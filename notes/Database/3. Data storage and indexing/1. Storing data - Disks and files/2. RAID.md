<!-- TOC titleSize:1 tabSpaces:2 depthFrom:1 depthTo:6 withLinks:1 updateOnSave:1 orderedList:0 skip:0 title:1 charForUnorderedList:* -->
# Table of Contents
- [Table of Contents](#table-of-contents)
- [RAID](#raid)
  - [Data striping](#data-striping)
  - [Redundancy](#redundancy)
  - [Levels of redundancy](#levels-of-redundancy)
    - [Level 1 - Mirrored](#level-1---mirrored)
    - [Level 0+1 - Striping and mirroring](#level-01---striping-and-mirroring)
    - [Level 2 - Error-correcting codes](#level-2---error-correcting-codes)
    - [Level 3 - Bit-interleaved parity](#level-3---bit-interleaved-parity)
    - [Level 4 - Block-interleaved parity](#level-4---block-interleaved-parity)
    - [Level 5 - Block-interleaved distributed parity](#level-5---block-interleaved-distributed-parity)
    - [Level 6 - P+Q redundancy](#level-6---pq-redundancy)
  - [Choice of RAID levels](#choice-of-raid-levels)
<!-- /TOC -->

# RAID
**Disks as bottlenecks**. Disks are potential bottlenecks for system performance and storage system reliability
* *Explain*.
    * Even though disk performance has been improving continuously, microprocessor performance has advanced much more rapidly
        
        $\to$ Disk performance is much worse than microprocessor performance
    * Since disks contain mechanical elements, they have much higher failure rates than electronic parts of a computer system
    
        $\to$ If a disk fails, all the data stored on it is lost

**Disk array**. An arrangement of several disks, organized so as to increase performance and improve reliability of the resulting storage system
* *Data striping*. Performance is increased through data striping
    * *Idea*. Distribute data over several disks to give the impression of having a single large, very fast disk
* *Redundancy*. Reliability is improved through redundancy
    * *Idea*. Instead of having a single copy of the data
    
        $\to$ Redundant information is maintained
        * *Redundant information organization*. The redundant information is carefully organized
        
            $\to$ In case of a disk failure, it can be used to reconstruct the contents of the failed disk
* *Redundant arrays of independent disks (RAID)*. Disk arrays implementing a combination of data striping and redundancy
    * *RAID levels*. Several RAID organizations, referred to as RAID levels, have been proposed
    
        $\to$ Each RAID level represents a different trade-off between reliability and performance
    * *Terminology*. Historically, the I in RAID stood for inexpensive, as a large number of small disks was much more economical than a single very large disk
    
        $\to$ Today, such very large disks are not even manufactured, i.e. a sign of the impact of RAID

## Data striping
**Data striping**. A disk array gives the user the abstraction of having a single, very large disk
* *Idea*. If the user issues an I/O request, we carry out the following steps
    1. Identify the set of physical disk blocks storing the data requested
        
        $\to$ These disk blocks may reside on a single disk in the array, or may be distributed over several disks in the array
    2. The set of blocks is retrieved from the disk(s) involved
        
        $\to$ How we distribute the data over the disks in the array influences how many disks are involved when an I/O request is processed
* *Data segmentation in data striping*. The data is segmented into equal-size partitions distributed over multiple disks
    * *Striping unit*. The size of a partition
    * *Partitions distribution*. Use a round robin algorithm, i.e. if the disk array consists of $D$ disks
        
        $\to$ Partition $i$ is written onto disk $i \mod D$

**Performance improvement**. Consider a striping unit of a bit, with $D$ successive data bits spreading over all $D$ data disks in the array

$\to$ All I/O requests involve all disks in the array
* *I/O operations*. Since the smallest unit of transfer from a disk is a block

    $\to$ Each I/O request involves transfer of at least $D$ blocks
    * *Transfare rate*. Since we can read the $D$ blocks from the $D$ disks in parallel
    
        $\to$ The transfer rate of each request is $D$ times the transfer rate of a single disk
    * *Bandwidth*. Each request uses the aggregated bandwidth of all disks in the array
    * *Disk access time*. The disk access time of the array is basically the access time of a single disk
        * *Explain*. All disk heads have to move for all requests
* *Conclusion*. For a disk array with a striping unit of a single bit

    $\to$ The number of requests per time unit that the array can process and the average response time for each individual request are similar to that of a single disk

## Redundancy
**Brief**. Having more disks increases storage system performance, and also lowers overall storage system reliability
* *Example*. Consider a single disk with the mean-time-to-failure (MTTF) of 50,000 hours

    $\to$ The MTTF of an array of 100 disks is only $50,000/100 = 500$ hours or about 21 days
* *Disk failure possibly*. Disks have a higher failure probability early and late in their lifetimes, i.e.
    * Early failures are often due to undetected manufacturing defects
    * Late failures occur since the disk wears out

>**NOTE**. Failures do not occur independently either, e.g. consider a fire in the building, an earthquake, or purchase of a set of disks that come from a ‘bad’ manufacturing batch

**Redundancy and reliability**. Reliability of a disk array can be increased by storing redundant information
* *Idea*. If a disk failure occurs, the redundant information is used to reconstruct the data on the failed disk
    
    $\to$ Redundancy can immensely increase the MTTF of a disk array
* *Incorporation of redundancy into disk array design*. We have to make two choices
    * *Where to store the redundant information*.
        * *Option 1*. Store the redundant information on a small number of check disks, or
        * *Option 2*. Distribute the redundant information uniformly over all disks
    * *How to compute the redundant information*. Most disk arrays store parity information

**Parity scheme**. An extra check disk contains information that can be used to recover from failure of any one disk in the array
* *Parity of a data bit*. Consider a disk array with $D$ disks and consider the first bit on each data disk 
    * *Assumptions*.
        * $i$ of the $D$ data bits are one
    * *Parity of the data bits*. The first bit on the check disk, which is set to one if $i$ is odd, otherwise it is set to zero
        
        $\to$ The check disk contains parity information for each set of corresponding $D$ data bits
* *Recovery of the first bit of a failed disk*. We can recover from failure of any one disk with parity
    1. Count the number $j$ of bits that are one on the $D − 1$ nonfailed disks
    2. If $j$ is odd and the parity bit is one, or if $j$ is even and the parity bit is zero
    
        $\to$ The value of the bit on the failed disk must have been zero
    3. Otherwise, the value of the bit on the failed disk must have been one
* *Reconstruction of the lost information*. Involve reading all data disks and the check disk

**Reliability groups**. In a RAID system, the disk array is partitioned into reliability groups
* *Reliability group*. Consist of a set of data disks and a set of check disks
    
    $\to$ A common redundancy scheme is applied to each group
* *Number of check disks*. Depend on the RAID level chosen
    
    >**NOTE**. In the remainder of this section, we assume for ease of explanation that there is only one reliability group
    
    >**NOTE**. Actual RAID implementations consist of several reliability groups, and that the number of groups plays a role in the overall reliability of the resulting storage system

## Levels of redundancy
**Brief**. Throughout the discussion of the different RAID levels, we consider sample data fitting on four disks
* *Explain*. Without any RAID technology, our storage system would consist of exactly four data disks

    $\to$ Depending on the RAID level chosen, the number of additional disks varies from zero to four

### Level 1 - Mirrored
**Brief**. A RAID Level 1 system is the most expensive solution

**Level 1 - Mirroring**. Instead of having one copy of the data

$\to$ Two identical copies of the data on two different disks are maintained, i.e. mirroring
* *Writes to disks*. Every write of a disk block involves a write on both disks
    * *Concurrent writes*. These writes may not be performed simultaneously
        * *Explain*. A global system failure could occur while writing the blocks
            
            $\to$ Both copies are left in an inconsistent state
        * *Consequence*. We always write a block on one disk first and then write the other copy on the mirror disk
* *Reads from disks*. Since two copies of each block exist on different disks
    
    $\to$ We can distribute reads between the two disks and allow parallel reads of different disk blocks conceptually residing on the same disk
    * *Read operation scheduling*. A read of a block can be scheduled to the disk having the smaller expected access time

**Data striping in RAID level 1**. RAID level 1 does not stripe the data over different disks

$\to$ The transfer rate for one request is comparable to the transfer rate of a single disk

**Effective space utilization**. In our example, we need four data and four check disks with mirrored data for a RAID
level 1 implementation

$\to$ The effective space utilization is 50 percent, independent of the number of data disks

### Level 0+1 - Striping and mirroring
**RAID level 0+1 (RAID level 10)**. Combine striping and mirroring
* *Reading operations*.
    * As in RAID level 1, read requests of the size of a disk block can be scheduled both to a disk or its mirror image
    * Read requests of the size of several contiguous blocks benefit from the aggregated bandwidth of all disks
* *Writing operations*. The cost for writes is analogous to RAID Level 1

**Effective space utilization**. As in RAID Level 1, our example with four data disks requires four check disks

$\to$ The effective space utilization is always 50 percent

### Level 2 - Error-correcting codes
**RAID level 2**.
* *Idea*.
    * The striping unit is a single bit
    * The redundancy scheme used is Hamming code
* *Number of check disks*. In our example with four data disks, only three check disks are needed
    * *Growth of the number of check disks*. The number of check disks grows logarithmically with the number of data disks

**Striping at the bit level**. In a disk array with $D$ data disks, the smallest unit of transfer for a read is a set of $D$ blocks 
* *Reading operations*.
    * *Pros*. Level 2 is good for workloads with many large requests
        * *Explain*. For each request the aggregated bandwidth of all data disks is used
    * *Cons*. RAID level 2 is bad for small requests of the size of an individual block for the same reason
* *Writing operations*. A write of a block involves a read-modify-write cycle
    * *Read-modify-write cycle*. Given $C$ check disks
        1. Read $D$ blocks into main memory
        2. Modify $D + C$ blocks
        3. Write $D + C$ blocks to disk

**Effective space utilization**. For a RAID Level 2 implementation with four data disks, three check disks are needed

$\to$ The effective space utilization is about 57 percent

>**NOTE**. The effective space utilization increases with the number of data disks

### Level 3 - Bit-interleaved parity
**Drawback of Level 2**. While the redundancy schema used in RAID Level 2 improves in terms of cost upon
RAID Level 1

$\to$ It keeps more redundant information than is necessary
* *Explain*. Hamming code has the advantage of being able to identify which disk has
failed
    
    $\to$ However, disk controllers can easily detect which disk has failed
* *Consequence*. The check disks do not need to contain information to identify the failed disk

    $\to$ Information to recover the lost data is sufficient

**RAID level 3**. Instead of using several disks to store Hamming code

$\to$ RAID Level 3 has a single check disk with parity information
* *Consequence*. The reliability overhead for RAID Level 3 is a single disk, i.e. the lowest overhead possible

**Performance of Level 3 compared with Level 2**. 
* The performance characteristics of RAID Level 2 and RAID Level 3 are very similar
* RAID Level 3 can also process only one I/O at a time
* The minimum transfer unit is $D$ blocks
* A write requires a read-modify-write cycle

### Level 4 - Block-interleaved parity
**RAID level 4**. Have a striping unit of a disk block, rather than a single bit as in RAID Level 3
* *Benefits of block-level striping*.
    * *Read operations*. 
        * Read requests of t`he size of a disk block can be served entirely by the disk, where the requested block resides
        * Large read requests of several disk blocks can still utilize the aggregated bandwidth of the $D$ disks
    * *Write operations*. The write of a single block still requires a read-modify-write cycle
        
        $\to$ However, only one data disk and the check disk are involved
        * The parity on the check disk can be updated without reading all $D$ disk blocks
            * *Explain*. The new parity can be obtained by 
                1. Notice the differences between the old data block and the new data block
                2. Apply the difference to the parity block on the check disk

                    $$\text{NewParity} = (\text{OldData} \oplus \text{NewData}) \oplus \text{OldParity}$$
        * The read-modify-write cycle involves four disk accesses per write
            1. Read the old data block and the old parity block
            2. Modify the two blocks
            3. Write the blocks back to disk
        * Since the check disk is involved in each write, it can easily become the bottleneck

**Effective space utilization**. RAID Level 3 and 4 configurations with four data disks require one check disk

$\to$ In our example, the effective space utilization is 80 percent

>**NOTE**. The effective space utilization increases with the number of data disks

### Level 5 - Block-interleaved distributed parity
**RAID Level 5**. Improve upon Level 4 by distributing the parity blocks uniformly over all disks, instead of storing them on a single check disk
* *Benefits*.
    * Several write requests can potentially be processed in parallel
        * *Explain*. The bottleneck of a unique check disk has been eliminated
    * Eead requests have a higher level of parallelism
        * *Explain*. 
            * Since the data is distributed over all disks
                
                $\to$ Read requests involve all disks
            * Whereas in systems with a dedicated check disk, the check disk never participates in reads

**Performance**. A RAID Level 5 system has the best performance of all RAID levels with redundancy for small and large read and large write requests
* *Explain*. Small writes still require a readmodify-write cycle and are thus less efficient than in RAID Level 1

**Effective space utilization**. The effective space utilization is the same as in RAID levels 3 and 4

### Level 6 - P+Q redundancy
**Motivation**. Recovery from failure of a single disk is not sufficient in very large disk arrays
* *Explain*.
    * In large disk arrays, a second disk might fail before replacement of an already failed disk could take place
    * The probability of a disk failure during recovery of a failed disk is not negligible

**RAID Level 6**. Use Reed-Solomon codes to recover from up to two simultaneous disk failures

$\to$ RAID Level 6 requires conceptually two check disks
* *Redundant information distribution*. RAID Level 6 uniformly distributes redundant information at the block level as in RAID Level
    
    $\to$ The performance characteristics for small and large read requests and for large write requests are analogous to RAID Level 5
    * *Explain*. Since two blocks with redundant information need to be updated
        
        $\to$ For small writes, the read-modify-write procedure involves six, instead of four disks, as compared to RAID Level 5

**Effective space utilization**. For a RAID Level 6 system with storage capacity equal to four data disks, six disks are required

$\to$ In our example, the effective space utilization is 66 percent.

## Choice of RAID levels
**RAID Level 0**. If data loss is not an issue, RAID Level 0 improves overall system performance at
the lowest cost

**RAID Level 0+1**. Superior to RAID Level 1, with the main application areas are small storage subsystems, where the cost of mirroring is moderate
* *RAID Level 0+1 for intensive-write applications*. RAID Level 0+1 is useful for applications with a high percentage of writes in their workload
    * *Explain*. RAID Level 0+1 provides the best write performance

**RAID levels 2 and 4**. Inferior to RAID levels 3 and 5, respectively
* *RAID Level 3*. Appropriate for workloads consisting mainly of large transfer requests of several contiguous blocks
      * *Explain*. The performance of a RAID Level 3 system is bad for workloads with many small requests of a single disk block
* *RAID Level 5*. A good general-purpose solution
    * *Explain*. It provides high performance for large requests as well as for small requests

**RAID Level 6**. Appropriate if a higher level of reliability is required