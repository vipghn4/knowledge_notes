<!-- TOC titleSize:1 tabSpaces:2 depthFrom:1 depthTo:6 withLinks:1 updateOnSave:1 orderedList:0 skip:0 title:1 charForUnorderedList:* -->
# Table of Contents
- [Table of Contents](#table-of-contents)
- [Buffer manager](#buffer-manager)
  - [Buffer replacement policies](#buffer-replacement-policies)
  - [Buffer management in DBMS versus OS](#buffer-management-in-dbms-versus-os)
- [Appendix](#appendix)
  - [Concepts](#concepts)
  - [Discussions](#discussions)
<!-- /TOC -->

# Buffer manager
**Motivating example**. To understand the role of the buffer manager, consider a simple example
* *Assumptions*.
    * The database contains 1,000,000 pages, with only 1,000 pages of main memory are available for holding data
    * There is a query that requires a scan of the entire file
* *Observations*. Since all the data cannot be brought into main memory at one time
    * The DBMS must bring pages into main memory as they are needed
    * The DBMS must, in the process, decide what existing page in main memory to replace to make space for the new page

**Buffer manager**.
* *Replacement policy*. The policy used to decide which page to replace
* *Buffer manager*. The software layer responsible for bringing pages from disk to main memory as needed
* *Buffer pool*. The buffer manager manages the available main memory by partitioning it into a collection of pages
    
    $\to$ This is collectively refer to as the buffer pool
    * *Frames*. The main memory pages in the buffer pool
    
        $\to$ It is convenient to think of them as slots that can hold a page, which usually resides on disk or other secondary storage media

**Benefits of buffer manager**. Higher levels of the DBMS code can be written without worrying about whether data pages are in memory or not, i.e.
1. Higher-level DBMS code asks the buffer manager for the page
2. The page is brought into a frame in the buffer pool if it is not available
3. The higher-level code must also inform the buffer manager if it modifies the requested page
4. The buffer manager then makes sure that the change is propagated to the copy of the page on disk
5. Higher-level code releases the page when it is no longer needed, by informing the buffer manager

    $\to$ The frame containing the page can be reused

**Additional information maanged by buffer manager**. 
* *Additional information maanged by buffer manager*. In addition to the buffer pool itself

    $\to$ The buffer manager maintains some bookkeeping information, and two variables for each frame in the pool, i.e. `pin_count` and `dirty`
    * *`pin_count`*. The number of times that the page currently in a given frame has been requested but not released
        * *Explain*. The number of current users of the page
    * *`dirty`*. A boolean variable indicating whether the page has been modified since it was brought into the buffer pool from disk
* *Workflow with `pin_count` and `dirty`*. Initially, the `pin_count` for every frame is set to `0`, and the `dirty` bits are turned off
    
    $\to$ When a page is requested the buffer manager does the following
    1. Checks the buffer pool to see if some frame contains the requested page
        
        $\to$ If so increments the `pin_count` of that frame, otherwise, the buffer manager brings it in as follows
        1. Chooses a frame for replacement, using the replacement policy, and increments its `pin_count`
        2. If the `dirty` bit for the replacement frame is on
            
            $\to$ Write the page it contains to disk
            * *Explain*. The disk copy of the page is overwritten with the contents of the frame
        3. Reads the requested page into the replacement frame
    2. Returns the main memory address of the frame containing the requested page to the requestor

**Pinning and unpinning a page**.
* *Pinning*. Incrementing `pin_count` is often called pinning the requested page in its frame
* *Unpinning*. When the code calling the buffer manager and requesting the page subsequently calls the buffer manager and releases the page
    
    $\to$ The `pin_count` of the frame containing the requested page is decremented, i.e. unpinning
    
>**NOTE**. If the requestor has modified the page, it informs the buffer manager that it unpins the page
>
>$\to$ The `dirty` bit for the frame is set

**Paging with `pin_count`**.
* *Idea*.
    * The buffer manager will not read another page into a frame until its `pin_count` becomes `0`, i.e. all requestors of the page have unpinned it
    * If a requested page is not in the buffer pool, and if a free frame is not available in the buffer pool
        
        $\to$ A frame with zero `pin_count` is chosen for replacement
        
        >**NOTE**. If there are many such frames, a frame is chosen according to the buffer manager’s replacement policy

* *Writing modifications to disk*. When a page is eventually chosen for replacement
    * If the `dirty` bit is not set, i.e. the page has not been modified since being brought into main memory
        
        $\to$ There is no need to write the page back to disk
        * *Consequence*. The copy on disk is identical to the copy in the frame
        
            $\to$ The frame can simply be overwritten by the newly requested page
    * Otherwise, the modifications to the page must be propagated to the copy on disk
* *Paging when there is no free page*. If there is no page in the buffer pool with zero `pin_count`, and a page that is not in the pool is requested
    
    $\to$ The buffer manager must wait until some page is released before responding to the page request
    * *Practical solution*. In practice, the transaction requesting the page may simply be aborted in this situation
        
        $\to$ Pages should be released by the code calling the buffer manager to request the page, as soon as possible

**Concurrently requested page**. What if a page is requested by several different transactions?
* *Interpretation*. What if the page is requested by programs executing independently on behalf of different users
* *Problem*. There is the potential for such programs to make conflicting changes to the page
* *Locking protocol*. Enforced by higher-level DBMS code, i.e. the transaction manager, to ensure that each transaction obtains a shared or exclusive lock before requesting a page to read or modify
    * *Idea*. Two different transactions cannot hold an exclusive lock on the same page at the same time
        
        $\to$ This is how conflicting changes are prevented
    * *Consequence*. The buffer manager simply assumes that the appropriate lock has been obtained before a page is requested

## Buffer replacement policies
**Brief**. The policy for choosing an unpinned page for replacement can affect the time taken for database operations considerably

$\to$ Many alternative policies exist, and each is suitable in different situations

**Least recently used (LRU)**. The best-known replacement policy
* *Implementation*. Use a queue of pointers to frames with zero `pin_count`
    * A frame is added to the end of the queue when it becomes a candidate for replacement, i.e. zero `pint_count`
    * The page chosen for replacement is the one in the frame at the head of the queue

**Clock replacement**. A variant of LRU with similar behavior but less overhead
* *Idea*. Choose a page for replacement using a `current` variable taking on values `1` through `N`, where N is the number of buffer frames, in circular order
    * *Interpretation*. Frames are arranged in a circle, i.e. a clock’s face, and `current` is a clock hand moving across the face
* *`referenced` bit*. To approximate LRU behavior, each frame has an associated `referenced` bit, which is turned on when the page `pin_count` goes to `0`
* *Choosing frame for replacement*.
    1. The `current` frame is considered for replacement
    2. If the frame is not chosen for replacement
        
        $\to$ `current` is incremented and the next frame is considered
    3. If the `current` frame has positive `pin_count`
        
        $\to$ It is not a candidate for replacement and `current` is incremented
    4. If the `current` frame has the `referenced` bit turned on
        
        $\to$ The clock algorithm turns the `referenced` bit off and increments `current`
        * *Consequence*. A recently referenced page is less likely to be replaced
    5. If the `current` frame has zero `pin_count`, and its `referenced` bit is off
        
        $\to$ The page in it is chosen for replacement
    6. If all frames are pinned in some sweep of the clock hand, i.e. `current` is incremented until it repeats
        
        $\to$ No page in the buffer pool is a replacement candidate
    7. The process is repeated until some frame is chosen

**Sequential flooding**.
* *Problems with LRU and clock policies*. The LRU and clock policies are not always the best replacement strategies for a database system, if many user requests require sequential scans of the data
* *Sequential flooding*. Consider the following example of sequential flooding
    * *Assumptions*. 
        * The buffer pool has 10 frames
        * The file to be scanned has $N$ pages
        * There are no competing requests for pages
    * If the file to be scanned has 10 or fewer pages
        
        $\to$ Only the first scan of the file does any I/O
        * *Consequence*. Page requests in subsequent scans will always find the desired page in the buffer pool
    * If the file to be scanned has 11 pages, i.e. one more than the number of available pages in the buffer pool
    
        $\to$ Using LRU, every scan of the file will result in reading every page of the file
* *LRU and sequential flooding*. LRU is the worst possible replacement strategy

## Buffer management in DBMS versus OS
**Brief**. Obvious similarities exist between virtual memory in operating systems and buffer management in database management systems
* *Explain*.
    * *Common objective*. Provide access to more data than will fit in main memory
    * *Common idea*. Bring in pages from disk to main memory as needed, replacing pages that are no longer needed in main memory
* *Question of interest*. Why can’t we build a DBMS using the virtual memory capability of an OS?
    * A DBMS can often predict the order in which pages will be accessed, or page reference patterns, more accurately than is typical in an OS environment
        
        $\to$ It is desirable to utilize this property
    * A DBMS needs more control over when a page is written to disk than an OS typically provides
    * A DBMS requires the ability to explicitly force a page to disk

        $\to$ This is to ensure that the copy of the page on disk is updated with the copy in memory
    * A DBMS must be able to ensure that certain pages in the buffer pool are written to disk before certain other pages are written
        
        $\to$ This is to implement the WAL protocol for crash recovery
        * *Virtual memory and write operations ordering*. Virtual memory implementations in operating systems cannot be relied upon to provide such control over when pages are written to disk
            * *Explain*. 
                * The OS command to write a page to disk may be implemented by
                    1. Record the write request
                    2. Defer the actual modification of the disk copy
                * Hence, if the system crashes in the interim, the effects can be catastrophic for a DBMS

**Reference pattern prediction**. A DBMS can often predict reference patterns since most page references are generated by higher-level operations with a known pattern of page accesses
* *Benefits*.
    * This allows for a better choice of pages to replace 
    * This makes the idea of specialized buffer replacement policies more attractive in the DBMS environment
    * This enables the use of a simple and very effective strategy, i.e. prefetching of pages
* *Page prefetching*. The buffer manager can anticipate the next several page requests and fetch the corresponding pages into memory before the pages are requested
    * *Benefits*. 
        * The pages are available in the buffer pool when they are requested
        * Reading in a contiguous block of pages is much faster than reading the same pages at different times in response to distinct requests
        * If the pages to be prefetched are not contiguous
            
            $\to$ Recognizing that several pages need to be fetched can nonetheless lead to faster I/O
            * *Explain*. An order of retrieval can be chosen for these pages that minimizes seek times and rotational delays
        * Since the I/O can typically be done concurrently with CPU computation
            
            $\to$ Once the prefetch request is issued to the disk,
            * The disk is responsible for reading the requested pages into memory pages
            * The CPU can continue to do other work

# Appendix
## Concepts
**Write-Ahead Log (WAL) protocol**. Special log records are used to describe the changes made to a page
* *Storing log records in buffer*. The log records pertaining to the page, which is to be replaced, may be in the buffer
    
    $\to$ The protocol requires that they be written to disk before the page is written to disk

**Prefetching**. In IBM DB2, both sequential and list prefetch, i.e. prefetching a list
of pages, are supported
* *Default prefetch size*. 32 pages of size 4KB
* *Operation-specific prefetch size*.
    * For some sequential type database utilities, e.g. `COPY`, `RUNSTATS`, DB2 will prefetch upto 64 4KB pages
    * For a smaller buffer pool, i.e. less than 1000 buffers, the prefetch quantity is adjusted to 16 or 8 pages
* *Manually-configured prefetch size*. Prefetch size can be configured by the user
    
    >**NOTE**. For certain environments, it may be best to prefetch 1000 pages at a time

## Discussions
**Buffer management in practice**. IBM DB2 and Sybase ASE allow buffers to be partitioned into named pools

$\to$ Each database, table, or index can be bound to one of these pools
* *Replacement policy of pools*. 
    * Each pool can be configured to use either LRU or clock replacement in ASE
    * DB2 uses a variant of clock replacement, with the initial clock value based on the nature of the page
        * *Example*. Index nonleaves get a higher starting clock value, which delays their replacement 
* *Page hating in DB2*. A buffer pool client in DB2 can explicitly indicate that it hates a page
    
    $\to$ The page will be the next choice for replacement
* *Other replacement policies in DB2*.
    * DB2 applies MRU for the pages fetched in some utility operations, e.g. `RUNSTATS`
    * DB2 V6 also supports FIFO