<!-- TOC titleSize:1 tabSpaces:2 depthFrom:1 depthTo:6 withLinks:1 updateOnSave:1 orderedList:0 skip:0 title:1 charForUnorderedList:* -->
# Table of Contents
- [Table of Contents](#table-of-contents)
- [Definition](#definition)
- [Conic form problems (cone programs)](#conic-form-problems-cone-programs)
- [Semi-definite programming](#semi-definite-programming)
- [Example](#example)
- [BONUS](#bonus)
<!-- /TOC -->

# Definition
**Idea**: generalize the standard form convex optimization by allowing inequality constraints to be vector valued

**Standard form**

* Assumptions:
  * $f_0: \textbf{R}^n \rightarrow \textbf{R}$
  * $K_i \subseteq \textbf{R}^{k_i}$ are proper cones
  * $f_i: \textbf{R} \rightarrow \textbf{R}^{k_i}$ are $K_i$-convex
* Formulation:

$\hspace{1.0cm} \text{minimize}$ $f_0(x)$

$\hspace{1.0cm} \text{subject to}$ $f_i(x) \preceq_{K_i} 0$ $\forall i \in [1, m]$

$\hspace{3.0cm} A x = b$

**Properties inherited from ordinary convex optimization**

* The feasible set, any sublevel set, and the optimal set are convex
* Any point that is locally optimal is also globally optimal
* The optimality condition for differentiable $f_0$ holds without any change

# Conic form problems (cone programs)
**Formulation**
  * Formulation:

  $\hspace{1.0cm} \text{minimize}$ $c^T x$

  $\hspace{1.0cm} \text{subject to}$ $F x + g \preceq_K 0$

  $\hspace{3.0cm} A x = b$
  
  * Intuition:
    * $F x + g \in -K$ and $-K$ is a proper cone
    
    $\hspace{1.0cm} \rightarrow \{x|F x + g \preceq_K 0\}$ is a proper cone
    * $\{x|A x = b\}$ is a linear sub-space
    * From above, our problem is to minimize $c^T x$ over a slice of the proper cone $\{x|F x + g \preceq_K 0\}$ (i.e. a convex set)

**Conic program and linear program**: conic program is a generalization of linear program
  * Explain: when $K$ is the non-negative orthant (i.e. $K = \{x|x \succeq 0\}$), conic program is linear program

**Special forms**:
  * Standard form:
  
  $\hspace{1.0cm} \text{minimize}$ $c^T x$

  $\hspace{1.0cm} \text{subject to}$ $x \succeq_K 0$

  $\hspace{3.0cm} A x = b$
  * Inequality form:
  
  $\hspace{1.0cm} \text{minimize}$ $c^T x$

  $\hspace{1.0cm} \text{subject to}$ $F x + g \preceq_K 0$

# Semi-definite programming
**Semi-definite program (SDP)**:
* Assumptions:
  * $X \succeq 0$ denote that a matrix $X$ is symmetric and positive semi-definite
  
  >**NOTE**: $X \succeq Y$ denotes that $X - Y \succeq 0$
  
  * $F_1, ..., F_n, G \in \textbf{S}^k$ (i.e. they're symmetric)
  * $A \in \textbf{R}^{p \times n}$
* SDP:

$\hspace{1.0cm} \text{minimize}$ $c^T x$

$\hspace{1.0cm} \text{subject to}$ $\sum_{i = 1}^n x_i F_i + G \preceq 0$

$\hspace{3.0cm} A x = b$

>**NOTE**: when $G, F_1, ..., F_n$ are all diagonal

$\hspace{1.0cm} \rightarrow$ SDP becomes LP

**SDP as a generalization of conic programming**:
* Symmetric semi-definite matrices: the set of $k \times k$ symmetric semi-positive matrices $\textbf{S}^k_+$ is a proper cone
  * Prove:
    * If $M \in \textbf{S}^k_+$ then $v^T M v \geq 0$ $\forall v \in \textbf{R}^k$
    
    $\hspace{1.0cm} \rightarrow v^T (a M) v \geq 0$ $\forall v \in \textbf{R}^k$ for any $a \geq 0$, which implies that $a M \in \textbf{S}^k_+$ $\forall a \geq 0$
    * If $M, N \in \textbf{S}^k_+$ then $v^T (\theta M + (1 - \theta) N) v \geq 0$ $\forall v \in \textbf{R}^k$ for any $\theta \in [0, 1]$
    
    $\hspace{1.0cm} \rightarrow \theta M + (1 - \theta) N \in \textbf{S}^k_+$
    * If $M \in \textbf{S}^k$ then $v^T M v \geq 0$ $\forall v \in \textbf{R}^k$
    
    $\hspace{1.0cm} \rightarrow v^T (-M) v \leq 0$, which implies that $-M \in \textbf{S}^k_+ \leftrightarrow M = 0$
    * From above, $\textbf{S}^k_+$ is a convex, peaked, solid and closed cone, thus it's a proper cone
* Observations:
  * $\sum_{i=1}^n x_i F_i + G = \textbf{F} x + G$ where $\textbf{F} = (F_1, ..., F_n)$ is a tensor of shape $k \times k \times n$
  * $\preceq$ can be written as $\preceq_{\textbf{S}^k_+}$
  * From above, the inequality constraint can be written as

  $\hspace{1.0cm} \textbf{F} x + G \preceq_{\textbf{S}^k_+} 0$
* Conclusion: SDP is conic programming with $F x + g \preceq_K 0$ replaced by $\textbf{F} x + G \preceq_{\textbf{S}^k_+} 0$

**Feasible set of a SDP problem**:
* Assumptions:
  * $\textbf{F}, x$ and $G$ are defined as above
  * $\text{rank}(\textbf{Nul}$ $\textbf{F}) = r$
* Observations: use the generalization of vector space to any matrix space
  * Without loss of generalization, we consider the inequality $\textbf{F} x \preceq_{\textbf{S}^k_+} 0$
  
  $\hspace{1.0cm} \rightarrow \textbf{F} x \in -\textbf{S}^k_+$
  * Since $\text{rank}(\textbf{Nul}$ $\textbf{F}) = r$, $\{x|\textbf{F} x = 0\}$ is a $r$-D space
  * $\{x|\textbf{F} x \in -\textbf{S}^k_+\}$ is a cone:
    * $\{x|\textbf{F} x = M\} = \{x|\textbf{F} x = 0\} + v$
    
    $\hspace{1.0cm}$ $v$ is any vector that $\textbf{F} v = M$
    * $\{x|\textbf{F} x = \alpha M\} = \{x|\textbf{F} x = 0\} + \alpha v$
    
    $\hspace{1.0cm}$ $\alpha \geq 0$
    
    $\hspace{1.0cm}$ $v$ is any vector that $\textbf{F} v = M$
    * Imagine $\{x|\textbf{F} x = 0\}$ as a line in a 3D space
    
    $\hspace{1.0cm} \rightarrow$ $\{x|\textbf{F} x = \alpha M, \alpha \geq 0\}$ for any $M \in \textbf{S}^k_+$ is a set of lines which:
    
    $\hspace{2.0cm}$ Are parallel to $\{x|\textbf{F} x = 0\}$
    
    $\hspace{2.0cm}$ Lie on the same side as $\{x|\textbf{F} x = M\}$ (relative to the origin)
  * $\{x|\textbf{F} x \in -\textbf{S}^k_+\}$ is convex:
    * $\{x|\textbf{F} x = \theta M + (1 - \theta) N\} = \{x|\textbf{F} x = 0\} + \theta v + (1 - \theta) u$ for any $M, N$
    
    $\hspace{1.0cm}$ $\theta \in [0, 1]$
    
    $\hspace{1.0cm}$ $u$ is any vector that $\textbf{F} u = N$
    
    $\hspace{1.0cm}$ $v$ is any vector that $\textbf{F} v = M$
    * $\theta v + (1 - \theta) u$ lies between $u, v$ and on the same line as $u, v$
    
    $\hspace{1.0cm} \rightarrow \{x|\textbf{F} x = \alpha (\theta M + (1 - \theta) N), \alpha \geq 0\}$ lies between $\{x|\textbf{F} x = \alpha M, \alpha \geq 0\}$ and $\{x|\textbf{F} x = \alpha N, \alpha \geq 0\}$
  * $\{x|\textbf{F} x \in -\textbf{S}^k_+\}$ is peaked:
    * If $M \in \textbf{S}^k_+$ then $-M \notin \textbf{S}^k_+$
    
    $\hspace{1.0cm} \rightarrow \{x|\textbf{F} x \in -\textbf{S}^k_+\}$ mustn't include $\{x|\textbf{F} x = \alpha (-M), \alpha \geq 0\}$
  * $\{x|\textbf{F} x \in -\textbf{S}^k_+\}$ is closed and not empty: since $\textbf{S}^k_+$ is closed and not empty
* Conclusion: $\{x|\textbf{F} x \in \textbf{S}^k_+\}$ is a proper cone

**Standard and inequality form SDP**:
* Standard form:
  * Assumptions:
    * $X \in \textbf{S}^n$
    * $C, A_1, ..., A_p \in \textbf{S}^n$
  * Standard form:

  $\hspace{1.0cm} \text{minimize}$ $\textbf{tr}$ $(C X)$

  $\hspace{1.0cm} \text{subject to}$ $\textbf{tr}$ $(A_i X) = b_i$ $\forall i \in [1, p]$

  $\hspace{3.0cm} X \succeq 0$
  * Explain: 
    * $\textbf{tr}$ $(C X) = \sum_{i = 1}^n (C X)_{ii} = \sum_{i = 1}^n \sum_{j = 1}^n C_{ij} X_{ji}$
    * $\sum_{i = 1}^n \sum_{j = 1}^n C_{ij} X_{ji} = \sum_{i = 1}^n \sum_{j = 1}^n C_{ij} X_{ij}$ since $C, X \in \textbf{S}^n$
    
    $\hspace{1.0cm} \rightarrow \textbf{tr}$ $(C X) =  \sum_{i = 1}^n \sum_{j = 1}^n C_{ij} X_{ij}$
    * From above, if we flatten $C$ and $X$, the objective function and the constraints is of the same form as those in SDP
* Inequality form:
  * Assumptions:
    * $x \in \textbf{R}^n$
    * $B, A_1, ..., A_n \in \textbf{S}^k$
    * $c \in \textbf{R}^n$
  * Inequality form:

  $\hspace{1.0cm} \text{minimize}$ $c^T x$

  $\hspace{1.0cm} \text{subject to}$ $\sum_{i = 1}^n x_i A_i \preceq B$

**Multiple LMI (linear matrix inequality) and linear inequalities**
* Motivation problem:

$\hspace{1.0cm} \text{minimize}$ $c^T x$

$\hspace{1.0cm} \text{subject to}$ $F^{(i)}(x) = \sum_{j = 1}^n x_j F^{(i)}_j \preceq 0$ $\forall i \in [1, K]$

$\hspace{3.0cm} G x \preceq h$

$\hspace{3.0cm} A x = b$

* From moviating problem to SDP:
  * Observations: if $A, B$ are semi-positive definite then $C = \begin{bmatrix} A && 0 \\ 0 && B\end{bmatrix}$ is semi-positive definite
    * Explain: $v^T C v = v^T \begin{bmatrix} A && 0 \\ 0 && 0\end{bmatrix} v + v^T \begin{bmatrix} 0 && 0 \\ 0 && B \end{bmatrix} v \geq 0$ $\forall v$
  * Formulation:

  $\hspace{1.0cm} \text{minimize}$ $c^T x$

  $\hspace{1.0cm} \text{subject to}$ $\textbf{diag}$ $(G x - h, F^{(1)}(x), ..., F^{(K)}(x)) \preceq 0$

  $\hspace{3.0cm} A x = b$

# Example
**Bounding portfolio risk with incomplete covariance information**:
* Assumptions:
    * $n$ is the number of assets or stocks
    * $x_i$ is the amount of asset $i$ held over some investment period
    * $p_i$ is the relative price change of asset $i$ over the period (see "Quadratic optimization problems")
        * Mean: $\bar{p} = E(p)$
        * Covariance: $\Sigma = E[(p = \bar{p}) (p - \bar{p})^T]$
* The change in totla value of the portfolio: $p^T x$
    * Mean: $E(p^T x) = \bar{p}^T x$
    * Standard deviation: $\sigma = \sqrt{x^T \Sigma x}$
* Classical portfolio optimization problem: 
    * Optimization variable: $x$
    * Objective function: $\sigma^2$
    * Constraints: there can be many constraints
    * Parameters: $\bar{p}$ and $\Sigma$ are known
* Risk bounding problem: 
    * Assumptions:
        * $x$ is known
        * $\Sigma$ is only known partially
            * $\Sigma_{ij} \in [L_{ij}, U_{ij}]$ where $i, j \in [1, ..., n]$
            * $L$ and $U$ are known
    * Question: what is the maximum risk for our portfolio
* Worst-case variance of the portfolio: $\sigma^2_\text{wc} = \sup \{x^T \Sigma x:\Sigma_{ij} \in [L_{ij}, U_{ij}], i, j = 1, ..., n, \Sigma \succeq 0\}$
    * Optimization problem:

    $\text{maximize } x^T \Sigma x$

    $\text{subject to } \Sigma_{ij} \in [L_{ij}, U_{ij}]$, $i, j = 1, ..., n$

    $\hspace{3.0cm} \Sigma \succeq 0$

    * Problem parameters:
        * $x, L$, and $U$
        * $\Sigma \in \textbf{S}^n$
    * Worst case relative price change: $p = \bar{p} + \Sigma^{1/2} v$ where $E(v) = 0$ and $\text{Cov}(v) = I$
* Other constraints for determining $\sigma_\text{wc}$:
    * Known variance of certain portfolios: $u_k^T \Sigma u_k = \sigma_k^2$
        * $\mu_k$ is the known portfolio
        * $\sigma_k^2$ is the variance of $p^T u_k$, which is very accurately estimated
    * Including effects of estimation error: $C (\Sigma - \hat{\Sigma}) \leq \alpha$
        * $\hat{\Sigma}$ is the given estimate of $\Sigma$
        * $C$ is a positive definite quadratic form on $\textbf{S}^n$
        * $\alpha$ determines the confidence level
    * Factor model: 
        * Formal: $\Sigma = F \Sigma_\text{factor} F^T + D$
            * $F \in \textbf{R}^{n \times k}$ is known
            * $\Sigma_\text{factor} \in \textbf{S}^k$ is known
            * $D$ is diagonal and known
        * Interpretation: $p = F z + d$ 
            * $z$ is random
            * $d_i$ are independent
    * Information about correlation coefficients:
        * $\Sigma_{ii}$ are known but $\Sigma_{ij}$ are not, for $i \neq j$ 
        * $l_{ij} \leq \rho_{ij} = \frac{\Sigma_{ij}}{\Sigma_{ii}^{1/2} \Sigma_{jj}^{1/2}} \leq u_{ij}$ where $l_{ij}, u_{ij}$ are known

**Fastest mixing Markov chain on a graph**:
* Assumptions:
    * $G = ({\cal{N}}, {\cal{E}})$ is an undirected graph
        * ${\cal{N}} = 1, ..., n$
        * ${\cal{E}} \subseteq \{1, ..., n\}^2$
    * ${\cal{E}}$ is symmetric since $G$ is undirected
    * $(i, i)$ are allowed to be in ${\cal{E}}$
    * $X(1) \to X(2) \to ... \to X(n)$ is a Markov chain
        * $X(t) \in \{1, ..., n\}$ for $t \in \textbf{Z}_+$
        * $\forall (i, j) \in {\cal{E}}$, $P_{ij}$ is the probability that $X$ makes a transition between node $i$ and node $j$
            * $P_{ij} = 0$ $\forall (i, j) \notin {\cal{E}}$
* Transition probability matrix: $P_{ij} = P[X(t+1) = i|X(t) = j]$ $\forall i, j = 1, ..., n$
    * Constraints:
        * $P_{ij} \geq 0$ $\forall i, j = 1, ..., n$
        * $\textbf{1}^T P = \textbf{1}^T$
        * $P = P^T$
        * $P_{ij} = 0$ $\forall (i, j) \notin {\cal{E}}$
    * An equilibrium distribution for the Markov chain: $\frac{1}{n} \textbf{1}$
* Eigenvalues of $P$: $1 = \lambda_1 \geq ... \geq \lambda_n \geq -1$ are eigenvalues of $P$
    * Prove: direct prove
        * Prove $1$ is an eigenvalue of $P$: direct prove
            * $P \textbf{1} = \textbf{1}$, thus $1$ is an eigenvalue of $P$ w.r.t eigenvector $\textbf{1}$
        * Prove $|\lambda| \leq 1$ for all eigenvalues $\lambda$ of $P$: direct prove using $|E(X)| \leq E(|X|)$
            * $P v = \lambda v \leftrightarrow P_i^T v = \lambda v_i$ where $P_i$ is the $i$-th row of $P$
            * Let $v_i = \max_j v_j$
            * From above, $|\lambda| |v_i| = |P_i^T v|$

            $\hspace{3.5cm} = |\sum_j P_{ij} v_j|$

            $\hspace{3.5cm} \leq \sum_j P_{ij} |v_j|$

            $\hspace{3.5cm} \leq \sum_j P_{ij} |v_i|$

            $\hspace{3.5cm} = |v_i|$
            * From above, $|\lambda| \leq 1$
* Converence of the distribution of $X(t)$ to $\frac{1}{n} \textbf{1}$: determined by the second largest (in magnitude) eigenvalue of $P$
    * Formal: $r = \max \{\lambda_2, - \lambda_n\}$
    * Mixing rate of the Markov chain: $r$
        * $r = 1$: the distribution of $X(t)$ need not converge to $\frac{1}{n} \textbf{1}$ (i.e. the chain doesn't mix)
        * $r < 1$: the distribution of $X(t)$ need not converge to $\frac{1}{n} \textbf{1}$ asymptotically as $r^t$ as $t \to \infty$
        * Smaller $r$: the Markov chain mixes faster
    * Prove:
        * Assumptions:
            * $\{\lambda_i\}_{i=1}^n$ are eigenvalues of $P$
            * $\{v_i\}_{i=1}^n$ are eigenvectors of $P$
                * For each eigenvalue $\lambda$ of $P$, $\{v_i\}_{\lambda_i = \lambda}$ form a basis of the eigenspace of $P$ w.r.t. $\lambda$
            * $\{\hat{\lambda}_i\}_{i=1}^m$ are distinct eigenvalues of $P$
        * Prove that $P$ has no complex eigenvalue: direct prove
            * $\|P x\|_2^2 = x^T P^2 x = \lambda^2 \|x\|_2^2$ for all eigenvectors $x$ of $P$

            $\hspace{1.0cm} \rightarrow \lambda^2 = \frac{\|P x\|_2^2}{\|x\|_2^2}$ is real
        * Prove that any $x$ can be shown as a linear combination of $\{v_i\}_{i=1}^n$
            * Base case: $V_1 = \{v_i\}_{\lambda_i \in \{\hat{\lambda}_1\}}$ is linearly independent due to the definition of $\{v_i\}_{i=1}^n$
            * Induction: let $V_k = \{v_i\}_{\lambda_i \in \{\hat{\lambda}_1, ..., \hat{\lambda}_k\}}$, which is linearly independent, and $v$ is an eigenvector corresponding to $\lambda = \hat{\lambda}_{k+1}$
                * Case 1: $v \notin \textbf{span } V_k$
                    * $\sum_i a_i v_i + a v = 0$ implies $a v = 0$ and $\sum_i a_i v_i = 0$
                    
                    $\hspace{1.0cm} \rightarrow a_i = 0$ $\forall i$ and $a = 0$
                    * From above, $V \cup \{v\}$ is linearly independent
                * Case 2: $v \in \textbf{span } V_k$
                    * $v = \sum_i b_i v_i$
                    
                    $\hspace{1.0cm} \rightarrow \sum_i a_i v_i + v = 0$ implies $a_i = -b_i$ $\forall i$, since $V_k$ is linearly independent
                    * From above, $v = - \sum_i a_i v_i$

                    $\hspace{0.4cm} \leftrightarrow P v = \lambda v = - \sum_i a_i \lambda_i v_i = - \sum_i a_i \lambda v_i$

                    $\hspace{0.4cm} \leftrightarrow \lambda_i = \lambda$ $\forall i$, since $V_k$ is linearly independent
                    * From above, $v \in \textbf{span } V_k$ leads to a contradiction
                * From above, $V_k \cup \{v\}$ is linearly independent

                $\hspace{1.0cm} \rightarrow V_{k+1} = V_k \cup \{v_i\}_{\lambda_i = \hat{\lambda}_{k+1}}$ is linearly independent
            * Conclusion: $V_m = \{v_i\}_{i=1}^n$ are linearly independent
            * Consequence: 
                * Any $x \in \textbf{R}^n$ can be written as a linear combination of $\{v_i\}_{i=1}^n$
                * Any matrix $P$ can be written as $P = V D V^{-1}$ since $P V = V D$ and $V$ is invertible 
        * Prove convergence of $x$ to $\frac{1}{n} \textbf{1}$: direct prove
            * If $r < 1$, $P^k x = \sum_i a_i \lambda_i^k v_i \to a_1 v_1$

            $\hspace{1.0cm} \rightarrow$ The distribution of $X(k)$ converges to uniform distribution
            * If $r = 1$, $P^k x = \sum_i a_i \lambda_i^k v_i \to a_1 v_1 + a_2 v_2 + c$ for some term $c$
                * $v_1$ and $v_2$ are independent since they correspond to the same eigenvalue $1$

                $\hspace{1.0cm} \rightarrow$ The distribution of $X(k)$ will not converge to uniform distribution
* The fastest mixing Markov chain problem: find $P$ which minimizes $r$
* Optimization problem:
    * Lemma: if $\lambda_i \neq \lambda_j$ then $\langle v_i, v_j, \rangle = 0$
        * Prove: $\lambda_i \langle v_i, v_j \rangle = \langle A v_i, v_j \rangle = \langle v_i, A v_j \rangle = \lambda_j \langle v_i, v_j \rangle$, thus $(\lambda_i - \lambda_j) \langle v_i, v_j \rangle = 0$

        $\hspace{1.0cm} \rightarrow \langle v_i, v_j \rangle = 0$
    * Formulation of $r$ restricted to $\textbf{1}^\perp$: $\|Q P Q\|_2$
        * $Q = I - (1/n) \textbf{1} \textbf{1}^T$ is the matrix representing orthogonal projection on $\textbf{1}^\perp$
    * Observation: since $P \textbf{1} = \textbf{1}$

    $\hspace{1.0cm} r = \|Q P Q\|_2$

    $\hspace{1.3cm} = \|(I - (1/n) \textbf{1} \textbf{1}^T) P (I - (1/n) \textbf{1} \textbf{1}^T)\|_2$

    $\hspace{1.3cm} = \|P - (1/n) \textbf{1} \textbf{1}^T\|_2$
    * Optimization problem:

    $\hspace{1.0cm} \text{minimize } r = \|P - (1/n) \textbf{1} \textbf{1}^T\|_2$

    $\hspace{1.0cm} \text{subject to } P \textbf{1} = \textbf{1}$

    $\hspace{3.0cm} P_{ij} \geq 0$ $\forall i, j = 1, ..., n$

    $\hspace{3.0cm} P_{ij} = 0$ $\forall (i, j) \notin {\cal{E}}$
    * Variable: $P \in \textbf{S}^n$

---

# BONUS
**1. Matrix inequality**: 
  * Matrix inequality: $B \succeq 0$ means that $B \in \textbf{S}^k_+$
  * Linear matrix inequality (LMI): $\text{LMI}$ $(y) = A_0 + \sum_{i = 1}^m y_i A_i$
    * $y = (y_1, ..., y_m)$ is a real vector
    * A_0, ..., A_m are $n \times n$ symmetric matrices