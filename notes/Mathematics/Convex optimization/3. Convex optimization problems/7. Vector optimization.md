<!-- TOC titleSize:1 tabSpaces:2 depthFrom:1 depthTo:6 withLinks:1 updateOnSave:1 orderedList:0 skip:0 title:1 charForUnorderedList:* -->
# Table of Contents
- [Table of Contents](#table-of-contents)
- [General and convex vector optimization problems](#general-and-convex-vector-optimization-problems)
- [Optimal points and values](#optimal-points-and-values)
- [Pareto optimal points and values](#pareto-optimal-points-and-values)
- [Scalarization](#scalarization)
- [Multicriterion optimization](#multicriterion-optimization)
- [Examples](#examples)
<!-- /TOC -->

# General and convex vector optimization problems
**General vector optimization problem**
* Assumptions:
    * $x \in \textbf{R}^n$ is the optimization variable
    * $K \subseteq \textbf{R}^q$ is a proper cone
    * $f_0: \textbf{R}^n \rightarrow \textbf{R}^q$ is the objective function
    * $f_i: \textbf{R}^n \rightarrow \textbf{R}$ are the inequality constraint functions
    * $h_i: \textbf{R}^n \rightarrow \textbf{R}$ are the equality constraint functions
* Problem:

$\hspace{1.0cm} \text{minimize (w.r.t K)} f_0(x)$

$\hspace{1.0cm} \text{subject to}$ $f_i(x) \leq 0$ $\forall v \in [1, m]$

$\hspace{3.0cm} h_i(x) = 0$ $\forall i \in [1, p]$

**Convex vector optimization problem**: general vector optimization problem when:
* $f_0$ is $K$-convex
* $f_i$ is convex for $i \in [1, m]$
* $h_i$ is affine for $i \in [1, p]$

**Confusing aspect of vector optimization**: $f_0(x)$ and $f_0(y)$ need not be comparable for some $x, y$

# Optimal points and values
**Achievable objective values**: $\cal{O} = \{f_0(x)|\exists x \in \cal{D}, f_i(x) \leq 0$ $\forall i \in [1, m], h_i(x) = 0$ $\forall i \in [1, p]\} \subseteq \textbf{R}^q$
* Verbal definition: the set of objective values of feasible points

**Optimal solution**: a feasible $x$ that $f_0(x) \preceq_K f_0(y)$ for all feasible $y$

>**NOTE**: optimal solution needs not exist

**Optimal value of the problem**: $f_0(x)$ that $f_0(x) \preceq_K f_0(y)$ for all feasible $y$
* Interpretation: $x^*$ is unambiguously a best choice for $x$, among feasible points

>**NOTE**: when a vector optimization problem has an optimal value, it's unique and can be compared to the objective at every other feasible point, and is better than or equal to it

**Theorem**: a point $x^*$ is optimal if and only if:
  * $x^*$ is feasible
  * $\cal{O} \subseteq f_0(x^*) + K$

>**NOTE**: most vector optimization problems don't have an optimal point & an optimal value, but this occurs in some special cases

# Pareto optimal points and values
**Introduction**: we now consider the case, where ${\cal{O}}$ doesn't have an minimum element

$\hspace{1.0cm} \rightarrow$ The problem doesn't have an optimal point or optimal value

**Pareto optimal (or efficient) solution**: a feasible $x$ is Pareto optimal if $f_0(x)$ is a minimal element of $\cal{O}$
* Definition: $x$ is Pareto optimal if it's feasible and $f_0(y) \preceq_K f_0(x)$ implies $f_0(y) = f_0(x)$
* Formal: $x$ is Pareto optimal if $(f_0(x) - K) \cap {\cal{O}} = \{f_0(x)\}$ 

**Perato optimal value**: $f_0(x)$ if $x$ is Pareto optimal

**Theorem**: a point $x$ is Pareto optimal if and only if:
* $x$ is feasible
* $(f_0(x) - K) \cap \cal{O} = \{f_0(x)\}$

**The set of Pareto optimal values**: $\cal{P} \subseteq \cal{O} \cap \textbf{bd}$ $\cal{O}$
* Interpretation: ${\cal{P}}$ lies in the boundary of ${\cal{O}}$

# Scalarization
**Scalarization**: a standard technique for finding Pareto optimal (or optimal) points for a vector optimization problem
* Motivation: the characterization of minimum and minimal points via dual generalized inequalities

**Method**:
* Procedure:
    * Step 1: choose any $\lambda \succ_{K^*} 0$ (i.e. $\lambda \in K^*$)
    * Step 2: solve the scalar optimization problem:

    $\hspace{1.0cm} \text{minimize}$ $\lambda^T f_0(x)$

    $\hspace{1.0cm} \text{subject to}$ $f_i(x) \leq 0$ $\forall i \in [1, m]$

    $\hspace{3.0cm} h_i (x) = 0$ $\forall i \in [1, p]$

    $\hspace{1.0cm} \rightarrow$ The optimal point $x$ of the scalar optimization problem above is Pareto optimal for the original optimization problem
* Explain: see the note for section 2.6
* Why $\lambda \succ_{K^*} 0$ instead of non-zero $\lambda \succeq_{K^*} 0$: to ensure that the optimal solution of the scalarized problem is Pareto optimal to the vector optimization problem
    * Prove: prove by contradiction
        * Given $x$ is the optimal solution to the scalarized problem
        * Assume that $x$ is not Pareto problem, so there exists some feasible $y$ that
            * $f_0(y) \preceq_{K} f_0(x)$ or equivalently $f_0(x) - f_0(y) \succeq_{K} 0$
            * $f_0(y) \neq f_0(x)$
        * Since $\lambda \succ_{K^*} 0$, $\lambda^T [f_0(x) - f_0(y)] > 0$ or equivalently $\lambda^T f_0(x) > \lambda^T f_0(y)$

        $\hspace{1.0cm} \rightarrow$ This leads to contradiction since $x$ is supposed to be optimal for the scalarized problem
        * From above, $x$ must be Pareto optimal
    * Intuition:
        * If $\lambda_{K^*} \succeq_{K^*} 0$ is allowed, then assume $\lambda \in \textbf{bd } K^*$

        $\hspace{1.0cm} \rightarrow \exists x, y$ satisfying $f_0(x) \succeq_{K} f_0(y)$, $f_0(x) \neq f_0(y)$, and $[f_0(x) - f_0(y)] \perp \lambda$
        * From above, $\lambda^T [f_0(x) - f_0(y)] = 0$, thus $\lambda^T f_0(x) = \lambda^T f_0(y)$

        $\hspace{1.0cm} \rightarrow$ We cannot conclude that $y$ is better than $x$ under $\lambda$, despite that $y$ is actually better than $x$ 

**Weight vector**: the vector $\lambda \succ_{K^*} 0$ used in scalarization

>**NOTE**: by varying $\lambda$, we possibly obtain different Pareto optimal solutions (i.e. due to the dual characterization of minimal elements)

**Scalarization of convex vector optimization problems**
* Assumption: the underlying vector optimization is convex
* Lemma: $\lambda^T f_0$ is a convex function
    * Prove: direct prove
        * Due to the definition of generalized inequality,
        
        $\hspace{1.0cm} \rightarrow \theta \lambda^T f_0(x_1) + (1 - \theta) \lambda^T f_0(x_2) \geq \lambda^T f_0[\theta x_1 + (1 - \theta) x_2]$ $\forall \lambda \succeq_{K^*} 0$
    * Consequence: we can find Pareto optimal points of a convex vector optimization problem by solving a convex scalar optimization problem
        * Explain: for each choice of non-zero $\lambda \succ_{K^*} 0$
         
        $\hspace{1.0cm} \rightarrow$ We get a, usually different, Pareto optimal point
* Converse for convex optimization: for every Pareto optimal point $x^{\text{po}}$

$\hspace{1.0cm} \rightarrow$ There's some non-zero $\lambda \succeq_{K^*} 0$ that $x^{\text{po}}$ is a solution of the scalarized problem
* Conclusion: for convex vector optimization problems

$\hspace{1.0cm} \rightarrow$ Method of scalarization yields all Pareto optimal points, as $\lambda$ varies over $K^*$-non-negative, non-zero values

>**NOTE**: 
* It's not true that every solution of the scalarized problem, with $\lambda \succeq_{K^*} 0$ and $\lambda \neq 0$, is a Pareto optimal point for the vector problem
    * Explain: see note for section 2.6
* Every solution of the scalarized problem with $\lambda \succ_{K^*} 0$ is Pareto optimal

**Find Pareto optimal points of a convex vector optimization problem**:
* Step 1: solve the scalarized problem with $\lambda \succ_{K^*} 0$

$\hspace{1.0cm} \rightarrow$ We get a set of Pareto optimal points
* Step 2: consider non-zero weight vectors $\lambda$ satisfying $\lambda \succeq_{K^*} 0$, for each $\lambda$
    * Identify all solutions of the scalarized problem with $\lambda$
    * Among the given solutions, we check which are Pareto optimal for the vector optimization problem

# Multicriterion optimization
**Multicriterion (multi-objective) optimization problem**: vector optimization problems w.r.t the cone $K = \textbf{R}^q_+$
* Assumptions:
    * $f_0 = (F_1, ..., F_q)$ where $F_i$ can be interpreted as a scalar objective
* Conclusion:
    * Problem: minimize $F_i(x)$ for all $i \in [1, q]$
    * The $i$-th objective of the problem: $F_i$

**Convex multicriterion optimization problem**: multicriterion optimization problems where:
* $f_i$ is convex for all $i \in [1, m]$
* $h_i$ is affine for all $i \in [1, p]$
* $F_i$ is convex for all $i \in [1, q]$

**Terminologies**:
* $x$'s score (or value) according to the $i$-th objective: $F_i(x)$
* $x$ is at least as good as $y$ according to the $i$-th objective: $F_i(x) \leq F_i(y)$
* $x$ is better than $y$ according to the $i$-th objective: $F_i(x) < F_i(y)$
* $x$ is better than $y$ (or $x$ dominates $y$): 
    * $F_i(x) \leq F_i(y)$ $\forall i \in [1, q]$
    * $F_j(x) < F_j(y)$ for at least one $j \in [1, q]$

**Optimal solution**: $x^*$ is optimal if $F_i(x^*) \leq F_i(y)$ $\forall i \in [1, q]$ for every feasible $y$
* Non-competing objectives: when there is an optimal point

$\hspace{1.0cm} \rightarrow$ The objectives are non-competing (i.e. each objective is as small as it could be, even if the others were ignored)

**Pareto optimal point**: 
* Definition: $x^{\text{po}}$ is Pareto optimal if:
    * If $y$ is feasible and $F_i(y) \leq F_i(x^{\text{po}})$ $\forall i \in [1, q]$

    $\hspace{1.0cm} \rightarrow$ $F_i(x^{\text{po}}) = F_i(y)$ $\forall i \in [1, q]$
* Another interpretation: $x^{\text{po}}$ is Pareto optimal if and only if 
    * It's feasible
    * There's no better feasible point

**Trade-off analysis**:
* Assumptions:
    * $x$ and $y$ are Pareto optimal, conretely, for some $A \cup B \cup C = \{1, ..., q\}$:
        * $F_i(x) < F_i(y)$ $\forall i \in A$
        * $F_i(x) = F_i(y)$ $\forall i \in B$
        * $F_i(x) > F_i(y)$ $\forall i \in C$
    * $A, C$ are nonempty
* Terminology: we have traded off better objective values for $i \in A$ for worse objective values for $i \in C$
* (optimal) Trade-off analysis: the study of how much worse we must do, in some objectives, to do better in some other objectives
    * Another interpretation: the study of what sets of objective values are achievable
    * Example: 
        * How much larger $F_i(z)$ would have to be, so that $F_j(z) \leq F_j(x) - a$ where $a > 0$ is some constant
            * Intuition: how much must we pay in $F_i$ to obtain an improvement of $a$ in $F_j$
        * How much smaller $F_i(z)$ would have to be, so that $F_j(z) \leq F_j(x) - a$ where $a > 0$ is some constant
            * Intuition: how much must we optimize in $F_i$ to obtain a benefit of $a$ in $F_j$
* Strong trade-off: if a large increase in $F_i$ is required to get a small decrease in $F_j$

$\hspace{1.0cm} \rightarrow$ There is a *strong trade-off* between the objectives
* Weak trade-off: when a small increase in $F_i$ is required to get a large decrease in $F_j$

**Optimal trade-off surface**:
* Optimal trade-off surface (or curve): the set of Pareto optimal values for a multicriterion problem

>**NOTE**: trade-off analysis is somtimes called *exploring the optimal trade-off surface*

**Scalarizing multicriterion problems**:
* The objective for the scalar optimization problem: $\lambda^T f_0(x) = \sum_{i=1}^q \lambda_i F_i(x)$ where $\lambda \succ 0$
* The weight attached to the $i$-th objective: $\lambda_i$
    * Interpretation: $\lambda_i$ is a way of quantifying our desire to make $F_i$ small
    * Value of $\lambda_i$: large $\lambda_i$ results in small $F_i$
* The relative weight (or importance) of the $i$-th objective compared to the $j$-th objective: $\lambda_i / \lambda_j$
* The exchange rate between the $i$-th objective and the $j$-th objective: $\lambda_i / \lambda_j$
    * Intuition: a decrease in $F_i$ by $\alpha$ is the same as an increase in $F_j$ by $(\lambda_i / \lambda_j) \alpha$

**Optimal trade-off surface exploration in practice**: optimal trade-off surfaces are explored by ad hoc adjustment of the weights, based on the intuitive ideas above

# Examples
**Regularized least-squares**:
* Assumptions:
    * $A \in \textbf{R}^{m \times n}$ and $b \in \textbf{R}^m$
* Objective functions: 
    * $F_1(x) = \|A x - b\|_2^2 = x^T A^T A x - 2 b^T A x + b^T b$ 
        * Interpretation: a measure of misfit between $A x$ and $b$
    * $F_2(x) = \|x\|_2^2 = x^T x$
        * Interpretation: a measure of the size of $x$
* Variable: $x$
* Goal: find $x$ that gives a good fit and that is not large
* Problem:

$\hspace{1.0cm} \text{minimize (w.r.t } \textbf{R}^2_+ \text{)} f_0(x) = [F_1(x), F_2(x)]$

**Risk-return trade-off in portfolio optimization**:
* Classical Markowitz portfolio optimization problem

$\hspace{1.0cm} \text{minimize (w.r.t } \textbf{R}^2_+ \text{)} [F_1(x), F_2(x)] = (- \bar{p}^T x, x^T \Sigma x)$

$\hspace{1.0cm} \text{subject to } \textbf{1}^T x = 1, x \succeq 0$

* Scalarized problem:

$\hspace{1.0cm} \text{minimize (w.r.t } \textbf{R}^2_+ \text{)} - \bar{p}^T x + \mu x^T \Sigma x$

$\hspace{1.0cm} \text{subject to } \textbf{1}^T x = 1, x \succeq 0$

* Limiting case: 
    * $\mu \to 0$: maximize mean return regardless for return variance 
    * $\mu \to \infty$: minimize variance return regardless of mean return