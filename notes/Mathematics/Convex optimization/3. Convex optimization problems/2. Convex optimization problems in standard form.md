<!-- TOC titleSize:1 tabSpaces:2 depthFrom:1 depthTo:6 withLinks:1 updateOnSave:1 orderedList:0 skip:0 title:1 charForUnorderedList:* -->
# Table of Contents
- [Table of Contents](#table-of-contents)
- [Convex optimization](#convex-optimization)
- [Local and global optima](#local-and-global-optima)
- [An optimally criterion for differentiable $f_0$](#an-optimally-criterion-for-differentiable-f_0)
- [Equivalent convex problems](#equivalent-convex-problems)
- [Quasiconvex optimization](#quasiconvex-optimization)
- [BONUS](#bonus)
- [NEW WORDS](#new-words)
<!-- /TOC -->

# Convex optimization
**Convex optimization**
  * Assumptions:
    * $f_i$ are convex functions $\forall i \in [0, m]$
  * Conclusion:
    * Convex optimization problem is one of the form:

    $\hspace{1.0cm} \text{minimize}$ $f_0(x)$

    $\hspace{1.0cm} \text{subject to}$ $f_i(x) \leq 0$, $i = 1..m$

    $\hspace{3.0cm} a_i^T x = b_i$, $i = 1..p$
  * Another form:

  $\hspace{1.0cm} \text{minimize}$ $f_0(x)$
  
  $\hspace{1.0cm} \text{subject to}$ $f_i(x) \leq 0$, $i = 1..m$
  
  $\hspace{3.0cm} A x = b$

**Convex problem compared to the general standard form problem**
  * $f_0$ must be convex
  * $f_i$ must be convex $\forall i \in [1, m]$
  * $h_i(x) = a_i^T x - b_i$ must be affine

**Quasiconvex optimization problem**: when $f_0$ is quasiconvex

**Properties of a convex optimization problem**
  * The feasible set $D = \bigcap_{i = 0}^m \textbf{dom}$ $f_i$ is convex
  * The $\epsilon$-suboptimal sets are convex
  >**NOTE**: this is true for quasiconvex problems as well

**Abstract form convex optimization problem**: the problem of minizimg a convex function over a convex set

# Local and global optima
**Theorem**: for convex optimization problems (not quasiconvex), any locally optimal point is also (globally) optimal

# An optimally criterion for differentiable $f_0$
**Optimal condition**
  * Assumptions:
    * $f_0$ is the objective in a convex optimization problem
    * $f_0$ is differentiable
    * $X$ is the feasible set
  * Observations:
    * $f_0(y) \geq f_0(x) + \nabla f_0(x)^T (y - x)$ $\forall x, y \in \textbf{dom}$ $f_0$
  * Conclusion:$x$ is optimal if and only if:
    * $x \in X$
    * $\nabla f_0(x)^T (y - x) \geq 0$ $\forall y \in X$
  * Intuition 1:
    * The supporting hyperplane to $f_0$ at $x$ divides $\textbf{dom}$ $f$ into two halfplanes
      * $R_1 = \{y|\nabla f_0(x)^T (y - x) \geq 0\}$
      * $R_2 = \{y|\nabla f_0(x)^T (y - x) < 0\}$
    * $f_0(y) \geq f_0(x)$ $\forall y \in R_1$ (proved by the observation above)
    
    $\hspace{1.0cm} \rightarrow$ $x$ is optimal if and only if $y \in R_1$ $\forall$ feasible $y$
  * Intuition 2: $\nabla f_0(x)^T (y - x)$ gives the changes of $f_0$ when moving from $x$ to $y$

**Unconstrained problems**: 
  * The optimal condition: $\nabla f_0(x) = 0$
  * Cases:
    * The optimal value is unbounded ($\nabla f_0(x) = 0$ has no solution)
    * The optimal value is bounded ($\nabla f_0(x) = 0$ has solutions)
  * Intuition: for unconstrained problems, $\nabla f_0(x) = 0$ means that $y \in R_1$ $\forall y \in \textbf{dom}$ $f_0$

**Problems with equality constraints only**
* Problem:

$\hspace{1.0cm} \text{minimize}$ $f_0(x)$

$\hspace{1.0cm} \text{subject to}$ $A x = b$
* The feasible set: an affine set
* Classical Lagrange multiplier optimality condition:
  * Formal:
    * There exists some $u$ so that $\nabla f_0(x) + A^T u = 0$
    * $A x = b$
  * Intuition: see section "1. The Lagrange dual function"

**Minimization over the non-negative orthant**
* Problem:

  $\hspace{1.0cm} \text{minimize} f_0(x)$
  
  $\hspace{1.0cm} \text{subject to} x \succeq 0$
* The optimality condition:
  * $x \succeq 0$
  * $\nabla f_0(x)^T (y - x) \geq 0$ $\forall y \succeq 0$  
* Another form of optimality condition:
  * Formal: the optimal conditions can be rewritten as
    * $x \succeq 0$
    * $\nabla f_0(x) \succeq 0$ (i.e. $f_0$ must be bounded below on $\{x|x \succeq 0\}$)
    * $x_i (\nabla f_0(x))_i = 0$ $\forall i \in [1, n]$
* Complementary: $(\nabla f_0(x))_i x_i = 0$ $\forall i \in [1, n]$
  * Intuition: the sparsity patterns (the set of indices corresponding to non-zero components) of $x$ and $\nabla f_0(x)$ have empty intersection

# Equivalent convex problems
**Eliminating equality constraints**
* Idea: replace $x$ by $F z + x_0$ for some $z$
  * $x_0$ is a particular solution of $A x = b$
  * $\textbf{col } F = \textbf{null } A$
* Equivalent problem:

$\hspace{1.0cm} \text{minimize}$ $f_0(F z + x_0)$

$\hspace{1.0cm} \text{subject to}$ $f_i(F z + x_0) \leq 0$ $\forall i = 1..m$
* Properties of the equivalent problem:
  * The convexity is preserved (since $f_i$ are convex and $F z + x_0$ is an affine function of $z$)
  * Eliminating constraints involves standard linear algebra operations
* Advantages and disadvantages: 
  * Advantages: we can restrict our attention to convex optimization problems with no equality constraints
  * Disadvantages: the problem maybe harder to understand and analyze, and maybe ruin the efficiency of the optimization solver

**Introducing equality constraints**

**Slack variables**: new contraints with slack variables: $f_i(x) + s_i = 0$

>**NOTE**: we must have $f_i$ affine since the equality constraints must be affine

**Epigraph problem form**
* Equivalent problem:

$\hspace{1.0cm} \text{minimize}$ $t$

$\hspace{1.0cm} \text{subject to}$ $f_0(x) - t \leq 0$

$\hspace{3.0cm} f_i(x) \leq 0$ $\forall i = 1..m$

$\hspace{3.0cm} a_i^T x = b_i$ $\forall i = 1..p$
* Universal objective for a class of problems: the objective, which any problem in the class is readily transformed to one with linear objective
* Advantages:
  * Simplify the theoretical analysis
  * Simplify algorithm development

**Minimizing over some variables**

# Quasiconvex optimization
**Standard form**
* Assumptions:
  * $f_i$ is convex $\forall i \in [1, m]$
  * $f_0$ is quasiconvex
* Problem:

    $\hspace{1.0cm} \text{minimize}$ $f_0(x)$

    $\hspace{1.0cm} \text{subject to}$ $f_i(x) \leq 0$, $i = 1..m$

    $\hspace{3.0cm} A x = b$

**Locally optimal solutions and optimality conditions**: quasiconvex optimization problem can have optimal solutions that aren't globally optimal
* Optimality condition: $x$ is optimal if
  * $x \in X$ where $X$ is the feasible set 
  * $\nabla f_0(x)^T (y - x) > 0$ $\forall y \in X / \{x\}$
* Intuition:
  * $\nabla f_0(x)^T (y - x) > 0$ means that $\nabla f_0(x) \neq 0$
  
  $\hspace{1.0cm} \rightarrow x$ isn't a local optimal in $X$
  * $\nabla f_0(x)^T (y - x) > 0$ means that $y$ is in the halfspace defined by $\nabla f_0(x)^T (y - x) > 0$ $\forall y \in X$
  
  $\hspace{1.0cm} \rightarrow f(y) \geq f(x)$ $\forall y \in X$
* Differences from the optimality condition of convex problems:
  * The condition above is only sufficient for optimality
  * $\nabla f_0$ is required to be non-zero

**Quasiconvex optimization via convex feasibility problems**
* Idea: represent the sublevel sets of a quasiconvex function via a family of convex inequalities
* Assumptions:
	* $\phi_t: \textbf{R}^n \rightarrow \textbf{R}$ where $t \in \textbf{R}$ is a family of convex functions
      * $f_0(x) \leq 0 \leftrightarrow \phi_t(x) \leq 0$
	  * $\phi_s(x) \leq \phi_t(x)$ whenever $s \geq t$
  	    * $\phi_t(x)$ is non-increasing in $t$
	* $p^*$ is the optimal value of the original quasiconvex problem
* The feasibility problem: determine whether $p^* \leq t$ or not by solving the feasibility problem

$\hspace{1.0cm} \text{find}$ $x$

$\hspace{1.0cm} \text{subject to}$ $\phi_t(x) \leq 0$

$\hspace{3.0cm} f_i(x) \leq 0$ $\forall i = 1..m$

$\hspace{3.0cm} A x = b$
* Observations: the feasibility problem is convex

$\hspace{1.0cm} \rightarrow$ By solving the feasibility problem, we can determine the lower bounds and upper bounds of $p^*$
* Algorithm: use binary search with criterion is the feasibility problem

---

# BONUS
**1. Orthant**: the intersection of $n$ mutually orthogonal half-spaces
  * Example: a ray in $R^1$ or a quadrant in $R_2$

***
# NEW WORDS
* Quadrant (n): góc phần tư