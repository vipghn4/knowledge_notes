<!-- TOC titleSize:1 tabSpaces:2 depthFrom:1 depthTo:6 withLinks:1 updateOnSave:1 orderedList:0 skip:0 title:1 charForUnorderedList:* -->
# Table of Contents
- [Table of Contents](#table-of-contents)
- [The Lagrangian](#the-lagrangian)
- [The Lagrange dual function](#the-lagrange-dual-function)
- [Lower bounds on optimal value](#lower-bounds-on-optimal-value)
- [The Lagrange dual function and conjugate functions](#the-lagrange-dual-function-and-conjugate-functions)
  - [Examples](#examples)
- [From Lagrange multipliers to Lagrangian](#from-lagrange-multipliers-to-lagrangian)
  - [Lagrange multipliers for equality constraints](#lagrange-multipliers-for-equality-constraints)
  - [Lagrange multipliers for inequality constraints](#lagrange-multipliers-for-inequality-constraints)
- [Weak and strong duality via set of values](#weak-and-strong-duality-via-set-of-values)
- [BONUS](#bonus)
<!-- /TOC -->

# The Lagrangian
**Assumptions**:
* Problem 1:

$\hspace{1.0cm} \text{minimize}$ $f_0(x)$

$\hspace{1.0cm} \text{subject to}$ $f_i(x) \leq 0$ $\forall i \in [1, m]$

$\hspace{3.0cm} h_i(x) = 0$ $\forall i \in [1, p]$
* $x \in \textbf{R}^n$
* The problem domain: $\cal{D} = \bigcap_{i = 0}^m \textbf{dom}$ $f_i \cap \bigcap_{i = 1}^p \textbf{dom}$ $h_i$
    * $\cal{D}$ isn't empty
* $p^*$ is the optimal value of problem 1

**Lagrangian duality**:
* Basic idea: augment $f_0$ with a weighted sum of $f_1, ..., f_m$

$\hspace{1.0cm} \rightarrow$ We take the constraints of problem 1 into account
* The Lagrangian associated with problem 1: $L: \textbf{R}^n \times \textbf{R}^m \times \textbf{R}^p \rightarrow \textbf{R}$
    * $L(x, \lambda, \nu) = f_0(x) + \sum_{i = 1}^m \lambda_i f_i(x) + \sum_{i = 1}^p \nu_i h_i(x)$
    * $\textbf{dom}$ $L = \cal{D} \times \textbf{R}^m \times \textbf{R}^p$

**Basic concepts**:
* The Lagrange multiplier associated with the i-th inequality constraint: $\lambda_i$
* The Lagrange multiplier associated with the i-th equality constraint: $\nu_i$
* The dual variables (or Lagrange multiplier vectors): $\lambda$ and $\nu$

# The Lagrange dual function
**Lagrange dual function (or dual function)**: $g(\lambda, \nu) = \text{inf}_{x \in \cal{D}} L(x, \lambda, \nu)$
* Domain: $\{(\lambda, \nu):g(\lambda, \nu) > -\infty\}$
* Interpretation: the minimum value of the $L(x, \lambda, \nu)$ over $x$, given $\lambda$ and $\nu$ fixed

>**NOTE**: when the $L(x, \lambda, \nu)$ is unbounded below in $x$, $g(\lambda, \nu)$ takes on the value $-\infty$

**Concavity of the dual function**: $g$ is concave even when problem 1 isn't convex
* Prove: direct prove 
    * $g(\cdot)$ the infimum of a family of affine functions of $(\lambda, \nu)$

# Lower bounds on optimal value
**Theorem**: $g(\lambda, \nu) \leq p^*$ for any $\lambda \succeq 0$ and any $\nu$
* Prove: direct prove
    * $g(\lambda, \mu) \leq L(x^\*, \lambda, \mu) \leq f(x^\*)$ where $x^*$ is the optimal solution, which is feasible, of the primal problem

>**NOTE**: the theorem holds, but is useless, when $g(\lambda, \nu) = -\infty$

$\hspace{1.0cm} \rightarrow$ To ensure $g(\lambda, \nu) > -\infty$, $\lambda \succeq 0$ and $(\lambda, \nu) \in \textbf{dom}$ $g$ must hold

**Dual feasible**: a pair $(\lambda, \nu) \in \textbf{dom}$ $g$ with $\lambda \succeq 0$

**Linear approximation interpretation of Lagrangian**:
* Idea: replace the hard constraints with soft versions 
* Formal: $L(x, \lambda, \nu) = f_0(x) + \sum_{i=1}^m \lambda_i f_i(x) + \sum_{i=1}^p \nu_i h_i(x)$ where $\lambda_i \geq 0$
    * $\lambda_i \geq 0$ penalizes the cases where $f_i(x) > 0$
    * $\nu_i$ doesn't make penalization effect on $h_i(x)$

# The Lagrange dual function and conjugate functions
**The conjugate $f^*$ of a function $f: \textbf{R}^n \rightarrow \textbf{R}$ (recall)**: $f^*(y) = \sup_{x \in \textbf{dom} \text{ f}} [y^T x - f(x)]$

**The conjugate function and Lagrange dual function**: 
* Problem of interest:

$\hspace{1.0cm} \text{minimize } f_0(x)$

$\hspace{1.0cm} \text{subject to } Ax \preceq b$

$\hspace{3.0cm} Cx = d$
* Observations:
    * The Lagrangian is $g(\lambda, \nu) = \inf_x[f_0(x) + \lambda^T (Ax - b) + \nu^T (Cx - d)]$
    * The conjugate of $f_0$ if $f^*_0(y) = \sup_x [y^T x - f_0(x)]$
    * From above, 
        * $g(\lambda, \nu) = - \lambda^T b - \nu^T d - f^*_0(-A^T \lambda - C^T \nu)$
        * $\textbf{dom }(g) = \{(\lambda, \nu):-A^T \lambda - C^T \nu \in \textbf{dom } f_0^*\}$
* Conclusion: these functions are closely related, in the sense that:
    * $g(\lambda, \nu)$ is the infimum of an affine function of $f_0(x)$ where the sign of $f_0(x)$ is positive
    * $f_0^*(y)$ is the supremum of an affine function of $f_0(x)$ where the sign of $f_0(x)$ is negative

**Applications**: obtain $g(\lambda, \nu)$ via $f_0^*$

## Examples
**Equality constrained norm minimization**:
* Problem:

$\hspace{1.0cm} \text{minimize}$ $\|x\|$

$\hspace{1.0cm} \text{subject to}$ $A x = b$
* Conjugate of $\|x\|$: $f^*_0(y) = \begin{cases} 0 && \|y\|_* \leq 1 \\ \infty && \text{otherwise} \end{cases}$

$\hspace{1.0cm} \rightarrow f^*(y)$ is the indicator function of the dual norm unit ball

* Dual function of the problem: $g(\nu) = \begin{cases} -b^T \nu && \|A^t \nu\|_* \leq 1 \\ -\infty && \text{otherwise}\end{cases}$

**Entropy maximization**:
* Problem:

$\hspace{1.0cm} \text{minimize}$ $f_0(x) = \sum_{i = 1}^n x_i \log x_i$

$\hspace{1.0cm} \text{subject to}$ $A x \preceq b$

$\hspace{3.0cm} \textbf{1}^T x = 1$
* Objective function's domain: $\textbf{dom}$ $f_0 = \textbf{R}^n_{++}$ 
* Conjugate of $g(u) = u \log u$: $g^*(v) = \exp(v - 1)$

$\hspace{1.0cm} \rightarrow f_0^*(y) = \sum_{i = 1}^n \exp(y_i - 1)$ with $\textbf{dom}$ $f^*_0 = \textbf{R}^n$

* Dual function of the problem: $g(\lambda, \nu) = -b^T \lambda - \nu - \exp(-\nu - 1) \sum_{i = 1}^n \exp(-a_i^T \lambda)$

$\hspace{1.0cm} a_i$ is the $i$-th column of $A$

# From Lagrange multipliers to Lagrangian
## Lagrange multipliers for equality constraints
**Problem**:

$\hspace{1.0cm} \text{minimize}$ $f_0(x)$

$\hspace{1.0cm} \text{subject to}$ $h_i(x) = 0$ $\forall i \in [1, p]$

**Geometric intuition**: we want to minimize $f_0$ over a curve $C$, which is defined by the given constraints
* Assumptions:
    * $\nabla f_0(x) = 0$ for some $x \in C$ 
        * Explain: otherwise, Lagrange multipliers method will result in no solution
* Observations:
    * The point $x^*$, at which, $f_0$ is minimized must be an extremum, thus $x^*$ must satisfy:
        * $\nabla f_0(x^*)$ is normal to $C$
        * $\nabla h_i(x^*)$ is normal to $C$
            * Explain: $C$ is in some of $h_i$'s level curves
    * From above, $\nabla f_0(x^*) \in \textbf{span } \{\nabla h_i(x^*), i = 1..p\}$

    $\hspace{1.0cm} \rightarrow \nabla f_0(x^*) = - \sum_{i = 1}^p \nu_i h_i(x^*)$)
* Conclusion: minimizing $L(x, \nu) = f_0(x) + \sum_{i = 1}^p \nu_i h_i(x)$, in fact, yields a point $(x, \nu)$ that:
    * $\nabla_x L = \nabla f_0(x) + \sum_{i = 1}^p \nu_i \nabla h_i(x) = 0$
    * $\nabla_{\nu_i} L = h_i(x) = 0$ $\forall i \in [1, p]$

## Lagrange multipliers for inequality constraints
**Problem**:

$\hspace{1.0cm} \text{minimize}$ $f_0(x)$

$\hspace{1.0cm} \text{subject to}$ $f_i(x) \leq 0$ $\forall i \in [1, m]$

**Geometric intuition**:
* Assumptions:
    * $f_0$ is convex (for simplicity) 
        * Explain: there's only one extremum over $\textbf{dom } f_0$
    * $S = \{x:f_i(x) \leq 0$ $\forall i \in [1, m]\}$ is the feasible set
    * $x^* = \arg \min_{x \in \textbf{dom } f_0} f_0(x)$
* Observations:
    * If $x^* \in S$, $\nabla f_0(x^*) = 0$
    
    $\hspace{1.0cm} \rightarrow \nabla f_0(x^*)$ lies in the cone $\{\sum_{i = 1}^m \lambda_i \nabla f_i(x^*):\lambda \succeq 0\}$
    * If $x^* \notin S$
        * Suppose $\tilde{x} = \arg \min_{x \in S} f_0(x)$
        
        $\hspace{1.0cm} \rightarrow \tilde{x} \in \textbf{bd } S \cap C$ for some level curve $C$ of $f_0$, which just touches $S$ without intersecting $S$
        * From above, $\nabla f_0(\tilde{x})$ and $\{\nabla f_i(\tilde{x}), i = 1..m\}$ lie on the same space
            * Explain: they're all orthogonal to the mutual tangent space of $C$ and $\textbf{bd } S$ at $\tilde{x}$
        * For $i \in [1, m]$, $\nabla f_i(x)$ points out of $S$ for all $x \in \textbf{bd } S$ since:
            * $f_i(x) = b$ $\forall x \in \textbf{bd } S$
            * $f_i(x) \leq b$ $\forall x \in S$
            * $\nabla f_i(x) \perp \textbf{bd } S$
            * $\nabla f_i(x)$ points to the direction, in which $f_i$ grows most
        * $\nabla f_0(x)$ points to $S$ since:
            * Moving further from $x^*$ help increasing the value of $f_0$
            * $\nabla f_0(x) \perp C$
        * From above, $\nabla f_0(\tilde{x}) = - \sum_{i = 1}^m \lambda_i \nabla f_i(\tilde{x})$ where $\lambda_i \geq 0$
* The Lagrangian for the  inequality constrained problem: $L(x, \lambda) = f_0(x) + \sum_{i = 1}^m \lambda_i f_i(x)$ where $\lambda_i \geq 0$
* Conclusion: minimizing $L(x, \lambda)$, in fact, will a point $(x, \lambda)$ that:
    * $\nabla_x L = \nabla f_0(\tilde{x}) + \sum_{i = 1}^m \lambda_i \nabla f_i(\tilde{x}) = 0$ where $\lambda_i \geq 0$
    * The more $f_i(x) \leq 0$ $\forall i \in [1, m]$ are violated, the higher $L(x, \lambda)$ is

>**NOTE**: if there's no point such at which some level curves of $\{f_i(x), i \in [0, m]\}$ and $\{h_i(x), i = \in [1, p]\}$ all just touch each other

$\hspace{1.0cm} \rightarrow \nabla f_0(x) + \sum_{i = 1}^m \nabla f_i(x) + \sum_{i = 1}^p \nabla h_i(x) = 0$ gives no solution

# Weak and strong duality via set of values
* Assumptions:
    * ${\cal{G}} = \{(f_1(x), \dots, f_m(x), h_1(x), \dots, h_p(x)) \in \textbf{R}^m \times \textbf{R}^p \times \textbf{R}:x \in {\cal{D}}\}$
* Optimal value: $p^* = \inf \{t:(u, v, t) \in {\cal{G}}, u \preceq 0, v = 0\}$
* The dual problem:
    * Objective function: $(\lambda, \nu, 1)^T (u, v, t) = \sum_{i=1}^m \lambda_i u_i + \sum_{i=1}^p \nu_i v_i + t$
    * Domain: ${\cal{G}}$
    * Infimum of Lagrangian: $g(\lambda, \nu) = \inf \{(\lambda, \nu, 1)^T (u, v, t):(u, v, t) \in {\cal{G}}\}$
* Supporting hyperplanes of ${\cal{G}}$: $(\lambda, \nu, 1)^T (u, v, t) \geq g(\lambda, \nu)$
    * Properties:
        * Non-verticality: the last component of the normal vector is non-zero
        * Positive-oriented: $\lambda \succeq 0$

---

# BONUS
* Dual norm: 
    * Formal: $\|u\|_* = \sup \{u^Tx: \|x\| \leq 1\}$
        * $\|\cdot\|$ is a norm on $\textbf{R}^n$
    * Intuition: maximize $u^T x$ over the unit sphere w.r.t norm $\|\cdot\|$