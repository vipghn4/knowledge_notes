<!-- TOC titleSize:1 tabSpaces:2 depthFrom:1 depthTo:6 withLinks:1 updateOnSave:1 orderedList:0 skip:0 title:1 charForUnorderedList:* -->
# Table of Contents
- [Table of Contents](#table-of-contents)
- [New definitions](#new-definitions)
- [Famous results](#famous-results)
- [Tricks](#tricks)
<!-- /TOC -->

# New definitions
**Indicator function of a set $C$**: $\tilde{I}_C(x) = \begin{cases} 0 && x \in C \\ \infty && x \notin C \end{cases}$

**The restriction of a function to an affine set**:
* Assumptions:
    * $f: \textbf{R}^n \to \textbf{R}$ is a function
    * $C = \{F z + \hat{x}|z \in \textbf{R}^m\}$ is an affine set
        * $F \in \textbf{R}^{n \times m}$
        * $\hat{x} \in \textbf{R}^n$
* Conclusion:
    * The restriction of $f$ to $C$: $\tilde{f}(z) = f(F z + \hat{x})$ with $\textbf{dom } \tilde{f} = \{z|F z + \hat{x} \in \textbf{dom } f\}$

**An interpretation of Jensen's inequality**:
* Assumptions:
    * $f$ is a convex function
    * $v$ is a zero-mean random variable
* Conclusion:
    * $E[f(x_0 + v)] \geq f(x_0)$
* Interpretation: randomization or dithering hurts (i.e. raises the expected value of a convex function)
* Explain: $E[f(x_0 + v)] = \sum_i p_i f(x_0 + v_i) \geq f(x_0)$ since $\sum_i p_i = 1$

**Monotone mappings (general definition)**:
* Assumptions:
    * $\psi: \textbf{R}^n \to \textbf{R}^n$ is a function
* Conclusion:
    * $\psi$ is monotone if $(\psi(x) - \psi(y))^T (x - y) \geq 0$ fpr all $x, y \in \textbf{dom } \psi$

**Convex-concave function**:
* Assumptions:
    * $f: \textbf{R}^n \times \textbf{R}^m \to \textbf{R}$
* COnclusion:
    * $f(x, z)$ is convex-concave if
        * $f$ is a concave function of $z$, for each fixed $x$
        * $f$ is a convex function of $x$, for each fixed $z$
        * $\textbf{dom } f = A \times B$ where $A \subseteq \textbf{R}^n$ and $B \in \textbf{R}^m$ are convex

**Saddle-point property and strong max-min property**:
* Assumptions:
    * $f(x, z)$ is a function
    * $(\tilde{x}, \tilde{z}) \in \textbf{dom } f$
* Conclusion:
    * Saddle-point property: $(\tilde{x}, \tilde{z})$ satisfies saddle-point property if $f(\tilde{x}, z) \leq f(\tilde{x}, \tilde{z}) \leq f(x, \tilde{z})$
    * Max-min property: $(\tilde{x}, \tilde{z})$ satisfies max-min property if $\sup_z \inf_x f(x, z) = \inf_x \sup_z f(x, z)$

**Harmonic mean**: $f(x) = \|x\|_{-1}$

**Utility functions**: $u_\alpha = f(x; \alpha)$ where $f$ is a concave function and $\alpha$ is the parameter of $f$
* Concavity of $u_\alpha$: the marginal utility decreases as the amount of goods increases
    * Marginal utility: the increase in utility obtained for a fixed increase in the goods

**Linear map**
* Other names: linear mapping, linear transformation or linear function (in some context)
* Informal definition: a mapping $V \to W$ between two spaces that preserves the operations of addition and scalar multiplication
* Formal definition:
    * Assumptions:
        * $f: V \to W$ is a function
        * $u, v \in V$ are vectors
        * $c \in K$ is a scalar
    * Conclusion:
        * $f$ is a linear map if
            * $f(u + v) = f(u) + f(v)$
            * $f(c u) = c f(u)$

**Support function of a set $C \subseteq \textbf{R}^n$**: $S_C = \sup\{y^T x|x \in C\}$

**Conjugate function of functions of matrices**:
* Assumptions:
    * $X \in \textbf{S}_{++}^n$
    * $f: \textbf{S}_{++}^n \to \textbf{R}$
* Conclusion:
    * Conjugate function of $f$: $f^*(Y) = \sup_{X \succ 0} (\textbf{tr}(Y X) - f(X))$
* Explain: $\textbf{tr}(Y X) = \sum_i \sum_j X_{ij} Y_{ji} = \sum_{i, j} X_{ij} Y_{ji}$

**Approximation width**:
* Assumptions:
    * $f: \textbf{R} \to \textbf{R}$ is the target function which we want to approximate
    * $\hat{f}: \textbf{R} \to \textbf{R}$ is our function which is used for approximation
* Conclusion:
    * $\hat{f}$approximates $f$ with tolerance $\epsilon > 0$ over the interval $[0, T]$ if $|\hat{f}(t) - f(t)| \leq \epsilon$ over $t \in [0, T]$
    * Approximation width w.r.t $\epsilon$: the largest $T$ such that $\hat{f}$ approximates $f$ over $[0, T]$ with tolerance $\epsilon$

# Famous results
**A general arithmetic-geometric mean inequality**: $a^\theta b^{1 - \theta} \leq \theta a + (1 - \theta) b$
* Prove: $\log(\cdot)$ is a concave function

$\hspace{1.0cm} \rightarrow \theta \log a + (1 - \theta) \log b \leq \log(\theta a + (1 - \theta) b)$

**Holder's inequality**: for $p > 1$, $\frac{1}{p} + \frac{1}{q} = 1$ and $x, y \in \textbf{R}^n$

$\hspace{1.0cm} \rightarrow \sum_i x_i y_i \leq \|x\|_p \|y\|_q$
* Prove: consider the inequality $a^\theta b^{1 - \theta} \leq \theta a + (1 - \theta) b$, we can obtain Holder's inequality by
    * Substitute $a$ by $\frac{|x_i|^p}{\sum_j |x_j|^p}$, $b$ by $\frac{|y_i|^q}{\sum_j |b_j|^q}$ and $\theta$ by $\frac{1}{p}$ (i.e. $1 - \theta$ by $\frac{1}{q}$)
    * Summing over $i$ two sides of the substituted inequality

**Notes on semi-positive definite matrices**: positive eigenvalues doesn't imply positive definite matrix, only positive eigenvalues of a symmetric matrix implies that the matrix is positive definite
* Explain: consider a symmetric matrix $A = P^T D P$, $x^T D x = (P x)^T D (P x)$ which is non-negative if and only if $D$'s entries are non-negative

**Concavity of geometric mean**: $(\prod_i x_i)^{1/n}$ is a concave function
* Explain:
    * Assumptions:
        * $f(x) = (\prod_i x_i)^{1/n}$ with $\textbf{dom } f = \{x|\prod_i x_i \geq 0\}$
        * $g(x) = \begin{bmatrix} 1/x_1 \\ \vdots \\ 1/x_n\end{bmatrix}$
        * $h(x) = \text{diag }(1/x_1^2, ..., 1/x_n^2)$
    * Observations:
        * $\frac{\partial f}{\partial^2 x_i} = \frac{1}{n^2} (1 - n) f(x) x_i^{-2}$
        * $\frac{\partial f}{\partial x_i x_j} = \frac{1}{n^2} f(x) x_i^{-1} x_j^{-1}$
        * $H_f = [\frac{\partial f}{\partial x_i x_j}] = \frac{1}{n^2} f(x) (g(x) g(x)^T - n h(x))$
        * Consider $G_f = g(x) g(x)^T - n h(x)$ (i.e. $\text{sign } (v^T G v) = \text{sign } (v^T H v)$ for all $v$)
            * $v^T G v = (\sum_i \frac{v_i}{x_i})^2 - n \sum_i (\frac{v_i}{x_i})^2$
            * By applying Cauchy-Schwarz inequality to the inner product $\sum_i 1 \cdot \frac{v_i}{x_i}$,
            
            $\hspace{1.0cm} \rightarrow (\sum_i \frac{v_i}{x_i})^2 \leq \sum_i (\frac{v_i}{x_i})^2$
            * From above, $v^T G v \leq (1 - n) \sum_i (\frac{v_i}{x_i})^2 \leq 0$ for all $v$
        * From above, $H_f$ is semi-negative definite
    * Conclusion: $f$ is concave

**Generalized Euclidean-space inequalities**:
* Assumptions:
    * $u$ and $v$ vectors in $\textbf{R}^n$
    * $p, q \geq 1$
* Conclusion:
    * Generalized cosine inequality: $\|u^T v\|_1 \leq \|u\|_p \|v\|_q$ if $\frac{1}{p} + \frac{1}{q} = 1$
        * Other name: Holder's inequality
        * Prove: 
            * Without loss of generality, we assume that $\|u\|_p = 1$ and $\|v\|_q = 1$
            
            $\hspace{1.0cm} \rightarrow$ The inequality becomes $\|u^T v\|_1 \leq 1$
            * Followed by $a b \leq \frac{1}{p} a^p + \frac{1}{q} b^q$ (due to the concavity of $\log(\cdot)$)
            
            $\hspace{1.0cm} \rightarrow u^T v \leq \frac{1}{p} \|u\|_p^p + \frac{1}{q} \|v\|_p^p$
            
            $\hspace{2.5cm} = 1$
    * Generalized triangle inequality: $\|u + v\|_p \leq \|u\|_p + \|v\|_p$
        * Other name: Minkowski inequality
        * Prove: 
            * $|u_i + v_i|^p \leq |u_i + v_i|^{p - 1} (|u_i| + |v_i|)$
            
            $\hspace{3.0cm} = |u_i| |u_i + v_i|^{p - 1} + |v_i| |u_i + v_i|^{p - 1}$
            * By Holder's inequality, $u^T w \leq \|u\|_p \|w\|_{\frac{1}{1 - 1/p}}$ where $w = \begin{bmatrix} |u_1 + v_1|^{p-1} \\ \vdots \\ |u_n + v_n|^{p-1}\end{bmatrix}$
            
            $\hspace{1.0cm} \rightarrow \|u + v\|_p^p \leq (\|u\|_p + \|v\|_p) \|w\|_{\frac{1}{1 - 1/p}}$
            * $\|w\|_{\frac{1}{1 - 1/p}} = \|w\|_{\frac{p}{p - 1}} = (\sum_i |u_i + v_i|^p)^{1 - 1/p} = \frac{\|u + v\|_p^p}{\|u + v\|_p}$
            
            $\hspace{1.0cm} \rightarrow \|u + v\|_p \leq \|u\|_p + \|v\|_p$


**Convexity of norm**: any norm $\|x\|$ is a convex function
* Explain: 
    * $\|\alpha x\| = \alpha \|x\|$
    
    $\hspace{1.0cm} \rightarrow \theta \|x\| + (1 - \theta) \|y\| = \|\theta x\| + \|(1 - \theta) y\| \geq \|\theta x + (1 - \theta) y\|$ (due to triangle inequality)
    * From above, $\|x\|$ is convex

**Functions of eigenvalues**:
* Assumptions:
    * $X \in \textbf{S}^n$ is a matrix
    * $\lambda_i(X)$ is an eigenvalue of $X$
* Special formulas:
    * $\textbf{tr } X = \sum_i \lambda_i(X)$
        * Explain: 
            * $X \in \textbf{S}^n$
            
            $\hspace{1.0cm} \rightarrow X$ can be orthogonally diagonalized to $\begin{bmatrix} v_1 && \cdots && v_n \end{bmatrix} \text{diag }(\lambda_1, ..., \lambda_n) \begin{bmatrix} v_1^T \\ \vdots \\ v_n^T \end{bmatrix}$
            * $\textbf{tr } X = \sum_i \lambda_i \textbf{tr } (v_i v_i^T) = \sum_i \lambda_i \|v_i\|_2^2 = \sum_i \lambda_i$
    * $\det X = \prod_i \lambda_i(X)$
        * Explain: $X \in \textbf{S}^n$
        
        $\hspace{1.0cm} \rightarrow \det X = \det (P D P^{-1}) = \det D = \prod_i \lambda_i(X)$
* Core observations: 
    * Each $\lambda_i$ is a linear combination of $X$'s entries
    
    $\hspace{1.0cm} \rightarrow \lambda_i$ is also a linear function of $X$ (since $f(X) = x_{ij}$ is a linear function of $X$ (by Jensen's inequality), for each of its entry $x_{ij}$)
    * Each entry $x_{ij}$ of $X$ is a linear combination of $\lambda_1, ..., \lambda_n$
* Conclusion:
    * Convexity:
        * $\max_i \lambda_i(X)$ is convex
            * Explain: maximum of linear functions
        * $\min_i \lambda_i(X)$ is concave
            * Explain: minimum of linear functions
        * $\textbf{tr } X = \sum_i \lambda_i(X)$ is linear
            * Explain: easy-to-derive
        * $\textbf{tr } X^{-1} = \sum_i 1/\lambda_i(X)$ is convex
            * Explain: sum of convex functions
        * $(\det X)^{1/n} = [\prod_i \lambda_i(X)]^{1/n}$ is concave
            * Explain: $\lambda = (\lambda_1, ..., \lambda_n)$ is a linear function of $X$
            
            $\hspace{1.0cm} \rightarrow$ Due to the concavity of geometric mean, the above result is true
        * $\log \det X = \sum_i \log \lambda_i(X)$ is concave
            * Explain: sum of concave functions

**Convex hull (convex evelope) of a function**:
* Assumptions:
    * $f: \textbf{R}^n \to \textbf{R}$
* Conclusion:
    * Convex hull of $f$: $g(x) = \inf \{t|(x, t) \in \textbf{conv } \textbf{epi } f\}$
* Another interpretation: $\textbf{epi } g = \textbf{conv } \textbf{epi } f$

**Homogeneous function**: $f: \textbf{R}^n \to \textbf{R}$ is homogeneous if $g(t x) = t g(x)$ for all $t \geq 0$

**Minkowski function of a convex set $C$**: $M_C(x) = \inf \{t > 0|t^{-1} x \in C\}$

**First-order condition for log concave function**:
* Assumptions:
    * $f: \textbf{R}^n \to \textbf{R}$ is differentiable
        * $\textbf{dom  } f$ is convex
    * $f(x) > 0$ for all $x \in \textbf{dom } f$
* Conclusion: $f$ is log concave if and only if $\frac{f(y)}{f(x)} \leq \exp(\frac{\nabla f(x)^T (y - x)}{f(x)})$
    * Explain: $\log f(y) \leq \log f(x) + \frac{\log f(x)}{x} (y - x)$

# Tricks
**Prove convexity of a function**:
* Jensen's inequality: $f(\theta x + (1 - \theta) y) \leq \theta f(x) + (1 - \theta) f(y)$
* A function is convex if it's convex when restricted to any line intersecting its domain
* Use first-order or second-order conditions
* Prove the convexity of epigraph
* Based on the operations which preserve convexity

**Minimizing a function over a convex set**:
* Assumptions:
    * $f$ is a convex defined on $\textbf{R}^n$
    * $C$ is a convex set
    * $\tilde{I}_C$ is the indicator function of $C$
* Conclusion: minimizing $f$ over $C$ is the same as minimizing $f + \tilde{I}_C$ over $\textbf{R}^n$

**Fast multiplication derivative**:
* Assumptions:
    * $f: \textbf{R} \to \textbf{R}$ is a function
    * $g: \textbf{R} \to \textbf{R}$ is a function
* Conclusion:
    * $\frac{\partial f g}{\partial^n x} = \sum_{i = 0}^n \frac{\partial f}{\partial^i x} \frac{\partial g}{\partial^{n - i} x}$ where $f g$ denotes the function $f(x) \cdot g(x)$

**Eigenvalue of sum of matrices**:
* Assumptions:
    * $A, B \in \textbf{R}^{n \times n}$ are diagonalizable matrices
    * $v$ is an eigenvector of both $A$ and $B$ corresponding to eigenvalues $\lambda_A$ and $\lambda_B$
* Conclusion:
    * $v$ is an eigenvector of $A + B$ corresponding to eigenvalue $\lambda_A + \lambda_B$
* Consequence: the set of eigenvalues of $I + A$ is $\{\lambda_i + 1|i = 1..n\}$ where $\lambda_i$ is an eigenvalue of $A$

**Area of parallelepiped**:
* Assumptions:
    * $a, b$ are two vectors
    * $\theta$ is the angle between $a$ and $b$
    * $P$ is the parallelepiped defined by $a$ and $b$
* Conclusion:
    * The area of $P$: $\|a\|_2 \|b\|_2 \sin \theta$
* Explain: $\|a\|_2 \sin \theta$ is the height of $P$ w.r.t base $b$

**Cauchy-Schwarz inequality for proving concavity**: $a^T b \leq \|a\|_2 \|b\|_2$
* Tricks:
    * Reduce the expression for better inference
    * $\|a\|_2$ (or $\|b\|_2$) can be a constant (naturally or eliminated by some outer factor) or an unknown value depending on some other variable
    
    $\hspace{1.0cm} \rightarrow$ Try to apply Cauchy-Schwarz inequality in both way

**Prove linearity of a function**: use Jensen's inequality (i.e. when we cannot write the function in closed form)

**Tricks for proving inequality**:
* Introduce assumptions which doesn't reduce the generality of the problem
* When we want to transform the expression to produce some sub-expression of a specific form

$\hspace{1.0cm} \rightarrow$ Just go straight forward to produce that sub-expression, then reduce the sub-expression containing the remaining terms and factors

**Prove log-concavity of $f$**: $f''(x) f(x) \leq f'(x)^2$