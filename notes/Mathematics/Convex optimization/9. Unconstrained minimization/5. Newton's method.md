<!-- TOC titleSize:1 tabSpaces:2 depthFrom:1 depthTo:6 withLinks:1 updateOnSave:1 orderedList:0 skip:0 title:1 charForUnorderedList:* -->
# Table of Contents
- [Table of Contents](#table-of-contents)
- [Introduction](#introduction)
- [Newton method for finding roots](#newton-method-for-finding-roots)
- [The Newton step](#the-newton-step)
- [Newton's method](#newtons-method)
- [Convergence analysis (see the text book)](#convergence-analysis-see-the-text-book)
- [BONUS](#bonus)
- [NEW WORD](#new-word)
<!-- /TOC -->

# Introduction
**Other name**: Newton-Raphson method

**Motivation**: Newton method is originally developed for finding a root of a function
* Formal: given $f: \textbf{R} \to \textbf{R}$, find $x^*$ that $f(x^*) = 0$

**History**:
* First application of Newton method: Babylonian used the method to find the square root of a number $S \in \textbf{R}_+$

$\hspace{1.0cm} \rightarrow$ This is a special case of Newton method

# Newton method for finding roots
**Task**: find a point $x^*$ that $f(x^*) = 0$

**Idea**:
* First-order Taylor approximation: $f(x + \Delta x) = f(x) + f'(x) \Delta x + o(|\Delta x|)$
  * $o(|\Delta x|)$ can be omitted when approximating $f$ linearly
* We set the approximation to $0$: $0 \approx f(x) + f'(x) \Delta x$

$\hspace{1.0cm} \rightarrow x^* - x = \Delta x = - \frac{f(x)}{f'(x)}$

**The iterative update rule**: $x_{k+1} = x_k - \frac{f(x)}{f'(x)}$

**Generalize to multivariate functions**:
* Assumptions:
  * $F: \textbf{R}^n \to \textbf{R}^m$ is a function
* Taylor approximation: $\textbf{0} = F(x^*) = F(x + \Delta x) = F(x) + \frac{\partial F(x)}{\partial x} \Delta x + o(|\Delta x|)$

$\hspace{1.0cm} \rightarrow \Delta x = -(\frac{\partial F(x)}{\partial x})^{-1} F(x)$
* The iterative update rule: $x_{k+1} = x_k - (\frac{\partial F(x)}{\partial x})^{-1} F(x)$

**Root-finding problem to minimization problem**:
* Assumptions:
  * $f: \textbf{R}^n \to \textbf{R}$ is differentiable
* Task: find $\min_{x \in \textbf{R}^n} f(x)$
* Equivalent root-finding problem: $\frac{\partial F(x)}{\partial x} = \textbf{0}$

# The Newton step
**Interpretations**: 
* Interpretation 1: at each point, take $f$ and develop a second order approximation $\hat{f}(x + v) = f(x) + \nabla f(x)^T v + \frac{1}{2} v^T \nabla^2 f(x) v$

$\hspace{1.0cm} \rightarrow$ We take $\Delta x_\text{nt} = -\nabla^2 f(x)^{-1} \nabla f(x)$, which minimizes the second order approximation instead of $f$
* Interpretation 2: we solve the linearized optimality condition $\nabla f(x + v) \approx \nabla \hat{f}(x + v) = \nabla f(x) + \nabla^2 f(x) v = 0$

**The Newton step for $f$ at $x \in \textbf{dom } f$**: $\Delta x_\text{nt} = -\nabla^2 f(x)^{-1} \nabla f(x)$
* Newton step as a descent direction: $\nabla f(x)^T \Delta x_\text{nt} = -\nabla f(x)^T \nabla^2 f(x)^{-1} \nabla f(x) < 0$ (unless $\nabla f(x) = 0$) due to positive definiteness of $\nabla^2 f(x)$

$\hspace{1.0cm} \rightarrow$ The Newton step is a descent direction (unless $x$ is optimal)

**More on Newton step**: see Newton's method - probability & statistics

**Affine invariance of the Newton step**: the Newton step is independent of linear (or affine) changes of coordinates
* Assumptions:
  * $T \in \textbf{R}^{n \times n}$ is non-singular
  * $\bar{f}(y) = f(T y)$
  * $x = T y$
* Conclusion: the Newton steps of $f$ and $\bar{f}$ are related by $x + \Delta x_\text{nt} = T(y + \Delta y_\text{nt})$
* Interpretation:
  * Assumptions:
    * Assume we want to minimize $f(x)$, we apply change of the coordinates $Ty  = x$
    
    $\hspace{1.0cm} \rightarrow$ We want to minimize $\tilde{f}(y) = f(Ty)$
  * Conclusion:
    * The gradient of $\tilde{f}$ would be $\nabla \tilde{f}(y) = T \nabla f(Ty)$

>**NOTE**: this is why Newton method works so well
* Explain:
  * Newton's method and change of coordinates do commute
  * Gradient descent and change of coordinates don't commute

$\hspace{1.0cm} \rightarrow$ First-order scaling is non-issue in Newton's method (i.e. in gradient descent, the geometry of $f$ has a strong effect on the algorithm while in Newton's method, it doesn't make sense)

**The Newton decrement at $x$**: $\lambda(x) = (\nabla f(x)^T \nabla^2 f(x)^{-1} \nabla f(x))^{1/2}$
* Use: measure the proximity of $x$ to $x^*$
* Assumptions:
  * $\hat{f}(x)$ is the second-order approximation of $f$ at $x$
* Expressions of the Newton decrement:
  * Expression 1: $f(x) - \inf_y \hat{f}(y) = f(x) - \hat{f}(x + \Delta x_\text{nt}) = \frac{1}{2} \lambda(x)^2$
  
  $\hspace{1.0cm} \rightarrow \lambda$ is an estimate of $f(x) - p^*$, based on the quadratic approximation of $f$ at $x$
  * Expression 2: $\lambda(x)$ can be expressed as $(\Delta x_\text{nt}^T \nabla^2 f(x) \Delta x_\text{nt})^{1/2}$
  
  $\hspace{1.0cm} \rightarrow \lambda$ is the norm of the Newton step, in the norm $\|u\|_{\nabla^2 f(x)} = (u^T \nabla^2 f(x) u)^{1/2}$
* Affine invariance: the Newton decrement is affine invariant
  * Formal: the Newton decrement of $\bar{f}(y) = f(T y)$ at $y$ where $T$ is non-singular, is the same as the Newton decrement of $f$ at $x = T y$

# Newton's method
**Other name**: the damped (or guarded) Newton method

>**NOTE**: the pure Newton method uses a fixed step size $t = 1$

**Algorithm**:

---

**Initialization**: choose a starting point $x \in \textbf{dom } f$ and a tolerance $\epsilon > 0$

**Iteration**:
* Compute the Newton step and decrement
  * $\Delta x_\text{nt} = -\nabla^2 f(x)^{-1} \nabla f(x)$
  * $\lambda^2 = \nabla f(x)^2 \nabla^2 f(x)^{-1} \nabla f(x)$
* Stopping criterion: $\lambda^2 / 2 \leq \epsilon$
* Line search: choose step size $t$ by backtracking line search
* Update: $x = x + t \Delta x_\text{nt}$

---

**Practical result**: this method works shockingly well

# Convergence analysis (see the text book)
**What Newton's method actually do**: see the interpretation 1 of Newton step

$\hspace{1.0cm} \rightarrow$ Newton's method is gonna work well if Hessian is slowly changing (i.e. the third derivative is small)
* Explain: for each step, we create a quadratic model

$\hspace{1.0cm} \rightarrow$ If the quadratic model is good, the Newton's method is going to work really well

**Assumptions**:
* $f$ is strongly convex on $S$ with constant $m$
* $\nabla^2 f$ is Lipschitz continuous on $S$, with constant $L > 0$
  * Formal: $\|\nabla^2 f(x) - \nabla^2 f(y)\|_2 \leq L \|x - y\|_2$
    * $L$ can be interpreted as a bound on the third derivative of $f$
    * $L$ measures how well $f$ can be approximated by a quadratic function

**Outline**: there exists a constant $\eta \in (0, m^2/L)$ and $\gamma > 0$ that:
* If $\|\nabla f(x)\|_2 \geq \eta$ then $f(x^{(k+1)}) - f(x^{(k)}) \leq -\gamma$ (i.e. a decrease in $f$ is guaranteed)
* If $\|\nabla f(x)\|_2 < \eta$ then

$\hspace{1.0cm} \frac{L}{2m^2} \|\nabla f(x^{(k+1)})\|_2 \leq (\frac{L}{2m^2} \|\nabla f(x^{(k+1)})\|_2)^2$ (i.e. the convergence is extremely fast)

>**NOTE**: $\frac{L}{2m^2} \|\nabla f(x^{(k+1)})\|_2$ can be thought of as a measure of error (i.e. a scaled residual)

**Damped Newton phase**: $\|\nabla f(x)\|_2 \geq \eta$
* Most iterations require backtracking steps 
* $f$ decreases by at least $\gamma$
* If $p^* > -\infty$, this phase ends after at most $(f(x^{(0)}) - p^*)/\gamma$ iterations

**Quadratic convergent phase**: $\|\nabla f(x)\|_2 < \eta$
* All iterations use step size $t = 1$
* $\|\nabla f(x)\|_2$ converges to zereo quadratically: if $\|\nabla f(x^{(k)})\|_2 < \eta$ then

$\hspace{1.0cm} \frac{L}{2m^2} \|\nabla f(x^{(l)})\| \leq (\frac{L}{2m^2} \|\nabla f(x^{(k)})\|)^{2^{l-k}} \leq (\frac{1}{2})^{2^{l-k}}$ where $l \geq k$ (i.e. number of accurate digits doubles every step)

$\hspace{1.0cm} \rightarrow$ We cannot execute this phase more than $4$ or $5$ times

**A bound on the required number of Newton step**:
* Damped phase: $f(x^{(k+1)}) - f(x^{(k)}) \leq -\gamma$

$\hspace{1.0cm} \rightarrow$ The maximum number of damped steps is $\frac{f(x^{(0)}) - p^*}{\gamma}$
* Quadratic phase:
  * Observations:
    * $\|\nabla f(x^{(k)})\|_2 < \eta \land \eta < m^2/L$
    
    $\hspace{1.0cm} \rightarrow (\frac{L}{2m^2} \|\nabla f(x^{(k+1)})\|_2)^2 \leq (1/2)^2$
    * From above, $\frac{L}{2m^2} \|\nabla f(x^{(l)})\|_2 \leq (\frac{L}{2m^2} \|\nabla f(x^{(k)})\|_2)^{2^{l-k}} \leq (1/2)^{2^{l-k}}$
    
    $\hspace{1.0cm} \rightarrow \|\nabla f(x^{(l)})\|_2 \leq \frac{2m^2}{L} (1/2)^{2^{l-k}}$
    * $f(x^{(l)}) - p^* \leq \frac{1}{2m} \|\nabla f(x^{(l)})\|_2^2 \leq (\frac{1}{2m} \frac{2m^2}{L} (1/2)^{2^{l-k}})^2 = \frac{2m^3}{L} (1/2)^{2^{l-k+1}}$
  * Conclusion: In order that $f(x) - p^* \leq \epsilon$
  
  $\hspace{1.0cm} \rightarrow$ The maximum number of quadratic steps is $- \log_2 \log_2 \frac{L^2}{2m^3} \epsilon = \log_2 \log_2 \epsilon_0 \epsilon$ where $\epsilon_0 = \frac{2m^3}{L}$
  
  >**NOTE**: $\log_2 \log_2 (\epsilon_0 / \epsilon)$ can be considered as a constant ($5$ or $6$) for practical purpose
* Total number of Newton steps: $\frac{f(x^{(0)}) - p^*}{\gamma} + 6$

**Complexity**:  
* Number of iterations until $f(x) - p^* \leq \epsilon$: bounded above by $\frac{f(x^{(0)}) - p^*}{\gamma} + \log_2 \log_2 (\epsilon_0 / \epsilon)$
  * $c, \epsilon_0$ are constants that depend on $m, L, x^{(0)}$
  * Practical notes: 
    * $\log_2 \log_2 (\epsilon_0 / \epsilon)$ is small (of the order of $6$) and almost constant for practical purpose
    * In practice, $m, L$ (hence $c, \epsilon_0$) are usually unknown
* Compute the Hessian: $O(n^3)$ (while the cost for computing the gradient is $O(n)$)

**NOTE**: in the past, $O(n^3)$ is terrible

$\hspace{1.0cm} \rightarrow$ Newton's method is approximated by other methods

---

# BONUS
* Metric induced by norm:
  * Assumptions: 
    * $V$ is a normed vector space
    * $\|\cdot\|$ is the norm of $V$
  * Conclusion:
    * The induced metric (or the metric induced by $\|\cdot\|$): the map $d: V \times V \to \textbf{R}_{\geq 0}$, which is defined as $d(x, y) = \|x - y\|$

# NEW WORD
* Proximity (n): sự gần gũi