<!-- TOC titleSize:1 tabSpaces:2 depthFrom:1 depthTo:6 withLinks:1 updateOnSave:1 orderedList:0 skip:0 title:1 charForUnorderedList:* -->
# Table of Contents
- [Table of Contents](#table-of-contents)
- [Pre-computation for line searches](#pre-computation-for-line-searches)
  - [Composition with an affine function](#composition-with-an-affine-function)
- [Computing the Newton step](#computing-the-newton-step)
  - [Cholesky factorization of $H$](#cholesky-factorization-of-h)
  - [Special structure of $H$ (see the text book)](#special-structure-of-h-see-the-text-book)
- [BONUS](#bonus)
- [NEW WORD](#new-word)
<!-- /TOC -->

# Pre-computation for line searches
**Simplest**: $f(x + t \Delta x)$ is evaluated, in the same way that $f(z)$ is evaluated for any $z \in \textbf{dom } f$, for each value of $t$

**Observation**: $f$ (and $\nabla f$, in exact line search) are to be evaluated at many points along the ray $\{x + t \Delta x|t \geq 0\}$

$\hspace{1.0cm} \rightarrow$ We can exploit this to reduce the total computational cost

**Idea**: do some pre-computation, which is often on the same order as computing $f$ at any point, so that $f$ (and $\nabla f$) can be computed more efficiently along the ray

## Composition with an affine function
**Assumptions**:
* $f(x) = \phi(Ax + b)$
  * $A \in \textbf{R}^{p \times n}$
  * $\phi$ is easy to evaluated
* $\tilde{f} = f(x + t \Delta x)$

**Task**: evaluate $\tilde{f}$ for $k$ values of $t$

**Naive approach**:
* Form $A(x + t \Delta x) + b$ for each $t$
  * This is done in the same way to compute $A z + b$ for any $z$
* Evaluate $\phi(A(x + t \Delta x) + b)$ for each $t$

**Linear interpolation of $A(x + t \Delta x) + b$**:
* Compute $A x + b$
* For each $t$, $A(x + t \Delta x) + b = (A x + b) + t (A \Delta x)$

# Computing the Newton step
**Problem**: the computational cost of computing $\Delta x_\text{nt}$ (i.e. the Newton step) dominates the cost of the line search

**Naive approach**:
* Compute the Hessian $H = \nabla^2 f(x)$ and the gradient $g = \nabla f(x)$ at $x$
  * This is done using a general linear equation solver
* Solve the equation $H \Delta x_\text{nt} = -g$ for $\Delta x_\text{nt}$

## Cholesky factorization of $H$
**Idea**: exploit the symmetry and positive definiteness of $H$

**Cholesky factorization of a matrix $A$**: $A = L L^T$ where $L$ is a lower triangular matrix

**Method**:
  * Solve $L w = -g$ by forward substitution to get $w = -L^{-1} g$
  * Solve $L^T \Delta x_\text{nt} = w$ by back substitution

**Formal**: $x_\text{nt} = L^{-T} w = -L^{-T} L^{-1} g = -H^{-1} g$

**Compute the Newton decrement**: $\lambda^2 = -\Delta x_\text{nt}^T g = \|L^{-1} g\|_2^2 = \|w\|_2^2$

## Special structure of $H$ (see the text book)
**Structure of $H$**: structure that is the same for all $x$
* Example: "$H$ is tri-diagonal" means $\nabla^2 f(x)$ is tri-diagonal for all $x \in \textbf{dom } f$

---

# BONUS
* The Newton system (or the normal equations): $\nabla^2 f(x) \Delta x_\text{nt} = -\nabla f(x)$
  * "the Newton system": the solution of the system is the Newton step
  * "the normal equations": the same type of equation arises in solving a least-squares problem
* Banded matrix: $A$ is banded with bandwidth $k$ if $A_{ij} = 0$ $\forall |i-j| > k$
  * Examples: 
    * Diagonal matrix is banded with bandwidth $0$
    * Tri-diagonal matrix is banded with bandwidth $1$
* Sparse matrix: a matrix in which most of the elements are zero
  * The sparsity of the matrix: the number of zero-valued elements divided by the total number of elements
* Dense matrix: a matrix in which most of the elements are nonzero
* Coupling: the degree of independence between modules (or components)

# NEW WORD
* Exploit (v): tận dụng