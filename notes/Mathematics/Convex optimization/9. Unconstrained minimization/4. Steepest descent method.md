<!-- TOC titleSize:1 tabSpaces:2 depthFrom:1 depthTo:6 withLinks:1 updateOnSave:1 orderedList:0 skip:0 title:1 charForUnorderedList:* -->
# Table of Contents
- [Table of Contents](#table-of-contents)
- [Steepest descent method](#steepest-descent-method)
- [Steepest descent for Euclidean and quadratic norms](#steepest-descent-for-euclidean-and-quadratic-norms)
- [Steepest descent for $L_1$-norm](#steepest-descent-for-l_1-norm)
- [Convergence analysis (see the text book)](#convergence-analysis-see-the-text-book)
- [Choice of norm](#choice-of-norm)
- [Bonus](#bonus)
<!-- /TOC -->

# Steepest descent method
**Problem**:
* The directional derivative of $f$ at $x$ in the direction $v$: $\nabla f(x)^T v$
* Task: choose $v$ to make $\nabla f(x)^T v$ as negative as possible

>**NOTE**: to make the ask sensible, we need to limit (or normalize) the length of $v$

**A normalized steepest descent direction (w.r.t the norm $\|\cdot\|$)**: $\Delta x_\text{nsd} = \arg \min \{\nabla f(x)^T v|\|v\| = 1\}$

**Unnormalized steepest descent direction**: $\Delta x_\text{sd} = \|\nabla f(x)\|_* \Delta x_\text{nsd}$
  * Notation: $\| \cdot \|_*$ is the dual norm
  * Change in $f$ when using unnormalized steepest descent direction: $\nabla f(x)^T \Delta x_\text{sd} = \|\nabla f(x)\|_* \nabla f(x)^T \Delta x_{nsd} = -\|\nabla f(x)\|_*^2$

**Normalized and unnormalized explain**:
* Normalized direction: $\|x_\text{nsd}\|_* = 1$
* Unnormalized direction: $\|x_\text{sd}\|_* = \|\nabla f(x)\|_*$

**Algorithm**

---

**Initialization**: choose a starting point $x \in \textbf{dom } f$

**Iteration**:
* Compute steepest descent direction $\Delta x_\text{sd}$
* Line search: choose $t$ via backtracking or exact line search
* Update: $x = x + t \Delta x_\text{sd}$

**Termination**: stopping criterion is satisfied

---

>**NOTE**: when exact line search is used, scale factors in the descent direction have no effect

$\hspace{1.0cm} \rightarrow$ The normalized or un-normalized direction can be used

**Intuition (given by Stephen Boyd in his course)**: 
* Assume that you ask people about the fastest direction to move downhill 
* By fastest decrease direction, you mean decrease in $f$ divided by the of number of meters you move, which is represented by norm
* Other people use different norms

$\hspace{1.0cm} \rightarrow$ They choose different directions to be the steepest descent

# Steepest descent for Euclidean and quadratic norms
**Steepest descent for Euclidean norm**: $\Delta x_\text{sd} = -\nabla f(x)$

**Steepest descent for quadratic norm**:
* Quadratic norm: $\|z\|_P = (z^T P z)^{1/2} = \|P^{1/2} z\|_2$ where $P \in \textbf{S}^n_{++}$
* Normalized steepest descent direction: $\Delta x_\text{nsd} = -[(P^{-1} \nabla f(x))^T P (P^{-1} \nabla f(x))]^{-1/2} P^{-1} \nabla f(x) = -(\nabla f(x)^T P^{-1} \nabla f(x))^{-1/2} P^{-1} \nabla f(x)$

* The dual norm: $\|z\|_* = \|P^{-1/2} z\|_2$

$\hspace{1.0cm} \rightarrow \Delta x_\text{sd} = -P^{-1} \nabla f(x)$

# Steepest descent for $L_1$-norm
**Normalized steepest descent**: $\Delta x_\text{nsd} = \arg \min \{\nabla f(x)^T v|\|v\|_1 \leq 1\}$
* Assumptions:
  * $i$ is any index that $\|\nabla f(x)\|_\infty = |(\nabla f(x))_i|$
  * $e_i$ is the $i$-th standard basis vector
* Conclusion:
  * $\Delta x_\text{nsd} = -\text{sign }(\frac{\partial f(x)}{\partial x_i}) e_i$
  * $\Delta x_\text{sd} = \Delta x_\text{nsd} \|\nabla f(x)\|_\infty = - \frac{\partial f(x)}{\partial x_i} e_i$

# Convergence analysis (see the text book)
**Convergence result**: for strongly convex $f$

$\hspace{1.0cm} f(x^{(k)}) - p^* \leq c^k (f(x^{(0)}) - p^*)$ where $c \in (0, 1)$ depends on $m, x^{(0)}$ and the line search type

# Choice of norm
>**NOTE**: this is the most important part of steepest descent

**Rough idea**: we want the norm to be consistent with the geometry of the sublevel sets

**Quadratic-norm steepest descent**: choose $P$ so that the sublevel sets of $f$, transformed by $P^{-1/2}$, are well conditioned
* Another representation of quadratic-norm steepest descent: steepest descent with quadratic norm $\|\cdot\|_P$ is actually gradient descent after change of variables $\tilde{x} = P^{1/2} x$

---

# Bonus
* **1. Dual norm**: $\|u\|_* = \sup \{u^T x|\|x\| \leq 1\}$ where $\|\cdot\|$ is a norm on $\textbf{R}^n$
* **2. Quadratic form**: $x^T A x$ where $A \in \textbf{S}^n$
	* Due to symmetricity, $A = P D P^T$ where $P$ is orthogonal and $D$ is diagonal
	
	$\hspace{1.0cm} \rightarrow x^T A x = (D^{1/2} P^T x)^T (D^{1/2} P^T x)$
	* From above, $x^T A x$ acts as following:
		* Rotate $x$ to $P^T x$
		* Scale (not uniform) $P^T x$ to $D^{1/2} P^T x$
		* Compute the $L_2$ norm of $D^{1/2} P^T x$