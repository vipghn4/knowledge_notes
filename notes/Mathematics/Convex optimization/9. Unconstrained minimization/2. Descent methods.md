<!-- TOC titleSize:1 tabSpaces:2 depthFrom:1 depthTo:6 withLinks:1 updateOnSave:1 orderedList:0 skip:0 title:1 charForUnorderedList:* -->
# Table of Contents
- [Table of Contents](#table-of-contents)
- [Descent methods](#descent-methods)
<!-- /TOC -->

# Descent methods
**Iterative method**: produce a minimizing sequence $x^{(k)}$
* Recursive formula: $x^{(k + 1)} = x^{(k)} + t^{(k)} \Delta x^{(k)}$ where $t^{(k)} > 0$ (except when $x^{(k)}$ is optimal)
  * Another form: focus on one iteration of the algorithm 
    * $x^+ = x + t \Delta x$
    * (or) $x = x + t \Delta x$
* Step (or search direction): $\Delta x^{(k)}$ (it needn't have unit norm)
  * Exact name: search step
* Iteration number: $k = 0, 1, ...$
* Step size (or step length) at iteration $k$: $t^{(k)} > 0$
  * Exact name: scale factor

**Descent method**: iterative methods which satisfy $f(x^{(k + 1)}) < f(x^{(k)})$ (except when $x^{(k)}$ is optimal)
* Descent direction for $f$ at $x^{(k)}$: any direction $\Delta x$ satisfying $\nabla f(x^{(k)})^T \Delta x^{(k)} < 0$

>**NOTE**: due to convexity, the search direction in a descent method must be a descent direction

* The outline of a general descent method:

---

**Initialization**: choose a starting point $x \in \textbf{dom } f$

**Iteration**:
* Determine a descent direction $\Delta x$
* Linear search: choose a step size $t > 0$
* Update: $x = x + t \Delta x$

**Termination**: stopping criterion is satisfied

---

* Line search (or ray search): selecting the step size $t$ determines where along $\{x + t \Delta x|t \in \textbf{R}_+\}$ the next iterate will be
* Stopping criterion: often of the form $\|\nabla f(x)\|_2 \leq \eta$ where $\eta > 0$ is small

**Exact line search**: choose $t$ to minimize $f$ along the ray $\{x + t \Delta x|t \geq 0\}$
* Formal: $t = \arg \min_{s \geq 0} f(x + s \Delta x)$
* Usage: when the cost of minimizing $f(x + s \Delta x)$ in $s$ is low, compared to the cost of computing the search direction $\Delta x$
* Find the minimizer of $f(x + s \Delta x)$ in $s$: can be found analytically in some special cases and computed efficiently in others

**Backtracking line search**:
* Inexact line search: the step length $t$ is chosen to approximately minimize $f$ along $\{x + t \Delta x|t \geq 0\}$ or even just to reduce $f$ 'enough'
* Backtracking line search:

---

**Initialization**: 
* Choose a descent direction $\Delta x$ for $x$ at $x \in \textbf{dom } f$
* Choose $\alpha \in (0, 0.5)$ and $\beta \in (0, 1)$
* Let $t = 1$

**Iteration**: if $f(x + t \Delta x) > f(x) + \alpha t \nabla f(x)^T \Delta x$ then $t = \beta t$

**Termination**: stopping criterion $f(x + t \Delta x) \leq f(x) + \alpha t \nabla f(x)^T \Delta x$ is satisfied 

---

* Intuition:
  * By replacing $t \Delta x$ by $y$, we got $g(y) = f(x) + \nabla f(x)^T y$
  * Consider the line $d = \{(y, g(y))|y \in \textbf{dom } g\}$, which is the tangent line of the curve $C = \{(x, f(x))|x \in \textbf{dom } f\}$
  * By multiply $\nabla f(x)^T y$ by $\alpha \in (0, 0.5)$, we make $d$ "more horizontal" so that it intersects $C$ at $(x, f(x))$ and some other point $(x_0, f(x_0))$ that $f(x_0) \leq f(x)$
  * Any point $z$ satisfying $f(z) > f(x) + \alpha \nabla f(x)^T (z - x)$ must lie within the line segment $[x, x_0]$
  
  $\hspace{1.0cm} \rightarrow$ Due to convexity, $f(z) \leq f(x)$
  * From above, we actually minimize $f$ at each iteration

* Termination: 
  * Observation: since $\Delta x$ is a descent direction, for small enough $t$

  $\hspace{1.0cm} \rightarrow f(x + t \Delta x) \approx f(x) + t \nabla f(x)^T \Delta x < f(x) + \alpha t \nabla f(x)^T \Delta x$
  * Conclusion: the backtracking line search eventually terminates

* The constant $\alpha$: the fraction of the decrease in $f$, predicted by linear extrapolation that we will accept
  * Explain: we extrapolate $f(y)$ by $\phi(y) = f(x) + \nabla f(x)^T y$
  
  $\hspace{1.0cm} \rightarrow$ If moving from $x$ to $y$ decreases $\phi(y)$ by $c$ then it decreases $f(x) + \alpha \nabla f(x)^T y$ by $\alpha c$

* The step length $t$ that satisfies the backtracking linear search stopping criterion:
  * Observation: $f(x + t \Delta x) \leq f(x) + \alpha t \nabla f(x)^T \Delta x$ holds for $t \geq 0$ and $t \in (0, t_0]$ for some $t_0$
  * The stopping value of $t$:
    * $t = 1$: when $t_0 \geq 1$
    * $t \in (\beta t_0, t_0]$: assume we terminates the algorithm at iteration $k$, then:
      * $t \geq t_0$ at iteration $k - 1$
      * $t \geq \beta t_0$ and $t \leq t_0$ at iteration $k$
  * A lower bound of the stopping value of $t$: $t \geq \min \{1, \beta t_0\}$

* Practical implementation: by convention, we define $f(x) = \infty$ $\forall x \notin \textbf{dom } f$

$\hspace{1.0cm} \rightarrow$ We first multiply $t$ by $\beta$ until $x + t \Delta x \in \textbf{dom } f$

* Choose hyperparameters:
  * $\alpha$: typically between $0.01$ and $0.3$ (i.e. we accept a decrease in $f$ between $1\%$ and $30\%$ of the prediction based on linear extrapolation)
  * $\beta$: often between $0.1$ (very crude search) and $0.8$ (less crude search)

**Practical efficiency**: 
* Exact line search: exact line search works a little bit better than ordinary line search , but sometimes, in many cases, it doesn't work any better at all, independent of the chosen value of $\beta$