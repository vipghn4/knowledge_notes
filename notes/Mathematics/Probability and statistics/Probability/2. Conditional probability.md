<!-- TOC titleSize:1 tabSpaces:2 depthFrom:1 depthTo:6 withLinks:1 updateOnSave:1 orderedList:0 skip:0 title:1 charForUnorderedList:* -->
# Table of Contents
- [Table of Contents](#table-of-contents)
- [The definition of conditional probability](#the-definition-of-conditional-probability)
  - [Conditional probability](#conditional-probability)
  - [Partition](#partition)
- [Independent events](#independent-events)
  - [Independent events](#independent-events-1)
  - [Conditionally independent events](#conditionally-independent-events)
- [Bayes' theorem](#bayes-theorem)
  - [Bayes' theorem](#bayes-theorem-1)
  - [Prior and posterior probabilities](#prior-and-posterior-probabilities)
- [Appendix](#appendix)
<!-- /TOC -->

# The definition of conditional probability
## Conditional probability
**Conditional probability**:
* Assumptions:
    * $A$ and $B$ are two events
* Conclusion:
    * Conditional probability of $A$ given $B$: $P(A|B) = \frac{P(A \cap B)}{P(B)}$
* Frequency interpretation: imagine when the same experiment is repeated for an infinitely larger number of times
* Behaviors of conditional probabilities: the same as probabilities

**Multiplication rule for conditional probabilities**:
* If $P(B) > 0$, $P(A \cap B) = P(B) P(A|B)$ 
* If $P(A) > 0$, $P(A \cap B) = P(A) P(B|A)$

## Partition
**Partition**:
* Assumptions:
    * $S$ is the sample space
    * $\{B_i\}_{i=1}^k$ are $k$ events where $\bigcup_{i=1}^k B_i = S$
* Conclusion:
    * $\{B_i\}_{i=1}^k$ is a partition of $S$
    * Law of total probability: $P(A) = \sum_j P(B_j) P(A|B_j)$ for any event $A$

**Augmented experiment**: if desired, any experiment can be augmented to include the potential or hypothetical observation of as much additional information as we find useful to help us calculate any desired probabilities
* Example idea: when it isn't clear from the initial description of the experiment that a partition exists

$\hspace{1.0cm} \rightarrow$ We can imagine that the experiment has some additional structure so that a partition exists
* Mixture (probability) model: an example of augmented experiment
    * Task: compute $P(A)$ knowing that possible values of $P(A)$ are $\{A_i\}_i$ based on observations
    * Augmented experiment: 
        * Observation: there is only one event $A$ but there is uncertainty about $P(A)$
        
        $\hspace{1.0cm} \rightarrow$ We believe that $P(A)$ has one of the values $\{A_i\}$
        * Solution: imagine that the experiment consists of 
            * Not only observing $A$ 
            * But also potentially observing enough additional outcomes to be able to compute $P(A)$
    * Procedure:
        * Step 1: make assumption (or guess based on experiments) that $P(P(A) = A_i) = p_i$
        * Step 2: $P(A|B) = \sum_i P(A|A_i) P(A_i)$
    * Example: Gaussian mixture model (GMM)

# Independent events
## Independent events
**Independent events**: $A$ and $B$ are independent if $P(A \cap B) = P(A) P(B)$
* Interpretation: learning $B$ doesn't change $P(A)$
* Corollaries: 
    * $P(A|B) = P(A)$ and $P(B|A) = P(B)$
    * $A$ and $B^c$ are independent

**Mutually independent events**: $\{A_i\}$ are (mutually) independent if $P(\bigcap_{i \in S_A} A_i) = \prod_i P(A_i)$ $\forall S_A \subseteq \{A_i\}$
* Difference from mutual exclusive events:
    * Mutually exclusive: $P(A|B) = 0$ (i.e. disjoint events)
    
    $\hspace{1.0cm} \rightarrow$ Learning that $B$ occurs means $P(A) = 0$
    * Mutually independent: $P(A|B) = P(A)$
    
    $\hspace{1.0cm} \rightarrow$ Learning that $B$ occurs doesn't change $P(A)$

**Determining event dependencies**: try to answer the question "if I were to learn that $B$ occurred, would I change $P(A)$ ?"

## Conditionally independent events
**Conditional independence**: $\{A_i\}$ are conditionally independent given $B$ if $P(\bigcap_{i \in S_A} A_i|B) = \prod_i (A_i|B)$ $\forall S_A \subseteq \{A_i\}$
* Corollaries: $P(A_2|A_1 \cap B) = P(A_2|B)$

**Conditionally independent events in experiment design**:
* Three common cases when assigning probabilities:
    * Case 1: we encounter a sequence of events which we believe that they have the same probability of occurring
    * Case 2: the order in which events are labeled doesn't affect the probabilities which we assign
    * Case 3: if we were to observe the events, we may assign probabilities in a different way
* The use of conditional independent: conditioning information removes an important source of uncertainty from the problem

$\hspace{1.0cm} \rightarrow$ We can partition the sample space according to the condition information

# Bayes' theorem
## Bayes' theorem
**Bayes' theorem**: 
* Assumptions:
    * $\{B_i\}$ form a partition of $S$ with $P(B_i) > 0$ $\forall i$
    * $A$ is an event with $P(A) > 0$
* Conclusion: $P(B_i|A) = \frac{P(B_i) P(A|B_i)}{\sum_j P(B_j) P(A|B_j)}$ $\forall i$

## Prior and posterior probabilities
**Prior probability**: $P(A)$
* Interpretation: probability of $A$ prior to knowing $B$

**Posterior probability**: $P(A|B)$
* Interpretation: probability of $A$ after knowing $B$
* Compute posterior probability in multiple stages: given $P(A|\bigcap_{j < i} B_j)$, compute $P(A|\bigcap_{j \leq i} B_j)$ 
    * Idea: using Bayes' theorem to incorporate $B_i$ into the condition term in $P(A|\bigcap_{j < i} B_j)$

**Convergence of posterior probability**:
* Assumptions:
    * $X_1, ..., X_n$ form a random sample from a distribution indexed by $\theta$
    * $p_i$ is the true distribution of $X_i$ given $\theta$
    * $\hat{p}_i$ is the assumed distribution of $X_i$ given $\theta$
* Observations: 
    * $\log P(\theta|X^n) = \log P(\theta) + \sum_i \log P(X_i|\theta)$

    $\hspace{2.8cm} \approx \log P(\theta) + n \sum_i p_i \log \hat{p}_i$

    $\hspace{2.8cm} \to n \cdot \text{Cross-entropy}(p, \hat{p})$
* Conclusion: the logarithm of the posterior of $\theta$, given $\{X_i\}_{i=1}^\infty$, converges to $n \cdot \text{Cross-entropy}(p, \hat{p})$
* Consequence: if the parametric model isn't very good, the posterior cannot be good, even after observing infinitely many data

---

# Appendix
* $P(\bigcap_i A_i) = \prod_i P(A_i|\bigcap_{j < i} A_j)$
* $P(\bigcap_i A_i|B) = \prod_i P(A_i|\bigcap_{j < i} A_j \cap B)$
* $P(A|C) = \sum_i P(B_i|C) P(A|B_i \cap C)$
* Conditional Bayes's theorem: $P(B_i|A \cap C) = \frac{P(B_i|C) P(A|B_i \cap C)}{\sum_j P(B_j|C) P(A|B_j \cap C)}$ $\forall i$