<!-- TOC titleSize:1 tabSpaces:2 depthFrom:1 depthTo:6 withLinks:1 updateOnSave:1 orderedList:0 skip:0 title:1 charForUnorderedList:* -->
# Table of Contents
- [Table of Contents](#table-of-contents)
- [Large random samples](#large-random-samples)
  - [Introduction](#introduction)
  - [The law of large numbers](#the-law-of-large-numbers)
    - [Inequalities](#inequalities)
    - [Types of convergence](#types-of-convergence)
    - [Properties of sample mean](#properties-of-sample-mean)
    - [The law of large numbers](#the-law-of-large-numbers-1)
  - [Central limit theorem](#central-limit-theorem)
    - [Central limit theorem](#central-limit-theorem-1)
    - [Proof of CLT](#proof-of-clt)
    - [Analysis of CLT](#analysis-of-clt)
    - [Delta method](#delta-method)
  - [The correction for continuity](#the-correction-for-continuity)
    - [Approximating a discrete distribution by a continuous distribution](#approximating-a-discrete-distribution-by-a-continuous-distribution)
    - [Approximating a bar chart](#approximating-a-bar-chart)
- [Appendix](#appendix)
  - [Concepts](#concepts)
<!-- /TOC -->

# Large random samples
## Introduction
**Brief**. This chapter introduces a number of approximation results, which simplify the analysis of large random samples
* *Basic question*. What can we say about the limiting behavior of a sequence of random variables $X_1,\dots,X_n$
* *Usage*. Extremely important for statistical inference

## The law of large numbers
### Inequalities
**Markov inequality**. If $P(X \geq 0) = 1$ then 

$$\forall t > 0, P(X \geq t) \leq \frac{E(X)}{t}$$

* *Explain*.

    $$\begin{aligned}
    E(X) &= \int_\mathbb{R} x f(x) dx\\
    &= \int_{-\infty}^t x f(x) dx + \int_t^\infty x f(x) dx\\
    &\leq \int_{-\infty}^t 0 f(x) dx + \int_t^\infty t f(x) dx\\\\
    &\leq t P(X\geq t)
    \end{aligned}$$

* *Purpose*. Give an upper bound for $P(X \geq t)$, for all $t\geq0$ using $E(X)$, when $t$ is large, i.e. at least greater than $E(X)$
* *Usage*. We can replace $X$ by some statistic
* *Extended version*. We can apply some tricks, e.g. use $P(X \geq t) \leq \frac{E[g(X)]}{g(t)}$ where $g(\cdot)$ is monotonically increasing, to obtain a tighter bound
    
    $$P(|X| \geq a) \leq \frac{E(\phi(|X|))}{\phi(a)}$$
    
    where $\phi(\cdot)$ is a monotically increasing non-negative function

**Chebyshev inequality**. 

$$\forall t > 0,P(|X - E(X)| \geq t) \leq \frac{\text{Var }(X)}{t^2}$$

* *Explain*. Apply Markov inequality to $[X - E(X)]^2$
* *Interpretation*. The variance of a random variable is a measure of how its distribution is the probability that $X$ is far from its mean

    $\to$ This probability is bounded by a quantity which increases as $\text{Var }(X)$ increases
* *Usage*. A valuable theoretical tool to prove the law of large numbers due to its universality
    * *Universality*. Chebyshev inequality can be applied to every distribution

**Chernoff bounds**. The idea of Chebyshev inequality can be generalized to other functions

$\to$ This leads to a sharper bound on the probability in the tail of a distribution when the bound applies
* *Assumptions*. $X$ is a random variable with m.g.f $\psi$
* *Conclusion*. 
    
    $$\forall t \in \mathbb{R}, P(X \geq t) \leq \min_{s > 0} \frac{\psi(s)}{e^{st}}$$

* *Explain*. Apply Markov inequality to $e^{s^* X}$ where 
    
    $$s^* = \arg \min_{s > 0} \frac{\psi(s)}{e^{st}}$$

* *Advantages*.
    * It is easy to deal with sums of independent random variables
    * The bound is exponentially decreasing in $t$, hence much stronger than Chebyshev bounds
    * The bound holds for all $s$ simultaneously
* *Usage*. When $X$ is the sum of $n$ i.i.d random variables and $t = n \mu$ for a large $n$ and some fixed $\mu$
    * Used to prove Hoeffdings inequality
    * Used to prove Bernstein inequality

**Bernstein inequalities**. Give bounds on the probability that the sum of random variables deviates from its mean
* *Simplest version*.
    * *Assumptions*.
        * $X_1,\dots,X_n$ are i.i.d Rademacher random variables 
        * $\bar{X}=\frac{1}{n} \sum_{i=1}^n X_i$ is the sample mean
    * *Rademacher random variables*. Random variables taking values $+1$ and $-1$ with probability $1/2$
    * *Conclusion*.

        $$\forall \varepsilon>0,P(|\bar{X}| > \epsilon) \leq 2 \exp \frac{-n\varepsilon^2}{2 (1+\frac{\varepsilon}{3})}$$

* *General version*.
    * *Assumptions*.
        * $X_1,\dots,X_n$ are i.i.d zero-mean random variables
        * $|X_i|\leq M$ almost surely, for all $i$
    * *Conclusion*.

        $$\forall t>0,P(\sum_{i=1}^n X_i\geq t)\leq \exp \frac{-\frac{1}{2} t^2}{\sum_{i=1}^n E(X_i^2) + \frac{1}{3} Mt}$$

**Hoeffdings inequality**.
* *Hoeffding's lemma*.
    * *Assumptions*. $X$ is a random variable where $a\leq X\leq b$ and $E(X) = 0$
    * *Conclusion*.

        $$\forall s>0,E(e^{sX})\leq e^\frac{s^2 (b-a)^2}{8}$$

* *Hoeffding's inequality*.
    * *Assumptions*. $X_1,\dots,X_n$ are independent random variables with $a_i\leq X_i\leq b_i$
    * *Conclusion*.

        $$P\bigg(\frac{1}{m}\sum_{i=1}^m X_i - \frac{1}{m}\sum_{i=1}^m E(X_i)\geq \varepsilon\bigg)\leq \exp \frac{-2\varepsilon^2 m^2}{\sum_{i=1}^m (b_i - a_i)^2}$$

* *Usage*. 
    * Simply produce a confidence interval for a binomial parameter $p$
    * Provide an upper bound on the probability that the sum of bounded independent random variables deviates from its expected value by more than a certain amount

**McDiarmid's inequality**. A useful generalization of Hoeffding's inequality for learning theory and other domains
* *Assumptions*.
    * $X_1,\dots,X_m$ are $m$ independent random variables taking values from some set $A$
    * $f:A^m\to\mathbb{R}$ satisfies the following boundedness condition

        $$\sup_{x_1,\dots,x_m,\hat{x}_i} |f(x_1,\dots,x_i,\dots,x_m) - f(x_1,\dots,\hat{x}_i,\dots,x_m)|\leq c_i$$

        for all $i\in\{1,\dots,m\}$
* *Conclusion*. For any $\epsilon>0$, we have

    $$P[f(X_1,\dots,X_m) - E[f(X_1,\dots,X_m)] \geq \epsilon] \leq \exp \frac{-2\epsilon^2}{\sum_{i=1}^m c_i^2}$$

* *Reference*. https://stanford.edu/~jduchi/projects/probability_bounds.pdf

**Other inequalities**.
* *Cauchy-Schwarz inequality*. $E(|X Y|) \leq \sqrt{E(X^2) E(Y^2)}$ where $X, Y$ have finite variances
* *Jensen inequality*. $E[g(X)] \geq g[E(X)]$ where $g$ is a convex function

### Types of convergence
**Assumptions**. $\{Z_i\}$ is a sequence of random variables

**Convergence in quadratic mean**. If $\lim_{n \to \infty} E[(Z_n - Z)^2] = 0$

$\to$ $\{Z_i\}$ converges to $Z$ in quadratic mean

**Convergence in probability**. 
* *Weak convergence*. If $\lim_{n \to \infty} P(|Z_n - Z| < \epsilon) = 1$ $\forall \epsilon > 0$

    $\to$ $\{Z_i\}$ converges to $Z$ in probability
    * *Notation*. $Z_n \overset{p}{\to} Z$
    * *Properties*. If $Z_n \overset{p}{\to} b$ and $g(z)$ is continuous at $z = b$ then $g(Z_n) \overset{p}{\to} g(b)$
        * *Explain*. Due to the definition of continuous function
* *Strong convergence*. If $P(\lim_{n \to \infty} Z_n = Z) = 1$

    $\to$ $\{Z_i\}$ converges to $Z$ with probability $1$
* *Difference from weak convergence*.
    * *Weak convergence*. For any $\delta >0$, there is some number $N_2\in\mathbb{R}$ so that

        $$P(|Z_n - Z|<\varepsilon) > 1 - \delta$$

        whenever $n\geq N_2$

        $\to$ When $n\geq N_2$, there is still some probability that $|Z_n - Z| \geq \varepsilon$, and this probability will get smaller as $n$ grows larger
    * *Strong convergence*. It is certain that, for any $\varepsilon > 0$ of $Z$, there is a number $N_2\in\mathbb{R}$ so that 
        
        $$|Z_n - Z| < \varepsilon$$
        
        whenever $n\geq N_2$
        
        $\to$ When $n\geq N_2$, it is certain that $|Z_n - Z|\geq \varepsilon$
    * *Consequence*. Strong convergence implies weak convergence but the reverse may not hold

**Convergence in distribution**. If $\lim_{n \to \infty} F_n(z) = F^*(z)$ for all $z$ at which $F^*(z)$ is continuous

$\to$ $\{Z_i\}$ converges to $F^*$ in distribution, i.e. $F^*$ is an asymptotic distribution of $Z_n$

**Relationships between convergence**.
* $X_n \to X$ (in quadratic mean) implies $X_n \to X$ (in probability)
    * *Idea*. $X_n$ will be close to $X$ (in value), no matter what value of $X$
    * *Explain*. Use Chebyshev inequality
        * By the definition of convergence in quadratic mean, for any $\delta > 0$
            
            $\to$ There exists some $N_2\in\mathbb{R}$ so that $E[(Z_n - Z)^2] < \delta$ whenever $n\geq N_2$
        * From Chebyshev inequality, we have that

            $$\begin{aligned}
            P(|Z_n-Z|\geq\varepsilon)&\leq \frac{E[(Z_n - Z)^2]}{\varepsilon^2}<\frac{\delta}{\varepsilon^2}
            \end{aligned}$$
        
        * Hence
            
            $$P(|Z_n - Z| < \varepsilon) > 1 - \frac{\delta}{\varepsilon^2}$$
            
            which is the weak convergence 

    >**NOTE**. The reverse isn't true

* $X_n \to X$ (in probability) implies $X_n \to X$ (in distribution)
    * *Idea*. $Z_n$ will be close to $X$ (in probability), no matter what value of $X$
    * *Explain*. 
        * $P(Z_n = x) \approx \lim_{n \to \infty, \epsilon \to 0} P(|Z_n - x| < \epsilon)$

        $\hspace{2.6cm} = \lim_{n \to \infty, \epsilon \to 0} P(|Z_n - X| < \epsilon) P(X = x)$
        
        $\hspace{2.6cm} \to P(X = x)$
        * Integrate through $x$ for both sides of the equation above, we obtain the results

    >**NOTE**. The reverse isn't true

* If $X_n \to X$ (in distribution) and $P(X = c) = 1$ for some $c \in \textbf{R}$, then $X_n \to X$ (in probability)
    * *Idea*. The distribution of $Z_n$ will be approximately the distribution of $X$, but $Z_n$'s value probably doesn't

### Properties of sample mean
**Mean and variance of sample mean**.
* *Sample mean*. $\bar{X}_n = \frac{1}{n} \sum_i X_i$
* *Mean and variance of sample mean*.
    * *Expectation of sample mean*. $E(\bar{X}_n) = E(X)$
    * *Variance of sample mean*. $\text{Var }(\bar{X}_n) = \frac{\text{Var }(X)}{n}$
* *Conclusion*. $\bar{X}_n$ is more likely to be close to $E(X)$ than $X_i$

**Chebyshev inequality for sample mean**. 

$$P(|\bar{X}_n - E(X)| \geq t) \leq \frac{\text{Var }(X)}{n t^2}$$

* *Usage*. Determine the sample size so that $\bar{X}_n$ is sufficiently close to $E(X)$
* *Drawback*. This inequality may results in a much greater sample size than is actually needed so that $\bar{X}_n$ is close to $E(X)$

### The law of large numbers
**Weak law of large numbers**. $\bar{X}_n \overset{p}{\to} E(X)$
* *Explain*. Due to the Chebyshev inequality for $\bar{X}_n$, we have that

    $$\lim_{n\to\infty} P(|\bar{X}_n - E(X)|<\varepsilon) = 1$$

* *Consequence*. We can use $\bar{X}_n$ to estimate $E(X)$ and its functions if $n$ is large

>**NOTE**. The law of large numbers holds for even infinite variance

**Strong law of large numbers**. $P(\lim_{n \to \infty} \bar{X}_n = E(X)) = 1$
* *Explain*. As $n\to\infty$, the empirical distribution of the sample will converge, in distribution, to the true distribution of the data

    $\to$ Hence $\lim_{n\to\infty} \bar{X}_n = E(X)$

**Difference between weak law and strong law**. Similar to difference between weak and strong convergence (in probability)

## Central limit theorem
### Central limit theorem
**Central limit theorem (Lindeberg and Lévy)**.
* *Assumptions*.
    * $\{X_i\}$ form a random sample of size $n$ from a given distribution with mean $\mu$ and finite variance $\sigma$
    * $\Phi(\cdot)$ is the c.d.f of ${\mathcal{N}}(0, 1)$
* *Conclusion*. $\frac{\bar{X}_n - \mu}{\sigma / \sqrt{n}}$ converges in distribution to the standard normal distribution
* *Consequences*.
    * $\bar{X}_n \sim {\mathcal{N}}(\mu, \sigma^2 / n)$ (approximately)
    * $\sum_i X_i \sim {\mathcal{N}}(n \mu, n \sigma^2)$ (approximately)
* *Prove*. Prove convergence in m.g.f or cumulants

**Central limit theorem (Liapounov) for sum of independent random variables**.
* *Assumptions*.
    * $\{X_i\}$ are independent random variables with finite third central moment
    * $\lim_{n \to \infty} \frac{\sum_i E(|X_i - \mu_i|^3)}{(\sum_i \sigma_i^2)^{3/2}} = 0$
    * $Y_n = \frac{\sum_{i=1}^n X_i - \sum_{i=1}^n \mu_i}{(\sum_{i=1}^n \sigma_i^2)^{1/2}}$
* *Conclusion*. $\lim_{n \to \infty} P(Y_n \leq x) = \Phi(x)$ given some fixed $x$
* *Consequence*.
    * $\sum_i X_i \sim {\mathcal{N}}(\sum_i \mu_i, \sum_i \sigma_i^2)$ (approximately)
* *CLT in practice*. The sample third moment is always finite due to the limited amount of data, thus $\sum_i X_i$ can always be approximated by a normal distribution

    $\to$ The question should be whether or not the asymptotic normal distribution provides a good approximation to the actual distribution of $\sum_i X_i$
* *Prove*. Prove convergence in m.g.f or cumulants

**Central limit theorem for unknown-variance case**. 

$$\frac{\bar{X}_n - \mu}{S_n/\sqrt{n}} \to {\mathcal{N}}(0, 1)$$ 

(in distribution) where $S_n^2 = \frac{1}{n-1} \sum_i (X_i - \bar{X}_n)^2$ is the corrected sample variance

**Central limit theorem for multivariate case**.
* *Assumptions*. 
    * $X_1, ..., X_n$ are i.i.d random vectors with mean $\mu$ and covariance $\Sigma$
    * $\bar{X}$ is the sample mean
* *Conclusion*. $\sqrt{n} (\bar{X} - \mu) \to {\mathcal{N}}(0, \Sigma)$ (in probability)

**Multiplicative central limit theorem**.
* *Assumptions*.
    * $X_1,\dots,X_n$ are i.i.d positive random variables
* *Conclusion*. As $n\to\infty$, the geometric, or multiplicative, mean of $X_1,\dots,X_n$ has a log-normal distribution with

    $$\mu=E(\ln X_i),\quad \sigma^2 = \frac{1}{n} \text{Var}(\ln X_i)$$

    assuming $\sigma^2$ is finite

**Gibrat's law**. The extension of multiplicative CLT, where $X_1,\dots,X_n$ are not identically distributed

### Proof of CLT
**Convergence of m.g.f**. M.g.f are important in the study of convergence in distribution, due to the following theorem
* *Assumptions*.
    * $\{X_i\}$ is a sequence of random variables
    * $F_n$ is the c.d.f of $X_n$
    * $\psi_n$ is the m.g.f of $X_n$
    * $X^*$ is another random variable with c.d.f $F^*$ and m.g.f $\psi^*$ 
    * $\psi_n$ and $\psi^*$ are supposed to exist
* *Conclusion*. If $\lim_{n\to\infty} \psi_n(t) = \psi^*(t)$ for all values of $t$ in some interval around $t=0$

    $\to$ $X_1,X_2,\dots$ converges in distribution to $X^*$
* *Explain*. Due to convergence of Laplace transform

**Proof of Lindeberg and Lévy's CLT**.
1. Let $Y_i=\frac{X_i - \mu}{\sigma}$ be i.i.d random variables with zero mean and unit variance
2. Let

    $$Z_n = \frac{\sqrt{n} (\bar{X}_n - \mu)}{\sigma} = \frac{1}{\sqrt{n}} \sum_{i=1}^n Y_i$$

3. We then prove that $Z_n$ converges in distribution to a random variable having the standard normal distribution

    $\to$ This is done by showing that the m.g.f of $Z_n$ converges to the m.g.f of standard normal distribution

### Analysis of CLT
**Accuracy of the normal approximation (Berry-Esseen)**. If $E(|X_1|^3) < \infty$ then 

$$\sup_z |P(Z_n \leq z) - \Phi(z)| \leq \frac{33}{4} \frac{E(|X_1 - \mu|^3)}{\sqrt{n} \sigma^3}$$

**Effect of central limit theorem**. The distribution of the sum of many random variables can be approximately normal
* *Consequence*. The distributions of many random variables studied in physical experiments are approximately normal
    * *Example*. the height of each person is determined by adding the values of individual factors
        
        $\to$ The distribution of people height is approximately normal

### Delta method
**Delta method**. Used to approximate the distribution of $\alpha(X_n)$ where $\{X_i\}$ is a sequence of random variables
* *Assumptions*.
    * $\{Y_i\}$ is a sequence of random variables converging to $\theta$
    * $\{a_i\}$ is a sequence of positive numbers converging to $\infty$ so that:
        * $a_n (Y_n - \theta)$ converges in distribution to $F^*$ for some continuous c.d.f $F^*$
    * $\alpha$ is a function with continuous derivative $\alpha'(\theta) \neq 0$
* *Conclusion*. $a_n [\alpha(Y_n) - \alpha(\theta)] / \alpha'(\theta)$ converges in distribution to $F^*$
* *Explain*. $Y_n - \theta = \frac{\alpha(Y_n) - \alpha(\theta)}{\alpha'(\theta)}$ according to Taylor approximation

**Delta method for average**. $\frac{1}{\sigma / \sqrt{n}} \frac{\alpha(\bar{X}_n) - \alpha(\mu)}{\alpha'(\mu)} \to {\mathcal{N}}(0, 1)$ (in distribution)

**Variance stabilizing transformations**. Transformations which transforms $\bar{X}_n$, with unknown variance, to some $\alpha(\bar{X}_n)$ with known variance
* *Find a variance stabilizing transformation*. Run Delta method in reverse
    * *Idea*. $\alpha(\bar{X}_n) \sim {\mathcal{N}}(\alpha(\mu), \sigma^2 \alpha'(\mu)^2 / n)$
        
        $\to$ We need $\alpha'(\mu) = c / \sigma$ where $c$ is some constant
* *Trick*. If $\sigma^2 = g(\mu)$ then choose $\alpha(\mu) = \int_a^\mu \frac{1}{g(x)^{1/2}} dx$

## The correction for continuity
**Brief**. Describe a standard method for improving the quality of approximating a discrete distribution by a normal distribution

### Approximating a discrete distribution by a continuous distribution
**Idea**. Approximate a discrete distribution by a continuous distribution

**Method**.
* *Assumptions*.
    * $X$ is a discrete random variable with p.f $f(x)$
        * $X$ take only integer values
    * $g(x)$ is a continuous distribution p.d.f, which we want to use to approximate $f(x)$
    * $Y$ is a random variable with p.d.f $g$
* *Conclusion*. If $g$ provides a good approximation to $f$

    $\to$ $P(X \in [a, b]) = \sum_{x = a}^b f(x) \approx P(Y \in [a, b]) = \int_a^b g(x) dx$
* *Drawbacks*. 
    * $P(Y \geq a) = P(Y > a)$ while $P(X \geq a) \neq P(X > a)$
    * $P(Y = x) = 0$ while $P(X = x) > 0$

### Approximating a bar chart
**Idea**. Approximate $f$ by a bar chart

**Method**.
* *Step 1*. For each integer $x$, represent $P(X = x)$ by the area of a rectangle, with base $[x - \frac{1}{2}, x + \frac{1}{2}]$ and height $f(x)$
* *Step 2*. Sketch $g(x)$ so that the sum of bars centered at $a, a+1, ..., b$ is approximated by $P[Y \in (a - 1/2, b + 1/2)] = \int_{a-1/2}^{b+1/2} g(x) dx$

**Correction for continuity**. The adjustment from $\int_a^b g(x) dx$ to $\int_{a-1/2}^{b+1/2} g(x) dx$

# Appendix
## Concepts
**Histogram as an approximation to a p.d.f**.
* *Assumptions*.
    * $\{X_i\}$ is a sequence of i.i.d random variables
    * $c_1$, $c_2$ are two constants where $c_1 < c_2$
    * $Y_i = \begin{cases} 1 & X_i \in [c_1, c_2) \\ 0 & \text{otherwise}\end{cases}$
    * $\bar{Y}_n = \frac{1}{n} \sum_i Y_i$
* *Conclusion*.
    * $\bar{Y}_n$ is the proportion of $\{X_i\}$ in $[c_1, c_2)$
    * $\bar{Y}_n \overset{p}{\to} P[X_1 \in [c_1, c_2)]$
* *Consequence*. If we draw a histogram with the area of the bar over each sub-interval being the proportion of a random sample lying in that sub-interval

    $\to$ The area of each bar converges in probability to the probability that a random variable lies in the sub-interval