<!-- TOC titleSize:1 tabSpaces:2 depthFrom:1 depthTo:6 withLinks:1 updateOnSave:1 orderedList:0 skip:0 title:1 charForUnorderedList:* -->
# Table of Contents
- [Table of Contents](#table-of-contents)
- [Introduction](#introduction)
- [Convergence](#convergence)
- [The law of large numbers](#the-law-of-large-numbers)
  - [Inequalities](#inequalities)
  - [Properties of sample mean](#properties-of-sample-mean)
  - [The law of large numbers](#the-law-of-large-numbers-1)
- [Central limit theorem](#central-limit-theorem)
  - [Central limit theorem](#central-limit-theorem-1)
  - [Conclusion](#conclusion)
  - [Delta method](#delta-method)
- [The correction for continuity](#the-correction-for-continuity)
  - [Approximating a discrete distribution by a continuous distribution](#approximating-a-discrete-distribution-by-a-continuous-distribution)
  - [Approximating a bar chart](#approximating-a-bar-chart)
- [BONUS](#bonus)
<!-- /TOC -->

# Introduction
**About this chapter**: introduce a number of approximation results which simplify the analysis of large random samples
* Basic question: what can we say about the limiting behavior of a sequence of random variables $\{X_i\}$
* Usage: extremely important for statistical inference

# Convergence
**Assumptions**:
* $\{Z_i\}$ is a sequence of random variables

**Convergence in quadratic mean**:
* Definition: if $\lim_{n \to \infty} E[(Z_n - Z)^2] = 0$

$\hspace{1.0cm} \rightarrow$ $\{Z_i\}$ converges to $Z$ in quadratic mean

**Convergence in probability (weak convergence)**: 
* Definition: if $\lim_{n \to \infty} P(|Z_n - Z| < \epsilon) = 1$ $\forall \epsilon > 0$

$\hspace{1.0cm} \rightarrow$ $\{Z_i\}$ converges to $Z$ in probability
* Notation: $Z_n \overset{p}{\to} Z$
* Properties: if $Z_n \overset{p}{\to} b$ and $g(z)$ is continuous at $z = b$ then $g(Z_n) \overset{p}{\to} g(b)$

**Convergence with probability $1$ (strong convergence)**:
* Definition: if $P(\lim_{n \to \infty} Z_n = Z) = 1$

$\hspace{1.0cm} \rightarrow$ $\{Z_i\}$ converges to $Z$ with probability $1$

**Convergence in distribution**:
* Definition: if $\lim_{n \to \infty} F_n(z) = F^*(z)$ for all $z$ at which $F^*(z)$ is continuous

$\hspace{1.0cm} \rightarrow \{Z_i\}$ converges to $F^*$ in distribution
* Asymptotic distribution of $Z_n$: $F^*$

**Relationships between convergence**:
* $X_n \to X$ (in quadratic mean) implies $X_n \to X$ (in probability)
    * Idea: $Z_n$ will be close to $X$ (in value), no matter what value of $X$
    * Explain: use Chebyshev inequality

>**NOTE**: the reverse isn't true

* $X_n \to X$ (in probability) implies $X_n \to X$ (in distribution)
    * Idea: $Z_n$ will be close to $X$ (in probability), no matter what value of $X$
    * Explain: 
        * $P(Z_n = x) \approx \lim_{n \to \infty, \epsilon \to 0} P(|Z_n - x| < \epsilon)$

        $\hspace{2.6cm} = \lim_{n \to \infty, \epsilon \to 0} P(|Z_n - X| < \epsilon) P(X = x)$
        
        $\hspace{2.6cm} \to P(X = x)$
        * Integrate through $x$ for both sides of the equation above, we obtain the results

>**NOTE**: the reverse isn't true

* If $X_n \to X$ (in distribution) and $P(X = c) = 1$ for some $c \in \textbf{R}$, then $X_n \to X$ (in probability)
    * Idea: the distribution of $Z_n$ will be approximately the distribution of $X$, but $Z_n$'s value probably doesn't

# The law of large numbers
## Inequalities
**Markov inequality**: if $P(X \geq 0) = 1$ then $P(X \geq t) \leq \frac{E(X)}{t}$ $\forall t > 0$
* Intuitive explanation: $t P(X \geq t)$ is obtained by removing the contribution of $\{x|x < t\}$ to $E(X)$ and clip $\{x|x \geq t\}$ using $t$ as the upper bound

$\hspace{1.0cm} \rightarrow t P(X \geq t) \leq E(X)$ is obvious
* Meaning: give an upper bound for $P(X \geq t)$ $\forall t > 0$ using $E(X)$
    * Use case: when $t$ is large (at least greater than $E(X)$)
* Usage: we can replace $X$ by some positive random variable (e.g. commonly a distance) and to obtain an upper bound on the probability of high distances

>**NOTE**: we can apply some tricks (e.g. use $P(X \geq t) \leq \frac{E[g(X)]}{g(t)}$ where $g(\cdot)$ is monotonically increasing) to obtain a tighter bound

* Extended version: $P(|X| \geq a) \leq \frac{E(\phi(|X|))}{\phi(a)}$ where $\phi(\cdot)$ is a monotically increasing non-negative function

**Chebyshev inequality**: $P(|X - E(X)| \geq t) \leq \frac{\text{Var }(X)}{t^2}$ $\forall t > 0$
* Explain: apply Markov inequality to $[X - E(X)]^2$
* Interpretation: the variance of a random variable is a measure of how its distribution is

$\hspace{1.0cm} \rightarrow$ The probability that $X$ is far from its mean is bounded by a quantity which increases as $\text{Var }(X)$ increases
* Usage: a valuable theoretical tool to prove the law of large numbers
    * Universality: Chebyshev inequality can be applied to every distribution

**Chernoff bounds**:
* Assumptions:
    * $X$ is a random variable with m.g.f $\psi$
* Conclusion: $P(X \geq t) \leq \min_{s > 0} \frac{\psi(s)}{e^{st}}$ $\forall t \in \textbf{R}$
* Explain: apply Markov inequality to $e^{s^* X}$ where $s^* = \arg \min_{s > 0} \frac{\psi(s)}{e^{st}}$
* Use case: when $X$ is the sum of $n$ i.i.d random variables and $t = n u$ for a large $n$ and some fixed $u$

**Hoeffdings inequality**:
* Theorem 1:
    * Assumptions:
        * $Y_1, ..., Y_n$ are independent observations where $E(Y_i) = 0$ and $Y_i \in [a_i, b_i]$
        * $\epsilon > 0$
    * Conclusion: for any $t > 0$, $P(\sum_i Y_i \geq \epsilon) \leq e^{-t \epsilon} \prod_i e^{t^2 (b_i - a_i)^2/8}$
* Theorem 2:
    * Assumptions:
        * $X_1, ..., X_n \sim \text{Bernoulli}(p)$
    * Conclusion: for any $\epsilon > 0$, $P(|\bar{X}_n - p| > \epsilon) \leq 2 e^{- 2 n \epsilon^2}$
* Usage: simply produce a confidence interval for a binomial parameter $p$

**Cauchy-Schwarz inequality**: $E(|X Y|) \leq \sqrt{E(X^2) E(Y^2)}$ where $X, Y$ have finite variances

**Jensen inequality**: $E[g(X)] \geq g[E(X)]$ where $g$ is a convex function

## Properties of sample mean
**Mean and variance of sample mean**:
* Sample mean: $\bar{X}_n = \frac{1}{n} \sum_i X_i$
* Mean and variance of sample mean:
    * Expectation of sample mean: $E(\bar{X}_n) = E(X)$
    * Variance of sample mean: $\text{Var }(\bar{X}_n) = \frac{\text{Var }(X)}{n}$
* Conclusion: $\bar{X}_n$ is more likely to be close to $E(X)$ than $X_i$

**Chebyshev inequality for sample mean**: $P(|\bar{X}_n - E(X)| \geq t) \leq \frac{\text{Var }(X)}{n t^2}$
* Usage: determine the sample size so that $\bar{X}_n$ is sufficiently close to $E(X)$
* Drawback: this inequality may results in a much greater sample size than is actually needed so that $\bar{X}_n$ is close to $E(X)$

## The law of large numbers
**(Weak) law of large numbers**: $\bar{X}_n \overset{p}{\to} E(X)$
* Conclusion: we can use $\bar{X}_n$ to estimate $E(X)$ and its functions if $n$ is large

>**NOTE**: the law of large numbers holds for even infinite variance

**Strong law of large numbers**: $P(\lim_{n \to \infty} \bar{X}_n = E(X)) = 1$

**Difference between weak law and strong law**:
* Weak law: as $n$ approaches infinity, the probability that $X_n$ won't be far from $E(X)$ converges to $1$
    * Explain: there are still a very small probability that $\bar{X}_n$ can be far from $E(X)$ as $n$ approaches infinity
* Strong law: as $n$ approaches infinity, $\bar{X}_n$ will eventually converge to $E(X)$
    * Explain: we can make sure that $\bar{X}_n$ will converge to $E(X)$ as $n$ approaches infinity no matter what

# Central limit theorem
## Central limit theorem
**Central limit theorem (Lindeberg and LÃ©vy)**:
* Assumptions:
    * $\{X_i\}$ form a random sample of size $n$ from a given distribution with mean $\mu$ and finite variance $\sigma$
    * $\Phi(\cdot)$ is the c.d.f of ${\cal{N}}(0, 1)$
* Conclusion: $\frac{\bar{X}_n - \mu}{\sigma / \sqrt{n}}$ converges in distribution to the standard normal distribution
* Consequences:
    * $\bar{X}_n \sim {\cal{N}}(\mu, \sigma^2 / n)$ (approximately)
    * $\sum_i X_i \sim {\cal{N}}(n \mu, n \sigma^2)$ (approximately)
* Prove: prove convergence in m.g.f or cumulants

**Central limit theorem (Liapounov) for sum of independent random variables**:
* Assumptions:
    * $\{X_i\}$ are independent random variables with finite third central moment
    * $\lim_{n \to \infty} \frac{\sum_i E(|X_i - \mu_i|^3)}{(\sum_i \sigma_i^2)^{3/2}} = 0$
    * $Y_n = \frac{\sum_{i=1}^n X_i - \sum_{i=1}^n \mu_i}{(\sum_{i=1}^n \sigma_i^2)^{1/2}}$
* Conclusion: $\lim_{n \to \infty} P(Y_n \leq x) = \Phi(x)$ given some fixed $x$
* Consequence:
    * $\sum_i X_i \sim {\cal{N}}(\sum_i \mu_i, \sum_i \sigma_i^2)$ (approximately)
* CLT in practice: the sample third moment is always finite due to the limited amount of data, thus $\sum_i X_i$ can always be approximated by a normal distribution

$\hspace{1.0cm} \rightarrow$ The question should be whether or not the asymptotic normal distribution provides a good approximation to the actual distribution of $\sum_i X_i$
* Prove: prove convergence in m.g.f or cumulants

**Central limit theorem for unknown-variance case**: $\frac{\bar{X}_n - \mu}{S_n/\sqrt{n}} \to {\cal{N}}(0, 1)$ (in distribution) where $S_n^2 = \frac{1}{n-1} \sum_i (X_i - \bar{X}_n)^2$ is the corrected sample variance

**Central limit theorem for multivariate case**:
* Assumptions: 
    * $X_1, ..., X_n$ are i.i.d random vectors with mean $\mu$ and covariance $\Sigma$
    * $\bar{X}$ is the sample mean
* Conclusion: $\sqrt{n} (\bar{X} - \mu) \to {\cal{N}}(0, \Sigma)$ (in probability)

## Conclusion
**Accuracy of the normal approximation (Berry-Esseen)**: if $E(|X_1|^3) < \infty$ then $\sup_z |P(Z_n \leq z) - \Phi(z)| \leq \frac{33}{4} \frac{E(|X_1 - \mu|^3)}{\sqrt{n} \sigma^3}$

**Effect of central limit theorem**: the distribution of the sum of many random variables can be approximately normal
* Consequence: the distributions of many random variables studied in physical experiments are approximately normal
    * Example: the height of each person is determined by adding the values of individual factors
    
    $\hspace{1.0cm} \rightarrow$ The distribution of people height is approximately normal

## Delta method
**Delta method**: used to approximate the distribution of $\alpha(X_n)$ where $\{X_i\}$ is a sequence of random variables
* Assumptions:
    * $\{Y_i\}$ is a sequence of random variables converging to $\theta$
    * $\{a_i\}$ is a sequence of positive numbers converging to $\infty$ so that:
        * $a_n (Y_n - \theta)$ converges in distribution to $F^*$ for some continuous c.d.f $F^*$
    * $\alpha$ is a function with continuous derivative $\alpha'(\theta) \neq 0$
* Conclusion: $a_n [\alpha(Y_n) - \alpha(\theta)] / \alpha'(\theta)$ converges in distribution to $F^*$
* Explain: $Y_n - \theta = \frac{\alpha(Y_n) - \alpha(\theta)}{\alpha'(\theta)}$ according to Taylor approximation

**Delta method for average**: $\frac{1}{\sigma / \sqrt{n}} \frac{\alpha(\bar{X}_n) - \alpha(\mu)}{\alpha'(\mu)} \to {\cal{N}}(0, 1)$ (in distribution)

**Variance stabilizing transformations**: transformations which transforms $\bar{X}_n$, with unknown variance, to some $\alpha(\bar{X}_n)$ with known variance
* Find a variance stabilizing transformation: run Delta method in reverse
    * Idea: $\alpha(\bar{X}_n) \sim {\cal{N}}(\alpha(\mu), \sigma^2 \alpha'(\mu)^2 / n)$
    
    $\hspace{1.0cm} \rightarrow$ We need $\alpha'(\mu) = c / \sigma$ where $c$ is some constant
* Trick: if $\sigma^2 = g(\mu)$ then choose $\alpha(\mu) = \int_a^\mu \frac{1}{g(x)^{1/2}} dx$

# The correction for continuity
**Introduction**: describe a standard method for improving the quality of approximating a discrete distribution by a normal distribution

## Approximating a discrete distribution by a continuous distribution
**Idea**: approximate a discrete distribution by a continuous distribution

**Method**:
* Assumptions:
    * $X$ is a discrete random variable with p.f $f(x)$
        * $X$ take only integer values
    * $g(x)$ is a continuous distribution p.d.f, which we want to use to approximate $f(x)$
    * $Y$ is a random variable with p.d.f $g$
* Conclusion: if $g$ provides a good approximation to $f$

$\hspace{1.0cm} \rightarrow P(X \in [a, b]) = \sum_{x = a}^b f(x) \approx P(Y \in [a, b]) = \int_a^b g(x) dx$
* Drawbacks: 
    * $P(Y \geq a) = P(Y > a)$ while $P(X \geq a) \neq P(X > a)$
    * $P(Y = x) = 0$ while $P(X = x) > 0$

## Approximating a bar chart
**Idea**: approximate $f$ by a bar chart

**Method**: 
* Step 1: for each integer $x$, represent $P(X = x)$ by the area of a rectangle, with base $[x - \frac{1}{2}, x + \frac{1}{2}]$ and height $f(x)$
* Step 2: sketch $g(x)$ so that the sum of bars centered at $a, a+1, ..., b$ is approximated by $P[Y \in (a - 1/2, b + 1/2)] = \int_{a-1/2}^{b+1/2} g(x) dx$

**Correction for continuity**: the adjustment from $\int_a^b g(x) dx$ to $\int_{a-1/2}^{b+1/2} g(x) dx$

---

# BONUS
* Histogram as an approximation to a p.d.f:
    * Assumptions:
        * $\{X_i\}$ is a sequence of i.i.d random variables
        * $c_1$, $c_2$ are two constants where $c_1 < c_2$
        * $Y_i = \begin{cases} 1 & X_i \in [c_1, c_2) \\ 0 & \text{otherwise}\end{cases}$
        * $\bar{Y}_n = \frac{1}{n} \sum_i Y_i$
    * Conclusion: 
        * $\bar{Y}_n$ is the proportion of $\{X_i\}$ in $[c_1, c_2)$
        * $\bar{Y}_n \overset{p}{\to} P[X_1 \in [c_1, c_2)]$
    * Consequence: if we draw a histogram with the area of the bar over each sub-interval being the proportion of a random sample lying in that sub-interval
    
    $\hspace{1.0cm} \rightarrow$ The area of each bar converges in probability to the probability that a random variable lies in the sub-interval