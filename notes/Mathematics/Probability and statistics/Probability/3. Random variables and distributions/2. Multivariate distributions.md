<!-- TOC titleSize:1 tabSpaces:2 depthFrom:1 depthTo:6 withLinks:1 updateOnSave:1 orderedList:0 skip:0 title:1 charForUnorderedList:* -->
# Table of Contents
- [Table of Contents](#table-of-contents)
- [Bivariate distributions](#bivariate-distributions)
  - [Discrete joint distributions](#discrete-joint-distributions)
  - [Continuous joint distribution](#continuous-joint-distribution)
  - [Mixed bivariate distributions](#mixed-bivariate-distributions)
  - [Bivariate cumulative distribution functions](#bivariate-cumulative-distribution-functions)
- [Marginal distributions](#marginal-distributions)
  - [Marginal p.f or marginal p.d.f](#marginal-pf-or-marginal-pdf)
  - [Independent random variables](#independent-random-variables)
- [Conditional distributions](#conditional-distributions)
  - [Conditional distributions](#conditional-distributions-1)
  - [Construction of the joint distribution](#construction-of-the-joint-distribution)
- [Multivariate distributions](#multivariate-distributions)
  - [Joint distribution](#joint-distribution)
  - [Marginal distributions](#marginal-distributions-1)
  - [Independent random variables](#independent-random-variables-1)
  - [Conditional distributions](#conditional-distributions-2)
<!-- /TOC -->

<!--Multivariate distributions-->
# Bivariate distributions
**Joint / bivariate distribution**: $\{P[(X, Y) \in C]\}_{\{(X, Y) \in C\} \text{ is an event}}$

## Discrete joint distributions
**Discrete joint distributions**:
* Assumptions:
    * $X, Y$ are random variables
    * $(X, Y)$ is an ordered pair taking finitely or at most countably many different values $(x, y)$
* Conclusion:
    * $X$ and $Y$ have a discrete joint distribution

**Joint probability function (p.d.)**: $f(x, y) = P(X = x, Y = y)$
* Properties: 
    * $\sum_{(x, y)} f(x, y) = 1$
    * $P[(X, Y) \in C] = \sum_{(x, y) \in C} f(x, y)$

## Continuous joint distribution
**Continuous joint distributions**:
* Assumptions:
    * $X, Y$ are random variables
    * There exists a non-negative function $f$ with $\textbf{dom } f = \textbf{R}^2$ so that 
        * $P[(X, Y) \in C] = \iint_C f(x, y) dx dy$ for any $C \subseteq \textbf{R}^2$
* Conclusion:
    * $X, Y$ have a continuous joint distribution

**Joint p.d.f of $X$ and $Y$**: $f(x, y)$
* Properties: $\iint_{\textbf{R}^2} f(x, y) dx dy = 1$

**Theorem**:
* Every individual point, and every infinite sequence of points, in $\textbf{R}^2$, has probability approximately $0$
* $P(\{(x, y)|y = f(x), x \in (a, b)\}) = 0$ and $P(\{(x, y)|x = f(y), y \in (a, b)\})$ 
    * $f$ is a definite (i.e. not random) continuous function of one real variable
    * Explain: since $f$ is a definite function, $\{(x, f(x))|x \in (a, b)\}$ is a curve in $\textbf{R}^2$

## Mixed bivariate distributions
**Joint p.f / p.d.f**:
* Assumptions:
    * $X$ is a discrete variable
    * $Y$ is a continuous variable
* Conclusion:
    * Joint p.f / p.d.f of $X$ and $Y$: $f(x, y)$ where $P(X \in A, Y \in B) = \int_B \sum_{x \in A} f(x, y) dy$

## Bivariate cumulative distribution functions
**Joint (cumulative) distribution functions (c.d.f)**: the function $F$ satifying $F(x, y) = P(X \leq x, Y \leq y)$
* Formal definition: $F(x, y) = \int_{-\infty}^y \int_{-\infty}^x f(r, s) dr ds$
    * Consequence: $f(x, y) = \frac{\partial^2 F(x, y)}{\partial x \partial y} = \frac{\partial^2 F(x, y)}{\partial y \partial x}$
* Properties: 
    * Given $x$ fixed, $F(x, y)$ is monotone increasing in $y$ 
    * Given $y$ fixed, $F(x, y)$ is monotone increasing in $x$
    * $P(X \in (a, b], Y \in (c, d]) = F(b, d) - F(a, d) - F(b, c) + F(a, c)$

**Theorem**:
* Assumptions:
    * $F_1$ is the c.d.f of $X$
    * $F_2$ is the c.d.f of $Y$
* Conclusion:
    * $F_1(x) = \lim_{y \to \infty} F(x, y)$
    * $F_2(y) = \lim_{x \to \infty} F(x, y)$

# Marginal distributions
## Marginal p.f or marginal p.d.f
**Marginal c.d.f / p.f / p.d.f**:
* Assumptions:
    * $X$ and $Y$ have a joint distribution
* Conclusion:
    * The marginal c.d.f (or p.f or p.d.f) of $X$: the c.d.f (or p.f or p.d.f) of $X$ derived from $f(x, y)$ and $F(x, y)$

**Formulas**:
* Discrete case: 
    * $f_1(x) = \sum_y f(x, y)$ 
    * $f_2(y) = \sum_x f(x, y)$
* Concrete case: 
    * $f_1(x) = \int_{-\infty}^\infty f(x, y) dy$
    * $f_2(y) = \int_{-\infty}^\infty f(x, y) dx$
* Mixed case:
    * $f_1(x) = \int_{-\infty}^\infty f(x, y) dy$
    * $f_2(y) = \sum_x f(x, y)$

## Independent random variables
**Independent random variables**: 
* Definition: $X$ and $Y$ are independent if $P(X \in A, Y \in B) = P(X \in A) P(Y \in B)$
* Interpretation in terms of probability function:
    * Assumptions: 
        * $F$ is the joint c.d.f of $X$ and $Y$
        * $F_1$ is the marginal c.d.f of $X$
        * $F_2$ is the marginal c.d.f of $Y$
    * Conclusion:
        * $X$ and $Y$ are independent if and only if $F(x, y) = F_1(x) F_2(y)$

**Theorem**: 
* Assumptions:
    * $f$ is the joint p.f, p.d.f, or p.f / p.d.f of $X$ and $Y$
* Conclusion:
    * $X$ and $Y$ are independent if and only if $f$ can be represented as $f(x, y) = h_1(x) h_2(y)$
        * $h_1, h_2$ are non-negative functions of $x$ and $y$ respectively
* Explain: 
    * $f(x, y) = h_1(x) h_2(y)$ implies $f(x, y) = f_1(x) f_2(y)$
    * Consider the discrete case
    
    $\hspace{1.0cm} \rightarrow \sum_{x, y} f(x, y) = \sum_{x, y} f_1(x) f_2(y) = \sum_x [ f_1(x) \sum_y f_2(y)] = \sum_x f_1(x) \cdot \sum_y f_2(y) = F_1(x) F_2(y)$
* Corollary: $X$ and $Y$ are independent if and only if $f(x, y) = f_1(x) f_2(y)$

# Conditional distributions
## Conditional distributions
**Discrete conditional distributions**
* Conditional p.f of $X$ given $Y$: $g_1(x|y) = \frac{f(x, y)}{f_2(y)}$
* Conditional distribution of $X$ given $Y = y$: $g(\cdot|y)$

**Continuous conditional distributions**:
* Conditional p.d.f of $X$ given $Y = y$: $g_1(x|y) = \frac{f(x, y)}{f_2(y)}$

>**NOTE**: for $y$ where $f_2(y) = 0$, we are free to define $g_1(x|y)$, as long as $g_1(x|y)$ is a p.d.f of $x$

**Conditional p.f / p.d.f of mixed distribution**:
* Conditional p.f of $X$ given $Y$: $g_1(x|y) = \frac{f(x, y)}{f_2(y)}$
* Conditional p.d.f of $Y$ given $X$: $g_2(y|x) = \frac{f(x, y)}{f_1(x)}$

## Construction of the joint distribution
**Multiplication rule for distribution**: $f(x, y) = g_1(x|y) f_2(y) = f_1(x) g_2(y|x)$ where $X$ and $Y$ can be either continuous or discrete

**Law of total probability for random variables**:
* Discrete case of $Y$: $f_1(x) = \sum_y g_1(x|y) f_2(y)$
* Continuous case of $Y$: $f_1(x) = \int g_1(x|y) f_2(y)$

**Bayes' theorem for random variables**: $g_2(y|x) = \frac{g_1(x|y) f_2(y)}{f_1(x)}$

**Independent random variables**: $X$ and $Y$ are independent if $g_1(x|y) = f_1(x)$

# Multivariate distributions
## Joint distribution
**Joint distribution function / c.d.f**: $F(\{x_i\}) = P[\bigcap_i (X_i \leq x_i)]$
* Vector notation: 
    * Random vector: $\textbf{X} = (X_1, ..., X_n)$
    * Distribution of random vector $\textbf{X}$: $F(\textbf{x})$

**Joint discrete distribution (p.f)**: $\{X_i\}$ have a discrete joint distribution if $\{X_i\}$ can have only a finite number or an infinite sequence of different possible values
* Joint p.f of $\{X_i\}$: $f(\{x_i\}) = P[\bigcap_i (X_i = x_i)]$
    * Vector notation: $f(\textbf{x}) = P(\textbf{X} = \textbf{x})$
* Sum of probability: $P(\textbf{X} \in C) = \sum_{\textbf{x} \in C} f(\textbf{x})$

**Joint continuous distribution (p.d.f)**: $\{X_i\}$ have a continuous joint distribution if there is some $f$ satisfying $P[\textbf{X} \in C] = \int_C f(\textbf{x}) d\textbf{x}$
* Joint p.d.f of $\{X_i\}$: $f(\textbf{x})$
    * Deriving $f(\textbf{x})$ from $F(\textbf{x})$: $\textbf{f}(\textbf{x}) = \frac{\partial^n F(\textbf{x})}{\partial x_1 ... \partial x_n}$

**Mixed distribution**: similar to bivariate case

## Marginal distributions
**Marginal distributions**: similar to bivariate case

## Independent random variables
**Independent random variables**: $\{X_i\}$ are independent if $P[\bigcap_i (X_i \in A_i)] = \prod_i P(X_i \in A_i)$
* Consequences: 
    * $F(\textbf{x}) = \prod_i F_i(x_i)$
    * $f(\textbf{x}) = \prod_i f_i(x_i)$

**Random samples / i.i.d / sample size**:
* Random sample: $\{X_i\}$ form a random sample from distribution with p.f / p.d.f $f$ if:
    * $\{X_i\}$ are independent
    * The marginal p.f / p.d.f $f_i(x_i) = f(x_i)$
* Independent and identically distributed (i.i.d): $\{X_i\}$ are i.i.d if they form a random sample
* Sample size: $|\{X_i\}|$

**Trick**: we can treat i.i.d variables equally for convenience when computing a probability

## Conditional distributions
**Conditional distributions**: $g_1(\textbf{y}|\textbf{z}) = \frac{f(\textbf{y}, \textbf{z})}{f_2(\textbf{z})}$
* Consequence: $f(\textbf{y}, \textbf{z}) = g_1(\textbf{y}|\textbf{z}) f_2(\textbf{z})$

>**NOTE**: this formulation allows us to deal with the case when $f_2(\textbf{z}) = 0$

**Notes**:
* Statement for creating conditional versions of results: "... conditional on $W = w$"
* Independence is a special case of conditional independence
    * Explain: "independence" means "independence given a $W = c$" where $W$ is some constant variable (i.e. $P(W = c) = 1$)