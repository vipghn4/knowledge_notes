<!-- TOC titleSize:1 tabSpaces:2 depthFrom:1 depthTo:6 withLinks:1 updateOnSave:1 orderedList:0 skip:0 title:1 charForUnorderedList:* -->
# Table of Contents
- [Table of Contents](#table-of-contents)
- [Functions of a random variable](#functions-of-a-random-variable)
- [Functions of two or more random variables](#functions-of-two-or-more-random-variables)
- [Probability distribution simulation](#probability-distribution-simulation)
  - [Probability integral transformation](#probability-integral-transformation)
  - [Simulation](#simulation)
- [BONUS](#bonus)
<!-- /TOC -->

<!--Functions of random variables-->
# Functions of a random variable
**Functions of a random variable**
* Random variable with a discrete distribution: $g(y) = P[r(X) = y] = \sum_{x:r(x) = y} f(x)$
* Random variable with a continuous distribution: 
    * c.d.f: $G(y) = P[r(X) \leq y] = \int_{x:r(x) \leq y} f(x) dx$
    * p.d.f: $g(y) = \frac{d G(y)}{d y}$

**Direct derivation of the p.d.f for one-to-one and differentiable function of a random variable**:
* Assumptions:
    * $X$ is a random variable with p.d.f $f$
        * $P[X \in (a, b)] = 1$
    * $Y = r(X)$ where $r$ is differentiable and one-to-one for $x \in (a, b)$
* Conclusion:
    * The p.d.f of $Y$: $g(y) = \begin{cases} f[r^{-1}(y)] |\frac{d r^{-1}(y)}{dy}| & y \in (r(a), r(b)) \\ 0 & \text{otherwise} \end{cases}$
* Explain: $\int g(y) dy = \int f[r^{-1}(y)] |\frac{d r^{-1}(y)}{d y}| dy$
    * Formal:
        * If $r$ is increasing, integrating $x$ from $a$ to $b$ means $y$ from $r(a)$ to $r(b)$, and $\frac{d r^{-1}(y)}{d y} > 0$
        * If $r$ is decreasing, integrating $x$ from $a$ to $b$ means $y$ from $r(b)$ to $r(a)$, and $\frac{d r^{-1}(y)}{d y} < 0$
    * Intuition: 
        * Since we are calculating an approximation of the volume by summing over volumes of boxes
        * The base size of each box is $|\frac{d r^{-1}(y)}{d y}| dy$, not $\frac{d r^{-1}(y)}{d y} dy$ (i.e. the moving distance from the left to the right of the box)

# Functions of two or more random variables
**Functions of two or more random variables**
* Random variables with a discrete joint distribution:
    * Assumptions:
        * $\{X_i\}_{i=1}^n$ is a set of discrete random variables whose joint p.f is $f$
        * $\{Y_i\}_{i=1}^m$ is defined as $Y_i = r_i(\textbf{X})$
    * Conclusion:
        * The joint p.f of $\{Y_i\}$: $g(\textbf{y}) = \sum_{\textbf{x}:r(\textbf{x}) = \textbf{y}} f(\textbf{x})$
* Random variables with a continuous joint distribution:
    * Assumptions:
        * $\{X_i\}_{i=1}^n$ is a set of continuous random variables whose joint p.f is $f$
        * $\{Y_i\}_{i=1}^m$ is defined as $Y_i = r_i(\textbf{X})$
    * Conclusion:
        * The joint c.d.f of $\{Y_i\}$: $G(\textbf{y}) = \int_{\textbf{x}:r(\textbf{x}) \preceq \textbf{y}} f(\textbf{x}) d\textbf{x}$
        * The joint p.d.f of $\{Y_i\}$: $g(\textbf{y}) = \frac{\partial^m G(\textbf{y})}{\partial y_1 ... \partial y_m}$

**Direct transformation of a multivariate p.d.f**:
* Assumptions:
    * $\{X_i\}_{i=1}^n$ are continuous random variables with joint p.d.f $f$
        * $P[X \in S] = 1$
    * $\{Y_i\}_{i=1}^n$ is defined as $Y_i = r_i(\textbf{X})$ 
        * $r_i$ is differentiable and one-to-one for $\textbf{x} \in S$
    * $J = \det A$ where $A_{ij} = \frac{\partial r_i^{-1}}{\partial y_j}$
* Conclusion:
    * The joint p.d.f of $\{Y_i\}$: $g(\textbf{y}) = \begin{cases} f[r_1^{-1}(\textbf{y}), ..., r_n^{-1}(\textbf{y})] |J| & y \in r(S) \\ 0 & \text{otherwise} \end{cases}$
* Intuition: similar to the univariate case, $|J|$ is the scale in volume of a hyper-box after applying the mapping $[y_i] = [r_i(\textbf{x})]$ to $[x_i]$

# Probability distribution simulation
## Probability integral transformation
**Theorem**: if $X$ have a continuous c.d.f $F$ and $Y = F(X)$ then $Y$ have the uniform distribution on $[0, 1]$
* Explain: $P(F(X) \leq p) = \begin{cases} 0 & p < 0 \\ p & p \in [0, 1] \\ 1 & p > 1\end{cases}$
* Corollary: if $Y$ have the uniform distribution on $[0, 1]$, and $F$ is a continuous c.d.f with quantile function $F^{-1}$

$\hspace{1.0cm} \rightarrow X = F^{-1}(Y)$ has c.d.f $F$

**Usage of the theorem**: transform an arbitrary continuous random variable $X$ into another random variable $Z$ with any desired continuous distribution
* Step 1: get a continuous random variable $X$ with c.d.f $F$
* Step 2: calculate $Y = F(X)$
* Step 3: calculate $Z = G^{-1}(Y)$ for some desired c.d.f $G$

$\hspace{1.0cm} \rightarrow Z$ has c.d.f $G$

## Simulation
**Pseudo-random numbers**: numbers appear to have some properties which a random sample would have, even when they're generated by deterministic algorithms

**Uniform pseudo-random number generators**: generators which generate pseudo-random numbers which appear to have the uniform distribution on $[0, 1]$
* Properties:
    * The generated numbers form a random sample from the uniform distribution over $[0, 1]$
    * The generated numbers appear to be observations of independent random variables

**Generating pseudo-random numbers having a specified distribution**: use uniform pseudo-random number generator to generate values of a random variable $Y$ having any desired distribution
* Idea: based on probability integral transformation

---

# BONUS
* Linear function:
    * Linear function of a random variable: $g(y) = \frac{1}{|a|} f(\frac{y - b}{a})$ where $Y = a X + b$ is a linear function of random variable $X$
    * Linear function of two random variables: $g(y) = \int_{-\infty}^\infty f(\frac{y - b - a_2 x_2}{a_1}, x_2) \frac{1}{|a_1|} dx_2$ where $Y = a_1 X_1 + a_2 X_2 + b$ is a linear function of $X_1$ and $X_2$
    * Linear function of multiple variables:
        * Assumptions:
            * $\textbf{X} = (X_1, ..., X_n)$ have a continuous joint distribution with p.d.f  $f$
            * $\textbf{Y} = \textbf{A} \textbf{X}$ where $\textbf{A}$ is non-singular
        * Conclusion: the joint p.d.f of $\textbf{Y}$ is $g(\textbf{y}) = \frac{1}{|\det \textbf{A}|} f(\textbf{A}^{-1} \textbf{y})$
* Range of a random sample: the distance from the minimum to the maximum
    * Usage: measure how spread out is a random sample
* Convolution:
    * Assumptions:
        * $X_1$ and $X_2$ are two random variables
        * $Y = X_1 + X_2$
    * Conclusion: the distribution of $Y$ is the convolution of the distributions of $X_1$ and $X_2$
    * Explain: $g(y) = \int_{-\infty}^\infty f_1(y - x_2) f_2(x_2) d x_2$