<!-- TOC titleSize:1 tabSpaces:2 depthFrom:1 depthTo:6 withLinks:1 updateOnSave:1 orderedList:0 skip:0 title:1 charForUnorderedList:* -->
# Table of Contents
- [Table of Contents](#table-of-contents)
- [Univariate distributions](#univariate-distributions)
  - [Random variables and discrete distributions](#random-variables-and-discrete-distributions)
    - [Random variables](#random-variables)
    - [Discrete distributions](#discrete-distributions)
    - [Continuous distributions](#continuous-distributions)
    - [Mixed distributions](#mixed-distributions)
  - [Cumulative distribution function](#cumulative-distribution-function)
    - [Cumulative distribution function](#cumulative-distribution-function-1)
    - [c.d.f of types of distributions](#cdf-of-types-of-distributions)
    - [Quantile function](#quantile-function)
- [Appendix](#appendix)
  - [Concepts](#concepts)
  - [Discussions](#discussions)
<!-- /TOC -->

# Univariate distributions
## Random variables and discrete distributions
### Random variables
**Random variable**. A real-valued function defined on the sample space $S$
* *Random variables*. A mapping $X: \Omega \to \textbf{R}$, which assigns a real number $X(\omega)$ to each outcome $\omega$, on a probability space $(\Omega, \mathcal{A}, P)$
* *Inverse transform*. For each subset $A \subseteq \textbf{R}$, $X^{-1}(A) = \{\omega \in \Omega:X(\omega) \in A\}$
* *Probability of random variables*.
    * $P(X \in A) = P(X^{-1}(A)) = P(\{\omega \in \Omega:X(\omega) \in A\})$
    * $P(X = x) = P(X^{-1}(x)) = P(\{\omega \in \Omega:X(\omega) = x\})$
* *References*.
    * https://www.math.cmu.edu/~ttkocz/teaching/1819/prob-notes.pdf
    * https://www.quora.com/Does-a-random-variable-always-take-on-values-that-are-numbers

**Generalized definition of random variable**. A measurable function $X:\Omega\to E$ from a set of possible outcomes to a measurable space $E$, i.e. state space

>**NOTE**. In some materials, the state space is a measurable space $(S',\mathcal{S}')$

* *Interpretation*. A random variable $X$ maps outcomes $\omega\in\Omega$ into states $E$
* *The probability that $X$ takes on a value in a measurable set $S\subseteq E$*.

    $$P(X\in S)=P(\{\omega\in\Omega:X(\omega)\in S\})$$

    * *Needs for measurability of $X$*. If $X$ is not measurable, then for some $P$-measurable set $S$

        $\to$ The set $\{\omega\in\Omega:X(\omega)\in S\}$ is not $P$-measurable, which is paradoxical
* *Examples*. Elements of $E$ can be random boolean values, categorical values, complex numbers, vectors, matrices, sequences, etc.
* *Usage*. Useful when one is interested in modeling the random variation of non-numerical data structures, e.g. as in machine learning
    * *Examples*. $E$ can be the set of one-hot vectors, or adjacent matrices, etc.

**Simple random variable**. 
* *Assumptions*.
    * $x_1,\dots,x_n\in\mathbb{R}$ are distinct values
    * $A_1,\dots,A_n$ are events, which form a partition of $\Omega$
* *Conclusion*. A random variable $X$ is simple if its image $X(\Omega)$ is a finite set, i.e.

    $$X=\sum_{k=1}^n x_k\mathbf{1}_{A_k}$$

* *Interpretation*. $X$ is a simple function of $\omega$

**Distribution**. $\{P(X \in C)\}_{\{X \in C\} \text{ is an event}}$

### Discrete distributions
**Discrete distribution / random variable**. $X$ is a discrete random variable (i.e. have a discrete distribution) if
* $X$ can take only a finite number $k$ of different values $\{x_i\}_{i=1}^k$
* (or) $X$ can take an infinite sequence of different values $\{x_i\}_{i=1}^\infty$

**Probability function (p.f) $f$ of $X$**. $f(x) = P(X = x)$
* *Other name*. Probability mass function (p.m.f)
* *Support of (the distribution of) $X$*. $\{x|f(x) > 0\}$

**Probability of a subset $C$**. $P(X \in C) = \sum_{x_i \in C} f(x_i)$

### Continuous distributions
**Continuous distribution / random variable**. $X$ is a continuous random variable (i.e. has a continuous distribution) if
* $P(X \in I) = \int_I f(x) dx$ $\forall I$ for some non-negative function $f$ where $I$ is an interval (bounded or unbounded) of real numbers

**Probability density function (p.d.f) $f$ of $X$**. $P(X \in I) = \int_I f(x) dx$
* *The support of (the distribution of) $X$*. $\{x|f(x) > 0\}$

>**NOTE**. For any individual value $c$, $\lim_{h \to 0} \int_{c-h}^{c+h} f(x) dx = 0$

>**NOTE**. p.d.f isn't probability (i.e. $f(x)$ can be unbounded, only the condition $\int_{-\infty}^\infty f(x) dx = 1$ is satisfied by $f$)

**Non-uniqueness of the p.d.f**.
* *Theorem*. If we change the value of the p.d.f $f$ at a finite number of points, or an infinite sequences of points, $\int f(x) dx$ will not be affected
    * *Explain*. Due to the definition of Riemann integral
* *Consequence*. The p.d.f of a random variable is not unique
* *Good practice*. Give only one version of p.d.f for each continuous distribution and refer to that version as the true p.d.f

### Mixed distributions
**Mixed distributions**. Distributions which is a mixture of discrete and continuous distribution

## Cumulative distribution function
### Cumulative distribution function
**(Cumulative) distribution function (c.d.f) of $X$**. $F(x) = P(X \leq x)$ for $x \in (-\infty, \infty)$

**Properties**.
* $F(x)$is non-decreasing as $x$ increases
* $\lim_{x \to -\infty} F(x) = 0$ and $\lim_{x \to \infty} F(x) = 1$
* $F(x) = F(x^+)$ $\forall x$

**Determine probabilities from distribution function**.
* $P(X \in (x_1, x_2]) = F(x_2) - F(x_1)$
* $P(X < x) = F(x^-)$
* $P(X = x) = F(x) - F(x^-)$

### c.d.f of types of distributions
**c.d.f of discrete distribution**. $F(x) = \sum_{x_i \leq x} f(x)$
* *Geometry of $F$*. 
    * If $P(a < X < b) = 0$ then $F(x)$ is constant and horizontal over $x \in (a, b)$
    * If $P(x) > 0$ then $F$ will jump by $P(x)$ at $x$

**c.d.f of continuous distribution**. $F(x) = \int_{-\infty}^x f(x) dx$
* *Consequence*. $\frac{dF(x)}{dx} = f(x)$ at all $x$ where $f$ is continuous

### Quantile function
**Quantile / percentiles**.
* *$p$ quantile (or $100p$ percentile) of $X$*. $F^{-1}(x)$
    * *Interpretation*. The smallest $x$ with $F(x) \geq p$
* *Quantile function of $X$*. $F^{-1}$ with $\textbf{dom } F = (0, 1)$

**Uniqueness**. $F^{-1}$ depends on the distribution only

**Median**.
* *Median*. the $0.5$ quantile of $X$
    * *Special case*. $F(x) = 1/2$ $\forall x \in [x_1, x_2)$
        * *Definition 1*. Any $x \in [x_1, x_2)$ is a median
        * *Definition 2 (most common)*. $\frac{x_1 + x_2}{2}$ is the median
* *Lower quantile*. The $0.25$ quantile of $X$
* *Upper quantile*. The $0.75$ quantile of $X$

# Appendix
## Concepts
**Normalizing constant**. Sometimes, a p.f (or p.d.f) is written as $f(x) = C g(x)$ where $\sum_x g(x) \neq 1$ (or $\int_{-\infty}^\infty g(x) dx \neq 1$)

$\to C$ is the normalizing constant to make $C g(x)$ a p.f (or p.d.f)

## Discussions
**Find p.d.f of functions of continuous randomv variables**.
1. Find $A_y = \{x:r(x) \leq y\}$
2. Find the c.d.f $F_Y(t) = P(Y \leq y) = \int_{A_y} f_X(x) dx$
3. Compute $f_Y(y) = F_Y'(y)$

**State space**. The range of possible values of the random variables in a stochastic process
* *Common state space*. The state space is usually, but not need to be, $\mathbb{R}$
* *References*. 
    * https://www.lkouniv.ac.in/site/writereaddata/siteContent/202003241550010566rajeev_pandey_Stoch_Process.pdf
    * https://math.stackexchange.com/questions/4084202/a-question-regarding-the-state-space-of-a-random-variables