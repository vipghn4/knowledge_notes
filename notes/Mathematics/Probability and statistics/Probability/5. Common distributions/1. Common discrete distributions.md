<!-- TOC titleSize:1 tabSpaces:2 depthFrom:1 depthTo:6 withLinks:1 updateOnSave:1 orderedList:0 skip:0 title:1 charForUnorderedList:* -->
# Table of Contents
- [Table of Contents](#table-of-contents)
- [Bernoulli distributions and binary sampling](#bernoulli-distributions-and-binary-sampling)
  - [Bernoulli distributions](#bernoulli-distributions)
  - [Binomial distributions](#binomial-distributions)
- [Hypergeometric distributions](#hypergeometric-distributions)
- [Poisson distributions](#poisson-distributions)
  - [Derivation](#derivation)
  - [Poisson distributions](#poisson-distributions-1)
- [Negative binomial distributions](#negative-binomial-distributions)
  - [Negative binomial distributions](#negative-binomial-distributions-1)
  - [Geometric distribution](#geometric-distribution)
- [Multinomial distributions](#multinomial-distributions)
  - [Multinomial distributions](#multinomial-distributions-1)
- [Other distributions](#other-distributions)
- [BONUS](#bonus)
<!-- /TOC -->

# Bernoulli distributions and binary sampling
## Bernoulli distributions
**Bernoulli distributions**:
* Descriptions: model the probability of whether an outcome is positive or negative
* Probability function (p.f): $f(x|p) = \begin{cases} p^x (1 - p)^{1 - x} & x \in \{0, 1\} \\ 0 & \text{otherwise} \end{cases}$
* Parameters: $p = P(X = 1)$

## Binomial distributions
**Binomial distributions**:
* Descriptions: model the probability of having $x$ red balls by sampling with replacement from a box with $p n$ red balls and $(1 - p) n$ blue balls
* Probability function (p.f): $f(x|n, p) = \begin{cases} \binom{n}{x} p^x (1 - p)^{n - x} & x \in \{0, ..., n\} \\ 0 & \text{otherwise}\end{cases}$
* Parameters:
    * $n$ is the number of Bernoulli trials
    * $p = P(X = 1)$

**Characteristics**:
* Mean: $E(X) = n p$
* Variance: $\text{Var }(X) = n p (1 - p)$

**Relation to other distributions**:
* From Bernoulli to Binomial: if $\{X_i\}$ are i.i.d and $X_i \sim \text{Bernoulli}(p)$ then $\sum_{i=1}^n X_i \sim \text{Binomial}(n, p)$
* From binomial to binomial: if $\{X_i\}$ are independent and $X_i \sim \text{Binomial}(n_i, p)$ then $\sum_i X_i \sim \text{Binomial}(\sum_i n_i, p)$

# Hypergeometric distributions
**Hypergeometric distributions**:
* Descriptions: model the probability of having $x$ red balls by sampling without replacement from a box with $A$ red balls and $B$ blue balls
* Probability function (p.f): $f(x|A, B, n) = \begin{cases} \frac{\binom{A}{x} \binom{B}{n - x}}{\binom{A + B}{n}} & x \in [\max\{0, n - B\}, \min\{n, A\}) \\ 0 \text{otherwise} \end{cases}$
* Parameters:
    * $A$ is the number of red balls
    * $B$ is the number of blue balls
    * $n \leq A + B$ is the number of sampled balls

**Characteristics**:
* Mean: $E(X) = \frac{n A}{A + B}$
* Variance: $\text{Var }(X) = \frac{n A B}{(A + B)^2} \frac{A + B - n}{A + B - 1}$

**Relationship to other distributions**:
* Assumptions:
    * $Y \sim \text{Binomial}(n, p)$
    * $\lim_{T \to \infty} A_T = \infty$, $\lim_{T \to \infty} B_T = \infty$ and $\lim_{T \to \infty} \frac{A_T}{A_T + B_T} = p$
    * $X_T \sim \text{Hypergeometric}(A_T, B_T, n)$
* Conclusion: $\lim_{T \to \infty} \frac{P(Y = x)}{P(X_T = x)} = 1$ for $x \in \{0, ..., n\}$
* Consequence: we can often model Bernoulli random variables as independent when we imagine selecting some at random without replacement from a huge finite population

# Poisson distributions
## Derivation
**Assumptions**:
* $\lambda = p n$ is the rate at which positive outcomes appear in each unit of time
    * $n$ is the number of observed outcomes within a unit of time, which is very large
    * $p$ is the proportion of positive outcomes among $n$ observed outcomes, which is very small
* $X$ is the random variable indicating the number of positive outcomes at a time unit

**Drawbacks of modeling $P(X)$ using binomial distributions**: the calculation are cumbersome due to the values of $n$ and $p$

**Arriving at the idea of Poisson distribution**:
* Relation between $f(x+1)$ and $f(x)$: consider $f$ as the p.f of Binomial distribution with parameters $n$ and $p$
    * $\frac{f(x + 1)}{f(x)} = \frac{\binom{n}{x + 1} p^{x+1} (1 - p)^{n-x-1}}{\binom{n}{x} p^x (1 - p)^{n-x}} = \frac{x! (n - x)! p}{(x + 1)! (n - x - 1)! (1 - p)} = \frac{(n - x) p}{(x + 1) (1 - p)}$
    * Since $n \to \infty$ and $p \to 0$, $\frac{f(x+1)}{f(x)} \approx \frac{n p}{x + 1}$
* Recursive relation: $f(x + 1) = \frac{\lambda}{x + 1} f(x) = \frac{\lambda^{x+1}}{(x + 1)!} f(0)$
* Compute $f(0)$: $f(0) = e^{-\lambda}$
    * Explain: $f(0) \sum_x \frac{\lambda^x}{x!} = 1$
    
    $\hspace{1.0cm} \rightarrow f(0) = \frac{1}{\sum_x \lambda^x / x!} = \frac{1}{e^\lambda}$

**Poisson distribution p.f**: $f(x|\lambda) = \begin{cases} \frac{e^{-\lambda} \lambda^x}{x!} & x \in \{0, 1, ...\} \\ 0 & \text{otherwise} \end{cases}$

## Poisson distributions
**Poisson distributions**:
* Descriptions: model the probability that there are $X$ positive outcomes (within a unit of time) given a rate $\lambda$ of positive outcomes at every time unit
    * Generalization: Poisson distribution can be used to model occurrences in any $N$-dimensional region which can be subdivided into arbitrarily small pieces
* Probability function (p.f): $f(x|\lambda) = \begin{cases} \frac{e^{-\lambda} \lambda^x}{x!} & x \in \{0, 1, ...\} \\ 0 & \text{otherwise} \end{cases}$
* Parameters: $\lambda > 0$ is the rate at which positive outcomes occur within each time unit

**Characteristics**:
* Mean: $\lambda$
* Variance: $\lambda$
* Moment generating function: $\psi(t) = e^{\lambda (e^t - 1)}$

**Relationship to other distributions**:
* From Poisson to Poisson: if $\{X_i\}$ are independent and $X_i \sim \text{Poisson }(\lambda_i)$ then $\sum_i X_i \sim \text{Poisson }(\sum_i \lambda_i)$
* From Binomial to Poisson:
    * Assumptions:
        * $f(x|n, p)$ is the p.f of the binomial distribution with parameters $n$ and $p$
        * $f(x|\lambda)$ is the p.f of the Poisson distribution with parameter $\lambda$
        * $\{p_n\}$ is a sequence of numbers in $[0, 1]$ so that $\lim_{n \to \infty} n p_n = \lambda$
    * Conclusion: $\lim_{n \to \infty} f(x|n, p_n) = f(x|\lambda)$ $\forall x \in \{0, 1, ...\}$
* From Hypergeometric and Poisson:
    * Assumptions:
        * $Y \sim \text{Poisson}(\lambda)$ where $\lambda > 0$
        * $\lim_{T \to \infty} A_T = \infty$, $\lim_{T \to \infty} B_T = \infty$ and $\lim_{T \to \infty} \frac{A_T}{A_T + B_T} = p$
        * $X_T \sim \text{Hypergeometric}(A_T, B_T, n)$
    * Conclusion: $\lim_{T \to \infty} \frac{P(Y = x)}{P(X_T = x)} = 1$ $\forall x \in \{0, 1, ...\}$

# Negative binomial distributions
## Negative binomial distributions
**Negative binomial distributions**:
* Descriptions: model the number of negative outcomes $X$ which occur before the $r$-th positive outcome
* Probability function (p.f): $f(x|r, p) = \begin{cases} \binom{r + x - 1}{x} p^r (1 - p)^x & x \in \{0, 1, ...\} \\ 0 & \text{otherwise}\end{cases}$
* Parameters: 
    * $r$ is the number of positive outcomes
    * $p = P(X = 1)$

**Characteristics**:
* Mean: $E(X) = \frac{r (1 - p)}{p}$
* Variance: $\text{Var }(X) = \frac{r (1 - p)}{p^2}$
* Moment generating function: $\psi(t) = (\frac{p}{1 - (1 - p) e^t})^r$ $\forall t < \log \frac{1}{1 - p}$

## Geometric distribution
**Geometric distribution**:
* Description: model the number of negative outcomes $X$ which occur before the first positive outcome
* Probability function (p.f): $f(x|p) = \begin{cases} p (1 - p)^x & x \in \{0, 1, ...\} \\ 0 & \text{otherwise}\end{cases}$
* Parameters: $p = P(X = 1)$

**Properties**:
* Memoryless: if $X \sim \text{Geometric}(p)$ and $k \geq 0$ then $P(X = k + t|X \geq k) = P(X = t)$ $\forall t \geq 0$

**Relationship to other distributions**:
* From Geometric to Negative binomial: if $\{X_i\}_{i=1}^r$ are i.i.d and $X_i \sim \text{Geometric}(p)$ then $\sum_i X_i \sim \text{Negative-Binomial}(r, p)$

# Multinomial distributions
## Multinomial distributions
**Multinomial distributions**:
* Description: model the probability of obtaining $x_i$ outcomes of each type $i$ by sampling $n$ outcomes from a population of $k$ different types
* Probability function (p.f): $f(\textbf{x}|n, \textbf{p}) = \begin{cases} \frac{n!}{x_1! ... x_k!} \prod_i p_i^{x_i} & \sum_i x_i = n \\ 0 & \text{otherwise} \end{cases}$
* Parameters: 
    * $p_i = P(X = i)$ where $\sum_i p_i = 1$
    * $n$ is the number of random outcomes

**Characteristics**:
* Means: $E(X_i) = n p_i$
* Variances: $\text{Var }(X_i) = n p_i (1 - p_i)$
* Covariances: $\text{Cov }(X_i, X_j) = - n p_i p_j$

**Relationship to other distributions**:
* From Multinomial to Binomial: if $(X_1, X_2) \sim \text{Multinomial}[n, (p_1, p_2)]$ then $X_1 \sim \text{Binomial}(n, p_1)$ and $X_2 = n - X_1$
    * Corollary 1: if $\textbf{X} \sim \text{Multinomial}(n, \textbf{p})$ then $X_i \sim \text{Binomial}(n, p_i)$
    * Corollary 2: 
        * Assumptions: 
            * $\textbf{X} \sim \text{Multinomial}(n, \textbf{p})$ where $\textbf{X} \in \textbf{N}^k$
            * $\{i_j\} \in \{1, ..., k\}$
        * Conclusion: $\sum_j X_{i_j} \sim \text{Binomial}(n, \sum_j p_{i_j})$

# Other distributions
**Point mass distribution at $a$**: $F(x) = \begin{cases} 0 & x < a \\ 1 & x \geq a\end{cases}$ (i.e. $P(X = a) = 1$)
* Notation: $X \sim \delta_a$

---

# BONUS
* Binomial coefficient: $\binom{m}{r} = \frac{m!}{r! (m - r)!}$ where $r \leq m$
    * Extended binomial coefficient: $\binom{m}{r} = \frac{\prod_{i=0}^{r-1} (m - i)}{r!}$ $\forall m, r \in \textbf{R}$
* Multinomial coefficient: $\binom{n}{x_1, ..., x_k} = \frac{n!}{x_1! ... x_k!}$
* Variance change between sampling methods:
    * Assumptions:
        * $T$ is the total number of balls in a box
        * $p$ is the proportion of red balls in the box
    * Conclusion: there is some reduction in $\text{Var }(X)$ caused by sampling without replacement from a finite population
        * Sampling with replacement: $\text{Var }(X) = n p (1 - p)$
        * Sampling without replacement: $\text{Var }(X) = n p (1 - p) \frac{T - n}{T - 1}$
    * Finite population correction: $\alpha = \frac{T - n}{T - 1} \in [0, 1]$