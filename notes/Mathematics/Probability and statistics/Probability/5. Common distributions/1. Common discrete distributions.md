<!-- TOC titleSize:1 tabSpaces:2 depthFrom:1 depthTo:6 withLinks:1 updateOnSave:1 orderedList:0 skip:0 title:1 charForUnorderedList:* -->
# Table of Contents
- [Table of Contents](#table-of-contents)
- [Common discrete distributions](#common-discrete-distributions)
  - [Bernoulli distributions and binary sampling](#bernoulli-distributions-and-binary-sampling)
    - [Bernoulli distributions](#bernoulli-distributions)
    - [Binomial distributions](#binomial-distributions)
  - [Hypergeometric distributions](#hypergeometric-distributions)
    - [Comparison of sampling methods](#comparison-of-sampling-methods)
    - [Population of unknown composition](#population-of-unknown-composition)
  - [Poisson distributions](#poisson-distributions)
    - [Derivation](#derivation)
    - [Poisson distributions](#poisson-distributions-1)
  - [Negative binomial distributions](#negative-binomial-distributions)
    - [Negative binomial distributions](#negative-binomial-distributions-1)
    - [Geometric distribution](#geometric-distribution)
  - [Multinomial distributions](#multinomial-distributions)
    - [Multinomial distributions](#multinomial-distributions-1)
  - [Other distributions](#other-distributions)
- [Appendix](#appendix)
  - [Concepts](#concepts)
<!-- /TOC -->

# Common discrete distributions
## Bernoulli distributions and binary sampling
### Bernoulli distributions
**Bernoulli distributions**. Model the probability of whether an outcome is positive or negative
* *Probability function (p.f)*. 

    $$f(x|p) = \begin{cases}
    p^x (1 - p)^{1 - x} & x \in \{0, 1\} \\
    0 & \text{otherwise}
    \end{cases}$$

* Parameters: $p = P(X = 1)$

### Binomial distributions
**Binomial distributions**. Model the probability of having $x$ red balls by sampling with replacement from a box with $p n$ red balls and $(1 - p) n$ blue balls
* Probability function (p.f): $f(x|n, p) = \begin{cases} \binom{n}{x} p^x (1 - p)^{n - x} & x \in \{0, ..., n\} \\ 0 & \text{otherwise}\end{cases}$
* Parameters:
    * $n$ is the number of Bernoulli trials
    * $p = P(X = 1)$
* *Characteristics*.
    * *Mean*. $E(X) = n p$
    * *Variance*. $\text{Var }(X) = n p (1 - p)$

**Relation to other distributions**.
* *From Bernoulli to Binomial*: If $\{X_i\}$ are i.i.d and $X_i \sim \text{Bernoulli}(p)$ then 
    
    $$\sum_{i=1}^n X_i \sim \text{Binomial}(n, p)$$

    * *Conditions for the theorem*. Not every sum of Bernoulli random variables has a binomial distribution

        $\to$ There are two conditions for this theorem
        * $X_1,\dots,X_n$ must be mutually independent
        * $X_1,\dots,X_n$ must have the same parameter $p$

* *From binomial to binomial*. If $\{X_i\}$ are independent and $X_i \sim \text{Binomial}(n_i, p)$ then 
    
    $$\sum_i X_i \sim \text{Binomial}(\sum_i n_i, p)$$

**Example**. We can use Bernoulli and Binomial calculations to improve efficiency when data collection is costly
* *Scenario*. Military and other large organizations are often faced with the need to test large numbers of members for rare diseases

    $\to$ Suppose that each test requires a small amount of blood, and it is guaranteed to detect the disease if it is anywhere in the blood
    * *Naive approach*. Perform $n$ tests where $n$ is the number of people
    * *Alternative approach*. Divide $n$ people into $m$ groups of size $n/m$ each
        1. For each group, take a portion of the blood sample from each of the $n/m$ people in the group
            
            $\to$ These samples are combined into one sample
        2. Test each of the $m$ combined samples
              * If none of the $m$ combined samples has the disease
                  
                  $\to$ Nobody has the disease, and we needed only $m$ tests instead of $n$
              * If only one of the combined samples has the disease
                
                $\to$ We can test those $n/m$ people separately, and we needed only $m+n/m$ tests
* *Mathematical modeling*.
    * $X_1,\dots,X_n$ are Bernoulli random variables with parameter $p$, where

        $$X_i=\begin{cases}
        1 & \text{person } i \text{ has disease}\\
        0 & \text{otherwise}
        \end{cases}$$
    
    * $Z_{1,i}$ be the number of people having disease in group $i$

        $\to$ $Z_{1,i}$ has the binomial distribution with parameters $n/m$ and $p$
    * $Y_{1,i} = 1$ if $Z_{1,i} > 0$, otherwise $Y_{1,i} = 0$

        $\to$ $Y_{1,i}$ has the Bernoulli distribution with parameter

        $$p_y = P(Z_{1,i}>0) = 1 - P(Z_{1,i}=0) = 1 - (1 - p)^{n/m}$$

        and they are independent

    * $Y_1=\sum_{i=1}^m Y_{1,i}$ is the number of groups, whose members we need to test individually

        $\to$ $Y_1$ has the binomial distribution with parameters $m$ and $p_y$
* *Analysis*.
    * The number of people we need to test individually is $\frac{n}{m}\cdot Y_1$, where

        $$E(\frac{n}{m} \cdot Y_1) = \frac{n}{m}\cdot m\cdot p_y = p_y n$$

        which is much smaller than $n$ if $p_y$ is small
    * Hence, the maximum number of tests required is given as $m + p_y n$
* *Multiple-stage version*. Each of the groups, which test positive, is split further into subgroups, which are tested together
    
    $\to$ The positive subgroups are then split further, and so on

## Hypergeometric distributions
**Hypergeometric distributions**. Model the probability of having $x$ red balls by sampling without replacement from a box with $A$ red balls and $B$ blue balls
* Pro*bability function (p.f)*.
    
    $$f(x|A, B, n) = \begin{cases} \frac{\binom{A}{x} \binom{B}{n - x}}{\binom{A + B}{n}} & x \in [\max\{0, n - B\}, \min\{n, A\}) \\ 0 & \text{otherwise} \end{cases}$$

* *Parameters*.
    * $A$ is the number of red balls
    * $B$ is the number of blue balls
    * $n \leq A + B$ is the number of sampled balls
* *Characteristics*.
    * *Mean*. $E(X) = \frac{n A}{A + B}$
    * *Variance*. $\text{Var }(X) = \frac{n A B}{(A + B)^2} \frac{A + B - n}{A + B - 1}$

### Comparison of sampling methods
**Finite population correction**. Used in the theory of sampling from finite populations without replacement
* *Assumptions*. $T = A+B$ is the total number of balls in the box
* *Observations*.
    * The variance of a hypergeometric random variable can be written as

        $$\text{Var}(X)=np(1-p)\frac{T-n}{T-1}$$

    * $np(1-p)$ is the variance of the binomial distribution modeling the number of red balls when sampling with replacement
* *Finite population correction*. $\alpha = \frac{T - n}{T - 1}$
    * *Interpretation*. Model the reduction in $\text{Var}(X)$ caused by sampling without replacement from a finite population

**Closeness of binomial and hypergeometric distributions**. For each fixed sample size $n$, it can be seen that $\alpha\to 1$ as $T\to\infty$

$\to$ There is almost no difference from sampling with replacement
* *Assumptions*.
    * $Y \sim \text{Binomial}(n, p)$
    * $\lim_{T \to \infty} A_T = \infty$, $\lim_{T \to \infty} B_T = \infty$
    * $\lim_{T \to \infty} \frac{A_T}{A_T + B_T} = p$
    * $X_T \sim \text{Hypergeometric}(A_T, B_T, n)$
* *Conclusion*. 
    
    $$\forall x \in \{0, ..., n\},\lim_{T \to \infty} \frac{P(Y = x)}{P(X_T = x)} = 1$$

* *Consequence*. We can often model Bernoulli random variables as independent when we imagine selecting some at random without replacement from a huge finite population

### Population of unknown composition
**Brief**. The hypergeometric distribution can arise as a conditional distribution when sampling is done without replacement from a finite population of unknown composition

**Simple scenario**. Suppose that we know $T=A+B$ without knowing $A$ and $B$
* *Mathematical modeling*.
    * $P = A/T$ is a random variable, whose possible values are $\{\frac{k}{T}\}_{k=0}^T$
* *Conclusion*. Conditional on $P=p$, we can behave as if we know that $A=pT$ and $B=(1-p)T$

    $\to$ The conditional distribution of $X$ is the hypergeometric distribution with parameters $pT$ and $(1-p)T$ and $n$
* *Extreme case*. If $T$ is so large, that hypergeometric distribution converges to binomial distribution with parameters $n$ and $p$

    $\to$ It is no longer necessary that we assume that $T$ is known
    * *Consequence*. Conditional on the proportion $A/T$, the individual draws become independent Bernoulli trials

    >**NOTE**. This is the motivation for using the binomial distributions as models for numbers of successes in samples from very large finite populations

## Poisson distributions
### Derivation
**Assumptions**.
* $\lambda = p n$ is the rate at which positive outcomes appear in each unit of time
    * $n$ is the number of observed outcomes within a unit of time, which is very large
    * $p$ is the proportion of positive outcomes among $n$ observed outcomes, which is very small
* $X$ is the random variable indicating the number of positive outcomes at a time unit

**Drawbacks of modeling $P(X)$ using binomial distributions**. The calculation are cumbersome due to the values of $n$ and $p$

**Arriving at the idea of Poisson distribution**.
* *Relation between $f(x+1)$ and $f(x)$*. Consider $f$ as the p.f of Binomial distribution with parameters $n$ and $p$
    
    $$\begin{aligned}
    \frac{f(x + 1)}{f(x)} &= \frac{\binom{n}{x + 1} p^{x+1} (1 - p)^{n-x-1}}{\binom{n}{x} p^x (1 - p)^{n-x}}\\ 
    &= \frac{x! (n - x)! p}{(x + 1)! (n - x - 1)! (1 - p)} \\
    &= \frac{(n - x) p}{(x + 1) (1 - p)}
    \end{aligned}$$

    * *Consequence*. As $n \to \infty$ and $p \to 0$, $\frac{f(x+1)}{f(x)} \approx \frac{n p}{x + 1}$
* *Recursive relation*. 
    
    $$f(x + 1) = \frac{\lambda}{x + 1} f(x) = \frac{\lambda^{x+1}}{(x + 1)!} f(0)$$

* *Compute $f(0)$*. $f(0) = e^{-\lambda}$
    * Explain: Since $f(0) \cdot \sum_x \frac{\lambda^x}{x!} = 1$, we have that $f(0) = \frac{1}{\sum_x \lambda^x / x!} = \frac{1}{e^\lambda}$

**Poisson distribution p.f**. 

$$f(x|\lambda) = \begin{cases}
\frac{e^{-\lambda} \lambda^x}{x!} & x \in \{0, 1, ...\} \\ 0 & \text{otherwise}
\end{cases}$$

### Poisson distributions
**Poisson distributions**. Model the probability that there are $X$ positive outcomes (within a unit of time) given a rate $\lambda$ of positive outcomes at every time unit
* *Generalization*. Poisson distribution can be used to model occurrences in any $N$-dimensional region which can be subdivided into arbitrarily small pieces
* *Probability function (p.f)*.
    
    $$f(x|\lambda) = \begin{cases} \frac{e^{-\lambda} \lambda^x}{x!} & x \in \{0, 1, ...\} \\ 0 & \text{otherwise} \end{cases}$$

* *Parameters*. $\lambda > 0$ is the rate at which positive outcomes occur within each time unit
* *Characteristics*.
    * *Mean*. $\lambda$
    * *Variance*. $\lambda$
    * *Moment generating function*. $\psi(t) = e^{\lambda (e^t - 1)}$

**Relationship to other distributions**.
* *From Poisson to Poisson*. If $\{X_i\}$ are independent and $X_i \sim \text{Poisson }(\lambda_i)$ then 
    
    $$\sum_i X_i \sim \text{Poisson }(\sum_i \lambda_i)$$

* *From Binomial to Poisson*.
    * *Assumptions*.
        * $f(x|n, p)$ is the p.f of the binomial distribution with parameters $n$ and $p$
        * $f(x|\lambda)$ is the p.f of the Poisson distribution with parameter $\lambda$
        * $\{p_n\}$ is a sequence of numbers in $[0, 1]$ so that $\lim_{n \to \infty} n p_n = \lambda$
    * *Conclusion*. 
        
        $$\forall x \in \{0, 1, ...\},\lim_{n \to \infty} f(x|n, p_n) = f(x|\lambda)$$

* *From hypergeometric and Poisson*.
    * *Assumptions*.
        * $Y \sim \text{Poisson}(\lambda)$ where $\lambda > 0$
        * $\lim_{T \to \infty} A_T = \infty$, $\lim_{T \to \infty} B_T = \infty$ and $\lim_{T \to \infty} \frac{A_T}{A_T + B_T} = p$
        * $X_T \sim \text{Hypergeometric}(A_T, B_T, n)$
    * *Conclusion*. 
        
        $$\forall x \in \{0, 1, ...\},\lim_{T \to \infty} \frac{P(Y = x)}{P(X_T = x)} = 1$$

## Negative binomial distributions
### Negative binomial distributions
**Negative binomial distributions**. Model the number of negative outcomes $X$ which occur before the $r$-th positive outcome
* *Probability function (p.f)*. 

    $$f(x|r, p) = \begin{cases} \binom{r + x - 1}{x} p^r (1 - p)^x & x \in \{0, 1, ...\} \\ 0 & \text{otherwise}\end{cases}$$

* *Parameters*. 
    * $r$ is the number of positive outcomes
    * $p = P(X = 1)$
* *Characteristics*.
    * *Mean*. $E(X) = \frac{r (1 - p)}{p}$
    * *Variance*. $\text{Var }(X) = \frac{r (1 - p)}{p^2}$
    * *Moment generating function*. $\psi(t) = (\frac{p}{1 - (1 - p) e^t})^r$ $\forall t < \log \frac{1}{1 - p}$

### Geometric distribution
**Geometric distribution**. Model the number of negative outcomes $X$ which occur before the first positive outcome
* *Probability function (p.f)*. 

    $$f(x|p) = \begin{cases} p (1 - p)^x & x \in \{0, 1, ...\} \\ 0 & \text{otherwise}\end{cases}$$

* *Parameters*. $p = P(X = 1)$

**Memoryless**. If $X \sim \text{Geometric}(p)$ and $k \geq 0$ then 

$$\forall t \geq 0,P(X = k + t|X \geq k) = P(X = t)$$

**Relationship to other distributions**.
* *From Geometric to Negative binomial*. If $\{X_i\}_{i=1}^r$ are i.i.d and $X_i \sim \text{Geometric}(p)$ then 
    
    $$\sum_i X_i \sim \text{Negative-Binomial}(r, p)$$

## Multinomial distributions
### Multinomial distributions
**Multinomial distributions**. Model the probability of obtaining $x_i$ outcomes of each type $i$ by sampling $n$ outcomes from a population of $k$ different types
* *Probability function (p.f)*. 
    
    $$f(\textbf{x}|n, \textbf{p}) = \begin{cases} \frac{n!}{x_1! ... x_k!} \prod_i p_i^{x_i} & \sum_i x_i = n \\ 0 & \text{otherwise} \end{cases}$$

* *Parameters*. 
    * $p_i = P(X = i)$ where $\sum_i p_i = 1$
    * $n$ is the number of random outcomes
* *Characteristics**:
    * *Means*. $E(X_i) = n p_i$
    * *Variances*. $\text{Var }(X_i) = n p_i (1 - p_i)$
    * *Covariances*. $\text{Cov }(X_i, X_j) = - n p_i p_j$

**Relationship to other distributions**:
* *From Multinomial to Binomial*. If $(X_1, X_2) \sim \text{Multinomial}[n, (p_1, p_2)]$ then 

    $$X_1 \sim \text{Binomial}(n, p_1),\quad X_2 = n - X_1$$
    * *Corollary 1*. If $\textbf{X} \sim \text{Multinomial}(n, \textbf{p})$ then $X_i \sim \text{Binomial}(n, p_i)$
    * *Corollary 2*. 
        * *Assumptions*. 
            * $\textbf{X} \sim \text{Multinomial}(n, \textbf{p})$ where $\textbf{X} \in \textbf{N}^k$
            * $\{i_j\} \in \{1, ..., k\}$
        * *Conclusion*. $\sum_j X_{i_j} \sim \text{Binomial}(n, \sum_j p_{i_j})$

## Other distributions
**Point mass distribution at $a$**. $F(x) = \begin{cases} 0 & x < a \\ 1 & x \geq a\end{cases}$ (i.e. $P(X = a) = 1$)
* Notation: $X \sim \delta_a$

# Appendix
## Concepts
**Multinomial coefficients**
* Binomial coefficient: $\binom{m}{r} = \frac{m!}{r! (m - r)!}$ where $r \leq m$
    * Extended binomial coefficient: $\binom{m}{r} = \frac{\prod_{i=0}^{r-1} (m - i)}{r!}$ $\forall m, r \in \textbf{R}$
* Multinomial coefficient: $\binom{n}{x_1, ..., x_k} = \frac{n!}{x_1! ... x_k!}$

**Stirling's fomrula**. Let

$$s_n=\frac{1}{2} \log 2\pi + \bigg(n+\frac{1}{2}\bigg) \log n - n$$

then

$$\lim_{n\to\infty} |s_n - \log n!| = 0$$

or, in other words

$$\lim_{n\to\infty} \frac{\sqrt{2\pi} n^{n+1/2} e^{-n}}{n!} = 1$$
* *Motivation*. For large values of $n$, it is nearly impossible to compute $n!$
    * *Observation*. In most cases, for which $n!$ is required, one only needs the ratio of $n!$ toa nother large number $a_n$

        $\to$ In such cases, we have that

        $$\frac{n!}{a_n} = e^{\log n! - \log a_n}$$
    
    * *Conclusion*. If we have a simple approximation $s_n$ to $\log n!$ as given above

        $\to$ The ratio $n!/a_n$ to $s_n/a_n$ will be close to $1$ for large $n$

**Theorem**. 
* *Assumptions*.
    * $a_n$ is a sequence of real numbers converging to $0$
    * $c_n$ is a sequence of real numbers so that $c_n a_n^2\to 0$
* *Conclusion*.

    $$\lim_{n\to\infty} (1+a_n)^{c_n} e^{-a_n c_n} = 1$$

* *Explain*. The key is to transform exponential function and multiplication into logarithm and addition, i.e. we have to prove

    $$\lim_{n\to\infty} c_n \log (1 + a_n) - a_n c_n = 0$$

    * We have that

        $$\log(1+a_n) = a_n - \frac{a_n^2}{2} + \frac{a_n^3}{3} - \frac{a_n^4}{4} + \dots$$
    
    * Hence

        $$\begin{aligned}
        \lim_{n\to\infty} c_n \log (1 + a_n) - a_n c_n &= \lim_{n\to\infty} c_n a_n - \frac{c_n a_n^2}{2} + \frac{c_n a_n^3}{3} - \frac{c_n a_n^4}{4} + \dots - c_na_n\\
        &= 0
        \end{aligned}$$

* *Other interpretation*. If $a_n c_n \to b$, then $(1+a_n)^{c_n} \to e^b$