<!-- TOC titleSize:1 tabSpaces:2 depthFrom:1 depthTo:6 withLinks:1 updateOnSave:1 orderedList:0 skip:0 title:1 charForUnorderedList:* -->
# Table of Contents
- [Table of Contents](#table-of-contents)
- [Normal distribution](#normal-distribution)
  - [Importance of normal distributions](#importance-of-normal-distributions)
  - [Normal distribution](#normal-distribution-1)
  - [Standard normal distribution](#standard-normal-distribution)
  - [Lognormal distribution](#lognormal-distribution)
  - [History of Gaussian distribution](#history-of-gaussian-distribution)
- [BONUS](#bonus)
- [The Gamma function](#the-gamma-function)
  - [Motivation](#motivation)
  - [The Gamma function](#the-gamma-function-1)
- [The Gamma distributions](#the-gamma-distributions)
- [Exponential distributions](#exponential-distributions)
  - [Derivation from Poisson distribution](#derivation-from-poisson-distribution)
  - [Exponential distributions](#exponential-distributions-1)
- [BONUS](#bonus-1)
- [The Beta function](#the-beta-function)
- [The Beta distribution](#the-beta-distribution)
<!-- /TOC -->

# Normal distribution
## Importance of normal distributions
**Mathematical properties of normal distributions**: if a random sample taken from a normal distribution

$\hspace{1.0cm} \rightarrow$ The distributions of various important functions of the observations in the sample can be derived explicitly and have simple forms

**Popularity in physical experiments**: the random variables studied in various physical experiments often have distributions which are approximately normal

**Central limit theorem**: if a large random sample is taken from some distribution

$\hspace{1.0cm} \rightarrow$ Many important functions of the observations will have distributions which are approximately normal

## Normal distribution
**Normal distribution**:
* Probability density function (p.d.f): $f(x|\mu, \sigma^2) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp[\frac{- (x - \mu)^2}{2 \sigma^2}]$
* Parameters: $\mu = E(X)$ and $\sigma^2 = \text{Var }(X)$

**Characteristics**:
* Mean: $\mu$
* Variance: $\sigma^2$
* Moment generating function: $\psi(t) = \exp(\mu t + \frac{1}{2} \sigma^2 t^2)$

**Properties**: 
* LInear combinations: if $\{X_i\}$ are independent and $X_i \sim {\cal{N}}(\mu_i, \sigma_i^2)$ then $\sum_i a_i X_i \sim {\cal{N}}(\sum_i a_i \mu_i, \sum_i a_i^2 \sigma^2)$

## Standard normal distribution
**Standard normal distribution**: 
* Description: normal distribution with zero mean and unit variance
* Probability density function (p.d.f): $\phi(x) = \frac{1}{\sqrt{2 \pi}} \exp(-\frac{x^2}{2})$
    * Cumulative density function (c.d.f): $\Phi(x) = \int_{-\infty}^x \phi(x) dx$

>**NOTE**: the c.d.f of normal distributions cannot be expressed in closed form

$\hspace{1.0cm} \rightarrow$ We have to use numerical approximations or use a table of values for them

**Properties**:
* Symmetry: $\Phi(-x) = 1 - \Phi(x)$ and $\Phi^{-1}(p) = -\Phi^{-1}(1 - p)$

**Relationship to other distributions**:
* From normal to standard normal: 
    * Assumptions:
        * $X \sim {\cal{N}}(\mu, \sigma^2)$
        * $F$ is the c.d.f of $X$
    * Conclusion:
        * $Z = \frac{X - \mu}{\sigma} \sim {\cal{N}}(0, 1)$
        * $F(x) = \Phi(\frac{x - \mu}{\sigma})$ for all $x$ and $p \in (0, 1)$
        * $F^{-1}(p) = \mu + |sigma \Phi^{-1}(p)$ for all $x$ and $p \in (0, 1)$

## Lognormal distribution
**Lognormal distribution**: $X$ has the lognormal distribution with parameters $\mu$ and $\sigma^2$ if $\log(X) \sim {\cal{N}}(\mu, \sigma^2)$

**Characteristics**:
* Mean: $E(X) = \exp(\mu + \frac{\sigma^2}{2})$
* Variance: $\text{Var }(X) = \exp(2 \mu + \sigma^2) [\exp(\sigma^2) - 1]$
* Moment generating function: $\psi(t) = E(X^t)$

## History of Gaussian distribution
**History**:
* First derivation: used by DeMoivre to approximate the probabilities of binomial experiments with very large $n$
* Later derivation: used to model the probabilities of random error

**Astronomy context**:
* In 1600s, Tycho Brahe suggested using repeated measurements but didn't specify how to use repeated measurements
    * Some astronomers used median of measurements
    * Some astronomers used mean of measurements
* In 1632, Galileo reasoned that:
    * There is a true distance, which is only one number
    * All observations have errors
    * The errors are symmetric around the true value
    * Small errors are more common than large ones
* In 1660s, Robert Boyle argued that people should focus on one very careful experiment, rather than repeated ones
* In 1756, Simpson introduced the notion of "probability distributions of errors" or "error curve"
    * Error could takes on values: $-v, ..., -2, -1, 0, 1, 2, ..., v$
    * Error probabilities are: 
        * $r^{-v}, ..., r^{-2}, r^{-1}, r^0, r^1, r^2, ..., r^v$
        * (or) $r^{-v}, 2 r^{1-v}, ..., (v + 1) r^0, ..., 2 r^{v-1}, r^v$
* In 1774, Laplace proposed the first of his probability distributions $\phi$ for error (i.e. Laplace distribution)
    * Idea:
        * $\phi$ must be symmetric about $0$
        * $\phi$ must be monotone decreasing with $|x|$
    * Conclusion: $\phi(x) \sim \frac{d \phi(x)}{d x}$
    
    $\hspace{1.0cm} \rightarrow \phi(x) = \frac{m}{2} e^{-m |x|}$
    * Prove: from the idea above, we conclude that $\phi(x) \sim e^{-m |x|}$
    
    $\hspace{1.0cm} \rightarrow$ By solving $c \int_{-\infty}^\infty e^{-m |x|} dx = 1$, we conclude that the normalizing constant $c$ is $\frac{m}{2}$
* Gauss concludes that the probability density for the error is $\phi(x) = \frac{h}{\sqrt{\pi}} e^{-h^2 x^2}$
    * Idea: $\frac{\phi'(x)}{\phi(x)} = k x$ (i.e. $\ln \phi(x) = \frac{k}{2}x^2 + c$)
    * Observations: $\phi(x) = A e^{\frac{k x^2}{2}}$ implies $k = -h^2$ and $A = \frac{h}{\sqrt{\pi}}$
    * Conclusion: $\phi(x) = \frac{h}{\sqrt{\pi}} e^{-h^2 x^2}$

**Beyond errors**:
* In 1846, Adolphe Quetelet examined chest measurements of Scottish soldiers from a medical journal
    * Observations: measurements of different objects of the same type would also take on the Normal Distribution
    * Idea: individuals were errors from some true, abstract form - the average value
* Galton: noticed the normal curve appearing in heights, chest measurements, exam scores, weight of sweet peas, etc.

$\hspace{1.0cm} \rightarrow$ He was one of the first statisticians to use the term "normal" to described Gaussian curve
* In 1893, Pearson called the curve "the normal curve", and credit it to Laplace

---

# BONUS
* $P(|X - \mu| \leq k \sigma) = P(|Z| \leq k)$ $\forall k, \mu, \sigma$ where $X \sim {\cal{N}}(\mu, \sigma^2)$ and $Z \sim {\cal{N}}(0, 1)$
* Sample mean: $\bar{X}_n = \frac{1}{n} \sum_i X_i$

# The Gamma function
## Motivation
**Motivating problem**: find a smooth curve which connects the points $(x, y)$ given by $y = (x - 1)!$ at the positive integer values for $x!$
* Purpose: make factorial function to be analytic
* Difficulty: there are no simple solutions for factorials (i.e. no combination of sums, products, powers, etc.)
* Idea: it's possible to find a general formula for factorials using integrals and limits from calculus 

**Derivation**:
* Translated version of the factorial function: $\begin{cases} f(1) = 1 \\ f(x+1) = x f(x)\end{cases}$ where $x \in \textbf{R}_{++}$

**Gamma function**: commonly used extension of the factorial function to complex numbers

## The Gamma function
**The Gamma function**: $\Gamma(\alpha) = \int_0^\infty x^{\alpha - 1} e^{-x} dx$ where $\alpha > 0$

**Theorem**: $\frac{\Gamma(\alpha)}{\Gamma(\alpha - 1)} = \alpha - 1$ where $\alpha > 1$
* Explain: use partial integral 
* Corollary: $\Gamma(n) = (n - 1)!$ where $n \in \textbf{Z}_+$

**Gamma function in practice**: $\Gamma(\alpha)$ must be evaluated when $\alpha = n$ or $\alpha = n + 1/2$ where $n \in \textbf{R}_{++}$
* Theorem: $\Gamma(n + \frac{1}{2}) = (n - \frac{1}{2}) (n - \frac{3}{2})... (\frac{1}{2}) \Gamma(\frac{1}{2})$

**Theorem**: $\int_0^\infty x^{\alpha - 1} e^{\beta x} dx = \frac{\Gamma(\alpha)}{\beta^\alpha}$ where $\alpha > 0$ and $\beta > 0$

# The Gamma distributions
**Gamma distributions**:
* Description: 
    * Assumptions:
    * Observation: $\frac{x^{\alpha-1} e^{-\beta x}}{\int_0^\infty x^{\alpha-1} e^{\beta x} dx}$
    * Conclusion: $f(x|\alpha, \beta)$ is the distribution of the time $x$ waiting for the $\alpha$-th positive outcome from a Poisson process with rate $\beta$
* Probability desnity function (p.d.f): $f(x|\alpha, \beta) = \begin{cases} \frac{\beta^\alpha}{\Gamma(\alpha)} x^{\alpha - 1} e^{-\beta x} & x > 0 \\ 0 & x \leq 0 \end{cases}$
* Parameters: 
    * $\alpha$ is the considered number of outcomes
    * $\beta$ is the rate parameter of the Poisson process

**Characteristics**:
* Mean: $E(X) = \frac{\alpha}{\beta}$
* Variance: $\text{Var}(X) = \frac{\alpha}{\beta^2}$
* Moments: $E(X^k) = \frac{\Gamma(\alpha + k)}{\beta^k \Gamma(\alpha)}$
* Moment generating function: $\psi(t) = (\frac{\beta}{\beta - t})^\alpha$ where $t < \beta$

**Relationship to other distributions**:
* From Gamma to Gamma: if $\{X_i\}_i$ are i.i.d and $X_i \sim \Gamma(\alpha_i, \beta)$ then $\sum_i X_i \sim \Gamma(\sum_i \alpha_i, \beta)$

# Exponential distributions
## Derivation from Poisson distribution
**Derivation from Poisson distribution**:
* Assumptions:
    * $\beta$ is the rate of positive outcomes within a time unit
    * $X_t \sim \text{Poisson}(\beta t)$ is the number of positive outcomes within $t$ units of time
    * $Y$ is the time (measured using the same time unit) until the first positive outcome
* Observations:
    * $P(Y \leq t) = P(X_t \geq 1) = 1 - P(X_t = 0) = 1 - e^{-\beta t}$
    * From above, $f(t) = \beta e^{-\beta t}$ is the p.d.f of $Y$
* Conclusion: $f(x|\beta) = \begin{cases} \beta e^{- \beta x} & x > 0 \\ 0 & x \leq 0\end{cases}$

**Corollary**: $\Gamma(\alpha, \beta)$ models the distribution of the waiting time until the $\alpha$-th positve outcomes
* Explain: if $\{X_i\}_{i=1}^\alpha$ are i.i.d and $X_i \sim \text{Exponential}(\beta)$ then $\sum_i X_i \sim \text{Gamma}(\alpha, \beta)$

## Exponential distributions
**Exponential distributions**:
* Description: model the distribution of waiting time $x$ between two consecutive positive outcomes from a Poisson process with rate $\beta$

>**NOTE**: waiting time is measured using w.r.t same time unit for the Poisson process 

* Probability density function: $f(x|\beta) = \begin{cases} \beta e^{- \beta x} & x > 0 \\ 0 & x \leq 0\end{cases}$
* Parameters: $\beta$ is the rate parameter of the Poisson process

**Characteristics**:
* Mean: $E(X) = \frac{1}{\beta}$
* Variance: $\text{Var}(X) = \frac{1}{\beta^2}$
* Moment generating function: $\psi(t) = \frac{\beta}{\beta - t}$ where $t < \beta$

**Memoryless property of exponential distributions**: if $X \sim \text{Exponential}(\beta)$ and $t > 0$ then $P(X \geq t + h|X \geq t) = P(X \geq h)$ $\forall h > 0$
* Interpretation: if we've waited for hours or days without success, the success isn't more likely to arrive soon
* Uniqueness: exponential distribution are the only continuous distributions with the memoryless property
    * Explain: suppose that there is some distribution $g$ having memoryless property    
        * From above, $G(t + h) = G(t) G(h)$ where $G(x) = P(X \geq x)$

        $\hspace{1.0cm} \rightarrow G'(t) = G(t) G'(0) + G'(t) G(0)$
        * Since $G(0)$ and $G'(0)$ are constants

        $\hspace{1.0cm} \rightarrow G'(t) = c G(t)$
        * By solving the differential equation, we come up that $G(t) = e^{c t}$

        $\hspace{1.0cm} \rightarrow P(X \leq t) = 1 - e^{c t}$ (i.e. c.d.f of $\text{Exponential}(-c)$)

**Relationships to other distributions**:
* Exponential and Geometric distributions:
    * Geometric distributions: model the number of failures before the first success in a sequence of Bernoulli trials
    * Exponential distributions: model the waiting time until the first success in continuous time, given success rate $\beta$
* Exponential to Exponential:
    * Assumptions:
        * $X_1, ..., X_n \sim \text{Exponential}(\beta)$
        * $Z_1 \leq ... \leq Z_n$ are sorted $X_1, ..., X_n$
        * $Y_k = Z_k - Z_{k-1}$
    * Conclusion: 
        * $\min\{X_1, ..., X_n\} \sim \text{Exponential}(n \beta)$
        * $Y_k \sim \text{Exponential}((n+1-k) \beta)$

---

# BONUS
* Stirling's fomrula: $\lim_{x \to \infty} \frac{\sqrt{2 \pi} x^{x - 1/2} e^{-x}}{\Gamma(x)} = 1$

# The Beta function
**Beta function**: $B(\alpha, \beta) = \int_0^1 x^{\alpha-1} (1 - x)^{\beta - 1} dx$

**Beta function and Gamma function**: $B(\alpha, \beta) = \frac{\Gamma(\alpha) \Gamma(\beta)}{\Gamma(\alpha + \beta)}$

# The Beta distribution
**Beta distributions**:

<div style="text-align:center">
    <img src="https://i.imgur.com/QpgD1Xx.png">
    <figcaption>Beta distribution PDF with different parameter values</figcaption>
</div>

* Description:
    * Assumptions:
        * $P$ is the proportion of postives among $n$ outcomes
            * The marginal distribution of $P$ is the uniform distribution over $[0, 1]$
        * $X$ is the number of positives among $n$ observed outcomes
    * Observation: $P(p|x) = \frac{P(x|p)}{\int_q P(x|q)} = \frac{p^x (1 - p)^{n-x}}{\int_0^1 q^x (1 - q)^{n-x} dq}$ for $p \in (0, 1)$
    * Conclusion: the conditional distribution of $P$, given $X$, is the Beta distribution with parameters $x+1$ and $n-x+1$
* Probability density function: $f(x|\alpha, \beta) = \begin{cases} \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha) \Gamma(\beta)} x^{\alpha-1} (1 - x)^{\beta - 1} & x \in (0, 1) \\ 0 & \text{otherwise}\end{cases}$
* Parameters: $\alpha > 0$ and $\beta > 0$

**Characteristics**:
* Mean: $E(X) = \frac{\alpha}{\alpha + \beta}$
* Variance: $\text{Var}(X) = \frac{\alpha \beta}{(\alpha + \beta)^2 (\alpha + \beta + 1)}$

**Relationships to other distributions**:
* Beta and Binomial: 
    * Assumptions:
        * $P \sim \text{Beta}(\alpha, \beta)$
        * The conditional distribution of $X$ given $P = p$ is the Binomial distribution with parameters $n$ and $p$
    * Conclusion: the conditional distribution of $P$ given $X$ is the Beta distribution with parameters $\alpha+x$ and $\beta + n - x$
    * Explain: 
        * $P(p|x) \propto P(x|p) P(p)$
        * $P(x|p) P(p) = p^{\alpha+x-1} (1 - p)^{\beta+n-x-1}$

**Intuition of Beta distribution when $\alpha < 1$ and $\beta < 1$**:
* *As sample size decreases, variance increases*. Variance provides one explanation for the U-shaped Beta distribution
    * *Sample size and Beta distribution variance*.
        * A larger sample size $\alpha + \beta$ decreases the distribution's variance
        * A smaller sample size $\alpha + \beta$ increases the distribution's variance
    * *U-shaped Beta distribution*. If Beta were limited to unimodel distributions, their variance could never reach its full potential

        $\to$ To maximize a Beta distribution's variance for a particular mean, the distribution must be bimodal, with its density concentrated at the two extremes
        * *Extreme case*. As the variance approaches its maximum, for any given mean

            $\to$ The Beta distribution approaches a Bernoulli distribution, and its variance approaches the variance of a Bernoulli distribution with its same mean
    * *Conclusion*. Since the domain of a Beta distribution is bounded by 0 and 1, to keep the mean and increase the variance

        $\to$ Most of the mass has to be pushed up against the boundaries, and there is no other way for the variance to be large enough
        * *Key idea*. The mode does not coincide with the mean
* *Different interpretations of $\alpha$ and $\beta$ - Mean-based versus mode-based*.
    * *Mean and mode of Beta distribution*.
        * *Mean*. Correspond to $\alpha$ successes and $\beta$ failures
            * *Formula*. $E(X)=\frac{\alpha}{\alpha + \beta}$
            * *Usage*. 
                * Bayesians generally prefer to use posterior mean rather than posterior mode as a point estimate

                    $\to$ This is justified by a quadratic loss function
                * The use for $\alpha$ and $\beta$ are more convenient mathematically
        * *Mode*. Correspond to $\alpha - 1$ successes and $\beta - 1$ failures
            * *Formula*. $E(X)=\frac{\alpha - 1}{\alpha + \beta - 2}$
            * *Usage*. Have advantage that $\Beta(1,1)$ prior corresponds to 0 successes and 0 failures
    * *Mean and mode in bimodal Beta distributions*. Bimodal Beta distributions have two modes but only one mean
        * *Bimodal Beta distribution - in "Doing Bayesian Data Analysis" by John Kruschke*. Consider the example of a fair coin
            
            $\to$ Bimodal Beta distributions mean we "believe that the coin is a trick coin which nearly always comes up heads or nearly always comes up tails, but we do not know which"

            >**NOTE**. This is rather contrived scenario, thus the coin tossing example has limit in describing bimodal Beta distribution
        
    * *Beta distribution as a conjugate distribution*. $\alpha < 1$ and $\beta < 1$ only appear as a prior distribution, when either no success or no failure is observed
        * *Conjugate hyperparameters*. It is often useful to think of the hyperparameters of a conjugate prior distribution as corresponding to having observed a certain number of pseudo-observations with properties specified by the parameters
        * *$\alpha - 1$ and $\beta - 1$ as the number of successes and failures*. Used if the posterior mode is used to choose an optimal parameter setting
            * *Explain*. We focus on the mode throughout the experiment
        * *$\alpha$ and $\beta$ as the number of successes and failures*. Used if the posterior mean is used to choose an optimal parameter setting
            * *Explain*. We focus on the mean throughout the experiment
* *References*.
    * https://math.stackexchange.com/questions/3494530/why-does-the-beta-distribution-become-u-shaped-when-alpha-and-beta-1?rq=1
    * https://stats.stackexchange.com/questions/121880/problem-interpreting-the-beta-distribution?rq=1
    * https://stats.stackexchange.com/questions/362728/whats-the-intuition-for-a-beta-distribution-with-alpha-and-or-beta-less-than