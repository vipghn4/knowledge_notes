<!-- TOC titleSize:1 tabSpaces:2 depthFrom:1 depthTo:6 withLinks:1 updateOnSave:1 orderedList:0 skip:0 title:1 charForUnorderedList:* -->
# Table of Contents
- [Table of Contents](#table-of-contents)
- [Common continuous distributions](#common-continuous-distributions)
  - [Normal distribution](#normal-distribution)
    - [Importance of normal distributions](#importance-of-normal-distributions)
    - [Normal distribution](#normal-distribution-1)
    - [Standard normal distribution](#standard-normal-distribution)
    - [Comparisons of normal distributions](#comparisons-of-normal-distributions)
    - [Lognormal distribution](#lognormal-distribution)
    - [History of Gaussian distribution](#history-of-gaussian-distribution)
  - [The Gamma distribution](#the-gamma-distribution)
    - [The Gamma function](#the-gamma-function)
      - [Motivation](#motivation)
      - [The Gamma function](#the-gamma-function-1)
    - [The Gamma distributions](#the-gamma-distributions)
    - [Exponential distributions](#exponential-distributions)
  - [The Beta distribution](#the-beta-distribution)
    - [The Beta function](#the-beta-function)
    - [The Beta distribution](#the-beta-distribution-1)
  - [Other distributions](#other-distributions)
- [Appendix](#appendix)
  - [Concepts](#concepts)
<!-- /TOC -->

# Common continuous distributions
## Normal distribution
### Importance of normal distributions
**Mathematical properties of normal distributions**. If a random sample taken from a normal distribution

$\to$ The distributions of various important functions of the observations in the sample can be derived explicitly and have simple forms

**Popularity in physical experiments**. The random variables studied in various physical experiments often have distributions which are approximately normal

**Central limit theorem**. If a large random sample is taken from some distribution

$\to$ Many important functions of the observations will have distributions which are approximately normal

### Normal distribution
**Normal distribution**.
* *Probability density function (p.d.f)*.
    
    $$f(x|\mu, \sigma^2) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp[\frac{- (x - \mu)^2}{2 \sigma^2}]$$

* *Parameters*. $\mu = E(X)$ and $\sigma^2 = \text{Var }(X)$

**Characteristics**.
* *Mean*. $\mu$
* *Variance*. $\sigma^2$
* *Moment generating function*. $\psi(t) = \exp(\mu t + \frac{1}{2} \sigma^2 t^2)$

**Distribution of linear combinations of Gaussian variables**. If $\{X_i\}$ are independent and $X_i \sim {\mathcal{N}}(\mu_i, \sigma_i^2)$ then 

$$\sum_i a_i X_i \sim {\mathcal{N}}(\sum_i a_i \mu_i, \sum_i a_i^2 \sigma^2)$$

* *Usage*. 
    * Determine the minimum sample size $n$, so that

        $$P(|\bar{X}_n - \mu| \leq \epsilon) \geq \delta$$
    
    * Find the confidence interval of $\mu$ given the observations

### Standard normal distribution
**Standard normal distribution**.
* *Description*. Normal distribution with zero mean and unit variance
* *Probability density function (p.d.f)*. 
    
    $$\phi(x) = \frac{1}{\sqrt{2 \pi}} \exp(-\frac{x^2}{2})$$

* *Cumulative density function (c.d.f)*. 
    
    $$\Phi(x) = \int_{-\infty}^x \phi(x) dx$$

    >**NOTE**. The c.d.f of normal distributions cannot be expressed in closed form
    >
    >$\to$ We have to use numerical approximations or use a table of values for them

**Consequences of symmetry**. Forall $x$ and all $0<p<1$ 

$$\Phi(-x) = 1 - \Phi(x),\quad \Phi^{-1}(p) = -\Phi^{-1}(1 - p)$$

**Converting normal distributions to standard**. 
* *Assumptions*.
    * $X \sim {\mathcal{N}}(\mu, \sigma^2)$
    * $F$ is the c.d.f of $X$
* *Conclusion*.
    * $Z = \frac{X - \mu}{\sigma} \sim {\mathcal{N}}(0, 1)$
    * $F(x) = \Phi(\frac{x - \mu}{\sigma})$ for all $x$ and $p \in (0, 1)$
    * $F^{-1}(p) = \mu + |sigma \Phi^{-1}(p)$ for all $x$ and $p \in (0, 1)$

### Comparisons of normal distributions
**Quantiles of normal distribution**. Every normal distribution contains the same total amount of probability within $n\in\mathbb{N}^*$ standard deviations of its mean, i.e.

$$\forall k, \mu, \sigma,P(|X - \mu| \leq k \sigma) = P(|Z| \leq k)$$

where $X \sim {\mathcal{N}}(\mu, \sigma^2)$ and $Z \sim {\mathcal{N}}(0, 1)$

### Lognormal distribution
**Lognormal distribution**. $X$ has the lognormal distribution with parameters $\mu$ and $\sigma^2$ if $\log(X) \sim {\mathcal{N}}(\mu, \sigma^2)$
* *Usage*. 
    * Popular for modeling times until failure
    * Used by Black and Scholes (1973) to develop a pricing scheme for options on stocks, whose prices follow a lognormal distribution

**Characteristics**.
* *Mean*. $E(X) = \exp(\mu + \frac{\sigma^2}{2})$
* *Variance*. $\text{Var }(X) = \exp(2 \mu + \sigma^2) [\exp(\sigma^2) - 1]$
* *Moment generating function*. $\psi(t) = E(X^t)$

**Example**. 
* *Scenario*. Products that are subject to wear and tear are generally tested for endurance in order to estimate their useful lifetimes
    * *Dataset*. Measurements of the numbers of millions of revolutions before failure for 23 ball bearings

        $\to$ A histogram of the 23 lifetimes are plotted together with a lognormal p.d.f with parameters chosen to match the observed data
* *Question*. How long to wait until there is a 90 percent chance that a ball bearing will have failed
* *Solution*. Use the $0.9$ quantile of the distribution of lifetimes

**Occurrence and applications**. Log-normal distribution is important in the description of natural phenomena
* *Motivation*. Many natural growth processes are driven by the accumulation of many small percentage changes
    
    $\to$ Such changes become additive on a log scale
    * *Observations*. Under appropriate regularity conditions
        
        $\to$ The distribution of the resulting accumulated changes will be increasingly well approximated by a log-normal
        * *Explain*. Due to multiplicative CLT
    * *Gibrat's law*. The size distributions at any age of things that grow over time tends to be log-normal
    * *Consequence*. Fundamental natural laws imply multiplications and divisions of positive variables
        
        $\to$ Assuming log-normal distributions of the variables involved leads to consistent models in these cases
* *Occurrences*.
    * The length of comments posted in Internet discussion forums follows a log-normal distribution
    * Users' dwell time on online articles (jokes, news etc.) follows a log-normal distribution
    * The length of chess games tends to follow a log-normal distribution
    * Onset durations of acoustic comparison stimuli that are matched to a standard stimulus follow a log-normal distribution

### History of Gaussian distribution
**History**.
* *First derivation*. Used by DeMoivre to approximate the probabilities of binomial experiments with very large $n$
* *Later derivation*. Used to model the probabilities of random error

**Astronomy context**.
* In 1600s, Tycho Brahe suggested using repeated measurements but didn't specify how to use repeated measurements
    * Some astronomers used median of measurements
    * Some astronomers used mean of measurements
* In 1632, Galileo reasoned that:
    * There is a true distance, which is only one number
    * All observations have errors
    * The errors are symmetric around the true value
    * Small errors are more common than large ones
* In 1660s, Robert Boyle argued that people should focus on one very careful experiment, rather than repeated ones
* In 1756, Simpson introduced the notion of "probability distributions of errors" or "error curve"
    * Error could takes on values $-v, ..., -2, -1, 0, 1, 2, ..., v$
    * Error probabilities are
        * $r^{-v}, ..., r^{-2}, r^{-1}, r^0, r^1, r^2, ..., r^v$
        * (or) $r^{-v}, 2 r^{1-v}, ..., (v + 1) r^0, ..., 2 r^{v-1}, r^v$
* In 1774, Laplace proposed the first of his probability distributions $\phi$ for error (i.e. Laplace distribution)
    * *Idea*.
        * $\phi$ must be symmetric about $0$
        * $\phi$ must be monotone decreasing with $|x|$
    * *Conclusion*. $\phi(x) \sim \frac{d \phi(x)}{d x}$
    
        $\to \phi(x) = \frac{m}{2} e^{-m |x|}$
    * *Prove*. From the idea above, we conclude that $\phi(x) \sim e^{-m |x|}$
    
    $\hspace{1.0cm} \rightarrow$ By solving $c \int_{-\infty}^\infty e^{-m |x|} dx = 1$, we conclude that the normalizing constant $c$ is $\frac{m}{2}$
* Gauss concludes that the probability density for the error is $\phi(x) = \frac{h}{\sqrt{\pi}} e^{-h^2 x^2}$
    * *Idea*. $\frac{\phi'(x)}{\phi(x)} = k x$ (i.e. $\ln \phi(x) = \frac{k}{2}x^2 + c$)
    * *Observations*. $\phi(x) = A e^{\frac{k x^2}{2}}$ implies $k = -h^2$ and $A = \frac{h}{\sqrt{\pi}}$
    * *Conclusion*. $\phi(x) = \frac{h}{\sqrt{\pi}} e^{-h^2 x^2}$

**Beyond errors**.
* In 1846, Adolphe Quetelet examined chest measurements of Scottish soldiers from a medical journal
    * *Observations*. Measurements of different objects of the same type would also take on the Normal Distribution
    * *Idea*. Individuals were errors from some true, abstract form - the average value
* Galton noticed the normal curve appearing in heights, chest measurements, exam scores, weight of sweet peas, etc.

    $\to$ He was one of the first statisticians to use the term "normal" to described Gaussian curve
* In 1893, Pearson called the curve "the normal curve", and credit it to Laplace

## The Gamma distribution
### The Gamma function
#### Motivation
**Motivating problem**. Find a smooth curve connecting the points $(x, y)$ given by $y = (x - 1)!$ at the positive integer values of $x!$
* *Purpose*. Make factorial function to be analytic
* *Difficulty*. There are no simple solutions for factorials (i.e. no combination of sums, products, powers, etc.)
* *Idea*. It is possible to find a general formula for factorials using integrals and limits from calculus

**Translated version of the factorial function**: $\begin{cases} f(0) = 1 \\ f(x+1) = x f(x)\end{cases}$ where $x \in \mathbb{R}_{++}$

$\to$ This is used to derive the Gamma function

**Gamma function**. Commonly used extension of the factorial function to complex numbers

#### The Gamma function
**The Gamma function**. 

$$\Gamma(\alpha) = \int_0^\infty x^{\alpha - 1} e^{-x} dx$$

where $\alpha > 0$

**Theorem**. $\frac{\Gamma(\alpha)}{\Gamma(\alpha - 1)} = \alpha - 1$ where $\alpha > 1$
* *Explain*. Use partial integral 
* *Corollary*. $\Gamma(n) = (n - 1)!$ where $n \in \mathbb{Z}_+$

**Gamma function in practice**. $\Gamma(\alpha)$ must be evaluated when $\alpha = n$ or $\alpha = n + 1/2$ where $n \in \mathbb{R}_{++}$
* *Theorem*. $\Gamma(n + \frac{1}{2}) = (n - \frac{1}{2}) (n - \frac{3}{2})... (\frac{1}{2}) \Gamma(\frac{1}{2})$

**Theorem**. $\int_0^\infty x^{\alpha - 1} e^{\beta x} dx = \frac{\Gamma(\alpha)}{\beta^\alpha}$ where $\alpha > 0$ and $\beta > 0$

### The Gamma distributions
**Gamma distributions**.
* *Probability desnity function (p.d.f)*. 

    $$f(x|\alpha, \beta) = \begin{cases} \frac{\beta^\alpha}{\Gamma(\alpha)} x^{\alpha - 1} e^{-\beta x} & x > 0 \\ 0 & x \leq 0 \end{cases}$$

* *Parameters*. 
    * $\alpha$ is the considered number of outcomes
    * $\beta$ is the rate parameter of the Poisson process

**Characteristics**.
* *Mean*. $E(X) = \frac{\alpha}{\beta}$
* *Variance*. $\text{Var}(X) = \frac{\alpha}{\beta^2}$
* *Moments*. $E(X^k) = \frac{\Gamma(\alpha + k)}{\beta^k \Gamma(\alpha)}$
* *Moment generating function*. $\psi(t) = (\frac{\beta}{\beta - t})^\alpha$ where $t < \beta$

**From Gamma to Gamma**. If $\{X_i\}_i$ are i.i.d and $X_i \sim \Gamma(\alpha_i, \beta)$ then 

$$\sum_i X_i \sim \Gamma(\sum_i \alpha_i, \beta)$$

### Exponential distributions
**Derivation from Poisson distribution**.
* *Assumptions*.
    * $\beta$ is the rate of positive outcomes within a time unit
    * $X_t \sim \text{Poisson}(\beta t)$ is the number of positive outcomes within $t$ units of time
    * $Y$ is the time (measured using the same time unit) until the first positive outcome
* *Observations*.
    * $P(Y \leq t) = P(X_t \geq 1) = 1 - P(X_t = 0) = 1 - e^{-\beta t}$
    * From above, $f(t) = \beta e^{-\beta t}$ is the p.d.f of $Y$
* *Conclusion*. $f(x|\beta) = \begin{cases} \beta e^{- \beta x} & x > 0 \\ 0 & x \leq 0\end{cases}$
* *Corollary*. $\Gamma(\alpha, \beta)$ models the distribution of the waiting time until the $\alpha$-th positve outcomes
    * *Explain*. If $\{X_i\}_{i=1}^\alpha$ are i.i.d and $X_i \sim \text{Exponential}(\beta)$ then 
        
        $$\sum_i X_i \sim \text{Gamma}(\alpha, \beta)$$

**Exponential distributions**. Model the distribution of waiting time $x$ between two consecutive positive outcomes from a Poisson process with rate $\beta$

>**NOTE**. Waiting time is measured using w.r.t same time unit for the Poisson process 

* *Probability density function*. $f(x|\beta) = \begin{cases} \beta e^{- \beta x} & x > 0 \\ 0 & x \leq 0\end{cases}$
* *Parameters*. $\beta$ is the rate parameter of the Poisson process

**Characteristics**.
* *Mean*. $E(X) = \frac{1}{\beta}$
* *Variance*. $\text{Var}(X) = \frac{1}{\beta^2}$
* *Moment generating function*. $\psi(t) = \frac{\beta}{\beta - t}$ where $t < \beta$

**Memoryless property of exponential distributions**. If $X \sim \text{Exponential}(\beta)$ and $t > 0$ then 

$$\forall h > 0,P(X \geq t + h|X \geq t) = P(X \geq h)$$

* *Interpretation*. If we've waited for hours or days without success, the success is not more likely to arrive soon
* *Uniqueness*. Exponential distribution are the only continuous distributions with the memoryless property
    * *Explain*. Suppose that there is some distribution $g$ having memoryless property    
        * From above, $G(t + h) = G(t) G(h)$ where $G(x) = P(X \geq x)$

            $\to G'(t) = G(t) G'(0) + G'(t) G(0)$
        * Since $G(0)$ and $G'(0)$ are constants

            $\to G'(t) = c G(t)$
        * By solving the differential equation, we come up that $G(t) = e^{c t}$

            $\to P(X \leq t) = 1 - e^{c t}$ (i.e. c.d.f of $\text{Exponential}(-c)$)

**Relationships to other distributions**.
* *Exponential and Geometric distributions*.
    * *Geometric distributions*. model the number of failures before the first success in a sequence of Bernoulli trials
    * *Exponential distributions*. model the waiting time until the first success in continuous time, given success rate $\beta$
* *Exponential to Exponential*.
    * *Assumptions*.
        * $X_1, ..., X_n \sim \text{Exponential}(\beta)$
        * $Z_1 \leq ... \leq Z_n$ are sorted $X_1, ..., X_n$
        * $Y_k = Z_k - Z_{k-1}$
    * *Conclusion*. 
        * $\min\{X_1, ..., X_n\} \sim \text{Exponential}(n \beta)$
        * $Y_k \sim \text{Exponential}((n+1-k) \beta)$

## The Beta distribution
### The Beta function
**Beta function (or Euler integral of the first kind)**. 

$$B(\alpha, \beta) = \int_0^1 x^{\alpha-1} (1 - x)^{\beta - 1} dx$$

**Beta function and Gamma function**. $B(\alpha, \beta) = \frac{\Gamma(\alpha) \Gamma(\beta)}{\Gamma(\alpha + \beta)}$

### The Beta distribution
**Beta distributions**:

<div style="text-align:center">
    <img src="https://i.imgur.com/QpgD1Xx.png">
    <figcaption>Beta distribution PDF with different parameter values</figcaption>
</div>

* *Derivation*.
    * *Assumptions*.
        * $P$ is the proportion of postives among $n$ outcomes
            * The marginal distribution of $P$ is the uniform distribution over $[0, 1]$
        * $X$ is the number of positives among $n$ observed outcomes
    * *Observation*. $P(p|x) = \frac{P(x|p)}{\int_q P(x|q)} = \frac{p^x (1 - p)^{n-x}}{\int_0^1 q^x (1 - q)^{n-x} dq}$ for $p \in (0, 1)$
    * *Conclusion*. The conditional distribution of $P$, given $X$, is the Beta distribution with parameters $x+1$ and $n-x+1$
* *Probability density function*. 

    $$f(x|\alpha, \beta) = \begin{cases} \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha) \Gamma(\beta)} x^{\alpha-1} (1 - x)^{\beta - 1} & x \in (0, 1) \\ 0 & \text{otherwise}\end{cases}$$

* *Parameters*. $\alpha > 0$ and $\beta > 0$

**Characteristics**.
* *Mean*. $E(X) = \frac{\alpha}{\alpha + \beta}$
* *Variance*. $\text{Var}(X) = \frac{\alpha \beta}{(\alpha + \beta)^2 (\alpha + \beta + 1)}$

**Beta and Binomial**: 
* *Assumptions*.
    * $P \sim \text{Beta}(\alpha, \beta)$
    * The conditional distribution of $X$ given $P = p$ is the Binomial distribution with parameters $n$ and $p$
* *Conclusion*. The conditional distribution of $P$ given $X$ is the Beta distribution with parameters $\alpha+x$ and $\beta + n - x$
* *Explain*. 
    * $P(p|x) \propto P(x|p) P(p)$
    * $P(x|p) P(p) = p^{\alpha+x-1} (1 - p)^{\beta+n-x-1}$

**Intuition of Beta distribution when $\alpha < 1$ and $\beta < 1$**.
* *As sample size decreases, variance increases*. Variance provides one explanation for the U-shaped Beta distribution
    * *Sample size and Beta distribution variance*.
        * A larger sample size $\alpha + \beta$ decreases the distribution's variance
        * A smaller sample size $\alpha + \beta$ increases the distribution's variance
    * *U-shaped Beta distribution*. If Beta were limited to unimodel distributions, their variance could never reach its full potential

        $\to$ To maximize a Beta distribution's variance for a particular mean, the distribution must be bimodal, with its density concentrated at the two extremes
        * *Extreme case*. As the variance approaches its maximum, for any given mean

            $\to$ The Beta distribution approaches a Bernoulli distribution, and its variance approaches the variance of a Bernoulli distribution with its same mean
    * *Conclusion*. Since the domain of a Beta distribution is bounded by 0 and 1, to keep the mean and increase the variance

        $\to$ Most of the mass has to be pushed up against the boundaries, and there is no other way for the variance to be large enough
        * *Key idea*. The mode does not coincide with the mean
* *Different interpretations of $\alpha$ and $\beta$ - Mean-based versus mode-based*.
    * *Mean and mode of Beta distribution*.
        * *Mean*. Correspond to $\alpha$ successes and $\beta$ failures
            * *Formula*. $E(X)=\frac{\alpha}{\alpha + \beta}$
            * *Usage*. 
                * Bayesians generally prefer to use posterior mean rather than posterior mode as a point estimate

                    $\to$ This is justified by a quadratic loss function
                * The use for $\alpha$ and $\beta$ are more convenient mathematically
        * *Mode*. Correspond to $\alpha - 1$ successes and $\beta - 1$ failures
            * *Formula*. $E(X)=\frac{\alpha - 1}{\alpha + \beta - 2}$
            * *Usage*. Have advantage that $\Beta(1,1)$ prior corresponds to 0 successes and 0 failures
    * *Mean and mode in bimodal Beta distributions*. Bimodal Beta distributions have two modes but only one mean
        * *Bimodal Beta distribution - in "Doing Bayesian Data Analysis" by John Kruschke*. Consider the example of a fair coin
            
            $\to$ Bimodal Beta distributions mean we "believe that the coin is a trick coin which nearly always comes up heads or nearly always comes up tails, but we do not know which"

            >**NOTE**. This is rather contrived scenario, thus the coin tossing example has limit in describing bimodal Beta distribution
        
    * *Beta distribution as a conjugate distribution*. $\alpha < 1$ and $\beta < 1$ only appear as a prior distribution, when either no success or no failure is observed
        * *Conjugate hyperparameters*. It is often useful to think of the hyperparameters of a conjugate prior distribution as corresponding to having observed a certain number of pseudo-observations with properties specified by the parameters
        * *$\alpha - 1$ and $\beta - 1$ as the number of successes and failures*. Used if the posterior mode is used to choose an optimal parameter setting
            * *Explain*. We focus on the mode throughout the experiment
        * *$\alpha$ and $\beta$ as the number of successes and failures*. Used if the posterior mean is used to choose an optimal parameter setting
            * *Explain*. We focus on the mean throughout the experiment
* *References*.
    * https://math.stackexchange.com/questions/3494530/why-does-the-beta-distribution-become-u-shaped-when-alpha-and-beta-1?rq=1
    * https://stats.stackexchange.com/questions/121880/problem-interpreting-the-beta-distribution?rq=1
    * https://stats.stackexchange.com/questions/362728/whats-the-intuition-for-a-beta-distribution-with-alpha-and-or-beta-less-than

## Other distributions
**Cauchy distribution**. A continuous probability distribution known especially among physicists
* *Other names*. Lorentz distribution, i.e. after Hendrik Lorentz, Cauchy–Lorentz distribution, Lorentz(ian) function, or Breit–Wigner distribution
* *Characteristics*. The moments of the distribution do not exist, since the integrals do not converge to finite values

    $\to$ Cauchy distribution is often used in statistics as the canonical example of a pathological distribution
* *Interpretation*. If a random variable is uniformly distributed on $(-\pi/2,\pi/2)$, then its tangent follows a Cauchy distribution
    
    $\to$ This is a very natural transformation to consider, making the distribution interesting
* *Usage*.
    * It is used in Bayesian analysis when one wishes to set a prior that does not provide too much information on some quantity
        * *Explain*. A normal, or t, or other choices, can greatly penalize large values, while a Cauchy does this to a far smaller extent
    * The Cauchy distribution has a similar bell shape to the Normal distribution, but much fatter tails
        
        $\to$ Events such as the behaviour of stock market prices, which are usually assumed to have a normal distribution, fit the Cauchy distribution better

# Appendix
## Concepts
**Sample mean**. $\bar{X}_n = \frac{1}{n} \sum_i X_i$

**Stirling's formula**.

$$\lim_{x\to\infty} \frac{\sqrt{2\pi} x^{x-1/2} e^{-x}}{\Gamma(x)}=1$$