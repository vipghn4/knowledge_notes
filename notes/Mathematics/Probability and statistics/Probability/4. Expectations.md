<!-- TOC titleSize:1 tabSpaces:2 depthFrom:1 depthTo:6 withLinks:1 updateOnSave:1 orderedList:0 skip:0 title:1 charForUnorderedList:* -->
# Table of Contents
- [Table of Contents](#table-of-contents)
- [Expectation of a random variable](#expectation-of-a-random-variable)
  - [Expectation of a random variable](#expectation-of-a-random-variable-1)
  - [Expectation of a function](#expectation-of-a-function)
  - [Properties of expectations](#properties-of-expectations)
- [Variance](#variance)
  - [History](#history)
  - [Variance and standard deviation](#variance-and-standard-deviation)
- [Moments](#moments)
  - [History](#history-1)
  - [Mathematical moment](#mathematical-moment)
  - [Physical moment](#physical-moment)
  - [Statistical moment](#statistical-moment)
  - [Moment generating functions](#moment-generating-functions)
- [Mean and median](#mean-and-median)
- [Covariance and correlation](#covariance-and-correlation)
- [Conditional expectation](#conditional-expectation)
  - [Conditional expectation](#conditional-expectation-1)
  - [Conditional variance](#conditional-variance)
- [Prediction](#prediction)
- [BONUS](#bonus)
- [NEW WORD](#new-word)
<!-- /TOC -->

# Expectation of a random variable
## Expectation of a random variable
**Expectation of a discrete distribution**: 
* Bounded random variable:
    * Assumptions:
        * $X$ is a bounded discrete random variable with p.f $f$
    * Conclusion: 
        * The expectation (the mean, or expected value) of $X$: $E(X) = \sum_x x f(x)$
* General random variables:
    * If $\sum_{x > 0} x f(x)$ or $\sum_{x < 0} x f(x)$ is finite
    
    $\hspace{1.0cm} \rightarrow E(X) = \sum_x x f(x)$
    * If $\sum_{x > 0} x f(x)$ and $\sum_{x < 0} x f(x)$ are infinite
    
    $\hspace{1.0cm} \rightarrow E(X)$ isn't well-defined (i.e. not exist)

**Expectation of a continuous distribution**:
* Bounded random variable:
    * Assumptions:
        * $X$ is a bounded continuous random variable with p.d.f $f$
    * Conclusion: 
        * The expectation (the mean, or expected value) of $X$: $E(X) = \int_{-\infty}^\infty x f(x) dx$
* General random variables:
    * If $\int_0^\infty x f(x) dx$ or $\int_{-\infty}^0 x f(x) dx$ is finite
    
    $\hspace{1.0cm} \rightarrow E(X) = \int_{-\infty}^\infty x f(x) dx$
    * If $\int_0^\infty x f(x) dx$ and $\int_{-\infty}^0 x f(x) dx$ are infinite
    
    $\hspace{1.0cm} \rightarrow E(X)$ isn't well-defined (i.e. not exist)

**Remarks**:
* Interpretation of expectation: the mean of a distribution can be regarded as being the center of gravity of that distribution
    * Explain: $f(x)$ refers to the amount of mass at $x$
* Existence of distribution: we have to make sure that $E(X)$ exists before calculating it
* Expectation and distribution: $E(X)$ depends only on the distribution of $X$ 
    * Explain: having the same distribution means having the same expectation
* Drawbacks of mean: can be affected by a very small change in $f(x)$ at very large $x$

## Expectation of a function
**Functions of a single random variable**: $E[r(X)] = E(Y) = \int_{-\infty}^\infty y g(y) dy$ if $E(Y)$ exists
* Law of unconscious statistician:
    * Continuous case: $E[r(X)] = \int_{-\infty}^\infty r(x) f(x) dx$ if $E[r(X)]$ exists
    * Discrete case: $E[r(X)] = \sum_x r(x) f(x)$ if $E[r(X)]$ exists

**Functions of multiple random variables**:
* Law of unconscious statistician:
    * Continuous case: $E[r(\textbf{X})] = \int_{\textbf{R}^n} r(\textbf{x}) f(\textbf{x}) d\textbf{x}$ if $E[r(\textbf{X})]$ exists
    * Discrete case: $E[r(\textbf{X})] = \sum_\textbf{x} r(\textbf{x}) f(\textbf{x})$ if $E[r(\textbf{X})]$ exists

## Properties of expectations
**Basic properties**:
* $E(\sum_i a_i X_i + b) = \sum_i a_i E(X_i) + b$
* $E[g(\textbf{X})] \geq g[E(\textbf{X})]$ if $g: \textbf{R}^n \to \textbf{R}$ is a convex function
    * Explain: Jensen's inequality
    * Usage: provide a lower bound for $E[g(\textbf{X})]$

**The rule of iterated expectations**: $E[E(r(X, Y)|X)] = E[r(X, Y)]$

**Expectation of a product of independent random variables**: $E(\prod_i X_i) = \prod_i E(X_i)$
* Intuition: consider $E(X_1 X_2)$
    * Consider a data matrix $\textbf{X} \in \textbf{R}^{n \times 2}$
    * For each row $i$ of $\textbf{X}$, add $x_{i1} x_{i2}$ to the sum
    * At the end, each possible value $\hat{x}_1$ of $X_1$ will be multiplied by $n_{\hat{x}_1} E(X_2)$
    * Dividing the sum by $n$, we obtain $\sum_{\hat{x}_1} \frac{n_{\hat{x}_1}}{n} \hat{x}_1 E(X_2) \approx E(X_1) E(X_2)$

# Variance
## History
**Gauss' work**: conceptually first introduced "variance"
* Gaussian distribution: Gauss modeled a series of individual measurements as scattering symmetrically and independently around some fixed center point

$\hspace{1.0cm} \rightarrow$ Gauss derived a probability distribution for the reasonable expectations about the "errors" with two parameters: "location" and "precision"
* Maximum likelihood position of the center point: based on Gaussian distribution

$\hspace{1.0cm} \rightarrow$ Gauss found the maximum likelihood point by minimizing the squared deviation
* Variance: the expected value of $(x - \mu)^2$ from the p.d.f of Gaussian distribution $\frac{1}{\sqrt{2 \pi \sigma^2}} e^{-\frac{(x - \mu)^2}{2 \sigma^2}}$

**Probable error of the mean**: studied by students of Gauss, and later by Student, Pearson, and Fisher
* t-distribution: $\frac{\bar{X}_n - \mu}{\sigma / \sqrt{n}}$ has a Student's t-distribution with $n-1$ degrees of freedom
    * Parameters: location $\mu$ and dispersion parameter $\sigma$
    * Usage: used in t-test for assessing the statistical of the difference between two sample means
* Standard deviation: the dispersion parameter of t-distribution

## Variance and standard deviation
**Variance and standard deviation**: 
* Variance: $\text{Var }(X) = E[(X - \mu)^2]$
    * Alternative formula: $\text{Var }(X) = E(X^2) - \mu^2$

>**NOTE**: variance depends only on the distribution

* Standard deviation: $\sqrt{\text{Var }(X)}$

**Properties of variance**:
* $\text{Var }(X) \geq 0$
* $\text{Var }(X)$ must exist and be finite if $X$ is a bounded random variable
* $\text{Var }(X) = 0$ if and only if $P(X = c) = 1$
* If $\{X_i\}$ are independent then $\text{Var }(\sum_i a_i X_i) = \sum_i a_i^2 \text{Var }(X_i)$

# Moments
## History
**Motivation**: from Archimedes' discovery of operating principle of the lever
* The moment of force: $M = r F$ where $F$ is the applied force and $r$ is the distance from the applied force to object
    * Interpretation: the amount of force applied to an object

**"Moment"**: means "importance" or "consequence"
* The moment of a force about an axis: the importance of the force w.r.t its power to generate in matter rotation about the axis

## Mathematical moment
**Mathematical moment about $c$**: $\mu_n = \int_{-\infty}^\infty (x - c)^n f(x) dx$ where $f$ is a real-valued continuous function
* Interpretation: a specific quantitative measure of the shape of a function

## Physical moment
**Physical moment**: $\mu_n = \int r^n \rho(r) dr$ where $\rho$ is the distribution of the density of mass

**Moment of mass**:
* Zeroth moment: the total mass
* First moment: the center of mass
* Second moment: the moment of inertia

**History**: motivated by the mathematical moments

## Statistical moment
**The $k$-th moment of $X$**: $E(X^k)$
* Usage: zeroth moment provides the expected values

**The $k$-th central moment of $X$**: $E[(X - \mu)^k]$
* Usage: higher-order central moments provide clearer information about the distribution shape
* Property: invariant to translation

**The $k$-th normalized moment of $X$**: $\frac{E[(X - \mu)^k]}{\sigma^k}$
* Usage: represent the distribution independently of any linear change of scale
* Property: invariant to scaling

## Moment generating functions
**Moment generating function (m.g.f) of $X$**: $\psi(t) = E(e^{t X})$
* Other name: Laplace transform of $X$
* Usage:
    * Compute the moments of a distribution
    * Find the distribution of sums of random variables
    * Used to prove the central limit theorem

**Derivation of moments**: if $\psi(t)$ is finite for all $t \in (0^-, 0^+)$

$\hspace{1.0cm} \rightarrow E(X^k) = \frac{\partial^k \psi}{\partial^k t}(0)$

**Uniqueness of m.g.f**: $\psi(t) = \int_{-\infty}^\infty f(x) x^k dx$ is the Laplace transform of the p.m.f $f(x)$
* Consequence: $\{\psi(t)|\forall t\}$ is unique for each p.m.f $f$

**Properties of m.g.f**:
* $\psi_2(t) = e^{bt} \psi_1(a t)$ where $\psi_2(t)$ is the m.g.f of $a X + b$
* $\psi(t) = \prod_i \psi_i(t)$ where $\psi(t)$ is the m.g.f of $\sum_i X_i$
* If the m.g.f of $X_1$ and $X_2$ are finite and identical for all $t \in (0^-, 0^+)$

$\hspace{1.0cm} \rightarrow f_1(X_1)$ and $f_2(X_2)$ must be identical

# Mean and median
**Median of the distribution of $X$**: any $m$ satisfying $P(X \leq m) \geq 1/2$ and $P(X \geq m) \geq 1/2$
* Interpretation: any $m$ such that $P(X < m) = P(X > m)$
* Multiple medians: a distribution may have multiple medians (i.e. shown before)

**Mean and median**:
* Median: more useful measure of the middle of the distribution than is the mean
* Mean: reflect the change in probability assignments
    * Explain: if we move some probability from a value $x$ larger than the median to some value $x_0$ larger than the median
    
    $\hspace{1.0cm} \rightarrow$ The median remains

**Properties of median**: if $r(\cdot)$ is a one-to-one function over $\{x|P(x) > 0\}$ and $m$ is the median of $X$

$\hspace{1.0cm} \rightarrow r(m)$ is a median of $r(X)$

# Covariance and correlation
**Covariance**: $\text{Cov }(X, Y) = E[(X - E(X))(Y - E(Y))]$ if it exists
* Idea: compute $E(X Y)$ where $X$ and $Y$ have zero-mean distributions
* Range of covariance: can be positive, negative or zero
* Another formulation: $\text{Cov }(X, Y) = E(X Y) - E(X) E(Y)$

**Correlation**: $\rho(X, Y) = \frac{\text{Cov }(X, Y)}{\sigma_X \sigma_Y}$
* Idea: compute $E(X Y)$ where $X$ and $Y$ have zero-mean and unit-variance distributions
* Motivation: 
    * $\text{Cov }(X, Y)$ is affected by the overall magnitude of $X$ and $Y$ (e.g. $\text{Cov }(a X, Y) = a \text{Cov }(X, Y)$)
    * Correlation isn't driven by arbitrary changes in scales
* Range of correlation: $[-1, 1]$

>**NOTE**: correlation measures only linear relationship
* Explain: correlation gives the goodness of the fit for the best possible linear function describing the relation between $X$ and $Y$

**Positively / negatively correlated and uncorrelated**:
* Positively correlated: $\rho(X, Y) > 0$
* Negatively correlated: $\rho(X, Y) < 0$
* Uncorrelated: $\rho(X, Y) = 0$

**Properties of covariance and correlation**:
* If $X$ and $Y$ are independent then $\text{Cov }(X, Y) = \rho(X, Y) = 0$

>**NOTE**: un-correlation doesn't mean independence

* If $Y = a X + b$ then $\rho(X, Y) = \text{sign }(a)$
    * Consequence: $|\rho(X, Y)| = 1$ implies that $X$ and $Y$ are linearly related
* $\text{Var }(X, Y) = \text{Var }(X) + \text{Var }(Y) + 2 \text{Cov }(X, Y)$
    * Generalization: $\text{Var }(\sum_i X_i) = \sum_i \text{Var }(X_i) + 2 \sum_{i < j} \text{Cov }(X_i, X_j)$

# Conditional expectation
## Conditional expectation
**Conditional expectation**: 
* Continuous case: $E(Y|x) = \int_{-\infty}^\infty y g_2(y|x) dy$
* Discrete case: $E(Y|x) = \sum_y y g_2(y|x)$

>**NOTE**: $E(Y|x)$ isn't uniquely defined for $\{x|f_1(x) = 0\}$ and some other $x$ (i.e. when $E(Y)$ is undefined)

**Conditional means as random variables**: $E(Y|x)$ is a random variable (i.e. a function of $X$) whose value when $X = x$ is $E(Y|x)$
* Usage: used as a useful concept when talking about conditional means
* Corollary: $E(Y|X)$ can have its own distribution derived from the distribution of $X$

**Law of total expectation**: $E(X) = E[E(X|Y)]$
* Other names:
    * Law of iterated expectations
    * Smoothing theorem

## Conditional variance
**Conditional variance**: $\text{Var }(Y|x) = E\{[Y - E(Y|x)]^2|x\}$

**Law of total variance**: $\text{Var }(Y) = E[\text{Var }(Y|X)] + \text{Var }[E(Y|X)]$
* Other names: 
    * Variance decomposition formula
    * Law of iterated variances
* Terminologies:
    * The expected value of the process variance: $E[\text{Var }(Y|X)]$
    * The variance of the hypothetical mean: $\text{Var }[E(Y|X)]$

**Law of total covariance**: $\text{Cov }(X, Y) = E[\text{Cov }(X, Y|Z)] + \text{Cov }[E(X|Z), E(Y|Z)]$
* Other names:
    * Covariance decomposition formula
    * Conditional covariance formula

# Prediction
**Task**:
* Assumptions:
    * $X$ and $Y$ are two random variables
* Task: predict the value of $Y$ given $X$ has been observed

**Error minimization**:
* Mean squared error (MSE) from of the prediction $d$: $E[(X - d)^2]$
    * Existence: when $\text{Var }(X)$ is finite
    * Minimizer: $E(X)$
* Mean absolute error (MAE) from the prediction $d$: $E(|X - d|)$
    * Existence: when $E(X)$ is finite
    * Minimizer: median of $X$

**Best predictions**: $d(X) = E(Y|X)$ minimizes $E\{[Y - d(X)]^2\}$
* If $X = x$ is observed, $d(X) = E(Y|x)$ is the best prediction (in terms of MSE)
    * MSE given $X = x$: $\text{Var }(Y|x)$
    * Overall MSE: $E[\text{Var }(Y|X)]$
* If no value of $X$ is observed, $d(X) = E(Y)$ is the best prediction (in terms of MSE)
    * MSE: $\text{Var }(Y)$

**MSE of the predictions**:
* Reduction in MSE using observation $X$: $\text{Var }[E(Y|X)] = \text{Var }(Y) - E[\text{Var }(Y|X)]$
    * Terminologies:
        * Explained variance: $\text{Var }[E(Y|X)]$
        * Unexplained variance: $E[\text{Var }(Y|X)]$
    * Usage: measure the usefulness of $X$ in predicting $Y$
* Overall MSE and MSE of a particular prediction when $X = x$:
    * MSE before observing $X$: $E[\text{Var }(Y|X)]$
    * MSE after observing $X = x$: $E[\text{Var }(Y|x)]$

---

# BONUS
* Mean and expectation:
    * "Mean" refers to the mean of a distribution
    * "Expectation" refers to the expected value of a random variable
* Expectation for non-negative distributions:
    * Integer-valued random variables:
        * Assumptions:
            * $X$ is a random variable taking only values $0, 1, 2, ...$
        * Conclusion: $E(X) = \sum_i P(X \geq i)$
        * Explain: $E(X) = \sum_{i=n}^1 i P(X = i)$
    * General non-negative random variable: $E(X) = \int_0^\infty [1 - F(x)] dx$
* Interquartile range: $F^{-1}(0.75) - F^{-1}(0.25)$
* Skewness of $X$: $E[(X - \mu)^3] / \sigma^3$
    * Usage: measure the asymmetry of a distribution

    >**NOTE**: $\frac{1}{\sigma^3}$ makes the skewness measure only the lack of symmetry, rather than the spread of the distribution
* $P(r(X, Y)|X = x) = P(r(x, Y)|X = x)$

# NEW WORD
* Inertia (n): quán tính
* Lever (n): đòn bẩy