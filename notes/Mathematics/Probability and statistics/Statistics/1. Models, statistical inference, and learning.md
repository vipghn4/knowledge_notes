<!-- TOC titleSize:1 tabSpaces:2 depthFrom:1 depthTo:6 withLinks:1 updateOnSave:1 orderedList:0 skip:0 title:1 charForUnorderedList:* -->
# Table of Contents
- [Table of Contents](#table-of-contents)
- [Models, statistical inference, and learning](#models-statistical-inference-and-learning)
  - [Introduction](#introduction)
  - [Fundamental concepts in inference](#fundamental-concepts-in-inference)
    - [Point estimation](#point-estimation)
    - [Confidence sets](#confidence-sets)
    - [Hypothesis testing](#hypothesis-testing)
- [BONUS](#bonus)
<!-- /TOC -->

<!--7. Models, statistical inference, and learning-->
# Models, statistical inference, and learning
## Introduction
**Statistical inference (or learning)**: the process of using data to infer the distribution which generated the data
* Formal: given observations $X_1, ..., X_n \sim F$, infer (or estimate, or learn) $F$ or some feature of $F$ (e.g. mean, variance, etc.)

**Statistical model**: a set of distributions (or a set of sensities) ${\cal{F}}$
* Parametric model: a set ${\cal{F}}$ which can be parametrized by a finite number of parameters
    * Parameter space: the set of values which can be taken by the parameters $\theta$
    * Nuisance parameters: elements of $\theta$ which we are interested in
* Non-parametric model: a set ${\cal{F}}$ which cannot be parametrized by a finite numer of parameters

**Statistical functional**: any function of $F$ (c.d.f)

**Regression, prediction, and classification**:
* Assumptiosn:
    * $\{(X_i, Y_i)\}_i$ is the observed pairs of data
* Conclusion:
    * Predictor: $X$
        * Other names: regressor, feature, independent variable
    * Outcome: $Y$
        * Other names: response variable, dependent variable
    * Regression function: $r(x) = E(Y|X = x)$
    * Prediction: the goal of predicting $Y$ for a new $X$
        * Classification: when $Y$ is discrete
        * Regression: when $Y$ is continuous
            * Other name: curve estimation

**Frequentists and Bayesians**: two dominant approaches to statistical inference

## Fundamental concepts in inference
### Point estimation
**Point estimation**: provide a single best guess of some quantity of interest (e.g. parameters, a c.d.f, a p.d.f, or a prediction of a future value)
* Point estimate of $\theta$: $\hat{\theta}_n = g(X_1, ..., X_n)$

**Error measurements**:
* Bias of point estimate $\hat{\theta}_n$: $\text{bias}(\hat{\theta}_n) = E_\theta(\hat{\theta}_n) - \theta$
* Standard error: the standard deviation of $\hat{\theta}_n$
    * Compute standard error: it's often impossible to compute the standard error, but we can usually estimate it
* Mean squared error: $\text{MSE} = E_\theta[(\hat{\theta}_n - \theta)^2]$
    * MSE and other error measures: $\text{MSE} = \text{bias}(\hat{\theta}_n)^2 + \text{Var}_\theta(\hat{\theta}_n)$
        * Explain: 
            * $E_\theta[(E_\theta(\hat{\theta}_n) - \theta) (\hat{\theta}_n - E_\theta(\hat{\theta}_n))] = 0$ (i.e. orthogonal random variables)
            * $[\hat{\theta}_n - E_\theta(\hat{\theta}_n)] + [E_\theta(\hat{\theta}_n) - \theta] = \hat{\theta}_n - \theta$
        * Interpretation: similar to the law of total variance

**Desired properties of $\hat{\theta}_n$**: 
* Unbiased estimator: $\hat{\theta}_n$ is unbiased if $E(\hat{\theta}_n) = \theta$
    * Importance of unbiasness: used to receive much attention but it's not very important these days
* Consistent estimator: $\hat{\theta}_n$ is consistent $\hat{\theta}_n \overset{p}{\to} \theta$
    * Importance of consistency: a reasonable requirement for estimators
    * Theorem: if $\text{bias} \to 0$ and $\text{standard-error} \to 0$ as $n \to \infty$, $\hat{\theta}_n$ is consistent
        * Explain: $\text{MSE} \to 0$ implies $\hat{\theta}_n \overset{p}{\to} \theta$ due to Chebyshev inequality
        * Intuition: $\text{MSE} \to 0$ means $\hat{\theta}_n \to \theta$ in quadratic mean

### Confidence sets
**Confidence interval**:
* $1 - \alpha$ confidence interval: $C_n = (a, b)$ where
    * $a = a(X_1, ..., X_n)$
    * $b = b(X_1, ..., X_n)$
    * $P_\theta(\theta \in C_n) \geq 1 - \alpha$ $\forall \theta \in \Theta$
* Coverage of $C_n$: $1 - \alpha$
    * Common coverage: $1 - \alpha = 0.95$ (i.e. $\alpha = 0.05$)

>**NOTE**: $C_n$ is random and $\theta$ is fixed

* Confidence set: generalization of confidence interval when $\theta \in \textbf{R}^d$ 

**Confusion**: a confidence interval isn't a probability statement about $\theta$ since $\theta$ is fixed
* Right interpretation of confidence interval: if one repeats the experiment over and over, $C_n$, which is random, will contain $\theta$ for $95\%$ of the time
    * Drawbacks: we don't usually repeat the same experiment over times

    $\hspace{1.0cm} \rightarrow$ This is useless
* Reasonable interpretation of confidence interval: if one constructs confidence intervals of a sequence of unrelated parameters $\{\theta_i\}$, then $95\%$ of $\{C_i\}$ contain the parameters

### Hypothesis testing
**Hypothesis testing**: decide if the data provide sufficient evidence to reject null hypothesis or not 
* Null hypothesis $H_0$: the default theory
* Alternative hypothesis $H_1$: the alternative theory of $H_0$

---

# BONUS
* Sobolev space: ${\cal{F}}_\text{SOB} = \{f:\int [f''(x)]^2 dx < \infty\}$
* Sampling distribution: distribution of $\hat{\theta}_n$
* Normal asymptotic estimator: $\hat{\theta}_n$ is asymptotically normal if $\frac{\hat{\theta}_n - \theta}{\text{standard-error}} \to {\cal{N}}(0, 1)$ (in distribution)
    * Another interpretation: $\hat{\theta}_n \to {\cal{N}}(\theta, \text{standard-error}^2)$ (in distribution)
* Asymptotic confidence interval:
    * Pointwise asymptotic confidence interval: $\lim \inf_{n\to\infty} P_\theta(\theta \in C_n) \geq 1 - \alpha$ $\forall \theta \in \Theta$
        * Intuition: $C_n$ converges to be a $1-\alpha$ confidence interval for all $\theta \in \Theta$
    * Uniform asymptotic confidence interval: $\lim \inf_{n \to \infty} \inf_{\theta \in \Theta} P_\theta(\theta \in C_n) \geq 1 - \alpha$
        * Intuition: $C_n$ converges to be $1 - \alpha$ confidence interval for all $\theta \in \Theta$ at the same time