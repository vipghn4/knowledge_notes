<!-- TOC titleSize:1 tabSpaces:2 depthFrom:1 depthTo:6 withLinks:1 updateOnSave:1 orderedList:0 skip:0 title:1 charForUnorderedList:* -->
# Table of Contents
- [Table of Contents](#table-of-contents)
- [Fisher information](#fisher-information)
  - [Definition and properties of Fisher information](#definition-and-properties-of-fisher-information)
  - [Information inequality](#information-inequality)
  - [Application of Fisher information](#application-of-fisher-information)
  - [Efficient estimators](#efficient-estimators)
  - [Properties of MLE for large samples](#properties-of-mle-for-large-samples)
  - [Matrix form of Fisher information](#matrix-form-of-fisher-information)
  - [Observed Fisher information](#observed-fisher-information)
- [Appendix](#appendix)
  - [Concepts](#concepts)
  - [Discussions](#discussions)
<!-- /TOC -->

# Fisher information
## Definition and properties of Fisher information
**Informal definition**. A way of measuring the amount of information, which an observable $X$ carries about an unknown parameter $\theta$ of a distribution modeling $X$
* *Assumptions*.
    * $f(X;\theta)$ is the p.d.f for $X$ conditioned on $\theta$
* *Informal definition*. Fisher information describes the probability that we observe a given outcome of $X$, given a known value of $\theta$
    * If $f$ is sharply peaked with respect to changes in $\theta$
        
        $\to$ It is easy to indicate the "correct" value of $\theta$ from the data
        * *Explain*. $X$ provides a lot of information about $\theta$
    * If $f$ is flat and spread-out, then it would take many samples of $X$ to estimate the actual "true" value of $\theta$
* *Application*. 
    * Choose among several sampling plans, i.e. which plan results in the most infomration about $\theta$
    * Provide a lower bound on the variance of unbiased estimators of $\theta$, i.e. via Cramer-Rao inequality

**Score (or informant)**. The partial derivative w.r.t $\theta$ of the log-likelihood, i.e. $\frac{\partial}{\partial \theta} \log f(X;\theta)$

>**NOTE**. Score is a function of the observations, which are subject to sampling errors

* *Interpretation*. Evaluated at a particular point of the parameter vector, the score indicates the steepness of the log-likelihood function
    
    $\to$ Score tells thereby the sensitivity to infinitesimal changes to the parameter values
* *Usage*. If the log-likelihood function is continuous over the parameter space, the score will vanish at a local maximum or minimum
    
    $\to$ This is used in MLE to find the parameter values that maximize the likelihood function
    * *Explain*. 
        * Consider fixing the observations $\mathbf{X}=(X_1,\dots,X_n)$ where $n\to\infty$, then $\frac{\partial}{\partial \theta} \log f(X;\theta)$ is a function of $\theta$

            $$\frac{\partial}{\partial\theta} \log f_n(\mathbf{X};\theta)\to E(\frac{\partial}{\partial\theta} \log f(X;\theta)|\theta)$$
        
        * Given that $\theta$ is the parameter we want to estimate, we have that

            $$\forall\hat{\theta}\in\Omega,\log f_n(\mathbf{X};\theta)\geq \log f_n(\mathbf{X};\hat{\theta})$$
        
        * Hence, we can conclude that

            $$\frac{\partial}{\partial\theta} \log f_n(\mathbf{X};\hat{\theta}) = 0$$

            is a criterion for finding MLE, and this leads to the following theorem
* *Theorem*. If $\theta$ is the true parameter, then, if derivatives can be passed under the integrals,

    $$E[\frac{\partial}{\partial \theta} \log f(X;\theta)|\theta] = 0$$

    * *Proof*.

        $$\begin{aligned}
        E[\frac{\partial}{\partial \theta} \log f(X;\theta)|\theta] &= \int_\mathbb{R} \frac{\partial}{\partial \theta} f(x;\theta) \frac{1}{f(x;\theta)} f(x;\theta) dx\\
        &= \frac{\partial}{\partial \theta} \int_\mathbb{R} f(x;\theta) dx\\
        &= \frac{\partial}{\partial \theta} 1 = 0
        \end{aligned}$$

* *Theorem*. If $X_1,\dots,X_n$ are independent variables then

    $$\frac{\partial}{\partial \theta} \log f(X_1,\dots,X_n;\theta) = \sum_{i=1}^n \frac{\partial}{\partial \theta} \log f(X_i;\theta)$$

    * *Interpretation*. Observing more data gives us more information about $\theta$

**Fisher information**. The variance of the score, i.e.

$$\mathcal{I}(\theta) = E\bigg[\bigg( \frac{\partial}{\partial \theta} \log f(X;\theta) \bigg)^2|\theta\bigg] = \int_\mathbb{R} \bigg(\frac{\partial}{\partial \theta} \log f(x;\theta)\bigg)^2 f(x;\theta) dx\geq 0$$

* *Interpretation*. A random variable carrying high Fisher information implies that the absolute value of the score is often high
* *Theorem*. If $f(x;\theta)$ is twice differentiable w.r.t $\theta$, then

    $$\mathcal{I}(\theta) = -E\bigg[\frac{\partial^2}{\partial \theta^2} \log f(X;\theta)|\theta\bigg]$$

    * *Interpretation*. $\mathcal{I}(\theta)$ can be seen as the curvature of the support curve, i.e. the graph of log-likelihood
    * *Consequence*. Near the maximum likelihood estimate
        * Low Fisher information indicates that the maximum is shallow and there are many nearby values with a similar log-likelihood
        * High Fisher information indicates that the maximum is sharp

**Fisher information in a random sample**.
* *Assumptions*.
    * $\mathbf{X}=(X_1,\dots,X_n)$ is a random sample from a distribution, whose p.f or p.d.f is $f(x;\theta)$
    * $\theta$ must lie in an open interval $\Omega$ of the real line
    * $f_n(\mathbf{x};\theta)$ is the joint p.f or p.d.f of $\mathbf{X}$
    * The set of $\mathbf{x}$ so that $f_n(\mathbf{x};\theta) > 0$ is the same for all $\theta$
    * $\log f_n(\mathbf{x};\theta)$ is twice differentiable w.r.t $\theta$
* *Fisher information in the random sample $\mathbf{X}$*.

    $$\mathcal{I}_n(\theta) = E\bigg[\frac{\partial}{\partial \theta} \log f_n(\mathbf{X};\theta)|theta\bigg]$$

* *Continuous distribution case*. $\mathcal{I}_n(\theta)$ in the entire sample is given by

    $$\mathcal{I}_n(\theta) = \int_\mathbb{R} \dots \int_\mathbb{R} \bigg(\frac{\partial}{\partial \theta} f_n(\mathbf{x};\theta)\bigg)^2 f_n(\mathbf{x};\theta) dx_1 \dots dx_n$$

* *Discrete distribution case*. $\mathcal{I}_n(\theta)$ is similar to continuous case, with integral replaced by summation
* *Theorem*. If derivatives can be passed under the integrals, then $\mathcal{I}_n(\theta)$ can be given as

    $$\mathcal{I}_n(\theta) = \text{Var}\bigg(\frac{\partial}{\partial \theta} f_n(\mathbf{X};\theta)|\theta\bigg)$$

    or

    $$\mathcal{I}_n(\theta) = -E\bigg(\frac{\partial^2}{\partial \theta^2} f_n(\mathbf{X};\theta)|\theta\bigg)$$

**Theorem**. Fisher information in a random sample of $n$ observations is $n$ times the Fisher information in a single observation, i.e.

$$\mathcal{I}_n(\theta) = n \mathcal{I}(\theta)$$

## Information inequality
**Brief**. Fisher information can be used to determine a lower bound for the variance of an arbitrary estimator of the parameter $\theta$ in a given problem

**Cramer-Rao infomration inequality**.
* *Assumptions*.
    * $\mathbf{X}=(X_1,\dots,X_n)$ is a random sample from a distribution with p.d.f $f(x;\theta)$
    * $T=r(\mathbf{x})$ is a statistic with finite variance
    * $E(T|\theta)$ is a differentiable function of $\theta$
* *Cramer-Rao infomration inequality*.

    $$\text{Var}(T|\theta) \geq \frac{1}{n\mathcal{I}_n(\theta)} \bigg(\frac{\partial E(T|\theta)}{\partial \theta}\bigg)^2$$

* *Equality case*. Happen if and only if there exist functions $u(\theta)$ and $v(\theta)$ satisfying

    $$T=u(\theta) \frac{\partial}{\partial\theta} \log f_n(\mathbf{X};\theta) + v(\theta)$$

* *Consequence*. Minimizing $\text{Var}(T|\theta)$ means maximizing $\mathcal{I}_n(\theta)$

    $\to$ This is used mainly in optimal design of experiments

**Cramer-Rao lower bound on the variance of an unbiased estimator**. Let $T$ be an unbiased estimator of $\theta$, then

$$\text{Var}(T|\theta) \geq \frac{1}{n\mathcal{I}(\theta)}$$

## Application of Fisher information
**Studying customer arrivals example**. A store owner is learning about customer arrivals

$\to$ She models arrivals during the day as a Poisson process with unknown rate $\theta$
* *Sampling plans*. The owner thinks of two sampling plans to obtain information about customer arrivals
    * *Plan 1*. Choose a fixed number $n$ of customers, and see how long, $X$, it takes until $n$ customers arrive
        * *Explain*. The owner can observe a gamma random variable $X$ with parameters $n$ and $\theta$
    * *Plan 2*. Observe for a fixed length of time $t$, and count how many customers, $Y$, arrive during that time
        * *Explain*. The owner can observe a Poisson random variable $Y$, with mean $t\theta$

**Plan analysis**. The owner can compute the Fisher information in each random variable, i.e. $\mathcal{I}_Y(\theta)$ and $\mathcal{I}_X(\theta)$

$$\mathcal{I}_X(\theta) = \frac{n}{\theta^2},\quad \mathcal{I}_Y(\theta) = \frac{t}{\theta}$$

$\to$ Which is larger will depend on the particular values of $n,t,\theta$, where $n,t$ are chosen by the owner and $\theta$ is unknown
* *Equality*. For $\mathcal{I}_X(\theta)=\mathcal{I}_Y(\theta)$ to happen, it is necessary and sufficient that $n=t\theta$
* *Consequence*. The equality makes intuitive sense, i.e.
    * If the owner chooses plan 1, then the length of time $T$ it takes to observe $n$ customers will be random
    * In fact, $T=X$, and $E(T\theta) = n$, hence, if the owner is comparing sampling plans, which are expected to observe the same numbers of customers, or observe for the same length of time

        $\to$ The two plans should provide the same amount of information

**Another way to choose between sampling plans**. 
* *Idea*. The owner compares the estimators that she will use to make inferential statements about customer arrivals, i.e.
    * *Option 1*. Estimate $\theta$, i.e. the rate of customer arrivals

        $\to$ The unbiased estimator is $(n-1)/X$
    * *Option 2*. Estimate $1/\theta$, i.e. the mean time between customer arrivals

        $\to$ The unbiased estimator is $X/n$
* *Plan analysis*. Suppose that the owner can obtain unbiased estimators for both $\theta$ and $1/\theta$

    $\to$ She can use Cramer-Rao lower bound to see which estimator has low variance, hence better
    * *Observation*. 
        * For option 1, the variance of $(n-1)/X$ is strictly larger than the lower bound
        * For option 2, the variance of $X/n$ equals to the lower bound
    * *Consequence*. Option 2 turns out to be less noisy, hence better

## Efficient estimators
**Brief**. An estimator, whose variance equals the Cramer-Rao lower bound makes the most efficient use of the data $\mathbf{X}$ in some sense

**Efficient estimator**. An estimator $T$ is an efficient estimator of its expectation $E(T|\theta)$ if there is equality in the Cramer-Rao inequality for every $\theta\in\Omega$
* *Existence of efficient estimator*. Given a problem, there may be no efficient estimator

**Theorem**. $T$ is an efficient estimator if and only if $T$ is a linear function of $\frac{\partial}{\partial\theta} \log f_n(\mathbf{X};\theta)$
* *Explain*. Since $T$ is an estimator, it cannot involve $\theta$

    $\to$ For $T$ to be efficient, it must be possible to find $u(\theta)$ and $v(\theta)$ so that $\theta$ is canceled from the equation

    $$T=u(\theta) \frac{\partial}{\partial\theta} \log f_n(\mathbf{X};\theta) + v(\theta)$$

    and the value of $T$ will depend only on $\mathbf{X}$, not on $\theta$

**Unbiased estimators with minimum variance**.
* *Assumptions*.
    * $T$ is an efficient estimator of its expectation $E(T|\theta)$ for a given problem
    * $T_1$ is any other unbiased estimator of $E(T|\theta)$
* *Conclusion*. For every $\theta\in\Omega$

    $$\frac{1}{n\mathcal{I}(\theta)} = \text{Var}(T|\theta) \leq \text{Var}(T_1|\theta)$$

## Properties of MLE for large samples
**Brief**. For each sample size $n$, let $\hat{\theta}_n$ be the MLE of $\theta$

$\to$ If $n$ is large, the distribution of $\hat{\theta}_n$ is approximately the normal distribution with mean $\theta$ and variance $\frac{1}{n\mathcal{I}(\theta)}$
* *Consequence*. This statement is applicable to machine learning

**Asymptotic distribution of an efficient estimator**.
* *Assumptions*.
    * $T$ is an efficient estimator of $E(T|\theta)$
    * $\frac{\partial}{\partial\theta} E(T|\theta)$ is never zero
* *Conclusion*. The asymptotic distribution of

    $$\big(\frac{\partial}{\partial\theta} E(T|\theta)\big)^{-1} \sqrt{n \mathcal{I}(\theta)} [T - E(T|\theta)]$$

    is the standard normal distribution

## Matrix form of Fisher information
**Matrix form of Fisher information**.
* *Assumptions*.
    * $\theta = \begin{bmatrix} \theta_1 & \cdots \theta_N \end{bmatrix}^T \in \mathbb{R}^N$ is the parameter vector
* *Fisher information matrix (FIM)*. $\mathcal{I}(\theta) \in\mathbb{R}^{N\times N}$ where

    $$[\mathcal{I}(\theta)]_{ij} = E\bigg[ \bigg(\frac{\partial}{\partial\theta_i} \log f(X;\theta) \cdot \frac{\partial}{\partial\theta_j} \log f(X;\theta)\bigg) \bigg| \theta \bigg]$$

    * *Properties*. The FIM is a square positive semi-definite matrix
* *Theorem*. Under certain regularity conditions, the FIM can be written as

    $$[\mathcal{I}(\theta)]_{ij} = -E\bigg[ \frac{\partial^2}{\partial\theta_i \partial\theta_j} \log f(X;\theta) \bigg| \theta\bigg]$$

* *Further topics*.
    * FIM canbe derived as the Hessian of the relative entropy 
    * Fisher information metric, i.e. a Riemannian metric on the $N$-dimensional parameter space
    * FIM can be understood as a metric induced from the Euclidean metric, after appropriate change of variable

**Orthogonal parameters**. $\theta_i$ and $\theta_j$ are orthogonal if $[\mathcal{I}(\theta)]_{ij} = 0$
* *Interpretation*.
* *Consequence of orthogonality*. The MLEs of $\theta_i$ and $\theta_j$ are independent and can be calculated separatedly

    $\to$ When dealing with research problems, it is very common for the researcher to invest some time searching for an orthogonal parametrization of the densities involved in the problem

## Observed Fisher information
**Observed Fisher information**. The negatiev of the Hessian matrix of log-likelihood
* *Assumptions*.
    * $X_1,\dots,X_n$ are i.i.d random variables with density $f(X;\theta)$
    * $\theta$ is a possibly unknown vector
* *Log-likelihood of $\theta$ given $X_1,\dots,X_n$*.

    $$l(\theta|X_1,\dots,X_n)=\sum_{i=1}^n \log f(X_i|\theta)$$

* *Observed information matrix at $\theta^*$*. A sample-based version of the Fisher information matrix

    $$\begin{aligned}
    \mathcal{J}(\theta^*) &= -\nabla\nabla^T l(\theta)|_{\theta=\theta^*}\\
    &=-\left.\left({\begin{array}{cccc}{\tfrac {\partial ^{2}}{\partial \theta _{1}^{2}}}&{\tfrac {\partial ^{2}}{\partial \theta _{1}\partial \theta _{2}}}&\cdots &{\tfrac {\partial ^{2}}{\partial \theta _{1}\partial \theta _{p}}}\\{\tfrac {\partial ^{2}}{\partial \theta _{2}\partial \theta _{1}}}&{\tfrac {\partial ^{2}}{\partial \theta _{2}^{2}}}&\cdots &{\tfrac {\partial ^{2}}{\partial \theta _{2}\partial \theta _{p}}}\\\vdots &\vdots &\ddots &\vdots \\{\tfrac {\partial ^{2}}{\partial \theta _{p}\partial \theta _{1}}}&{\tfrac {\partial ^{2}}{\partial \theta _{p}\partial \theta _{2}}}&\cdots &{\tfrac {\partial ^{2}}{\partial \theta _{p}^{2}}}\\\end{array}}\right)\ell (\theta )\right|_{\theta =\theta ^{*}}
    \end{aligned}$$

* *Alternative definition*. Observed information can be modeled in terms of the parameters' posterior probability $p(\theta|x)$
    
    $$I(\theta) = -\frac{d^2}{d\theta^2} \log p(\theta|x)$$

* *Fisher information*. The expected value of the observed information, givne a single observation $X$ distributed according to a hypothetical model with parameter $\theta$, i.e.

    $$\mathcal{I} = E(\mathcal{J}(\theta))$$

**Scoring algorithm (Fisher's scoring algorithm)**. A form of Newton's method used in statistics to solve maximum likelihood equations numerically
* *Derivation*.
    * *Assumptions*.
        * $Y_1,\dots,V_n$ are i.i.d random variables with p.d.f $f(y;\theta)$
        * $\theta^*$ is the MLE of $\theta$, which we wish to calculate
    * *Initialization*. Pick a starting point $\theta_0$
    * *Iteration*.
        * *Observation*.
            * Consider a Taylor expansion of the score function $V(\theta)$ about $\theta_0$

                $$V(\theta)\approx V(\theta_0) - \mathcal{J}(\theta_0) (\theta - \theta_0)$$

                where $\mathcal{J}(\theta_0)$ is the observed information matrix at $\theta_0$
            * By setting $\theta=\theta^*$, using that $V(\theta^*)=0$ and rearranging gives us

                $$\theta^*\approx\theta_0 + \mathcal{J}^{-1}(\theta_0) V(\theta_0)$$
        
        * *Conclusion*. Under certain regularity conditions, it can be shown that $\theta_m\to\theta^*$, where

            $$\theta_m = \theta_{m-1} + \mathcal{J}^{-1}(\theta_{m-1}) V(\theta_{m-1})$$

* *Fisher scoring algorithm*. $\mathcal{J}(\theta)$ is replaced by $\mathcal{I}(\theta) = E[\mathcal{J}(\theta)]$, i.e.

    $$\theta_m = \theta_{m-1} + \mathcal{I}^{-1}(\theta_{m-1}) V(\theta_{m-1})$$

# Appendix
## Concepts
**Desired properties of an estimator (under Fisher information)**. Unbiased estimator with low variance, i.e. no underfit and no overfit in machine learning terminology

$\to$ Fisher information is used to choose sampling plan to obtain as much information as possible, and lowest variance as possible

**Traditional experiment design methods**. Traditionally, statisticians have evaluated estimators and designs by considering some summary statistic of the covariance matrix, i.e. of an unbiased estimator, usually with positive real values, e.g. the determinant or matrix trace

**Bias-variance tradeoff**.
* *Assumptions*.
    * $Y = f(X) + \epsilon$ is the model underlying the data
        * $\epsilon$ is a noise element with zero mean and variance $\sigma^2$
    * $\hat{Y} = \hat{f}(X)$ is the predicted model
    * The evaluation metric is MSE
* *Bias-variance tradeoff*.

    $$E[(y - \hat{f}(x))^2] = \text{Bias}[\hat{f}(x)]^2 + \text{Var}[\hat{f}(x)] + \sigma^2$$

    where 

    $$\text{Bias}[\hat{f}(x)] = E[\hat{f}(x)] - f(x),\quad \text{Var}[\hat{f}(x)] = E[(E[\hat{f}(x)] - \hat{f}(x))^2]$$

* *Derivation*. Use the formula

    $$E(X^2) = E(X)^2 + \text{Var}(X)$$

* *Desired estimator*. Unbiased estimator with low variance
    * *High bias and low variance*. Imply underfit
    * *High variance and low bias*. Imply overfit

## Discussions
**Why log likelihood is used instead of likelihood itself**. For computational convenience
* *Main reason*. Theoretically, the math is a little simpler for optimizing a sum instead of a product