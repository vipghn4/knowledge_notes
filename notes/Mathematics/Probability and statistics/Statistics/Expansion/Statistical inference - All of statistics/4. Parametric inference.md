<!-- TOC titleSize:1 tabSpaces:2 depthFrom:1 depthTo:6 withLinks:1 updateOnSave:1 orderedList:0 skip:0 title:1 charForUnorderedList:* -->
# Table of Contents
- [Table of Contents](#table-of-contents)
- [Parametric inference](#parametric-inference)
  - [Introduction](#introduction)
  - [Method of moments](#method-of-moments)
  - [Maximum likelihood estimator](#maximum-likelihood-estimator)
  - [The Delta method](#the-delta-method)
  - [Multi-parameter models](#multi-parameter-models)
    - [Fisher information matrix and MLE](#fisher-information-matrix-and-mle)
    - [Multivariate Delta method](#multivariate-delta-method)
    - [The parametric boostrap](#the-parametric-boostrap)
- [Technical appendix](#technical-appendix)
  - [Exponential families](#exponential-families)
- [NEW WORD](#new-word)
<!-- /TOC -->

# Parametric inference
## Introduction
**Parametric model**: ${\cal{F}} = \{f(x; \theta): \theta \in \Theta\}$ where $\Theta \subset \textbf{R}^k$

**Parametric inference**: the problem of estimating $\theta$
* How can we know that the distribution generating data is in some parametric model:
    * Answer: we rarely have such knowledge

    $\hspace{1.0cm} \rightarrow$ This is why non-parametric methods are preferable
* Reasons for studying parametric models:
    * In some cases, we know that a parametric model provides a reasonable approximation
    * The inferential concepts for parametric models provide background for understanding for certain non-parametric methods

**Parameter of interest**:
* Parameter of interest: parameters we want to estimate
* Nuisance parameters: parameters, which is
    * Not of interest
    * Required for the analysis of the parameters of interest

## Method of moments
**Consistency**:
* Assumptions:
    * $\hat{\theta}_n$ is the method of moments estimator
* Conditions of convergence:
* Convergence: under the conditions of convergence,
    * $\hat{\theta}_n$ exists with probability tending to $1$
    * $\hat{\theta}_n \overset{p}{\to} \theta$
    * $\hat{\theta}_n$ is asymptotically normal
        * Formal: $\sqrt{n} (\hat{\theta}_n - \theta) \sim {\cal{N}}(0, \Sigma)$
            * $\Sigma = g E_\theta(Y Y^T) g^T$
            * $Y = (X^1, ..., X^k)^T$
            * $g = (g_1, ..., g_k)$ where $g_i = \frac{\partial \alpha^{-1}_i(\theta)}{\partial \theta}$
                * $\alpha_i(\theta) = E_\theta(X^i)$
    
    >**NOTE**: the last convergence statement can be used to find standard errors and confidence intervals
    * Recommended easier way: the boostrap

## Maximum likelihood estimator
**Properties of maximum likelihood estimator**: under certain regularity conditions and smoothness conditions on $f(x; \theta)$,
* Consistency: $\hat{\theta}_n \overset{p}{\to} \theta$
* Equivariance: if $\hat{\theta}_n$ is the MLE of $\theta$ then $g(\hat{\theta}_n)$ is the MLE of $g(\theta)$
* Asymptotically normal: $\sqrt{n} (\hat{\theta}_n - \theta)/\hat{\text{se}} \to {\cal{N}}(0, 1)$ (in distribution) where $\hat{\text{se}}$ can be computed analytically
* Asymptotically optimal or efficient: among all well behaved estimators

$\hspace{1.0cm} \rightarrow$ The MLE has the smallest variance, at least for large samples
* The MLE is approximately the Bayes estimator

>**NOTE**: for sufficiently complicated problems

$\hspace{1.0cm} \rightarrow$ These properties won't hold and the MLE won't be a good estimator

## The Delta method
**The Delta method**:
* Assumptions:
    * $\hat{\tau}_n = g(\hat{\theta}_n)$ is the MLE of $\tau$
    * $\hat{\text{se}}(\hat{\tau}_n) = |g'(\hat{\theta})| \hat{\text{se}}(\hat{\theta}_n)$
    * $C_n = (\hat{\tau}_n - z_{\alpha/2} \hat{\text{se}}(\hat{\tau}_n), \hat{\tau}_n + z_{\alpha/2} \hat{\text{se}}(\hat{\tau}_n))$
* Conclusion:
    * If $g$ is differentiable and $g'(\theta) \neq 0$
    
    $\hspace{1.0cm} \rightarrow \frac{\hat{\tau}_n - \tau}{\hat{\text{se}}(\hat{\tau}) / \sqrt{n}} \to {\cal{N}}(0, 1)$ (in distribution)
    * Hence, $P_\theta(\tau \in C_n) \to 1-\alpha$ as $n \to \infty$

**Usage**: find the distribution of the MLE

## Multi-parameter models
### Fisher information matrix and MLE
**Fisher information matrix**:
* Assumptions:
    * $\theta = (\theta_1, ..., \theta_k)$ is the parameters to estimate
    * $\hat{\theta} = (\hat{\theta}_1, ..., \hat{\theta}_k)$ is the MLE of $\theta$
    * $l_n = \sum_i \log f(X_i; \theta)$
    * $H$ is the Hessian of $l_n$ w.r.t $\theta$
* Fisher information matrix: $I_n(\theta) = -\begin{bmatrix} E_\theta(H_{ij}) \end{bmatrix}$

**Theorem**:
* Assumptions:
    * $J_n(\theta) = I^{-1}_n(\theta)$
    * $\hat{\text{se}}_j^2 = J_n(j, j)$
* Conclusion: under appropriate regularity conditions,
    * $\sqrt{n} (\hat{\theta} - \theta) \to {\cal{N}}(0, J_n)$ (in distribution)
    * $\frac{\hat{\theta}_j - \theta_j}{\hat{\text{se}}_j / \sqrt{n}} \to {\cal{N}}(0, 1)$ (in distribution)
    * The approximate covariance of $\hat{\theta}_j$ and $\hat{\theta}_k$ is $J_n(j, k)$

### Multivariate Delta method
**Multivariate Delta method**:
* Assumptions:
    * $\tau = g(\theta_1, ..., \theta_k)$
    * $\nabla g$ is the gradient of $g$ w.r.t $\theta$
        * $\nabla g \neq 0$ when evaluated at $\hat{\theta}$
    * $\hat{\tau} = g(\hat{\theta})$
    * $\hat{\text{se}}(\hat{\tau}) = \sqrt{(\hat{\nabla} g)^T \hat{J}_n (\hat{\nabla} g)}$
* Conclusion:
    * $\frac{\hat{\tau} - \tau}{\hat{\text{se}}(\hat{\tau}) / \sqrt{n}} \to {\cal{N}}(0, 1)$ (in distribution)

### The parametric boostrap
**Estimate standard errors and confidence for parametric models using boostrap**:
* Non-parametric boostrap: we sample $X_1^*, ..., X_n^*$ from the empirical distribution $\hat{F}_n$
* Parametric boostrap: we sample $X_1^*, ..., X_n^*$ from $f(x; \hat{\theta}_n)$ where $\hat{\theta}_n$ is some estimator

**Boostrap and delta method**:
* Boostrap: much easier than delta method
* Delta method: give a closed form expression for the standard error

# Technical appendix
## Exponential families
**One-parameter exponential family**: $\{f(x; \theta): \theta \in \Theta\}$ where $f(x; \theta)$ can be written as $f(x; \theta) = h(x) \exp[\eta(\theta) T(x) - B(\theta)]$
* Other form: $f(x; \eta) = h(x) \exp[\eta T(x) - A(\eta)]$ 
    * $\eta = \eta(\theta)$
    * $A(\eta) = \log \int h(x) \exp[\eta T(x)] dx$

**Quantities**:
* Natural sufficient statistic: $T(X)$
* Natural parameter: $\eta = \eta(\theta)$

**Theorem**: if $X_1, ..., X_n$ are i.i.d from a exponential family then $f(x^n; \theta)$ is an exponential family
* Consequence: $\sum_i T(X_i)$ is sufficient

**Theorem**:
* Assumptions:
    * $X$ have density in an exponential family
* Conclusion:
    * $E[T(X)] = A'(\eta)$ and $\text{Var}[T(X)] = A''(\eta)$

**Multi-parameter exponential family**:
* Assumptions:
    * $\theta = (\theta_1, ..., \theta_k)$
* Multi-parameter exponential family: $\{f(x; \theta):\theta \in \Theta\}$ where $f$ can be written as $f(x; \theta) = h(x) \exp[\sum_{i=1}^k \eta_i(\theta) T_i(x) - B(\theta)]$
* Theorems:
    * $T = (T_1, ..., T_k)$ is sufficient
    * $n$ i.i.d samples from $f$ has exponential form with sufficient statistic $(\sum_i T_1(X_i), ..., \sum_i T_k(X_i))$

---

# NEW WORD
* Nuisance (adj): phiền toái