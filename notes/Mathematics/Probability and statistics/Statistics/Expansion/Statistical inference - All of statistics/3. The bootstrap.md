<!-- TOC titleSize:1 tabSpaces:2 depthFrom:1 depthTo:6 withLinks:1 updateOnSave:1 orderedList:0 skip:0 title:1 charForUnorderedList:* -->
# Table of Contents
- [Table of Contents](#table-of-contents)
- [The bootstrap](#the-bootstrap)
  - [Simulation](#simulation)
  - [Boostrap variance estimation](#boostrap-variance-estimation)
  - [Boostrap confidence intervals](#boostrap-confidence-intervals)
    - [Normal interval](#normal-interval)
    - [Pivotal intervals](#pivotal-intervals)
    - [Percentile intervals](#percentile-intervals)
- [Technical appendix](#technical-appendix)
  - [The Jackknife](#the-jackknife)
<!-- /TOC -->

# The bootstrap
**The bootstrap**: a non-parametric method for estimating standard errors and computing confidence intervals

## Simulation
**Simulation**: 
* Step 1: draw an i.i.d sample $Y_1, ..., Y_B$ from a distribution $G$
* Step 2: estimate $E[h(Y)]$ by $\frac{1}{B} \sum_i h(Y_i)$

**Consistency**: due to the law of large numbers, $\frac{1}{B} \sum_i h(Y_i) \to E[h(Y)]$

**Application**: estimate $\text{Var}(Y)$ using the sample variance of simulated values

## Boostrap variance estimation
**Task**: simulate from the distribution of $T_n = g(X_1, ..., X_n)$ when the data are assumed to have distribution $\hat{F}_n$

**Idea**:
* Real world: $F \implies X_1, ..., X_n \implies T_n = g(X_1, ..., X_n)$
* Bootstrap world: $\hat{F}_n \implies X^*_1, ..., X^*_n \implies T^*_n = g(X^*_1, ..., X^*_n)$

**Procedure**:
* Step 1: simulate $X_1^*, ..., X_n^*$ from $\hat{F}_n$
* Step 2: compute $T^*_n = g(X^*_1, ..., X^*_n)$

**Simulate from $\hat{F}_n$**:
* Observation: $\hat{F}_n$ puts mass $\frac{1}{n}$ at each data point $X_1, ..., X_n$
* Conclusion: draw an observation from $\hat{F}_n$ is equivalent to drawing one point at random from the original dataset

**Boostrap variance estimation**:
* Step 1: draw $X^*_1, ..., X^*_n \sim \hat{F}_n$
* Step 2: compute $T^*_n = g(X^*_1, ..., X^*_n)$
* Step 3: repeat step 1 and 2 for $B$ times to obtain $\{T^*_{n, i}\}_i$
* Step 4: $v_\text{boot} = \frac{1}{B} \sum_b (T^*_{n, b} - \frac{1}{B} \sum_r T^*_{n, r})^2$

**Consistency**: we are using two approximations to compute $\text{Var}_F(T_n)$
* Approximation 1: $\text{Var}_F(T_n) \overset{\text{not so small}}{\approx} \text{Var}_{\hat{F}_n}(T_n)$
* Approximation 2: $\text{Var}_{\hat{F}_n}(T_n) \overset{\text{small}}{\approx} v_\text{boot}$

## Boostrap confidence intervals
### Normal interval 
**Normal interval**: $T_n \pm z_{\alpha/2} \hat{\text{se}}_\text{boot}$ where $\hat{\text{se}}_\text{boot}$ is the boostrap estimate of standard error
* Drawbacks: not accurate unless $T_n$ is close to Gaussian

### Pivotal intervals
**Pivotal intervals**:
* Assumptions:
    * $\theta = T(F)$ and $\hat{\theta}_n = T(\hat{F}_n)$
    * $R_n = \hat{\theta}_n - \theta$ is the pivot
        * $H(r) = P_F(R_n \leq r)$ is the c.d.f of $R_n$ w.r.t $F$
    * $\{\hat{\theta^*}_{n, i}\}_i$ are bootstrap replications of $\hat{\theta}_n$
* Conclusion:
    * $1-\alpha$ confidence interval of $\theta$: $C^*_n = (a, b)$ where
        * $a = \hat{\theta}_n - H^{-1}(1 - \frac{\alpha}{2})$
        * $b = \hat{\theta}_n - H^{-1}(\frac{\alpha}{2})$
    * Pivot: $R_n$
* Intuition: flip then shift the interval $[H^{-1}(1 - \frac{\alpha}{2}), H^{-1}(\frac{\alpha}{2})]$ by $\hat{\theta}_n$ 
    * Explain: $H^{-1}(\cdot)$ is the quantile function of $\theta$ flipped and shifted by $\hat{\theta}_n$ 
* Analysis:
    * Advantage: $C^*_n$ is an exact $1-\alpha$ confidence interval for $\theta$
    * Drawbacks: $H$ is unknown

**Boostrap pivotal confidence interval**:
* Assumptions: 
    * $R^*_{n, b} = \hat{\theta^*}_{n, b} - \hat{\theta_n}$
    * $\theta^*_\beta$ is the $\beta$ sample quantile of $\theta^*_{n, 1}, ..., \theta^*_{n, B}$
    * $r^*_\beta$ is the $\beta$ sample quantile of $R^*_{n, 1}, ..., R^*_{n, B}$
        * $r^*_\beta = \theta^*_\beta - \hat{\theta}_n$
* Idea: form a boostrap estimate of $H$: $\hat{H}(r) = \frac{1}{B} \sum_b I(R^*_{n, b} \leq r)$
* Formal: $C_n = (\hat{a}, \hat{b})$ where
    * $\hat{a} = \hat{\theta}_n - \hat{H}^{-1}(1 - \frac{\alpha}{2}) = 2 \hat{\theta}_n - \theta^*_{1 - \alpha/2}$
    * $\hat{b} = \hat{\theta}_n - \hat{H}^{-1}(\frac{\alpha}{2}) = 2 \hat{\theta}_n - \theta^*_{\alpha/2}$

>**NOTE**: $C_n = (\hat{a}, \hat{b})$ is, typically, a pointwise, asymptotic confidence interval

**Theorem**: under weak conditions on $T(F)$, $P_F(T(F) \in C_n) \to 1 - \alpha$ as $n \to \infty$ where $C_n = (\hat{a}, \hat{b})$

### Percentile intervals
**Boostrap percentile intervals**: $C_n = (\theta^*_{\alpha/2}, \theta^*_{1 - \alpha/2})$

# Technical appendix
## The Jackknife
**Introduction**: a method introduced in 1949 for computing standard errors
* Advantage: less computationally expensive than boostrap
* Drawback: less general than boostrap

**The Jackknife**: special case of $k$-fold cross validation (i.e. $k = 1$)
* Assumptions:
    * $T_n = T(X_1, ..., X_n)$ is a statistic
    * $\hat{T}_n$ is the calculated estimator of $T_n$ using $n$ observations
    * $T_{(-i)}$ is the statistic with $i$-th observation removed
    * $\bar{T}_n = \frac{1}{n} \sum_i T_{(-i)}$
* Jackknife estimate of $\text{Var}(T_n)$: $v_\text{jack} = \frac{n-1}{n} \sum_i (T_{(-i)} - \bar{T}_n)^2$
* Jackknife estimate of $\text{bias}(T_n) = \frac{n - 1}{n} \sum_i (T_{(-i)} - \hat{T}_n)$
    * Interpretation:
        * $\bar{T}_n$ plays the role of the estimated $T_n$
        * $\hat{T}_n$ plays the role of the true $T_n$
    * Bias-corrected jackknife estimate of $T_n$: $\hat{T}_n - \text{bias}(T_n) = n \hat{T}_n - (n - 1) \bar{T}_n$

**Consistency**: 
* Under suitable conditions on $T$, $v_\text{jack}$ consistently estimates $\text{Var}(T_n)$, in the sense that $\frac{v_\text{jack}}{\text{Var}(T_n)} \overset{p}{\to} 1$
* The Jackknife doesn't produce consistent estimates of the standard error of sample quantiles