<!-- TOC titleSize:1 tabSpaces:2 depthFrom:1 depthTo:6 withLinks:1 updateOnSave:1 orderedList:0 skip:0 title:1 charForUnorderedList:* -->
# Table of Contents
- [Table of Contents](#table-of-contents)
- [Maximum likelihood estimators](#maximum-likelihood-estimators)
  - [Introduction](#introduction)
  - [Definition of a maximum likelihood estimator](#definition-of-a-maximum-likelihood-estimator)
  - [Limitations](#limitations)
- [Properties of maximum likelihood estimators](#properties-of-maximum-likelihood-estimators)
  - [Maximum likelihood estimator of a function of parameters](#maximum-likelihood-estimator-of-a-function-of-parameters)
  - [Consistency](#consistency)
  - [Numerical computation](#numerical-computation)
  - [MLEs and Bayes estimators](#mles-and-bayes-estimators)
- [BONUS](#bonus)
<!-- /TOC -->

# Maximum likelihood estimators
## Introduction
**History**: introduced by R. A. Fisher in 1912

**Desirable properties**: MLE is the most widely used method of estimation in statistics
* Strong intuitive appeal
* Often yield a reasonable estimator of $\theta$
* If the sample is large, the method will typically yield an excellent estimatof of $\theta$

**Terminologies**:
* Old statement: $X_1, ..., X_n$ are i.i.d with p.m.f $f(x|\theta)$ conditional on $\theta$ (i.e. Bayesian perspective)
* New statement: $X_1, ..., X_n$ form a random sample from a distribution with p.m.f $f(x|\theta)$ where $\theta$ is unknown (i.e. frequentist perspective)

## Definition of a maximum likelihood estimator
**Idea**: 
* Discussion: given observed vector $\textbf{x}$ from a discrete distriubtion
    * We would certainly not consider any $\theta \in \Omega$ for which it's impossible to obtain $\textbf{x}$
    * If $f(\textbf{x}|\theta)$ is very high for $\theta = \theta_0$ and is very small for other $\theta \in \Omega$

    $\hspace{1.0cm} \rightarrow$ We would naturally estimate $\theta$ to be $\theta_0$ (unless we had strong prior information which outweighted the evidence in the sample and pointed toward some other value)
* Conclusion: we will consider $\theta^* = \arg \max_\theta f_n(\textbf{x}|\theta)$ and use it as an estimate of $\theta$

**Maximum likelihood estimator (MLE)**:
* Maximum likelihood estimator of $\theta$: $\hat{\theta} = \delta(\textbf{x}) = \arg \max_\theta f_n(\textbf{x}|\theta)$ for each observed vector $\textbf{x}$
* Maximum estimate of $\theta$: $\hat{\delta}(\textbf{x})$ when $\textbf{X} = \textbf{x}$ is observed

**Notes**: 
* The MLE is required to be an element of the parameter space $\Omega$
* For certain observed $\textbf{x}$, the maximum value of $f_n(\textbf{x}|\theta)$ may not actually be attained for any $\theta \in \Omega$

$\hspace{1.0cm} \rightarrow$ In this case, MLE is defined separately for each different $\textbf{x}$

* For certain observed $\textbf{x}$, the maximum value of $f_n(\textbf{x}|\theta)$ may actually be attained at more than one point in $\Omega$

$\hspace{1.0cm} \rightarrow$ In this case, MLE can be chosen among those optimal points

## Limitations
**Underestimate and overestimate**: MLE sometimes surely underestimates or overestimates the parameter

**Existence**: MLE doesn't always exists
* Solution: choose an appropriate parametrization of the distribution

>**NOTE**: this difficulty cannot always be avoided

**Maximum likelihood**: MLE isn't necessarily the value of the parameter that appears to be most likely given the data
* Explain: we maximize $f_n(\textbf{x}|\theta)$, not $f(\theta|\textbf{x})$
    * Solution: use posterior distribution (Bayes estimator)
* When to use MLE:
    * When we have a lot of data
    * When there is very little prior information

# Properties of maximum likelihood estimators
## Maximum likelihood estimator of a function of parameters
**Invariance**: if $\hat{\theta}$ is the MLE of $\theta$ and $g(\cdot)$ is one-to-one then $g(\hat{\theta})$ is the MLE of $g(\theta)$

**MLE of a function**:
* Assumptions:
    * $g: \Omega \to G$ is defined as $g(\theta)$
    * $G_t = \{\theta|g(\theta) = t\}$
        * Another interpretation: $g: G_t \to \{t\}$ for all $t \in G$
    * $L^*(t) = \max_{\theta \in G_t} \log f_n(\textbf{x}|\theta)$
        * Another interpretation: maximum likelihood over all $\theta \in G_t$
    * $L^*(\hat{t}) = \max_{t \in G} L^*(t)$
        * Another interpretation: maximum likelihood over all $\theta \in \Omega$
* Conclusion:
    * If $\hat{\theta}$ is an MLE of $\theta$ obtained by sovling $g(\hat{\theta}) = \hat{t}$
    
    $\hspace{1.0cm} \rightarrow g(\hat{\theta})$ is an MLE of $g(\theta)$

## Consistency
**Consistency**:
* Assumptions: for every $n \geq n_0$ for some certain $n_0$, there exists a unique MLE of $\theta$
* Conclusion: under certain conditions (typically satisfied in practical problems), the sequence of MLEs is a consistent sequence
    * Explain: the sequence of MLE converges in probability to the unknown value of $\theta$ as $n \to \infty$

**Bias from Bayes estimator**: for a given prior and a sufficiently large $n$, Bayes estimator of $\theta$ is also a consistent sequence of estimators

$\hspace{1.0cm} \rightarrow$ Bayes estimator and MLE of $\theta$ wil typically very close to each other, and be very close to the true $\theta$

## Numerical computation
**Problem**: in many problems, we cannot express the MLE of $\theta$ in closed form

**Solutions**: use iterative optimization algorithms 
* Newton's method
* Method of moments
* EM algorithm
* Gradient descent

**Newton's method**: an iterative optimization algorithm to find MLEs
* Assumptions:
    * $f(\theta)$ is a real-valued function of a real variable
* Task: solve $f(\theta) = 0$
* Newton's method:
    * Initialization: $\theta = \theta_0$
    * Iteration: $\theta_i = \theta_{i-1} - \frac{f(\theta_{i-1})}{f'(\theta_{i-1})}$
* Failures: when $\frac{f'(\theta_{i-1})}{f(\theta_{i-1})} \to 0$ between $\theta_0$ and the actual solution to $f(\theta) = 0$

**Method of moments**: used to obtain an initial guess for iterative optimization algorithms
* Assumptions:
    * $X_1, ..., X_n$ form a random sample indexed by $\theta \in \textbf{R}^k$
    * $\mu(\theta) = (\mu_1(\theta), ..., \mu_k(\theta))$ is supposed to be a one-to-one function of $\theta$
        * $\mu_j(\theta) = E(X_1^j|\theta)$ for $j = 1, ..., k$
    * $m_j = \frac{1}{n} \sum_i X_i^j$ is the $j$-th sample moment
* Method of moments: $\theta = \mu^{-1}(m_1, ..., m_k)$
    * Solve for $\theta$: solve $m_j = \mu_j(\theta)$ $\forall j$ for $\theta$
* Consistency: the sequence of method of moments estimators is a consistent sequence of estimators of $\theta$
    * Explain: sample moments converge to true moments in probability due to the law of large numbers

**EM algorithm**: solve data missing problem when computing MLEs
* Assumptions:
    * $X = (x_a, X_m)$ is the dataset
    * $x_a$ is the available data
    * $X_m$ is the missing data, which are treated as random variables
        * $f_m(X_m)$ is the distribution of $X_m$
    * $f_n(X|\theta)$ is the likelihood function
* Algorithm:
    * Initialization: $\theta = \theta^{(0)}$
    * Iteration:
        * E-step: compupte $f_m(X_m|x_a; \theta^{(j)})$
        * M-step: $\theta^{(j+1)} = \arg \max_\theta E_{X_m}[\log f_n(X|\theta)]$
            * Explain: treat $E_{X_m}[\log f_n(X|\theta)]$ as a function of $\theta$

* Convergence: 
    * $\log f_n(\textbf{x}|\theta)$ increases with each iteration of the EM algorithm
    * The algorithm converges to a local maximum of $f_n(\textbf{x}|\theta)$

## MLEs and Bayes estimators
**General idea**: both MLEs and Bayes estimators depend on the likelihood function but they use likelihood function in different ways

**Distribution of MLE and Bayes estimator**:
* Assumptions:
    * $\hat{\theta}$ is the MLE of $\theta$
    * $V_n(\theta)$ is a sequence of random variables which converges to some $v_\infty(\theta)$ as $n \to \infty$
* Observations:
    * Convergence of $f(x|\theta)$:
        * When $f(x|\theta)$, as a function of $\theta$, satisfies certain smoothness conditions

        $\hspace{1.0cm} \rightarrow \lim_{n \to \infty} f_n(\textbf{x}|\theta) \propto \exp[-\frac{1}{2 V_n(\theta)/n} (\theta - \hat{\theta})^2]$
        * From above, as $n \to \infty$, $f_n(\textbf{x}|\theta)$ has a sharp peak at $\theta = \hat{\theta}$
    * If the prior of $\theta$ is relatively flat compared to the very peak $f(x|\theta)$, the posterior will look a lot like the likelihood multiplied by a constant

    $\hspace{1.0cm} \rightarrow E(\theta|\textbf{x}) \approx \hat{\theta}$
    * From above, the posterior of $\theta$ will be approximately normal with mean $\hat{\theta}$ and variance $V_n(\hat{\theta})/n$
    * The distribution of maximum likelihood estimator (given $\theta$) will be approximately normal with mean $\theta$ and variance $v_\infty(\theta) / n$
* Convergence failures: when $n$ is not very large, or $f(x|\theta)$ is not smooth

$\hspace{1.0cm} \rightarrow$ The posterior distributions and distributions of MLEs aren't normal as discussed above

---

# BONUS
* Missing data: miss observations which we had planned or hoped to observe but were not observed
* Sampling plans:
    * Task: take observations from a distribution with p.m.f $f(x|\theta)$ to gain information about $\theta$
        * Naive approach: take a random sample of pre-determined size
    * Proposed approach:
        * Step 1: observe a few amount of data to estimate the observation cost
        * Step 2: decide to continue or stop collecting data
        * Step 3: in case of continue collecting data, stop collecting data if
            * We think that we have enough information to obtain a good estimate of $\theta$
            * (or) We cannot afford to collect more data
    * Theorem: 
        * Assumptions: 
            * The sample size $n$ is a random variable
            * The decision of continue sampling or not, after $n$ observations, is a function of $n$ observed examples
        * Conclusion: if the decision to continue collecting data is based solely on the observations, not $\theta$

        $\hspace{1.0cm} \rightarrow$ The MLE is independent of the sampling plan
        * Explain: the only way in which the MLE can depend on the sampling plan is through the likelihood function