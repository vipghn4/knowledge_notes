<!-- TOC titleSize:1 tabSpaces:2 depthFrom:1 depthTo:6 withLinks:1 updateOnSave:1 orderedList:0 skip:0 title:1 charForUnorderedList:* -->
# Table of Contents
- [Table of Contents](#table-of-contents)
- [Prior and posterior distributions](#prior-and-posterior-distributions)
  - [Prior distribution](#prior-distribution)
  - [Posterior distribution](#posterior-distribution)
  - [Sequential observations and predictions](#sequential-observations-and-predictions)
  - [The likelihood function](#the-likelihood-function)
- [Conjugate prior distributions](#conjugate-prior-distributions)
  - [Conjugate prior distribution](#conjugate-prior-distribution)
- [Bayes estimators](#bayes-estimators)
  - [Nature of an estimation problem](#nature-of-an-estimation-problem)
  - [Loss functions](#loss-functions)
  - [Definition of a Bayes estimator](#definition-of-a-bayes-estimator)
  - [More general parameters and estimators](#more-general-parameters-and-estimators)
- [BONUS](#bonus)
<!-- /TOC -->

<!--10:31-->
# Prior and posterior distributions
## Prior distribution
**Prior distribution**: the distribution $\xi(\theta)$ of $\theta$ over the parameter space $\Omega$, before observing other random variables of interest
* Interpretation: summary of previous information and knowledge about where in $\Omega$ the value of $\theta$ should be

**Sensitivity analysis and imnproper priors**:
* Sensitivity analysis: analyze how much impact the prior distribution had on the posterior probability of a single important event
    * Procedure:
        * Step 1: use a large collection of prior distributions for $\theta$
        * Step 2: compare the posterior distributions arising from the priors
    * Impact of prior distributions: different prior distributions don't make much difference if
        *  We have a lot of data
        *  (or) the prior distributions being compared are very spread out (i.e. weak priors)
    * Drawbacks of prior distribution: if one doesn't specify some prior distribution

    $\hspace{1.0cm} \rightarrow$ There is no way to calculate a conditional distribution of the parameter given the data
* Improper priors: priors $\xi(\theta)$ so that $\int \xi(\theta) d\theta = \infty$
    * Interpretation: the data contain much more information than is available a priori
    
    >**NOTE**: when the data don't contain much information, improper priors may be highly inappropriate

    * Choosing an improper prior: methods for choosing improper prior all lead to similar posterior
        * Most straightforward: start with the family of conjugate prior distributions (if available)
        * Observation: in most cases, if the parametrization of the conjugate family is chosen carefully

        $\hspace{1.0cm} \rightarrow$ The posterior hyperparameters will each equal the corresponding prior hyperparameters plus a statistic

        * Trick: set each of the prior hyperparameter by $0$
    * Note: one must choose an improper prior so that the posteriors are proper distributions

## Posterior distribution
**Posterior distribution**: the distribution $\xi(\theta|x_1, ..., x_n)$ of $\theta$ over the parameter space $\Omega$, after observing other random variables of interest
* From prior to posterior: $\xi(\theta|\textbf{x}) = \frac{\xi(\theta) \prod_i f(x_i|\theta)}{g_n(\textbf{x})}$ $\forall \theta \in \Omega$ where $g_n$ is the marginal joint p.m.f of $\{X_i\}$

**Trick**: $\xi(\theta|\textbf{x}) \propto f_n(\textbf{x}|\theta) \xi(\theta)$ (i.e. $g_n(\textbf{x})$ is constant)
* Solving for $g_n(\textbf{x})$: solve $\int_\Omega \xi(\theta|\textbf{x}) d\theta = 1$
* Uses:
    * Recognize if $f_n(\textbf{x}|\theta) \xi(\theta)$ is equal to one of the available p.m.f or not
    * Drop $g_n(\textbf{x})$ from $f_n(\textbf{x}|\theta)$

## Sequential observations and predictions
**Another way to compute $\xi(\theta|\textbf{x})$**: compute $\xi(\theta|\textbf{x})$ sequentially by $\xi(\theta|\textbf{x}_{1:i}) \propto f(x_i|\theta) \xi(\theta|\textbf{x}_{1:i-1})$

## The likelihood function
**Likelihood function**: $f_n(\textbf{x}|\theta)$ as a function of $\theta$ given $x_1, ..., x_n$

# Conjugate prior distributions
## Conjugate prior distribution
**Conjugate family of prior distributions**: $\Psi$ is a conjugate family of prior distribution if both $\xi(\theta)$ and $\xi(\theta|\textbf{x})$ belong to $\Psi$ no matter which $\xi(\cdot) \in \Psi$ is chosen
* Interpretation: $\Psi$ is closed under sampling from $f(x|\theta)$

**Hyperparameters**: 
* Prior hyperparameters: assocaited parameters of the prior distributions
* Posterior hyperparameters: assocaited parameters of the posterior distributions

**Usage**: simplify the procedure of calculating the posterior distribution

# Bayes estimators
## Nature of an estimation problem
**Estimator**:
* Assumptions:
    * $X_1, ..., X_n$ are observable data whose joint distribution is indexed by $\theta$
    * $\theta \in \Omega$ where $\Omega$ is a subset of real line
* Conclusion:
    * An estimator of $\theta$: $\delta(X_1, ..., X_n)$
        * Another interpretation: a function of random variables $X_1, ..., X_n$
    * The estimate of $\theta$: $\delta(x_1, ..., x_n)$
        * Another interpretation: observed value of $\delta(X_1, ..., X_n)$

**Constraints**: every possible value of $\delta(X_1, ..., X_n)$ must belong to $\Omega$
* Discussion:
    * We shall not require this restriction
        * Explain: if $\delta(x_1, ..., x_n) \notin \Omega$, one will need to decide whether that seems appropriate or not
    * Every estimator which takes values only inside $\Omega$ has other even less desirable properties

## Loss functions
**Loss function**: 
* Assumptions:
    * $\theta \in \Omega$ is the parameter to estimate
        * $\xi(\theta)$ is the prior p.m.f of $\theta$ on $\Omega$
    * $a$ is a real number
* Loss function: a real-valued function $L(\theta, a)$
    * Expected loss: $E[L(\theta, a)] = \int_\Omega L(\theta, a) \xi(\theta) d \theta$
* Interpretation: if the parameter equals $\theta$ and the estimate equals $a$

$\hspace{1.0cm} \rightarrow$ The statistician loses $L(\theta, a)$

**Target of estimation**: minimize $E[L(\theta, a)]$

## Definition of a Bayes estimator
**Bayes estimator**:
* Bayes estimator of $\theta$: $\delta^*$, where $\delta^*(\textbf{x})$ minimizes $E[L(\theta, a)|\textbf{x}]$ for each possible $\textbf{x}$
    * Formal: $E[L(\theta, \delta^*(\textbf{x}))] = \min_a E[L(\theta, a)|\textbf{x}]$
* Bayes estimate of $\theta$: $\delta^*(\textbf{x})$

**Existence of Bayes estimator**: there are situations in which no $\delta^*$ satisfies

**Properties**:
* Effect of different prior distributions: when the number of observations in the sample is so large

$\hspace{1.0cm} \rightarrow$ Bayes estimates w.r.t different priors are almost the same
* Consistency: when large number of observations are taken

$\hspace{1.0cm} \rightarrow$ The Bayes estimator will converge in probability to the unknown value of $\theta$ as $n \to \infty$

>**NOTE**: for a wide class of loss functions, the Bayes estimators of $\theta$ will form a consistent sequence of estimators as $n \to \infty$

>**NOTE**: if conugate prior distribution (if available) is assigned to $\theta$ and the squared error loss is used, the Bayes estimators will form a consistent sequence of estimators

**Limitations**:
* Loss function is required and also a prior distribution for the parameter
* $\theta$ maybe a vector of all unknowns
    * Explain: this requires multivariate prior for $\theta$ and multivariate loss $L(\theta, \textbf{a})$

	>**NOTE**: even when the statistician wants to estimate only one or two components of $\theta$, he still needs to assign multivariate prior to the entire $\theta$

## More general parameters and estimators
**Generalizations**:
* Estimate multi-dimensional parameters
* Estimate functions of the parameters

---

# BONUS
* Observable and hypothetically observable random variables:
    * Observable random variables: variables which we are essentially certain that we could observe if we devoted sufficient effort to observe it
    * Hypothetically observable random variables: variables which require infinite resources to observe
        * Example: $\lim_{n \to \infty} \bar{X}_n$
* Consistent estimator: a sequence of estimators which converges in probability to the unknown value of $\theta$, as $n \to \infty$