<!-- TOC titleSize:1 tabSpaces:2 depthFrom:1 depthTo:6 withLinks:1 updateOnSave:1 orderedList:0 skip:0 title:1 charForUnorderedList:* -->
# Table of Contents
- [Table of Contents](#table-of-contents)
- [Sufficient statistics](#sufficient-statistics)
  - [Sufficient statistics](#sufficient-statistics-1)
    - [Introduction](#introduction)
    - [Definition of a sufficient statistics](#definition-of-a-sufficient-statistics)
    - [The factorization criterion](#the-factorization-criterion)
  - [Jointly sufficient statistics](#jointly-sufficient-statistics)
    - [Definition of a jointly sufficient statistics](#definition-of-a-jointly-sufficient-statistics)
    - [Minimal sufficient statistics](#minimal-sufficient-statistics)
    - [Maximum likelihood estimators and Bayes estimators as sufficient statistics](#maximum-likelihood-estimators-and-bayes-estimators-as-sufficient-statistics)
  - [Improving an estimator](#improving-an-estimator)
    - [Conditional expectation when a sufficient statistic is known](#conditional-expectation-when-a-sufficient-statistic-is-known)
    - [Limitation of the use of sufficient statistics](#limitation-of-the-use-of-sufficient-statistics)
    - [Complete statistics](#complete-statistics)
- [Appendix](#appendix)
  - [Concepts](#concepts)
<!-- /TOC -->

# Sufficient statistics
## Sufficient statistics
### Introduction
**Drawbacks of maximum likelihood and Bayes estimators**.
* MLEs may not exist in some problem, or there are many MLEs
* If an MLE is unique, it may not be a suitable one, e.g. it always underestimate or overestimate $\theta$
* Bayes estimator may be hard to find, i.e. the search for a good estimator must be extended beyond the known methods

**Sufficient statistics**. There are methods based on the conditional distribution of various statistics given the parameter

$\to$ Sufficient statistics are the most useful statistics
* *Usage*. Get rid of the dependency of $f_n(\textbf{x}|\theta)$ from directly observing $\textbf{x}$

### Definition of a sufficient statistics
**Idea**.
* *Assumptions*.
    * Statistician $A$ can observe $X_1, ..., X_n$
    * Statistician $B$ can observe $T = r(X_1, ..., X_n)$, i.e. he cannot observe $X_1,\dots,X_n$
* *Observations*.
    * $A$ should be able to find a better estimator than $B$
        * *Explain*. $A$ can use any function of $X_1, ..., X_n$ while $B$ can only use functions of $T$
    * If $T$ summarizes all the information of the random sample
        
        $\to$ Individual values of $X_1, ..., X_n$ are irrelevant, given $T$, in the search for a good estimator of $\theta$
        * *Consequence*. $B$ can use $T$ to do as well as $A$
* *Intuition*. $B$ can learn $T$, and then can simulate random variables $X'_1, \dots, X'_n$, given $T=t$, so that 
    
    $$\forall\theta,f_n(X'_1, ..., X'_n|\theta) = f_n(X_1, ..., X_n|\theta)$$

    $\to$ $T$ is sufficient in the sense that $B$ can use $X'_1, ..., X'_n$ in the same way $A$ uses $X_1, \dots, X_n$ to estimate $\theta$
    * *Explain*. $B$ can simulate $X'_1,\dots,X'_n$ since, once learned $T=t$, $B$ no longer needs $\theta$ to derive the true p.d.f of the data

        $\to$ Since $T$ has summarized all such information to derive the p.d.f, hence it is sufficient
* *Interpretation*. $T$ has wrapped all information about $\theta$ given by the sample, which is required to derive the p.d.f $f(x;\theta)$
    * *Example*. When the p.d.f $f(x;\theta)$ depends on $\theta$ only via some function $g(\theta)$, which can be estimated by $T$
* *Auxiliary randomization*. The process of simulating $X'_1, ..., X'_n$
    * *Distinction from ordinary statistics*. The auxiliary randomization, after $T$ has been observed, does not require any knowledge about $\theta$

        $\to$ If $T$ is not sufficient, then the randomization involves the value of $\theta$, which is unknown, hence cannot be carried out

**Giang's intuition**. We can imagine the Markov chain for the sufficient statistic $T$ and the p.d.f $f$

$$\theta\to T\to f$$

**Sufficient statistics**.
* *Assumptions*.
    * $X_1, ..., X_n$ form a random sample from a distribution indexed by $\theta$
    * $T$ is a statistic so that $f_n(X_1, ..., X_n|T = t, \theta) = f_n(X_1, ..., X_n|T = t)$
* *Conclusion*. $T$ is a sufficient statistic for $\theta$ 
* *Informal*. Statistics which summarize all the information of the sample

### The factorization criterion
**Factorization criterion for statistical sufficiency**.
* *Assumptions*.
    * $X_1, ..., X_n$ form a random sample from a distribution with p.m.f $f(x|\theta)$
        * $\theta \in \Omega$ is unknown
    * $T = r(X_1, ..., X_n)$ is a statistic
* *Conclusion*. $T$ is sufficient for $\theta$ if and only if $f_n(\textbf{x}|\theta)$ can be factored as 
    
    $$\forall \textbf{x}\in\mathbb{R}^n, \forall\theta\in\Omega,f_n(\textbf{x}|\theta) = u(\textbf{x}) v[r(\textbf{x}), \theta]$$
    * $u(\textbf{x})$ is independent of $\theta$
    * $v$ depends on $\theta$, but depends on $\textbf{x}$ via $r(\textbf{x})$ only
* *Explain*. 
    * *Factorization of $f_n(\mathbf{x}|\theta)$ implies sufficiency of $T$*. Recall that, in frequentist perspective, $\theta$ is fixed and unknown
        
        $\to$ Given $T=t$, $v[r(\mathbf{x}),\theta]$ is fixed and hence $f_n(\mathbf{x}|\theta)$ depends only on $u(\mathbf{x})$
    * *Sufficiency of $T$ implies factorization of $f_n(\mathbf{x}|\theta)$*. Sufficiency of $T$ implies that

        $$P(\mathbf{X}=\mathbf{x}|T=t,\theta)=u(\mathbf{x})$$

        which further implies

        $$\begin{aligned}
        f_n(\mathbf{x}|\theta)&=P(\mathbf{X}=\mathbf{x}|T=t,\theta) P(T=t|\theta)\\
        &=u(\mathbf{x}) v(t,\theta)
        \end{aligned}$$

* *Intuition*. $f_n(\mathbf{x}|\theta)$, as a function of $\theta$, depends on the data only via $r(\mathbf{x})$
* *Another interpretation*. $T = r(X_1, ..., X_n)$ is sufficient for $\theta$ if and only if 
    
    $$f_n(\textbf{x}|\theta) \propto v[r(\textbf{x}), \theta]$$
    
    * *Explain*. To estimate $\theta$ based on $f_n(\textbf{x}|\theta)$, we only have to care about $r(\textbf{x})$, since $u(\textbf{x})$ is a constant
* *Corollary*. $T$ is sufficient if and on if $g(\theta|\textbf{x})$ depends on $\textbf{x}$ via $r(\textbf{x})$ only, no matter what prior distribution we use
    * *Explain*. Use Bayes' rule

        $$\begin{aligned}
        f(\theta|x) &= \frac{f(x|\theta) f(\theta)}{\int_{\theta'\in\Omega} f(x|\theta') f(\theta') d\theta'}\\
        &= \frac{v[r(x),\theta] f(\theta)}{\int_{\theta'\in\Omega} v[r(x),\theta']d\theta'}
        \end{aligned}$$

    * *Interpretation*. When using the likelihood function for finding posterior distributions
        
        $\to$ Any factor not depending on $\theta$, e.g. $u(\mathbf{x})$, can be removed from the likelihood without affecting the calculation of the posterior distribution

**Theorem**. If $T = r(X_1, ..., X_n)$ is sufficient then $h(T)$ is sufficient if $h(\cdot)$ is one-to-one

## Jointly sufficient statistics
**Needs for multidimensional sufficient statistics**. Sometimes, we need to extend the concept of sufficient statistic to deal with cases in which more than one statistic is needed to be sufficient
* *Observations*.
    * When a parameter $\theta$ is multidimensional, sufficient statistics will typically need to
    be multidimensional
    * Sometimes, no one-dimensional statistic is sufficient even when $\theta$ is one-dimensional
* *Conclusion*. We need to find two or more statistics $T_1,\dots,T_k$, which together are jointly sufficient statistics

### Definition of a jointly sufficient statistics
**Jointly sufficient statistics**. Sometimes, there's no 1D sufficient statistic
* *Assumptions*.
    * $X_1, ..., X_n$ form a random sample from a distribution indexed by $\theta$
    * $T_1, ..., T_k$ are $k$ statistics defined as $T_i = r_i(\textbf{X})$
* *Conclusion*. $(T_1, ..., T_k)$ are jointly sufficient if 
    
    $$f_n(\textbf{X}|\{T_i = t_i\}, \theta) = f_n(\textbf{X}|\{T_i = t_i\})$$

* *Informal*. $T_1, ..., T_k$ are jointly sufficient if they summarize all the information of the sample

**Factorization criterion for statistical jointly sufficiency**:
* *Assumptions*. $\{T_i|T_i = r_i(\textbf{X})\}$ are statistics
* *Conclusion*. $\{T_i\}$ are jointly sufficient for $\theta$ if $f_n(\textbf{x}|\theta)$ can be factored as 
    
    $$f(\textbf{x}|\theta) = u(\textbf{x}) v[r_1(\textbf{x}), ..., r_k(\textbf{x}), \theta]$$

* *Interpretation*. $f_n(\mathbf{x}|\theta)$ depends on $\theta$ only via $r_1(\mathbf{x}),\dots,r_k(\mathbf{x})$

**Theorem**. If $k$ other statistics $T'_1,\dots,T'_k$ are obtained from $T_1,\dots,T_k$ by a one-to-one transformation

$\to$ $T'_1,\dots,T'_k$ are jointly sufficient statistics for $\theta$

### Minimal sufficient statistics
**Benefits for sufficient statistics**. We want to try to find a sufficient statistic or a set of jointly sufficient statistics for $\theta$
* *Explain*. The values of such statistics summarize all the relevant information about $\theta$ contained in the random sample
    
    $\to$ The search for a good estimator of $\theta$ is simplified, as we need consider only functions of these statistics as possible estimators
* *Consequence*. It is desirable to find, not merely any set of jointly sufficient statistics, but the simplest set of jointly sufficient statistics
    * *Explain*. We want the set of sufficient statistics, which requires us to consider the smallest collection of posible estimators

**Minimal sufficient statistics**. The simplest set of joint sufficient statistics
* "Simplest". The set which requires us to consider the smallest collection of possible estimators
* *Minimal suffiency*. $T$ is a minimal sufficient statistic if
    * $T$ is sufficient
    * $T$ is a function of every other sufficient statistic
* *Minimal joint sufficiency*. $\mathbf{T} = (T_1, ..., T_k)$ are minimal joint sufficient statistic if
    * $\mathbf{T}$ are jointly sufficient
    * $\mathbf{T}$ is a function of every other sufficient statistics

### Maximum likelihood estimators and Bayes estimators as sufficient statistics
**MLE and sufficient statistics**.
* *Assumptions*.
    * $T = r(X_1, ..., X_n)$ is a sufficient statistic for $\theta$
    * $\hat{\theta}$ is the MLE of $\theta$
* *Conclusion*. 
    * $\hat{\theta}$ depends on $X_1, ..., X_n$ via $T$ only
    * If $\hat{\theta}$ is sufficient, then it is minimal sufficient
* *Explain*. 
    * *Prove statement 1*.
        * It follows from the factorization criterion that

            $$f_n(\mathbf{x}|\theta)=u(\mathbf{x}) v[r(\mathbf{x}),\theta]$$

        * Hence, maximizing $f_n(\mathbf{x}|\theta)$ is equivalent to maximizing $v[r(\mathbf{x}),\theta]$

            $\to$ $\hat{\theta}$ depends on $X_1,\dots,X_n$ only via $T=r(\mathbf{X})$
    * *Prove statement 2*. Since $\hat{\theta}$ is a function of any sufficient statistic $T=r(\mathbf{X})$

**Bayes estimator and sufficient statistics**:
* *Assumptions*.
    * $T = r(X_1, ..., X_n)$ is a sufficient statistic for $\theta$
    * $\hat{\theta}$ is a Bayes estiamtor of $\theta$
* *Conclusion*.
    * $\hat{\theta}$ depends on $X_1, ..., X_n$ via $T$ only
    * If $\hat{\theta}$ is sufficient, then it is minimal sufficient
* *Explain*.
    * *Prove statement 1*. Similar to the previous theorem
    * *Prove statement 2*. Similar to the previous theorem

## Improving an estimator
**Brief**. This section shows how to improve upon an estimator, which is not a function of a sufficient statistic, by using an estimator, which is a function of a sufficient statistic

**The mean squared error of an estimator**.
* *Assumptions*.
    * $\textbf{X} = (X_1, ..., X_n)$ form a random sample from a distribution with p.m.f $f(x|\theta)$ 
        * $\theta \in \Omega$ is unknown
    * $Z = g(X_1, ..., X_n)$ is a statistic with mean 
        
        $$E(Z|\theta)=\int g(\mathbf{x}) f_n(\mathbf{x}|\theta) d\mathbf{x}$$
        
        denoted by $E_\theta(Z)$
    * $h(\theta)$ is the function we want to estimated
    * $R(\theta) = E_\theta([\delta(\textbf{X}) - h(\theta)]^2)$ is the loss function, for each estimator $\delta(\textbf{X})$ of $\theta$
    * $\textbf{T}$ is a vector of jointly statistics for $\theta$
    * Statistician $A$ plans to use a particular estimator $\delta(\mathbf{X})$, after observing $\mathbf{X}$
    * Statistician $B$ can only observe $\textbf{T}$
        
        $\to$ $B$ can generate, by means of auxiliary randomization, an estimator with the same distribution as $\delta(\mathbf{X})$
* *Estimation of $\theta$*. Without a prior distribuition for $\theta$
    
    $\to$ It is desired to find an estimator $\delta$, for which $R(\theta,\delta)$ is small for every $\theta\in\Omega$, or at least for a wide range of $\theta$
* *Conclusion*. Even without auxiliary randomization, $B$ can find an estimator $\delta_0$ depending on $\textbf{X}$ via $\textbf{T}$ only, which is at least as good as $\delta$, i.e. 
    
    $$\forall \theta \in \Omega,R(\theta, \delta_0) \leq R(\theta, \delta)$$

* *Explain*. See below

### Conditional expectation when a sufficient statistic is known
**Rao-Blackwell theorem**.
* *Assumptions*.
    * $\delta(\textbf{X})$ is an estimator of $\theta$
    * $\delta_0(\textbf{T}) = E_\theta[\delta(\textbf{X})|\textbf{T}]$
* *Observations*. Since $\mathbf{T}$ is sufficient, $f_n(\mathbf{X}|\mathbf{T},\theta)$ is the same for every $\theta\in\Omega$, given $\mathbf{T}$

    $\to$ $E_\theta[\delta(\mathbf{X})|\mathbf{T}]$ is the same for all $\theta\in\Omega$

    * *Consequence*. $\delta_0(\mathbf{T})$ is an estimator of $\theta$, since it depends only on $\mathbf{X}$

        $\to$ We denote the relation as $\delta_0(\mathbf{T})=E[\delta(\mathbf{X})|\mathbf{T}]$
* *Conclusion*. 

    $$\forall \theta \in \Omega,R(\theta, \delta_0) \leq R(\theta, \delta)$$
    
    * *Strict case*. 
        
        $$\forall \theta \in \Omega,R(\theta, \delta) < \infty\implies R(\theta, \delta_0) < R(\theta, \delta)$$
        
        unless $\delta(\textbf{X})$ is a function of $\textbf{T}$
* *Generalization*. When $R$ is any convex loss
    
    $$E_\theta[R(\theta, \delta_0)] \leq E_\theta[R(\theta, \delta)]$$

**Rao-Blackwell estimator**.
* *Rao-Blackwell estimator*. $\delta_0(\textbf{X}) = E[\delta(\textbf{X})|\textbf{T}]$
    * *Original estimator*. $\delta(\textbf{X})$
    * *Improved estimator*. $\delta_0(\textbf{X})$
* *Rao-Blackwellization*. Modification of $\theta$ to $\theta_0$
* *Intuition*. $\textbf{T}$ contains all the required information that the data can supply

    $\to$ Conditioning on $\textbf{T}$ can provide an estimator which is not worse than the original one, and is often better in the sense of variability
* *Properties of $\theta_0$*.
    * *Unbias estimator*. $\delta_0$ is unbiased if and only if $\delta$ is unbiased
    * *Idempotence*. Using Rao-Blackwellization to improve the already improved estimator does not obtain a further improvement
* *Prove the improvements*.
    * *Prove*.
        * $E_\theta[R(\theta, \delta)|\textbf{T}] \geq R[\theta, E_\theta(\delta|\textbf{T})] = R(\theta, \delta_0)$ due to Jensen's inequality
        * From above, $E_\theta[R(\theta, \delta)] = E_\theta\{E_\theta[R(\theta, \delta)|\textbf{T}]\} \geq E_\theta[R(\theta, \delta_0)]$
    * *Why $\textbf{T}$ must be sufficient*. If $\textbf{T}$ isn't sufficient, $\delta_0$ can be a function of $\theta$, which is unknown
* *Practical use*. 
    * Rao-Blackwell estimator is less useful in practice
        * *Explain*. It is usually very difficult to calculate $E[\delta(\mathbf{X})|\mathbf{T}]$
    * The estimator is valuable mainly because it provides further strong evidence that we can restrict our search for a good estimator of $\theta$ to those estimators depending on the observations only through a sufficient statistics

**Inadmissible, admissible, and dominates**.
* *Assumptions*. 
    * $R(\theta,\delta)$ is a loss function
    * $\delta$ is an estimator of $\theta$
* *Admissibility*. 
    * $\delta$ is inadmissible if

        $$\forall\theta\in\Omega,\exists \delta_0,R(\theta\delta_0)\leq R(\theta,\delta)$$

        and

        $$\exists\theta\in \Omega,\exists\delta_0,R(\theta\delta_0)< R(\theta,\delta)$$

        and, under these conditions, $\delta_0$ is said to dominates $\delta$
    * $\delta$ is admissible if there is no $\delta_0$ dominating it

### Limitation of the use of sufficient statistics
**Limitation of the use of sufficient statistics**. The existence and the form of a sufficient statistic depend critically on the form of the assumed p.d.f. or the p.f
* *Explain*. A statistic may be sufficient when p.d.f $f(x|\theta)$ is assumed, but not for p.d.f $g(x|\theta)$ even when the two are similar

**Robust estimator**.
* *Assumptions*.
    * A statistician is in doubt about the exact form of the p.d.f. in a specific problem, but assumes for convenience that the p.d.f. is $f(x|\theta)$
    * $\mathbf{T}$ is a sufficient statistic under this assumption
* *Observations*. Because of the statistician’s uncertainty about the exact form of the p.d.f
    
    $\to$ He may wish to use an estimator of $\theta$, which performs well for a wide variety of possible p.d.f.’s
    
    >**NOTE**. Even though the selected estimator may not meet the requirement that it should depend on the observations only through the statistic $\mathbf{T}$

* *Robust estimator*. An estimator performing well for a wide variety of p.d.f's

    >**NOTE**. A robust estimator needs not be the best available for any particular family of p.d.f

**Needs for sensitivity analysis**. Sensitivity analysis can be applied to any feature of a chosen statistical model
* *Example*. The data distribution given the parameters, i.e. $f(x|\theta)$, is often chosen for convenience, rather than through a careful analysis, hence
    1. One can perform inference repeatedly using different distributions for the observable data
    2. The comparison of the resulting inferences from each choice is another form of sensitivity analysis

### Complete statistics
**Brief**. Completeness is a property of a statistic in relation to a model for a set of observed data

$\to$ In essence, it ensures that the distributions corresponding to different values of the parameters are distinct

**Completeness**. 
* *Assumptions*.
    * $X$ is a random variable, whose probability distribution belongs to a parametric model $P_\theta$
    * $T$ is a statistic
* *Complete statistics*. A statistic $T$ is complete w.r.t $\theta$ if 

$$\forall \theta,E_\theta[h(T)] = 0 \implies \forall \theta,P_\theta[h(T) = 0] = 1$$

* *Intuition*. $T$ is complete if $0$ is the only function of $T$, whose expectation is $0$
    * *Another interpretation*. $E_\theta[h(T)] \neq 0$ for all $h$, except for $h(T) = 0$ only
    * *Corollary*. if $T$ is complete then $E_\theta[h_1(T)] = E_\theta[h_2(T)] \implies h_1(T) = h_2(T)$
* *Boundedly complete*. $T$ is boundedly complete if $E_\theta[h(T)] = 0$ $\forall \theta \implies h(t) = 0$ holds for every $h$, which is bounded

**Completeness and sufficiency**.
* *Assumptions*. 
    * $T$ is a complete and sufficient statistic
    * $U = h(T)$ is a function of $t$ 
* *Conclusion*. $U$ is the unique unbiased estimator, based on $T$, of its expectation

# Appendix
## Concepts
**Order statistic**. $\{Y_i\}$ where $Y_i$ is the $i$-th smallest value in $X_1, ..., X_n$
* *Sufficiency*. Order statistics are sufficient for $\theta$

    $\to$ It is sufficient to know the set of $n$ numbers obtained in the sample, without knowing which particular one of these numbers was
* *Admissibility*.
    * *Assumptions*.
        * $\delta$ and $\delta_0$ are two estimators of $\theta$
        * $R(\theta, \delta)$ is the loss function
    * *Inadmissible*. $\delta$ is inadmissible if there exists some $\delta_0$ such that $R(\theta, \delta_0) \leq R(\theta, \delta)$

    $\hspace{1.0cm} \rightarrow \delta_0$ dominates $\delta$
    * *Admissible*. $\delta$ is admissible if there exists no $\delta_0$ such that $R(\theta, \delta_0) \leq R(\theta, \delta)$
* *Limitations of sufficient statistics*. The existence and the form os a sufficient statistic depend critically on the hypothetical p.m.f
* *Robust estimator*. Estimators which perform reasonably well for a wide variety of possible p.m.f's