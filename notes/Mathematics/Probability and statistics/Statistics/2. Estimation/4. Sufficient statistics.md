<!-- TOC titleSize:1 tabSpaces:2 depthFrom:1 depthTo:6 withLinks:1 updateOnSave:1 orderedList:0 skip:0 title:1 charForUnorderedList:* -->
# Table of Contents
- [Table of Contents](#table-of-contents)
- [Sufficient statistics](#sufficient-statistics)
  - [Introduction](#introduction)
  - [Definition of a sufficient statistics](#definition-of-a-sufficient-statistics)
  - [The factorization criterion](#the-factorization-criterion)
- [Definition of a jointly sufficient statistics](#definition-of-a-jointly-sufficient-statistics)
- [Minimal sufficient statistics](#minimal-sufficient-statistics)
- [Improving an estimator using sufficient statistics](#improving-an-estimator-using-sufficient-statistics)
- [Maximum likelihood estimators and Bayes estimators as sufficient statistics](#maximum-likelihood-estimators-and-bayes-estimators-as-sufficient-statistics)
- [Complete statistics](#complete-statistics)
- [BONUS](#bonus)
<!-- /TOC -->

# Sufficient statistics
## Introduction
**Drawbacks of maximum likelihood and Bayes estimators**.
* MLEs may not exist in some problem, or there are many MLEs
* If an MLE is unique, it may not be a suitable one, e.g. it always underestimate or overestimate $\theta$
* Bayes estimator may be hard to find, i.e. the search for a good estimator must be extended beyond the known methods

**Sufficient statistics**. There are methods based on the conditional distribution of various statistics given the parameter

$\to$ Sufficient statistics are the most useful statistics
* *Usage*. Get rid of the dependency of $f_n(\textbf{x}|\theta)$ from directly observing $\textbf{x}$

## Definition of a sufficient statistics
**Idea**.
* *Assumptions*.
    * Statistician $A$ can observe $X_1, ..., X_n$
    * Statistician $B$ can observe $T = r(X_1, ..., X_n)$, i.e. he cannot observe $X_1,\dots,X_n$
* *Observations*.
    * $A$ should be able to find a better estimator than $B$
        * *Explain*. $A$ can use any function of $X_1, ..., X_n$ while $B$ can only use functions of $T$
    * If $T$ summarizes all the information of the random sample
        
        $\to$ Individual values of $X_1, ..., X_n$ are irrelevant, given $T$, in the search for a good estimator of $\theta$
        * *Consequence*. $B$ can use $T$ to do as well as $A$
* *Intuition*. $B$ can learn $T$, and then can simulate random variables $X'_1, \dots, X'_n$, given $T=t$, so that 
    
    $$\forall\theta,f_n(X'_1, ..., X'_n|\theta) = f_n(X_1, ..., X_n|\theta)$$

    $\to$ $T$ is sufficient in the sense that $B$ can use $X'_1, ..., X'_n$ in the same way $A$ uses $X_1, \dots, X_n$ to estimate $\theta$
    * *Explain*. $B$ can simulate $X'_1,\dots,X'_n$ since, once learned $T=t$, $B$ no longer needs $\theta$ to derive the true p.d.f of the data

        $\to$ Since $T$ has summarized all such information to derive the p.d.f, hence it is sufficient
* *Interpretation*. $T$ has wrapped all information about $\theta$ given by the sample, which is required to derive the p.d.f $f(x;\theta)$
    * *Example*. When the p.d.f $f(x;\theta)$ depends on $\theta$ only via some function $g(\theta)$, which can be estimated by $T$
* *Auxiliary randomization*. The process of simulating $X'_1, ..., X'_n$
    * *Distinction from ordinary statistics*. The auxiliary randomization, after $T$ has been observed, does not require any knowledge about $\theta$

        $\to$ If $T$ is not sufficient, then the randomization involves the value of $\theta$, which is unknown, hence cannot be carried out

**Giang's intuition**. We can imagine the Markov chain for the sufficient statistic $T$ and the p.d.f $f$

$$\theta\to T\to f$$

**Sufficient statistics**.
* *Assumptions*.
    * $X_1, ..., X_n$ form a random sample from a distribution indexed by $\theta$
    * $T$ is a statistic so that $f_n(X_1, ..., X_n|T = t, \theta) = f_n(X_1, ..., X_n|T = t)$
* *Conclusion*. $T$ is a sufficient statistic for $\theta$ 
* *Informal*. Statistics which summarize all the information of the sample

## The factorization criterion
**Factorization criterion for statistical sufficiency**.
* *Assumptions*.
    * $X_1, ..., X_n$ form a random sample from a distribution with p.m.f $f(x|\theta)$
        * $\theta \in \Omega$ is unknown
    * $T = r(X_1, ..., X_n)$ is a statistic
* *Conclusion*. $T$ is sufficient for $\theta$ if and only if $f_n(\textbf{x}|\theta)$ can be factored as 
    
    $$\forall \textbf{x}\in\mathbb{R}^n, \forall\theta\in\Omega,f_n(\textbf{x}|\theta) = u(\textbf{x}) v[r(\textbf{x}), \theta]$$
    * $u(\textbf{x})$ is independent of $\theta$
    * $v$ depends on $\theta$, but depends on $\textbf{x}$ via $r(\textbf{x})$ only
* *Explain*. Recall that, in frequentist perspective, $\theta$ is fixed and unknown
    
    $\to$ Given $T=t$, $v[r(\mathbf{x}),\theta]$ is fixed and hence $f_n(\mathbf{x}|\theta)$ depends only on $u(\mathbf{x})$
* *Intuition*. $\theta$ affects with $f_n(\mathbf{x}|\theta)$ only via $v[r(\mathbf{x}),\theta]$, in a multiplicative manner, and interacts with $\mathbf{x}$ only via $r(\mathbf{x})$
* *Another interpretation*. $T = r(X_1, ..., X_n)$ is sufficient for $\theta$ if and only if 
    
    $$f_n(\textbf{x}|\theta) \propto v[r(\textbf{x}), \theta]$$
    
    * *Explain*. To estimate $\theta$ based on $f_n(\textbf{x}|\theta)$, we only have to care about $r(\textbf{x})$, since $u(\textbf{x})$ is a constant
* *Corollary*. $T$ is sufficient if and on if $g(\theta|\textbf{x})$ depends on $\textbf{x}$ via $r(\textbf{x})$ only
    * *Explain*. Use Bayes' rule

**Theorem**. If $T = r(X_1, ..., X_n)$ is sufficient then $h(T)$ is sufficient if $h(\cdot)$ is one-to-one

# Definition of a jointly sufficient statistics
**Jointly sufficient statistics**: sometimes, there's no 1D sufficient statistic
* Assumptions:
    * $X_1, ..., X_n$ form a random sample from a distribution indexed by $\theta$
    * $T_1, ..., T_k$ are $k$ statistics
        * $T_i = r_i(\textbf{X})$
* Conclusion: $(T_1, ..., T_k)$ are jointly sufficient if $f_n(\textbf{X}|\{T_i = t_i\}, \theta) = f_n(\textbf{X}|\{T_i = t_i\})$
* Informal: $T_1, ..., T_k$ are jointly sufficient if they summarize all the information of the sample

**Factorization criterion for statistical jointly sufficiency**:
* Assumptions:
    * $\{T_i|T_i = r_i(\textbf{X})\}$ are statistics
* Conclusion: $\{T_i\}$ are jointly sufficient for $\theta$ if $f_n(\textbf{x}|\theta)$ can be factored as $f(\textbf{x}|\theta) = u(\textbf{x}) v[r_1(\textbf{x}), ..., r_k(\textbf{x}), \theta]$

# Minimal sufficient statistics
**Minimal sufficient statistics**: the simplest set of joint sufficient statistics
* "Simplest": the set which requires us to consider the smallest collection of possible estimators
* Formal: 
    * Minimal suffiency: $T$ is a minimal sufficient statistic if:
        * $T$ is sufficient
        * $T$ is a function of every other sufficient statistic
    * Minimal joint sufficiency: $\textbf{T} = (T_1, ..., T_k)$ are minimal joint sufficient statistic if:
        * $\textbf{T}$ are jointly sufficient
        * $\textbf{T}$ is a function of every other sufficient statistics

# Improving an estimator using sufficient statistics
**The mean squared error of an estimator**:
* Assumptions:
    * $\textbf{X} = (X_1, ..., X_n)$ form a random sample from a distribution with p.m.f $f(x|\theta)$ 
        * $\theta \in \Omega$ is unknown
    * $Z = g(X_1, ..., X_n)$ is a statistic
        * $E_\theta(Z)$ denotes $E(Z|\theta)$
    * $h(\theta)$ is to be estimated using
    * $R(\theta) = E_\theta([\delta(\textbf{X}) - h(\theta)]^2)$ is the loss function
        * $\delta(\textbf{X})$ is an estimator of $\theta$
    * $\textbf{T}$ is a (jointly) statistic for $\theta$
    * Statistician $B$ can only observe $\textbf{T}$
* Conclusion: $B$ can find an estimator $\delta_0$ depending on $\textbf{X}$ via $\textbf{T}$ only, so that $R(\theta, \delta_0) \leq R(\theta, \delta)$ $\forall \theta \in \Omega$
* Explain: see below

**Rao-Blackwell theorem**:
* Assumptions:
    * $\delta(\textbf{X})$ is an estimator of $\theta$
    * $\delta_0(\textbf{T}) = E_\theta[\delta(\textbf{X})|\textbf{T}] = E[\delta(\textbf{X})|\textbf{T}]$
* Conclusion:
    * Rao-Blackwell estimator: $\delta_0(\textbf{X}) = E[\delta(\textbf{X})|\textbf{T}]$
        * Original estimator: $\delta(\textbf{X})$
        * Improved estimator: $\delta_0(\textbf{X})$
    * Rao-Blackwellization: modify $\theta$ to $\theta_0$
* Intuition: $\textbf{T}$ contains all the required information that the data can supply

$\hspace{1.0cm} \rightarrow$ Conditioning on $\textbf{T}$ can provide an estimator which is not worse than the original one, and is often better in the sense of variability
* Properties of $\theta_0$: 
    * $R(\theta, \delta_0) \leq R(\theta, \delta)$ $\forall \theta \in \Omega$
        * Strict case: $R(\theta, \delta) < \infty$, $R(\theta, \delta_0) < R(\theta, \delta)$ $\forall \theta \in \Omega$ unless $\delta(\textbf{X}) = g(\textbf{T})$ for some $g(\cdot)$
        * Generalization: $E_\theta[R(\theta, \delta_0)] \leq E_\theta[R(\theta, \delta)]$ where $R$ is any convex loss
    * Unbias estimator: $\delta_0$ is unbiased if and only if $\delta$ is unbiased
    * Idempotence: using Rao-Blackwellization to improve the already improved estimator doesn't obtain a further improvement
* Prove the improvements:
    * Prove:
        * $E_\theta[R(\theta, \delta)|\textbf{T}] \geq R[\theta, E_\theta(\delta|\textbf{T})] = R(\theta, \delta_0)$ due to Jensen's inequality
        * From above, $E_\theta[R(\theta, \delta)] = E_\theta\{E_\theta[R(\theta, \delta)|\textbf{T}]\} \geq E_\theta[R(\theta, \delta_0)]$
    * Why $\textbf{T}$ must be sufficient: if $\textbf{T}$ isn't sufficient, $\delta_0$ can be a function of $\theta$, which is unknown

# Maximum likelihood estimators and Bayes estimators as sufficient statistics
**MLE and sufficient statistics**:
* Assumptions:
    * $T = r(X_1, ..., X_n)$ is a sufficient statistic for $\theta$
    * $\hat{\theta}$ is the MLE of $\theta$
* Conclusion: 
    * $\hat{\theta}$ depends on $X_1, ..., X_n$ via $T$ only
    * If $\hat{\theta}$ is sufficient, then it's minimal sufficient

**Bayes estimator and sufficient statistics**:
* Assumptions:
    * $T = r(X_1, ..., X_n)$ is a sufficient statistic for $\theta$
    * $\hat{\theta}$ is a Bayes estiamtor of $\theta$
* Conclusion:
    * $\hat{\theta}$ depends on $X_1, ..., X_n$ via $T$ only
    * If $\hat{\theta}$ is sufficient, then it's minimal sufficient

# Complete statistics
**Completeness**: a statistic $T$ is complete w.r.t $\theta$ if $E_\theta[h(T)] = 0$ $\forall \theta \implies P_\theta[h(T) = 0] = 1$ $\forall \theta$
* Intuition: $T$ is complete if $0$ is the only function of $T$, whose expectation is $0$
    * Another interpretation: $E_\theta[h(T)] \neq 0$ for all $h$, except for $h(T) = 0$ only
    * Corollary: if $T$ is complete then $E_\theta[h_1(T)] = E_\theta[h_2(T)] \implies h_1(T) = h_2(T)$
* Boundedly complete: $T$ is boundedly complete if $E_\theta[h(T)] = 0$ $\forall \theta \implies h(t) = 0$ holds for every $h$, which is bounded

**Completeness and sufficiency**:
* Assumptions: 
    * $T$ is a complete and sufficient statistic
    * $U = h(T)$ is a function of $t$ 
* Conclusion: $U$ is the unique unbiased estimator, based on $T$, of its expectation

---

# BONUS
* Order statistic: $\{Y_i\}$ where $Y_i$ is the $i$-th smallest value in $X_1, ..., X_n$
    * Sufficiency: order statistics are sufficient for $\theta$
* Admissibility:
    * Assumptions:
        * $\delta$ and $\delta_0$ are two estimators of $\theta$
        * $R(\theta, \delta)$ is the loss function
    * Conclusion:
        * Inadmissible: $\delta$ is inadmissible if there exists some $\delta_0$ such that $R(\theta, \delta_0) \leq R(\theta, \delta)$

        $\hspace{1.0cm} \rightarrow \delta_0$ dominates $\delta$
        * Admissible: $\delta$ is admissible if there exists no $\delta_0$ such that $R(\theta, \delta_0) \leq R(\theta, \delta)$
* Limitations of sufficient statistics: the existence and the form os a sufficient statistic depend critically on the hypothetical p.m.f
* Robust estimator: estimators which perform reasonably well for a wide variety of possible p.m.f's