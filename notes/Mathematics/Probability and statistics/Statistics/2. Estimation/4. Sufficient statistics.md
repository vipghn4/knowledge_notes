<!-- TOC titleSize:1 tabSpaces:2 depthFrom:1 depthTo:6 withLinks:1 updateOnSave:1 orderedList:0 skip:0 title:1 charForUnorderedList:* -->
# Table of Contents
- [Table of Contents](#table-of-contents)
- [Introduction](#introduction)
- [Definition of a sufficient statistics](#definition-of-a-sufficient-statistics)
- [Definition of a jointly sufficient statistics](#definition-of-a-jointly-sufficient-statistics)
- [Minimal sufficient statistics](#minimal-sufficient-statistics)
- [Improving an estimator using sufficient statistics](#improving-an-estimator-using-sufficient-statistics)
- [Maximum likelihood estimators and Bayes estimators as sufficient statistics](#maximum-likelihood-estimators-and-bayes-estimators-as-sufficient-statistics)
- [Complete statistics](#complete-statistics)
- [BONUS](#bonus)
<!-- /TOC -->

# Introduction
**Drawbacks of maximum likelihood and Bayes estimators**:
* MLEs may not exist in some problem, or there are many MLEs
* If an MLE is unique, it may not be a suitable one (e.g. it always underestimate or overestimate $\theta$)
* Bayes estimator may be hard to find

**Sufficient statistics**: there are methods based on the conditional distribution of various statistics given the parameter

$\hspace{1.0cm} \rightarrow$ Sufficient statistics are the most useful statistics
* Usage: get rid of the dependency of $f_n(\textbf{x}|\theta)$ from directly observing $\textbf{x}$

# Definition of a sufficient statistics
**Idea**:
* Assumptions:
    * Statistician $A$ can observe $X_1, ..., X_n$
    * Statistician $B$ can observe $T = r(X_1, ..., X_n)$
* Observations:
    * $A$ should be able to find a better estimator than $B$
        * Explain: $A$ can use any function of $X_1, ..., X_n$ while $B$ can only use functions of $T$
    * If $T$ summarizes all the information of the random sample, and individual values of $X_1, ..., X_n$ are irrelevant in the search for a good estimator of $\theta$

    $\hspace{1.0cm} \rightarrow B$ can use $T$ to do as well as $A$
* Intuition: $B$ can simulate random variables $X'_1, ..., X'_n$ by learning $T$, so that $f_n(X'_1, ..., X'_n) = f_n(X_1, ..., X_n)$

$\hspace{1.0cm} \rightarrow B$ can use $X'_1, ..., X'_n$ in the same way $A$ uses $X_1, ..., X_n$ to estimate $\theta$

**Auxiliary randomization**: the process of simulating $X'_1, ..., X'_n$

**Sufficient statistics**:
* Assumptions:
    * $X_1, ..., X_n$ form a random sample from a distribution indexed by $\theta$
    * $T$ is a statistic so that $f_n(X_1, ..., X_n|T = t, \theta) = f_n(X_1, ..., X_n|T = t)$
* Conclusion: $T$ is a sufficient statistic for $\theta$ 
* Informal: statistics which summarize all the information of the sample

**Factorization criterion for statistical sufficiency**:
* Assumptions:
    * $X_1, ..., X_n$ form a random sample from a distribution with p.m.f $f(x|\theta)$
        * $\theta \in \Omega$ is unknown
    * $T = r(X_1, ..., X_n)$ is a statistic
* Conclusion: $T$ is sufficient for $\theta$ if and only if $f_n(\textbf{x}|\theta)$ can be factored as $f_n(\textbf{x}|\theta) = u(\textbf{x}) v[r(\textbf{x}), \theta]$ $\forall \textbf{x}, \theta$
    * $u(\textbf{x})$ is independent of $\theta$
    * $v$ depends on $\theta$, but depends on $\textbf{x}$ via $r(\textbf{x})$ only
* Another interpretation: $T = r(X_1, ..., X_n)$ is sufficient for $\theta$ if and only if $f_n(\textbf{x}|\theta) \propto v[r(\textbf{x}, \theta)]$
    * Explain: to estimate $\theta$ based on $f_n(\textbf{x}|\theta)$, we only have to care about $r(\textbf{x})$, since $u(\textbf{x})$ is a constant
* Corollary: $T$ is sufficient if and on if $g(\theta|\textbf{x})$ depends on $\textbf{x}$ via $r(\textbf{x})$ only

**Theorem**: if $T = r(X_1, ..., X_n)$ is sufficient then $h(T)$ is sufficient if $h(\cdot)$ is one-to-one

# Definition of a jointly sufficient statistics
**Jointly sufficient statistics**: sometimes, there's no 1D sufficient statistic
* Assumptions:
    * $X_1, ..., X_n$ form a random sample from a distribution indexed by $\theta$
    * $T_1, ..., T_k$ are $k$ statistics
        * $T_i = r_i(\textbf{X})$
* Conclusion: $(T_1, ..., T_k)$ are jointly sufficient if $f_n(\textbf{X}|\{T_i = t_i\}, \theta) = f_n(\textbf{X}|\{T_i = t_i\})$
* Informal: $T_1, ..., T_k$ are jointly sufficient if they summarize all the information of the sample

**Factorization criterion for statistical jointly sufficiency**:
* Assumptions:
    * $\{T_i|T_i = r_i(\textbf{X})\}$ are statistics
* Conclusion: $\{T_i\}$ are jointly sufficient for $\theta$ if $f_n(\textbf{x}|\theta)$ can be factored as $f(\textbf{x}|\theta) = u(\textbf{x}) v[r_1(\textbf{x}), ..., r_k(\textbf{x}), \theta]$

# Minimal sufficient statistics
**Minimal sufficient statistics**: the simplest set of joint sufficient statistics
* "Simplest": the set which requires us to consider the smallest collection of possible estimators
* Formal: 
    * Minimal suffiency: $T$ is a minimal sufficient statistic if:
        * $T$ is sufficient
        * $T$ is a function of every other sufficient statistic
    * Minimal joint sufficiency: $\textbf{T} = (T_1, ..., T_k)$ are minimal joint sufficient statistic if:
        * $\textbf{T}$ are jointly sufficient
        * $\textbf{T}$ is a function of every other sufficient statistics

# Improving an estimator using sufficient statistics
**The mean squared error of an estimator**:
* Assumptions:
    * $\textbf{X} = (X_1, ..., X_n)$ form a random sample from a distribution with p.m.f $f(x|\theta)$ 
        * $\theta \in \Omega$ is unknown
    * $Z = g(X_1, ..., X_n)$ is a statistic
        * $E_\theta(Z)$ denotes $E(Z|\theta)$
    * $h(\theta)$ is to be estimated using
    * $R(\theta) = E_\theta([\delta(\textbf{X}) - h(\theta)]^2)$ is the loss function
        * $\delta(\textbf{X})$ is an estimator of $\theta$
    * $\textbf{T}$ is a (jointly) statistic for $\theta$
    * Statistician $B$ can only observe $\textbf{T}$
* Conclusion: $B$ can find an estimator $\delta_0$ depending on $\textbf{X}$ via $\textbf{T}$ only, so that $R(\theta, \delta_0) \leq R(\theta, \delta)$ $\forall \theta \in \Omega$
* Explain: see below

**Rao-Blackwell theorem**:
* Assumptions:
    * $\delta(\textbf{X})$ is an estimator of $\theta$
    * $\delta_0(\textbf{T}) = E_\theta[\delta(\textbf{X})|\textbf{T}] = E[\delta(\textbf{X})|\textbf{T}]$
* Conclusion:
    * Rao-Blackwell estimator: $\delta_0(\textbf{X}) = E[\delta(\textbf{X})|\textbf{T}]$
        * Original estimator: $\delta(\textbf{X})$
        * Improved estimator: $\delta_0(\textbf{X})$
    * Rao-Blackwellization: modify $\theta$ to $\theta_0$
* Intuition: $\textbf{T}$ contains all the required information that the data can supply

$\hspace{1.0cm} \rightarrow$ Conditioning on $\textbf{T}$ can provide an estimator which is not worse than the original one, and is often better in the sense of variability
* Properties of $\theta_0$: 
    * $R(\theta, \delta_0) \leq R(\theta, \delta)$ $\forall \theta \in \Omega$
        * Strict case: $R(\theta, \delta) < \infty$, $R(\theta, \delta_0) < R(\theta, \delta)$ $\forall \theta \in \Omega$ unless $\delta(\textbf{X}) = g(\textbf{T})$ for some $g(\cdot)$
        * Generalization: $E_\theta[R(\theta, \delta_0)] \leq E_\theta[R(\theta, \delta)]$ where $R$ is any convex loss
    * Unbias estimator: $\delta_0$ is unbiased if and only if $\delta$ is unbiased
    * Idempotence: using Rao-Blackwellization to improve the already improved estimator doesn't obtain a further improvement
* Prove the improvements:
    * Prove:
        * $E_\theta[R(\theta, \delta)|\textbf{T}] \geq R[\theta, E_\theta(\delta|\textbf{T})] = R(\theta, \delta_0)$ due to Jensen's inequality
        * From above, $E_\theta[R(\theta, \delta)] = E_\theta\{E_\theta[R(\theta, \delta)|\textbf{T}]\} \geq E_\theta[R(\theta, \delta_0)]$
    * Why $\textbf{T}$ must be sufficient: if $\textbf{T}$ isn't sufficient, $\delta_0$ can be a function of $\theta$, which is unknown

# Maximum likelihood estimators and Bayes estimators as sufficient statistics
**MLE and sufficient statistics**:
* Assumptions:
    * $T = r(X_1, ..., X_n)$ is a sufficient statistic for $\theta$
    * $\hat{\theta}$ is the MLE of $\theta$
* Conclusion: 
    * $\hat{\theta}$ depends on $X_1, ..., X_n$ via $T$ only
    * If $\hat{\theta}$ is sufficient, then it's minimal sufficient

**Bayes estimator and sufficient statistics**:
* Assumptions:
    * $T = r(X_1, ..., X_n)$ is a sufficient statistic for $\theta$
    * $\hat{\theta}$ is a Bayes estiamtor of $\theta$
* Conclusion:
    * $\hat{\theta}$ depends on $X_1, ..., X_n$ via $T$ only
    * If $\hat{\theta}$ is sufficient, then it's minimal sufficient

# Complete statistics
**Completeness**: a statistic $T$ is complete w.r.t $\theta$ if $E_\theta[h(T)] = 0$ $\forall \theta \implies P_\theta[h(T) = 0] = 1$ $\forall \theta$
* Intuition: $T$ is complete if $0$ is the only function of $T$, whose expectation is $0$
    * Another interpretation: $E_\theta[h(T)] \neq 0$ for all $h$, except for $h(T) = 0$ only
    * Corollary: if $T$ is complete then $E_\theta[h_1(T)] = E_\theta[h_2(T)] \implies h_1(T) = h_2(T)$
* Boundedly complete: $T$ is boundedly complete if $E_\theta[h(T)] = 0$ $\forall \theta \implies h(t) = 0$ holds for every $h$, which is bounded

**Completeness and sufficiency**:
* Assumptions: 
    * $T$ is a complete and sufficient statistic
    * $U = h(T)$ is a function of $t$ 
* Conclusion: $U$ is the unique unbiased estimator, based on $T$, of its expectation

---

# BONUS
* Order statistic: $\{Y_i\}$ where $Y_i$ is the $i$-th smallest value in $X_1, ..., X_n$
    * Sufficiency: order statistics are sufficient for $\theta$
* Admissibility:
    * Assumptions:
        * $\delta$ and $\delta_0$ are two estimators of $\theta$
        * $R(\theta, \delta)$ is the loss function
    * Conclusion:
        * Inadmissible: $\delta$ is inadmissible if there exists some $\delta_0$ such that $R(\theta, \delta_0) \leq R(\theta, \delta)$

        $\hspace{1.0cm} \rightarrow \delta_0$ dominates $\delta$
        * Admissible: $\delta$ is admissible if there exists no $\delta_0$ such that $R(\theta, \delta_0) \leq R(\theta, \delta)$
* Limitations of sufficient statistics: the existence and the form os a sufficient statistic depend critically on the hypothetical p.m.f
* Robust estimator: estimators which perform reasonably well for a wide variety of possible p.m.f's