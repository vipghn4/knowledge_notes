<!-- TOC titleSize:1 tabSpaces:2 depthFrom:1 depthTo:6 withLinks:1 updateOnSave:1 orderedList:0 skip:0 title:1 charForUnorderedList:* -->
# Table of Contents
- [Table of Contents](#table-of-contents)
- [The information theory diagram](#the-information-theory-diagram)
  - [Source coding (see note 4) and channel coding](#source-coding-see-note-4-and-channel-coding)
- [Channel capacity](#channel-capacity)
  - [Introduction](#introduction)
  - [Definitions](#definitions)
  - [Properties of channel capacity](#properties-of-channel-capacity)
- [Another intuition about channel capacity](#another-intuition-about-channel-capacity)
- [Types of channel](#types-of-channel)
- [Definitions](#definitions-1)
- [Jointly typical sequences](#jointly-typical-sequences)
- [Channel coding theorem](#channel-coding-theorem)
  - [Idea of proof](#idea-of-proof)
  - [Main theorem](#main-theorem)
  - [Communication procedure (used for analysis only)](#communication-procedure-used-for-analysis-only)
  - [Proof](#proof)
  - [Best code](#best-code)
- [Zero-error codes](#zero-error-codes)
- [Hamming codes](#hamming-codes)
  - [Historical motivations](#historical-motivations)
  - [Hamming code](#hamming-code)
  - [Discussion](#discussion)
- [Feedback capacity](#feedback-capacity)
- [Source-channel separation theorem](#source-channel-separation-theorem)
  - [Motivation](#motivation)
  - [Main theorem](#main-theorem-1)
- [BONUS](#bonus)
- [NEW WORD](#new-word)
<!-- /TOC -->

# The information theory diagram
**Shannon's statement**: The fundamental problem
of communication is that
of reproducing at one
point either exactly or
approximately a message
selected at another point.
Frequently the messages
have meaning . . . \[which
is\] irrelevant to the
engineering problem

## Source coding (see note 4) and channel coding
**Source coding**:
* Source (Shannon's viewpoint): a generator of discrete messages
  * The messages come from a discrete set or a continuous set with a probability distribution associated with them
* Problem: given a noiseless communication system, what's the most efficient way of communicating the source message to the receivers
  * Idea: highly probable massages are assigned the shortest coded transmissions

**Channel coding**:
* Channel (Shannon's viewpoint): characterized by a set of chances of getting particular output symbols given particular input symbols
* Problem: what's the maximum rate, at which we can convey information with small chance of error through a probabilistic channel

# Channel capacity
## Introduction
**Communication**:
* $A$ communicates with $B$: the physical acts of $A$ have induced a desired physical state in $B$
* Successful communication: the communication is successful if $B$ (receiver) and $A$ (transmitter) agree on what was sent
* Issue with communication: the communication between $A$ and $B$ is a physical process

$\hspace{1.0cm} \rightarrow$ There might be uncontrollable ambient noise and imperfections of the physical signaling process

**Problem**: find the highest information rate (in units of information per unit time for continuous case or in units of information per channel use for discrete case) that can be achieved with arbitrarily small error probability (i.e. channel capacity)

>**NOTE**: the characterization of the channel capacity as the maximum mutual information is the central and most famous success of information theory

**Duality between the problems of data compression and data transmission**:
* Compression: we remove all the redundancy in the data to form the most compressed version possible
* Transmission: we add redundancy in a controlled fashion to combat errors in the channel

## Definitions
**Discrete channel**: a system consisting of
* An input alphabet $\cal{X}$ 
* An output alphabet $\cal{Y}$
* A probability transition matrix $p(y|x)$ where $p(y|x)_{ij} = p(y_j|x_i)$
  * $p(y|x)$ is the probability of observing the output symbol $y$ given the input symbol $x$ was sent

**Memoryless channel**: the probability distribution of the output depends only on the input at that time and conditionally independent of previous channel inputs or outputs

**Channel capacity of a discrete memoryless channel**: 
* Information channel capacity:
  * Assumptions:
    * $W$ is the message to be transmitted (i.e. the input of the encoder)
    
    >**NOTE**: in this note, we assume that $W$ comes from some source (i.e. $W$ is the raw message, which hasn't been compressed)
    
    * $X$ is the input symbol ($X^n$ is the input sequence) of the channel
    * $Y$ is the output symbol ($Y^n$ is the output sequence) of the channel
    * $\hat{W}$ is the estimation of the transmitted message $W$ (i.e. the output of the decoder)
    * $p(y|x)$ is the conditional probability of $y$ given $x$
  * Conclusion:
    * The basic mathematical model for a communication system:
    
    $\hspace{1.0cm} \xrightarrow[]{W} {\text{Encoder}} \xrightarrow[]{X^n} {\text{Channel} \\ p(y|x)} \xrightarrow[]{Y^n} {\text{Decoder}} \xrightarrow[]{\hat{W}}$
    * The rate of information flow (corresponding to the input distribution $p(x)$) through a channel: $I(X; Y)$ (units of symbols per channel use)
    * Channel capacity: $C = \max_{p(x)} I(X; Y)$
      * Explain: the maximum information flow over all possible input distributions $p(x)$
      * Intuition:
        * The rate of information flow is the amount of information flow from input $X$ to output $Y$ (i.e. the amount of information in $X$, which can be completely retrieved from $Y$)

        $\hspace{1.0cm} \rightarrow$ In order to completely retrieve some information of $X$ from $Y$, that information must be in both $X$ and $Y$ (i.e. the mutual information of $X$ and $Y$)

* Operational channel capacity: the highest rate in bits per channel use, at which information can be sent with arbitrarily low probability of error
* Shannon's second theorem: the information channel capacity equals to the operational channel capacity

$\hspace{1.0cm} \rightarrow$ We can drop the word "information" in most discussion of channel capacity

## Properties of channel capacity
**Properties**:
  * $C \geq 0$
  * $C \leq \log \min (|\cal{X}|, |\cal{Y}|)$
    * Intuition: see the intuition of channel capacity
    * Explain:
      * $I(X; Y) = H(X) - H(X|Y) \leq H(X) \leq \log |\cal{X}|$ $\forall p(x)$
      * $I(X; Y) = H(Y) - H(Y|X) \leq H(Y) \leq \log |\cal{Y}|$ $\forall p(x)$
  * $I(X; Y)$ is a continuous function of $p(x)$
  * $I(X; Y)$ is a concave function of $p(x)$

**Observations**:
* $I(X; Y)$ is a concave function over a close convex set $\{p(x)|p(x) \in [0, 1]\}$

$\hspace{1.0cm} \rightarrow$ A local maximum is a global maximum
* $C \leq \min (\log |\cal{X}|, \log |\cal{Y}|)$ (i.e. $\max C$ is finite)

$\hspace{1.0cm} \rightarrow$ The maximum can then be found by standard non-linear optimization techniques

>**NOTE**: in general, there's no closed-form solution for the capacity

# Another intuition about channel capacity
**Assumptions**:
* $X^n$ is the (typical) input sequence of i.i.d random variables
  * $X_i$ has distribution $p(X)$
* $Y^n$ is the (typical) output sequence, obtained by passing $X^n$ through a channel characterized by $p(Y|X)$
* $X^n$ and $Y^n$ are represented using symbols taken from a $D$-ary alphabet

**Observations**:
* The expected number of symbols used to represent $X_i$ is $H(X_i)$

$\hspace{1.0cm} \rightarrow$ Due to AEP, the expected number of symbols used to represent $X^n$ is $n H(X)$
* In other words, there are approximately $D^{n H(X)}$ possible cases of $X^n$ as $n$ grows larger
* From above,
  * The expected number of possible cases of $Y^n$ given some $X^n$ is $D^{n H(Y|X)}$
  * The expected number of possible cases of $Y^n$ is $D^{n H(Y)}$
* In communication, each possible input sequence $x^n$, after passing through the system, can be mapped to itself (if the system works perfectly) or some other sequence (if there is some noise or error during the process)
  * Let $Y_x$ be the set of possible output sequences that can be obtained by passing $x^n$ through the system
  * Consider the set of possible input sequences $\{x^n\}$, to ensure that we can always determine the input sequence given the output sequence
  
  $\hspace{1.0cm} \rightarrow$ The sets of possible output sequences corresponding to different input sequences must be disjoint
* From above, to ensure that we can always determine the input sequence given the output sequence

$\hspace{1.0cm} \rightarrow$ The size of the set of possible input sequences must not exceed $\frac{D^{n H(Y)}}{D^{n H(Y|X)}} = D^{n(H(Y) - H(Y|X)} = D^{n I(X; Y)}$ (approximately)

# Types of channel
**Noiseless binary channel**: the binary input is re-produced exactly at the output
* Properties:
  * Any transmitted bit is received without error

**Noisy channel with non-overlapping outputs**: there are two possible outputs corresponding to each of the two inputs
* Properties:
  * Every transmitted bit can be recovered without error (since each output symbol corresponds to only one input symbol)

**Noisy typewriter**: the channel input is either received unchanged at the output with probability $\frac{1}{2}$ or is transformed into the next letter with probability $\frac{1}{2}$
* Properties:
  * The expected number of symbols, which are transmitted without error, is $\frac{n}{2}$ where $n$ is the number of symbols

**Binary symmetric channel (BSC)**: a binary channel, in which the input symbols are transmitted incorrectly with probability $p$
* Properties:
  * The bits received don't reveal where the errors have occurred
  
  $\hspace{1.0cm} \rightarrow$ All bits received are unreliable
  * We can still use such communication channel to send information at a non-zero rate with an arbitrarily small probability of error

**Binary erasure channel**: the analog of the binary symmetric channel, in which some bits are lost (rather than corrupted) (i.e. a fraction $\alpha$ of the bits are erased)
* Properties:
  * The receiver knows which bits have been erased
  * The binary erasure channel has two inputs ($0$ and $1$) and three outputs ($0$, $1$ and $\epsilon$)

**Symmetric channel**
* Assumptions:
  * $p(y|x)$ is the channel transition matrix
* Symmetric channel: a channel is symmetric if
  * The rows of $p(y|x)$ are permutations of each other
  * The columns of $p(y|x)$ are permutations of each other

* Weakly symmetric: a channel is weakly symmetric if
  * Every row of $p(y|x)$ is a permutation of every other row
  * All the column sums $\sum_x p(y|x)$ are equal

* Theorem: 
  * Statement: for a weakly symmetric channel, $C = \log |\cal{Y}| -$ $H(\text{row of transition matrix})$, which is achieved by a uniform distribution on the input alphabet

# Definitions
**Idea about a the communication system**:
* $W$ represents a message, which is the raw message (i.e. hasn't been compressed yet)
* The encoder receives $W$ and encodes it to $X^n$
* The encoder puts $X^n$ into the channel and $Y^n$ is the output the decoder receives
* The decoder tries to estimate $W$ through $Y^n$
  * The decoder's estimate is denoted by $\hat{W}$

**Discrete channel**: denoted by $(\cal{X}, p(y|x), \cal{Y})$
* $\cal{X}, \cal{Y}$ are finite sets
* $p(y|x)$ is a collection of conditional p.m.f, one for each $x \in \cal{X}$

**$n$-th extension of the discrete memoryless channel (DMC)**: 
* DMC: $(\cal{X}^n, p(y^n|x^n), \cal{Y}^n)$ where $p(y_k|x^k, y^{k-1}) = p(y_k|x_k)$ $\forall k \in [1, n]$
  * Intuition: the current output symbol is conditionally (given the current input symbol) independent of the previous output symbols

  $\hspace{1.0cm} \rightarrow$ This is the memorylessness of the channel
* DMC without feedback: a DMC $(\cal{X}^n, p(y^n|x^n), \cal{Y}^n)$ where $p(x_k|x^{k-1}, y^{k-1}) = p(x_k|x^{k-1})$
  * Explain: the input symbols is conditionally (given the past input symbol) independent of the past output symbols
  * The channel transition function for the $n$-th extension of the DMC: $p(y^n|x^n) = \prod_{i = 1}^n p(y_i|x_i)$

>**NOTE**: by default, DMC means DMC without feedback (unless we state explicitly otherwise)

**An $(M, n)$ code for the channel $(\cal{X}, p(y|x), \cal{Y})$**: 
* Definition: consist of
  * A set of possible messages $\{1, ..., M\}$
  * An encoding function $X^n: \{1, ..., M\} \to \cal{X}^n$
    * Codebook (the range of the encoding function): $\{x^n(i)|i \in [1, M]\}$
  * A decoding function $g: \cal{Y}^n \to \{1, ..., M\}$
* Another interpretation:
  * The number of possible messages to be transmitted is $M$
  * Each message is encoded using $n$ symbols

>**NOTE**: some documents denote a code as $(M, n, X^n, \hat{W})$ instead of $(M, n)$

**Conditional probability of error given that index $i$ was sent**: 
* Formula: $\lambda_i = \text{Pr}(g(Y^n) \neq i|X^n = x^n(i)) = \sum_{y^n} p(y^n|x^n(i)) I(g(y^n) \neq i)$
  * $I(\cdot)$ is the indicator function

**The maximal probability of error $\lambda^{(n)}$ for an $(M, n)$ code**: $\lambda^{(n)} = \max_{i \in \{1, ..., M\}} \lambda_i$

**The (arithmetic) average probability of error for an $(M, n)$ code**: $P^{(n)}_e = \frac{1}{M} \sum_{i=1}^M \lambda_i$

**The rate $R$ of an $(M, n)$ code**: $R = \frac{\log M}{n}$ bits per transmission (i.e. channel use)
* Informal definition: the ratio between of how many symbols of message are transmitted and how many symbols are used for encoding
* Intuition: 
  * Since we initially don't know the exact distribution of the message $W$ coming from source

  $\hspace{1.0cm} \rightarrow$ For integrity, we assume that $W$ has the uniform distribution over the message space $\{1, ..., M\}$ and take $\log M$ bits to represent it
  * The encoder uses $n$ bits to encode $W$
  
  $\hspace{1.0cm} \rightarrow R$ can be understood as the amount of information (from source) that is carried within each bit of the output of the encoder
* Achievable rate: $R$ is achievable if there exists a sequence of $(\lceil 2^{nR}\rceil, n)$ codes that $\lambda^{(n)} \to 0$ as $n \to \infty$

**The operational capacity of a channel**: the supremum of all achievable rates

$\hspace{1.0cm} \rightarrow$ Rates less than capacity yield arbitrarily small probability of error for sufficiently large block lengths

>**NOTE**: we must distinguish operational capacity of a channel and information capacity of a channel, which is $C = \max_{p(x)} I(X; Y)$

# Jointly typical sequences
**The set of jointly typical sequences $\{(x^n, y^n)\}$ w.r.t the distribution $p(x, y)$**: 
* Informal definition: the set of $n$-sequences with empirical entropies $\epsilon$-close to the true entropies
* Formal: $A_\epsilon^{(n)}$ is the set of$(x^n, y^n) \in {\cal{X}}^n \times {\cal{Y}}^n$ satisfying:
  * $|\frac{1}{n} \log \frac{1}{p(x^n)} - H(X)| < \epsilon$
  * $|\frac{1}{n} \log \frac{1}{p(y^n)} - H(Y)| < \epsilon$
  * $|\frac{1}{n} \log \frac{1}{p(x^n, y^n)} - H(X, Y)| < \epsilon$ where $p(x^n, y^n) = \prod_{i = 1}^n p(x_i, y_i)$

**Joint AEP**:
* Assumptions:
  * $(X^n, Y^n)$ is the sequences of length $n$ drawn i.i.d according to $p(x^n, y^n)$
    * $p(x^n, y^n) = \prod_{i = 1}^n p(x_i, y_i)$
* Conclusion:
  * $\text{Pr}[(X^n, Y^n) \in A_\epsilon^{(n)}] \to 1$ as $n \to \infty$
    * Explain: due to AEP
  * $(1 - \delta) D^{n (H(X, Y) - \epsilon)} \leq |A_\epsilon^{(n)}| \leq D^{n (H(X, Y) + \epsilon)}$ for large $n$
    * Explain: see AEP note
  * If $(\tilde{X}^n, \tilde{Y}^n) \sim p(x^n) p(y^n)$ (i.e. $X^n, Y^n$ are independent sequences) then
    * $\text{Pr}[(\tilde{X}^n, \tilde{Y}^n) \in A_\epsilon^{(n)}] \leq D^{-n(I(X; Y) - 3 \epsilon)}$
      * Intuition:
        * There are about $D^{n H(X)}$ typical sequences and about $D^{n H(Y)}$ typical $Y$ sequences but only $D^{n H(X, Y)}$ jointly typical sequences
          
        $\hspace{1.0cm} \rightarrow$ Not all pairs of typical $X^n$ and typical $Y^n$ are jointly typical
        * From above, the probability that any randomly chosen $(\tilde{X}^n, \tilde{Y}^n)$ is jointly typical is about $D^{-n I(X; Y)}$
    * $\text{Pr}[(\tilde{X}^n, \tilde{Y}^n) \in A_\epsilon^{(n)}] \geq (1 - \delta) D^{-n(I(X; Y) + 3 \epsilon)}$ for sufficient large $n$
* Consequences to jointly typical decoding (see below):
  * The probability that any randomly chosen $(\tilde{X}^n, \tilde{Y}^n)$ is jointly typical is about $D^{-n I(X; Y)}$
  
  $\hspace{1.0cm} \rightarrow$ There are about $D^{n I(X; Y)}$ distinguishable $X^n$
  * Consider the set of jointly typical sequences for a fixed output $Y^n$ resulting from the true input $X^n$,
    * There are about $D^{n H(X|Y)}$ conditionally typical inputs
    
    $\hspace{1.0cm} \rightarrow$ The probability that some randomly chosen (other) input $\hat{X}^n$ is jointly typical with $Y^n$ is $D^{n H(X|Y)} / D^{n H(X)} = D^{-n I(X; Y)}$
    * From above, we can choose about $D^{n I(X; Y)}$ codewords $\hat{X}^n$ before one of these get confused with the true input $X^n$

**Advantage jointly typical sequences**: the set of jointly typical sequences is a relative small set whose probability to occur is approximately $100\%$

# Channel coding theorem
>**NOTE**: channel coding theorem is rather counter-intuitive

**The law of large numbers (recall)**:
* Assumptions:
    * $\bar{X} = \frac{1}{n} (X_1 + ... + X_n)$
* Conclusion:
    * $\bar{X} \to E(X)$ for $n \to \infty$

## Idea of proof
**Ideas of Shannon to prove that information can be sent reliably over a channel at all rates up to the channel capacity**:
* Allow an arbitrarily small but non-zero probability of error
* Use the channel many times in succession, so that the law of large numbers comes into effect
* Calculate the average of probability of error over a random choice of codebooks, which symmetrizes the probability, and which can then be used to show the existence of at least one good code

**Shannon's outline of proof**: based on the idea of typical sequences
* Jointly typical decoding rule: in our proof (not Shannon's), our decoding rule is to decode by joint typicality 
    * Procedure:
        * Step 1: look for a codeword that is jointly typical with the received sequence
        * Step 2: if we find a unique codeword satisfying this property
        
        $\hspace{1.0cm} \rightarrow$ We declare that word to be the transmitted codeword
    * Explain the validity:
        * By the properties of joint typically stated previously, with high probability the transmitted codeword and the received sequence are jointly typical
            * Explain: they are probabilistically related
        * The probability that any other codeword looks jointly typical with the received sequence is $D^{-n I}$
        
        $\hspace{1.0cm} \rightarrow$ If we have fewer than $D^{n I}$ codewords, then with high probability there will be no other codewords that can be confused with the transmitted codeword and the probability of error is small
    
    >**NOTE**: this decoding rule is sub-optimal, but it's simple to analyze and still achieves all rates below capacity

## Main theorem
**Channel coding theorem**: 
* Statement:
    * For a discrete memoryless channel, all rates below capacity $C$ is achievable
    * For every rate $R < C$, there exists a sequence of $(D^{n R}, n)$ codes with maximum probability of error $\lambda^{(n)} \to 0$
    * Any sequence of $(D^{n R}, n)$ codes with $\lambda^{(n)} \to 0$ must have $R \leq C$

**Intuition**: 
* We consider the encoding function of a $(M, n)$ code as a mapping from the set of raw messages $\{1, ..., M\}$ to a set of sequences of messages ${\cal{X}}^n$
    * Explain: each message $W$ is mapped into a sequence $X^n(W) = (X_1, ..., X_n)$ where $X_i$ is a input message to the channel
* As stated in the definition of channel capacity

$\hspace{1.0cm} \rightarrow$ The amount of information, measured in bits, flow from the transmiter to the receiver is $I(X; Y)$
* From above, if each message $X$ in the transmitted sequence $X^n$ carries more information than $I(X; Y)$ bits

$\hspace{1.0cm} \rightarrow$ A proportion of information, measured in bits, carried by the messages maybe dropped out and error may occur

## Communication procedure (used for analysis only)
**Code generation**:
* Assumptions:
    * $p(x)$ is fixed
* Code generating procedure:
    * Step 1: generate a $(D^{nR}, n)$ code at random according to $p(x)$
    * Step 2: generate $D^{nR}$ codewords independently according to $p(x^n) = \prod_{i = 1}^n p(x_i)$
    * Step 3: write the generated codewords as the rows of a matrix
    
    $\hspace{1.0cm} {\cal{C}} = \begin{bmatrix} x_1(1) & \cdots & x_n(1) \\
                                        \vdots & \ddots & \vdots \\
                                        x_1(D^{nR}) & \cdots & x_n(D^{nR})\end{bmatrix}$
    
>**NOTE**: each entries in $C$ is generated i.i.d according to $p(x)$

$\hspace{1.0cm} \rightarrow \text{Pr}({\cal{C}}) = \prod_{w = 1}^{D^{nR}} \prod_{i = 1}^n p(x_i(w))$

**Communication**:
* Step 1: a random code $\cal{C}$ is generated as above according to $p(x)$
* Step 2: $\cal{C}$ is revealed to both sender and receiver

>**NOTE**: both sender and receiver are assumed to know the channel transition matrix $p(y|x)$ for the channel

* Step 3: a message $W$ is chosen according to a uniform distribution
    * Formal: $\text{Pr}(W = w) = D^{-nR}$ $\forall w \in [1, D^{nR}]$
* Step 4: the $w$-th codeword $X^n(w)$ (i.e. row $w$ in $\cal{C}$) is sent over the channel
* Step 5: the receiver receives a sequence $Y^n$ according to $P(y^n|x^n(w)) = \prod_{i = 1}^n p(y_i|x_i(w))$
* Step 6: the receiver guesses which message was sent by a decoding scheme

**Decoding scheme**:
* Target: minimize probability of error
* Optimal decoding scheme: maximum likelihood decoding (i.e. choose the posteriori most likely message)
    * Disadvantage: difficult for analysis
    
    $\hspace{1.0cm} \rightarrow$ We will use the decoding scheme described below instead
* Joint typical decoding:
    * Description: 
        * The receiver declares that $\hat{W}$ was sent if:
            * $(X^n(\hat{W}), Y^n)$ is jointly typical (i.e. in $A_\epsilon^{(n)}$)
            * There's no index $W' \neq \hat{W}$ that $(X^n(W'), Y^n) \in A_\epsilon^{(n)}$
        * Possible errors:
            * Error 1: no such $\hat{W}$ exists or if there's more than one such
            * Error 2: $\hat{W} \neq W$
            
            $\hspace{1.0cm} \rightarrow$ In this case, we denote $\varepsilon$ as the event $\{\hat{W} \neq W\}$
    * Advantage: easier for analysis and asymptotically optimal

## Proof
* If we take the rate $R$, there would be $D^{nR}$ possible inputs to the channel
* According to the consequences of joint AEP, 

$\hspace{1.0cm} \rightarrow$ Given the received $Y^n$, the probability that there are some $X^n(W')$ confused with the true input $X^n(\hat{W})$ is $D^{nR} D{-nI(X; Y)}$
* Achievability:
  * For any $R = I(X; Y) - \epsilon$ for some $\epsilon > 0$ and $n \to \infty$, 
  
  $\hspace{1.0cm} \text{Pr}(\varepsilon) = \text{Pr}[(X^n(\hat{W}), Y^n) \notin A_\epsilon^{(n)}] + \text{Pr}[\exists W' \neq \hat{W}, (X^n(W'), Y^n) \in A_\epsilon^{(n)}]$
  
  $\hspace{2.0cm} \approx \delta + D^{nR} D{-n I(X; Y)}$ where $\delta \to 0$ as $n \to \infty$
  
  $\hspace{2.0cm} = \delta + D^{- n \epsilon} \to 0$
  
  $\hspace{1.0cm} \rightarrow$ Any rate $R < C$ is achievable
* Upper bound on rate:
  * Assume that $W$ is sampled uniformly from the set of possible messages
  * According to Fano's inequality,
  
  $\hspace{1.0cm} H(W|Y^n) \leq \log D^{nR} \text{Pr}(\varepsilon) + 1$
  
  * From above,
  
  $\hspace{1.0cm} \text{Pr}(\varepsilon) \geq \frac{H(W|Y^n) - 1}{nR}$
  
  $\hspace{2.2cm} = \frac{H(W) - I(W; Y^n) - 1}{nR}$
  
  $\hspace{2.2cm} \geq \frac{nR - I(X^n; Y^n) - 1}{nR}$ due to data processing inequality
  
  $\hspace{2.2cm} \geq \frac{nR - nC - 1}{nR} \to \frac{R - C}{R}$
  * For any $R = I(X; Y) + \epsilon$ for some $\epsilon > 0$ and $n \to \infty$, $\frac{R - C}{R}$ doesn't converge to $0$
  
  $\hspace{1.0cm} \rightarrow \text{Pr}(\varepsilon) \to 0$ if $R \leq C$ only

## Best code
**Constructing the best code**: the theorem shows that there exists good codes with arbitrarily small probability of error for long block lengths, it doesn't provide a way of constructing the best codes
* The scheme suggested by the proof: 
  * Advantage: if we generate a code at random with appropriate distribution
  
  $\hspace{1.0cm} \rightarrow$ The constructed code is likely to be good for long block lengths
  * Disadvantage: without some structure in the code
  
  $\hspace{1.0cm} \rightarrow$ It's very difficult to decode (i.e. the simple scheme of table lookup requires an exponentially large table)
* Conclusion: the theorem doesn't provide a practical coding scheme

# Zero-error codes
**Observations**:
* If $W$ can be determined uniquely from $Y^n$

$\hspace{1.0cm} H(W) = H(W|Y^n) + I(W; Y^n) = I(W; Y^n)$
* Since $W \to X^n(W) \to Y^n(W)$

$\hspace{1.0cm} \rightarrow I(W; Y^n) \leq I(X^n; Y^n)$

$\hspace{3.6cm} = \sum_{i = 1}^n I(X_i, Y_i)$

$\hspace{3.6cm} \leq n C$

**Conclusion**: zero error implies $R \leq C$

# Hamming codes
## Historical motivations
**History of coding theory**:
* Since the appearance of channel coding theorem (in Shannon's original 1948 paper)

$\hspace{1.0cm} \rightarrow$ People have searched for codes that allow us to transmit information with an arbitrarily small probability of error if the block length is large enough

>**NOTE**: besides achieving low probabilities of error, simplicity should be achieved so that we can encode and decode efficiently

* Coding theory has been developed as an entire field of searching for simple good codes
  * One of the simplest coding scheme was proposed by Hamming

**The object of coding**: introduce redundancy so that even if some of the information is lost or corrupted

$\hspace{1.0cm} \rightarrow$ It will still be possible to recover the message at the receiver

**Examples of simple coding schemes**:
* The most obvious coding scheme: 
  * Encoding: repeat information
    * Example: to send a $1$, we send $11111$ (i.e. rate $\frac{1}{5}$)
  * Decoding: take the majority vote of each block of five received bits
  * Discussion: by using longer repetition codes, we can achieve an arbitrarily low probability of error
* Another simple coding scheme:
  * Encoding: combine the bits in some intelligent fashion so that extra bit checks whether there is an error in some subset of information bits
    * Example: use parity check code (i.e. the simplest example of error-detecting code)
      * Step 1: start with a block of $n-1$ information bits
      * Step 2: choose the $n$-th bit so that the number of $1$'s in the block is even
      
      $\hspace{1.0cm} \rightarrow$ If there's an odd number of $1$'s during the transmission, the receiver will notice that the parity has changed and detect the error
  * Discussion: the code doesn't give any information about how to correct the errors

## Hamming code
**Introduction**: the simplest example of linear parity check codes

**Main principle**:
* Initialization:
  * Consider the set of non-zero binary vectors of length $r$
  
  $\hspace{1.0cm} \rightarrow$ Arrange them in columns to form a matrix $H$ (i.e. there should be $2^r - 1$ columns)
* Code generation:
  * We consider a binary code of block length $n = 2^r - 1$
  * Any operations will be done modulo $2$
  * The set of codewords are the set of vectors of length $n$ in $\textbf{null} H$

**Properties**:
* Number of codewords:
  * $H$ has rank $r$
  
  $\hspace{1.0cm} \rightarrow$ We expect $\textbf{null} H$ to have dimension $d = 2^r - 1 - r$
  * Consider all possible linear combinations (modulo $D$) of the vectors in a basis of $\textbf{null} H$
  
  $\hspace{1.0cm} \rightarrow$ There should be $2^d$ codewords
* Linearity of codewords: 
  * Consider any two codewords $w_1, w_2$ from the constructed code
  
  $\hspace{1.0cm} \rightarrow w_1 + w_2$ is a codeword (i.e. since $H (w_1 + w_2) = 0 \pmod{2}$)
  * From above, the set of codewords forms a linear subspace of dimension $d$
* Minimum weight of the code: the minimum number of $1$'s in any codeword
  * Minimum weight of the constructed code: 3
    * Explain: all columns of $H$ are different
    
    $\hspace{1.0cm} \rightarrow$ There must be at least three columns to sum up to the zero vector
* Minimum distance: the minimum number of places in which two codewords differ
  * Usage: measure of how far apart the codewords are and how distinguishable the codewords will be at the output of the channel
    * Explain: if a codeword $\textbf{c}$ is corrupted in $k_1$ places, it will differ from any codeword in at least $k_2 = k - k_1$ places where $k$ is the minimum distance of the code
    
    $\hspace{1.0cm} \rightarrow$ If $k_1 < k_2$, the corrupted code will be closer to $\textbf{c}$ than any other codeword
  
  >**NOTE**: for a linear code, the minimum distance equals to the minimum weight
    * Explain: for a linear code, the difference between any two codewords is also a codeword
  
  * Purpose of developing a code: develop codes that have a large minimum distance

**Correct the corrupted code**:
* The parity check matrix: $H$
* Assumptions:
  * $r = c + e_i$ is the corrupted message (i.e. corrupted at position $i$)
    * $e_i$ is the $i$-th column of the identity matrix
* Conclusion: if $H r$ equals the $i$-th column of $H$ then the bit at position $i$ is corrupted
  * Explain: $H r = H c + H e_i = H e_i$

**Systematic code**:
* Observations: the first $d$ bits of the codewords cycle through all $D^d$ combinations of $d$ bits
* Conclusion: we could use 
  * The first $d$ bits to be the $d$ bits of the message we want to send
  * The last $r$ bits to be determined by the code (i.e. parity check bits)

**Notation for Hamming code**: $(n, d, r)$
* $n$ is the block length
* $d$ is the number of information bits
* $k$ is the minimum distance

## Discussion
**Block codes**: a mapping that map a block of information bits onto a channel codeword and there's no dependence on past information bits

**Convolutional code**: a highly structured form of a code where each output block depends not only on the current input block, but also on some of the past inputs as well

**The promise of Shannon's channel capacity theorem**: for many years, none of the known coding algorithms came close to achieving the promise of Shannon's channel capacity theorem

# Feedback capacity
**Channel with feedback**:
* Step 1: all the received symbols are sent back immediately and noiselessly to the transmitter
* Step 2: the transmitter use the symbols from the receiver to decide which symbol to send next

**Definitions**:
* $(D^{nR}, n)$ feedback code: consist of
  * A sequence of mapping $x_i(W, Y^{i-1})$
  * A sequence of decoding functions $g: {\cal{Y}}^n \to \{1, ..., D^{nR}\}$
* The probability of error: $P_e^{(n)} = \text{Pr}(g(Y^n) \neq W)$
  * $W$ is uniformly distributed over $\{1, ..., D^{nR}\}$

**The capacity with feedback of a discrete memoryless channel**: the supremum of all rates achievable by feedback codes

**Theorem**: feedback can help enormously in simplifying encoding and decoding but cannot increase the capacity of the channel
* Formal: $C_\text{FB} = C = \max_{p(x)} I(X; Y)$
* Intuition: the definition of channel suggest that the only way to increase channel capacity is to modify $p(y|x)$, which is characterized by the channel, not to modify the source or the method of transmission (e.g. use feedback)

# Source-channel separation theorem
## Motivation
**Recall**:
* Data compression: $R > H$
* Data transmission: $R < C$

**Question**: is the condition $H < C$ necessary and sufficient for sending a source over a channel

**Types of sending messages**:
* Type 1: design a code to map the sequence of messages directly into the input of the channel
* Type 2: compress the messages into their most efficient representation then use appropriate channel code to send it over the channel
    * Observations: 
        * Data compression doesn't depend on the channel
        * Channel coding doesn't depend on the source distribution
    * Conclusion: we're not losing something by using the two-stage method

**Task**: prove that the two-stage method is as good as any other method of transmitting information over a noisy channel
* Consequences: this result has some important practical implications
    * We can consider the design of a communication system as a combination of two parts - source coding and channel coding
    * We can design source codes for most efficient representation of data
    * We can, separately and independently, design channel codes appropriate for the channel

## Main theorem
**Assumptions**:
* $V$ is a source that generates symbols from a finite alphabet $\cal{V}$ and satisfies the AEP

>**NOTE**: we won't make any assumptions about the kind of stochastic process produced by $V$

* $V^n = V_1, ..., V_n$ is the sequence of symbols to send over the channel so that the receiver can reconstruct it

**Communication procedure**:
* Step 1: map $V^n$ into a codeword $X^n(V^n)$
* Step 2: send $X^n(V^n)$ over the channel
* Step 3: the receiver looks at the received sequence $Y^n$ and estimate $V^n$ by $\hat{V}^n$
* Step 4: the receiver makes an error if $V^n \neq \hat{V}^n$

**Probability of error**: $\text{Pr}(V^n \neq \hat{V}^n) = \sum_{y^n} \sum_{v^n} p(v^n) p(y^n|x^n(v^n)) I(g(y^n) \neq v^n)$
* $I$ is the indicator function
* $g(\cdot)$ is the decoding function

**Source-channel coding theorem**:
* Assumptions:
    * Consider the assumptions above
    * $H({\cal{V}}) < C$ where $C$ is the channel capacity
* Conclusion:
    * There exists a source-channel code with probability of error converging to $0$
    * For any stationary stochastic process $V^n$, if $H({\cal{V}}) > C$
    
    $\hspace{1.0cm} \rightarrow$ The probability of error is bounded away from $0$ and it's not possible to send $V^n$ over the channel with arbitrarily low probability of error
* Consequences:
    * The separate encoders can achieve the same rates as the joint encoder
    * We can consider the problem of source coding separately from the problem of channel coding
        * The source coder tries to find the most efficient represent of the source
        * The channel coder encodes the message to combat the noise and errors introduced by the channel
* Intuition: the same as intuition of feedback channel capacity

---

# BONUS
* Symmetrization: a process that converts any function in $n$ variables to a symmetric function in $n$ variables
    * Symmetric function of $n$ variables: a function whose value given $n$ arguments is the same no matter the order of the arguments
* Intuition of channel capacity and code rate:
    * Code rate: indirectly tell how many symbols is used to represent a single message
        * The more symbols we use, the less error we get since we can have more symbols for error detection and error correction or we can enclose more detail about the sent message
        * The less symbols we use, the more error we get since we have less symbols for error detection and error correction
        
        $\hspace{1.0cm} \rightarrow$ If the number of symbols used for each message is below some threshold, we cannot distinguish the sent messages even when there's no error
    * Channel coding theorem: according to the intuition about code rate,
    
    $\hspace{1.0cm} \rightarrow$ If the code rate exceeds some threshold (called "channel capacity"), the number of symbols used for each message is small enough that the probability of error doesn't converge to $0$

# NEW WORD
* Induce (v): gây ra
* Counter-intuitive (adj): phản trực giác