<!-- TOC titleSize:1 tabSpaces:2 depthFrom:1 depthTo:6 withLinks:1 updateOnSave:1 orderedList:0 skip:0 title:1 charForUnorderedList:* -->
# Table of Contents
- [Table of Contents](#table-of-contents)
- [Entropy, relative entropy, and mutual information](#entropy-relative-entropy-and-mutual-information)
  - [Introduction to information theory](#introduction-to-information-theory)
  - [Information content](#information-content)
  - [Entropy](#entropy)
  - [Information entropy](#information-entropy)
  - [Joint entropy, conditional entropy, and cross entropy](#joint-entropy-conditional-entropy-and-cross-entropy)
    - [Joint entropy and conditional entropy](#joint-entropy-and-conditional-entropy)
    - [Cross entropy](#cross-entropy)
  - [Mutual information](#mutual-information)
    - [Data processing inequality](#data-processing-inequality)
  - [Kullback - Leibler divergence (relative entropy)](#kullback---leibler-divergence-relative-entropy)
    - [Conditional KL-divergence](#conditional-kl-divergence)
  - [Sufficient statistics](#sufficient-statistics)
  - [Fano's inequality](#fanos-inequality)
    - [Fano's inequality](#fanos-inequality-1)
    - [Another relationship of error rate and entropy](#another-relationship-of-error-rate-and-entropy)
    - [Intuition](#intuition)
- [Appendix](#appendix)
  - [Concepts](#concepts)
<!-- /TOC -->

# Entropy, relative entropy, and mutual information
## Introduction to information theory
**Fundamental questions in information theory**.
* What is the ultimate data compression
    * *Answer*. The entropy $H$
* What is the ultimate transmission rate of communication
    * *Answer*. The channel capacity $C$

**Contributions of information theory**.
* *Statistical physics*. Thermodynamics
* *Computer science*. Kolmogorov complexity or algorithmic complexity
* *Statistical inference*. Occam's Razor
* Error exponents for optimal hypothesis testing and estimation

**Applications of information theory in probability theory and statistics**. The fundamental quantities of information theory - entropy, relative entropy, and mutual information - are defined as functionals of probability distributions
* They characterize the behavior of long sequences of random variables 
* They allow us to estimate the probabilities of rare events
* They allow us to find the best error exponent in hypothesis tests

## Information content
**Information content**.
* *Assumptions*. $X$ is a random variable with p.m.f $p_X(\cdot)$
* *Self-information of measuring $X$ as outcome $x$*. $I_X(x) = \log \frac{1}{p_X(x)}$
    * Other names: self-information or surprisal

    >**NOTE**. The base of the logarithmic depends on unit of information used

* *Interpretation*. If we are told that $X$ has occured

    $\to$ We have received $I_X(x)$ bits of information
* *Intuition*.
    * Dividng an unit-length segment into $\frac{1}{p_X(x)}$ sub-segments of length $p_X(x)$
    * From above, if we use a string with symbols from a $D$-ary alphabet
    
    $\hspace{1.0cm} \rightarrow$ We need at least $\log_D \frac{1}{p_X(x)}$ symbols to exactly index each sub-segment
    * For events with probability $p \neq p_X(x)$, we represent such event with different number of symbols

    $\hspace{1.0cm} \rightarrow$ We don't need to seperate them from $x$, since they differ from $x$ (in the number of symbols)

**Self information as a measure of informativeness**.
* *Assumptions*.
    * $\mathcal{X}$ is a set of symbols
    * $\forall x \in \mathcal{X}$, $I_X(x) = \log_{|\mathcal{X}|} \frac{1}{p_X(x)}$
* *Conclusion*. $I_X(x)$ measures the proportion of informative part of $x$
    
    >**NOTE**. This perspective can be applied to entropy, relative entropy, mutual information, etc.

**Properties**.
* Rarer events yield more information
* If $X$ and $Y$ are independent events, then

    $$I_{XY}(x, y) = I_X(x) + I_Y(y)$$

**Information content and entropy**: 
* *Difference*. Information content acts on events, while entropy acts on random variables
* *Relationship*. $H(X) = I(X; X)$ where $I(X; X)$ is the mutual information of $X$ with itself
    * *Intuition*. 
        * $I(X; Y)$ measures the shared information of $X$ and $Y$
        * From above, if $Y = f(X)$ where $f$ is deterministic
        
        $\hspace{1.0cm} \rightarrow I(X; Y)$ measures the information (or uncertainty) of $X$ 
    * *Consequence*. if $Y = f(X)$ where $f$ is deterministic then 
        
        $$H(X, Y) = I(X; Y) = H(X)$$
        
        or equivalently $H(Y|X) = 0$

## Entropy
## Information entropy
**Entropy**. Measure the uncertainty of a random variable
* *Assumptions*.
    * $X$ is a discrete random variable whose set of possible values is $\mathcal{X}$
    * $p(X)$ is the p.f of $X$
* *Entropy of $X$*. 
    
    $$H(X) = \sum_{x \in \mathcal{X}} p(x) \log \frac{1}{p(x)}$$

* *Intuition*. Assume that $X$ is represented by a string whose symbols are taken from a $D$-ary alphabet
    
    $\to$ The expected length of the string is $H(X)$ (see intuition of self-information)
* *Convention*. $0\log 0 = 0$, i.e. since

    $$\lim_{x\to 0} x\log x=0$$

**Other intuitions of entropy**.
* The amount of randomness in $X$, measured in bits
* The minimum number of random bits needed to generate a drawn from $X$
* Average number of bits needed to store a draw from $X$ with compression
* Minimum number of bits needed for $A$ to communicate one draw from $X$ to $B$
* Number of yes / no questions needed, on average, to guess a draw from $X$

**Properties of $H$**.
* *Minimum entropy*. $H = 0$ if and only if all $p_i$ but one are zero
    * *Intuition*. Only when we are certain of the outcome, $H$ will vanish, otherwise $H$ is positive
* *Maximum entropy*. Given $n$, $H$ is a maximum and equal to $\log n$ when $p_i = \frac{1}{n}$ $\forall i$
    * *Proof 1*. Solve the equation $\frac{\partial H}{\partial p_i}=0$ for $i=1,\dots,n$
    * *Proof 2*. Direct prove using Jensen inequality (better prove)
        * Recall that a concave function $f$ will satisfy 
            
            $$\forall \sum_i c_i = 1, f(\sum_i c_i x_i) \geq \sum_i c_i f(x_i)$$ 
        * $x \log \frac{1}{x}$ is a concave function, hence

            $$(\sum_{i=1}^n c_i x_i) \log \frac{1}{\sum_{i=1}^n c_i x_i} \geq \sum_{i=1}^n c_i x_i \log \frac{1}{x_i}$$
        * Replace $c_i$ by $\frac{1}{n}$ and $x_i$ by $p_i$, we get

            $$\frac{1}{n} \log n \geq \frac{1}{n} H(X)$$

* If $X$ and $Y$ are two events with $m$ and $n$ possibilities respectively, then 
    
    $$H(X, Y) \leq H(X) + H(Y)$$
    
    * *Intuition*. The uncertainty of a joint event is no greater than the sum of individual uncertainties
    * *Proof*. direct prove using Jensen's inequality
        
        $$\begin{aligned}
        H(X) + H(Y) - H(X, Y) &= \sum_{x, y} p(x, y) \log \frac{p(x, y)}{p(x) p(y)}\\
        &\geq \log \frac{1}{\sum_{x, y} p(x) p(y)} = 0
        \end{aligned}$$
* Any change toward equalization of $p_1, ..., p_n$ increases $H$
    * *Formal*. 
        * *Assumptions*.
            * $p'_i = \sum_j a_{ij} p_j$
            * $\sum_i a_{ij} = \sum_j a_{ij} = 1$ and $a_{ij} \geq 0$
        * *Conclusion*. $H$ increases if we transform $p$ to $p'$ 
    * *Proof*: direct prove using Jensen inequality, i.e. let $H(p) = \sum_i p_i \log \frac{1}{p_i}$ for any set of probabilities $\{p_1, ..., p_n\}$, then
        
        $$\begin{aligned}
        H(p') &= \sum_i (\sum_j a_{ij} p_j) \log \frac{1}{\sum_j a_{ij} p_j}\\
        &\geq \sum_{i, j} a_{ij} p_j \log \frac{1}{p_j}\\
        &= \sum_j p_j \log \frac{1}{p_j}\\
        &= H(p)
        \end{aligned}$$

**Concavity of $H(X)$**. $H(X)$ is a concave function of $p$

## Joint entropy, conditional entropy, and cross entropy
### Joint entropy and conditional entropy
**Joint entropy**. $H(X, Y) = - \sum_{x, y} p(x, y) \log p(x, y)$

**Conditional entropy of $Y$ given $X$**.
* *Formal*. 
    
    $$H(Y|X) = \sum_x p(x) [- \sum_y p(y|x) \log p(y|x)] = - \sum_{x, y} p(x, y) \log p(y|x)$$
    
    * *Original notation*. $H_X(Y)$
    * *Another equivalent formulation*. $H(X, Y) = H(X) + H(Y|X)$
* *Interpretation*. The average of the entropy of $Y$, for each value of $X$, weighted according to $p(x)$
* *Intuition*. Uncertainty of the joint event $X, Y$ is the uncertainty of $Y$ plus the uncertainty of $Y|X$

**Compare conditional entropy and entropy**. $H(Y) \geq H(Y|X)$
* *Intuition*. 
    * The uncertainty of $Y$ is never increased by knowing $X$
    * The uncertainty of $Y$ will be decreased unless $X$ and $Y$ are independent
* *Proof*. Direct prove
    
    $$H(Y) + H(X) \geq H(X, Y) \leftrightarrow H(Y) \geq H(Y|X)$$

**Chain rule for entropy**. $H(X, Y) = H(X) + H(Y|X)$
* *Proof*. Direct prove
* *Corollary*. $H(X, Y|Z) = H(X|Y, Z) + H(Y|Z)$

### Cross entropy
**Application**. Measure the number of symbols needed to identity an event drawn from a set

**Cross-entropy**.
* *Assumptions*. $p$ and $q$ are discrete distributions
* *Conclusion*.
    
    $$\begin{aligned}
    H(p, q) &= \sum_i p(x) \log \frac{1}{q(x)}\\
    &= E_p(-\log q)\\
    &= H(p) + D_{KL}(p\|q)
    \end{aligned}$$
* *Intuition*. The cost of represent $X$, in bits, based on $q$ instead of its distribution $p$

>**NOTE**. Cross entropy MUST be understood as a cost function rather than a distance since $d(A, B) = d(B, A)$ for any distance function $d$

## Mutual information
**Mutual information**.
* *Assumptions*. $X$ and $Y$ are discrete random variables
* *Mutual information*: 
    
    $$I(X; Y) = \sum_{(x, y) \in \mathcal{X} \times \mathcal{Y}} p(x, y) \log \frac{p(x, y)}{p(x) p(y)}$$

    * *Other name*. Information gain
    * *Equivalant formulations*.
        * $I(X; Y) = \sum_{(x, y) \in \mathcal{X} \times \mathcal{Y}} p(x, y) \log \frac{p(x|y)}{p(x)}$
        * $I(Y; X) = \sum_{(x, y) \in \mathcal{X} \times \mathcal{Y}} p(x, y) \log \frac{p(y|x)}{p(y)}$
* *Intuition*. 

    $$I(X; Y) = H(X) - H(X|Y) = H(X) + H(Y) - H(X, Y)$$
    
    * *Intuition 1*. $I(X; Y)$ measures the shared information, measured in bits, of $X$ and $Y$
    * *Intuition 2*. $I(X; Y)$ measures the change, in $H(X)$, from a prior state $X$ to a state $X|Y$ with more information

**Properties**.
* $I(X; Y) \geq 0$
* $I(X; Y|Z) \geq 0$
* $I(X; Y) = I(Y; X)$

**Convexity and concavity of $I(X; Y)$**.
* $I(X; Y)$ is a concave function of $p(x)$ for fixed $p(y|x)$
* $I(X; Y)$ is a convex function of $p(y|x)$ for fixed $p(x)$

**Conditional mutual information**. 

$$I(X; Y|Z) = H(X|Z) - H(X|Y, Z)$$

* *Interpretation*. The mutual information of $X$ and $Y$, given $Z$
* *Non-negativity*. $I(X; Y|Z) \geq 0$

**Chain rule for mutual information**. 

$$I(X; Y_1, Y_2) = I(X; Y_2|Y_1) + I(X; Y_1)$$

* *Interpretation*. The mutual information of $X$ and $Y_1 \cup Y_2$
* *Proof*. Direct prove
    
    $$\begin{aligned}
    I(X; Y_1, Y_2) &= H(X) - H(X|Y_1, Y_2)\\
    &= H(X) - H(X|Y_1) + H(X|Y_1) - H(X|Y_1, Y_2)\\
    &= I(X; Y_1) + I(X; Y_2|Y_1)
    \end{aligned}$$
* *Intuition*. Mutual information of $X$ and $Y_1 \cup Y_2$ is composed of
    * Mutual information of $X$ and $Y_1$ (i.e. explained by $Y_1$)
    * Mutual information of $X|Y_1$ and $Y_2$ (i.e. unexplained by $Y_1$ but explained by $Y_2$)

### Data processing inequality
**Data processing inequality**. If $X \to Y \to Z$ is a Markov chain then $I(X; Y) \geq I(X; Z)$
* *Equality*. When $X \to Z \to Y$
* *Informal*. Post-processing cannot increase information
* *Intuition*. Given a raw message $X$ and its encoded one $Y$, $I(X; Y)$ gives the amount of information that $Y$ tells us about $X$

    $\to$ Any posterior transformation on $Y$, which produces some variable $Z$, may make the information about $X$ lost

**Proof**. Direct prove
* Due to Markov assumption
    
    $$\begin{aligned}
    I(X; Y, Z) &= I(X; Y) + I(X; Z|Y)\\
    &= I(X; Y)\\
    &=I(X; Z) + I(X; Y|Z)
    \end{aligned}$$
* From above, $I(X; Y) \geq I(X; Z)$ since $I(X; Y|Z) \geq 0$

**Corollaries**. 
* If $Z = g(Y)$ then $I(X; Y) \geq I(X; g(Y))$
    * *Explain*. $X \to Y \to g(Y)$
* If $X \to Y \to Z$ then $I(X; Y|Z) \leq I(X; Y)$
    * *Explain*. From the prove
    * *Intuition*. Mutual information of $X$ and $Y$ decreases if we know the the downstream variable $Z$

## Kullback - Leibler divergence (relative entropy)
**Application**. Measure how one probability distribution is different from a some reference probability distribution
* *"Divergence"*. This term is confusing, should be changed into "cost" since relative entropy is a cost, rather than a distance in a Euclidean space, where $d(A, B) = d(B, A)$ for any distance function $d$

**KL-divergence**.
* *Assumptions*. $P, Q$ are discrete probability distributions defined on the same probability space
* *KL divergence*. 
    
    $$D_{KL}(P\|Q) = \sum_i P(i) \log \frac{P_i}{Q_i}$$

* *Intuition*. The expeceted amount of cost increased, in number of symbols needed, of represeting $P$ via $Q$ instead of $P$ itself
* *Interpretations*.
    * *Machine learning*. The information gain achieved if $Q$ is used instead of $P$
    * *Bayesian inference*. The information gained when one revises one's beliefs from the prior distribution $Q$ to the posterior distribution $P$
        * *Explain*. The amount of information lost when $Q$ is used to approximate $P$

**Symmetrized divergence**. $D_{KL}(P\|Q) + D_{KL}(Q\|P)$
* *Application*. Feature selection in classification problems

**Information inequality**. $D_{KL}(P\|Q) \geq 0$
* *Proof*. Direct prove with Jensen's inequality, i.e. since $\log \frac{1}{x}$ is a convex function of $x$
    
    $\to \sum_i P(i) \log \frac{P_i}{Q_i} \geq \log \frac{1}{\sum_i Q_i} = 0$

**Properties**.
* If $P_1, P_2$ are independent distributions with joint distribution $P(x, y) = P_1(x) P_2(y)$ and $Q, Q_1, Q_2$ likewise,

    $\to D_{KL}(P\|Q) = D_{KL}(P_1\|Q_1) + D_{KL}(P_2\|Q_2)$
* $D(p\|q)$ is convex in $(p, q)$

### Conditional KL-divergence
**Conditional KL divergence**. 

$$D[p(y|x)\|q(y|x)] = \sum_{x, y} p(x, y) \log \frac{p(y|x)}{q(y|x)}$$

* *Non-negativity*. $D[p(y|x)\|q(y|x)] \geq 0$

**Chain rule for KL divergence**. 

$$D_{KL}(p(x, y)\|q(x, y)) = D_{KL}(p(x)\|q(x)) + D_{KL}(p(y|x)\|q(y|x))$$

* *Prove*. Direct prove
* *Intuition*. Use "number of bits" intuition from Information Entropy

**Relation to other quantities of information theory**.
* *Self-information*. 
    * *Formal*. $I(m) = D_{KL}(\delta_{im}\|\{p_i\})$
        * $\delta_{im}$ is the Kronecker delta function
        * $\{p_i\}$ is the distribution, from which the event $m$ is taken
    * *Intuition*. Use "number of bits" intuition from Information Entropy
* *Mutual information*. $I(X; Y) = D_{KL}(p(x, y)\|p(x) p(y))$
    * *Intuition*. $A \cap B = A + B - A \cup B$
* *Entropy*: 
    * *Formal*. $H(X) = \log N - D_{KL}(p_X(x)\|p_U(X))$
        * $N$ is the number of symbols in the $N$-ary alphabet used for encoding
        * $p_X$ is the true distribution of $X$
        * $p_U$ is the uniform distribution over $N$ items
    * *Intuition*. Use "number of bits" intuition from Information Entropy
* *Cross entropy*. $H(p, q) = H(p) + D_{KL}(p\|q)$

## Sufficient statistics
**Introduction**. Show the power of the data-processing inequality

**Sufficient statistic**.
* *Assumptions*.
    * $\{f_\theta(x)\}$ is a family of probability mass functions indexed by $\theta$
    * $X \sim \{f_\theta(x)\}$
    * $T(X)$ is any statistic
* *Observations*.
    * Since $\theta \to X \to T(X)$

        $$\forall\theta\in\Omega,I[\theta; T(X)] \leq I(\theta; X)$$

    * If $I[\theta; T(X)] = I(\theta; X)$ then no information is lost
* *Sufficient statistic*. $T(X)$ is sufficient for $\theta$ if it contains all the information in $X$ about $\theta$
    * *Formal*. $I[\theta; T(X)] = I(\theta; X)$

        $\to$ $T(X)$ summarizes all information about $\theta$, which can be provided by $X$

**Definition of sufficient statistic in statistics and information theory**.
* *Sufficient statistic in statistic*. $T$ is a sufficient statistic if $P(x|T, \theta) = P(x|T)$
    * Formal: $\theta \to T(X) \to X$ is a Markov chain
* *Equivalence to "sufficient statistic" in information theory*. We know $\theta \to X \to T(X)$ is a Markov chain also

    $\to \theta \to T(X) \to X$ implies $I[\theta; T(X)] = I(\theta; X)$

**Minimal sufficient statistic**. $T(X)$ is a minimal sufficient statistic relative to $\{f_\theta(x)\}$ if it's a function of every other sufficient statitic $U$
* *Formal*. $\theta \to T(X) \to U(X) \to X$
* *Interpretation*. 
    * $T$ compresses the information about $\theta$ in the sample
    * $U$ may contain additional irrelevant information

## Fano's inequality
### Fano's inequality
**Other names**. Fano converse or Fano lemma

**Motivation**.
* $H(X|Y) = 0$ if and only if $X = f(Y)$

    $\to$ We can estimate $X$ from $Y$ with zero probability of error if any only if $H(X|Y) = 0$ (i.e. $X$ is a function of $Y$)
* From above, we expect to be able to estimate $X$ with a low probability of error if any only if $H(X|Y)$ is small

    $\to$ This idea is quantified by Fano's inequality

**Problem of interest**.
* *Assumptions*.
    * We wish to estimate a random variable $X$ with a distribution $p(x)$
        * $\mathcal{X}$ is the support of $X$
    * We observe a random variable $Y$ that is related to $X$ by the conditional distribution $p(y|x)$
    * We estimate $X$ by $\tilde{X} = g(Y)$
        * $\tilde{X}$ takes on values in $\tilde{\mathcal{X}}$
        
        >**NOTE**. We won't restrict $\tilde{\mathcal{X}}$ to be equal to $\mathcal{X}$ and we allow $g(Y)$ to be random
    
    * $P_e = \text{Pr}(\tilde{X} \neq X)$
* *Problem*. Bound the probability that $\tilde{X} \neq X$ by bounding $H(X|Y)$ (i.e. how deterministic $X$ is, given $Y$)

**Fano's inequality**.
* *Observation*. $X \to Y \to \tilde{X}$ forms a Markov chain
* *Fano's inequality*.
    * *Original form*. $H(P_e) + P_e \log{|\mathcal{X}|} \geq H(X|\tilde{X}) \geq H(X|Y)$
    * *Weakened form*. 
        
        $$1 + P_e \log{|\mathcal{X}|} \geq H(X|Y)$$
        
        or 
        
        $$P_e \geq \frac{H(X|Y) - 1}{\log |\mathcal{X}|}$$

* *Corollaries*.
    * $H(p) + p \log{|\mathcal{X}|} \geq H(X|Y)$ where $p = \text{Pr}(X \neq Y)$
    * $H(P_e) + P_e \log({|\mathcal{X}|} - 1) \geq H(X|Y)$

>**NOTE**. $P_e = 0$ implies $H(X|Y) = 0$ as intuition suggests

* *Proof*: 
    * *Original form*. Direct prove using axiom (3) of information entropy
        * From axiom (3) of information theory
            
            $\to H(X) = H(P_e) + P_e \log |{\mathcal{X}}| \geq H(X|\tilde{X})$
        * From the data processing inequality

            $\to H(X|\tilde{X}) = H(X) - I(X; \tilde{X}) \geq H(X) - I(X; Y) = H(X|Y)$
    * *Weakened form*. since $1 \geq \log 2 \geq H(P_e)$ for any logarithm base $D \geq 2$

**Remark**. If there's no knowledge of $Y$, $X$ must be guessed without any information
* *Assumptions*.
    * $X \in \{1, ..., m\}$
    * $p_1 \geq p_2 \geq ... \geq p_m$
* *Best guess of $X$*. $\tilde{X} = 1$
    * *Error rate*. $P_e = 1 - p_1$
* *Fano's inequality*. $H(P_e) + P_e \log (m-1) \geq H(X)$
    * *Equality case*. $(p_1, ..., p_m) = (1 - P_e, \frac{P_e}{m-1}, ..., \frac{P_e}{m-1})$
* *Proof*. Direct prove from Fano's inequality

### Another relationship of error rate and entropy
**Theorem**.
* *Assumptions*.
    * $X$ and $X'$ are two i.i.d random variables
        * $X \sim p(x)$
    * Entropy of $X$ and $X'$ is $H(X)$
* *Conclusion*. $P(X = X') = \sum_x p^2(x)$
* *Proof*. $P(X = X') = \sum_x p(X = x) \cdot p(X' = x)$

**Lemma**. $P(X = X') \geq 2^{-H(X)}$
* *Proof*. Direct prove using Jensen's inequality 
    * $\log P(X = X') = \log [\sum_x p^2(x)] \geq \sum_x p(x) \log p(x) = - H(X)$ due to the concavity of $\log x$
* *Intuition*. Use "number of bits" intuition from information entropy

**Corollary**.
* *Assumptions*.
    * $X, X'$ are independent
    * $X \sim p(x)$
    * $X' \sim r(x)$
    * $x, x' \in {\mathcal{X}}$
* *Conclusion*.
    * $P(X = X') \geq 2^{-H(p)-D(p\|r)}$
    * $P(X = X') \geq 2^{-H(r)-D(r\|p)}$

### Intuition
**Original form**. To represent $X$ given $\tilde{X}$, we need to carry out two steps:
* Verify whether $\tilde{X} = X$

    $\to$ This takes $H(P_e)$ symbols
* If $\tilde{X} \neq X$, which happens with probability $P_e$, we need to specify $X$ from $\mathcal{X}$

    $\to$ This takes $P_e H(X)$ symbols, which equals $P_e \log {|\mathcal{X}|}$ in the worst case
* From above, we need $H(P_e) + P_e \log {|\mathcal{X}|}$ symbols to represent $X$ given $\mathcal{X}$ in the worst case

    $\to H(P_e) + P_e \log {|\mathcal{X}|} \geq H(X|\tilde{X}) \geq H(X|Y)$

**Weakened form**. $H(P_e) \leq \log_D 2 \leq 1$ where $D \geq 2$

$\to 1 + P_e \log {|\mathcal{X}|} \geq H(P_e) + P_e \log {|\mathcal{X}|} \geq H(X|\tilde{X}) \geq H(X|Y)$

**Conclusion**. Given $\tilde{X} \in \mathcal{X}$, we only need to determine the true value of $X$ among $|\mathcal{X}| - 1$ other values in $\mathcal{X}$

$\to H(P_e) + P_e \log(|{\mathcal{X}}| - 1)\geq H(X|\tilde{X}) \geq H(X|Y)$

# Appendix
## Concepts
**Kronecker delta function**.

$$\delta_{ij} = \begin{cases} 0 && i \neq j \\ 1 && i = j\end{cases}$$

**Another notation of Markov chain**. 
* *Assumptions*.
    * $X, Y, Z$ form a Markov chain
* *Notation*. $X \to Y \to Z$

**Kolmogorov complexity of an object (e.g. a piece of text)**. The length of the shortest computer program (in a predetermined programming language) that produces the object as output

**Linear homogeneous difference equation**. $\sum_{t=1}^n a_t X(t) = 0$
* *Idea*. Guess $X(t) = x^t$ and solve $\sum_{t=0}^n a_t x^t = 0$ for solutions $x_1, ..., x_n$
* *General solution*. $\sum_{i=1}^n c_i x_i$
    * *Explain*. Due to linearity of the equation and the difference operation

**Linear inhomogeneous difference equation**. $\sum_{t=1}^n a_t X(t) = b$
* *Idea*.
    1. Solve $\sum_{t=1}^n a_t X(t) = 0$ for $x_1, ..., x_n$
    2. Find a particular solution $x_0$ for $\sum_{t=1}^n a_t X(t) = b$
* *General solution*. $x_0 + \sum_{i=1}^n c_i x_i$