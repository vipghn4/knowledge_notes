<!-- TOC titleSize:1 tabSpaces:2 depthFrom:1 depthTo:6 withLinks:1 updateOnSave:1 orderedList:0 skip:0 title:1 charForUnorderedList:* -->
# Table of Contents
- [Table of Contents](#table-of-contents)
- [Introduction to information theory](#introduction-to-information-theory)
- [Entropy](#entropy)
  - [Derivation of information entropy from Shannon's paper](#derivation-of-information-entropy-from-shannons-paper)
  - [Information entropy](#information-entropy)
  - [Information content](#information-content)
- [Joint entropy, conditional entropy, and cross entropy](#joint-entropy-conditional-entropy-and-cross-entropy)
  - [Joint entropy and conditional entropy](#joint-entropy-and-conditional-entropy)
  - [Cross entropy](#cross-entropy)
- [Mutual information](#mutual-information)
  - [Data processing inequality](#data-processing-inequality)
- [Kullback - Leibler divergence (relative entropy)](#kullback---leibler-divergence-relative-entropy)
  - [Conditional KL-divergence](#conditional-kl-divergence)
- [Sufficient statistics](#sufficient-statistics)
- [Fano's inequality](#fanos-inequality)
  - [Fano's inequality](#fanos-inequality-1)
  - [Another relationship of error rate and entropy](#another-relationship-of-error-rate-and-entropy)
  - [Intuition](#intuition)
- [BONUS](#bonus)
<!-- /TOC -->

# Introduction to information theory
**Fundamental questions in information theory**:
* What is the ultimate data compression

$\hspace{1.0cm} \rightarrow$ Answer: the entropy $H$
* What is the ultimate transmission rate of communication

$\hspace{1.0cm} \rightarrow$ Answer: the channel capacity $C$

**Contributions of information theory**:
* Statistical physics: thermodynamics
* Computer science: Kolmogorov complexity or algorithmic complexity
* Statistical inference: Occam's Razor
* Error exponents for optimal hypothesis testing and estimation

**Applications of information theory in probability theory and statistics**: the fundamental quantities of information theory - entropy, relative entropy, and mutual information - are defined as functionals of probability distributions
* They characterize the behavior of long sequences of random variables 
* They allow us to estimate the probabilities of rare events
* They allow us to find the best error exponent in hypothesis tests

# Entropy
## Derivation of information entropy from Shannon's paper
**Question**: 
* How to measure the information produced by a stochastic process
* What rate information is produced

**Idea**:
* Assumptions:
    * $p_1, ..., p_n$ are probabilities of occurrence of $n$ events
    * $p_i$ is known in advance
* Task: 
    * Measure how much "choice" is involved in the selection of the event
    * How uncertain we are of the outcome

**Axioms on the measure $H(p_1, ..., p_n)$**:
* $H$ should be continuous in $p_i$
* If $p_i = \frac{1}{n}$ $\forall i$, $H$ should be monotonic increasing function of $n$
    * Intuition: with equally likely events, there is more choice when there are more possible events
* If a choice is broken into two successive choice, $H$ should be the weighted sum of individual values of $H$
    * Formal: $H(X_1, X_2) = p_1 H(X_1) + p_2 H(X_2)$
        * $p_1$ is the probability of having to make choice on $X_1$
        * $p_2$ is the probability of having to make choice on $X_2$

        >**NOTE**: $p_1 + p_2$ need not to be $1$
    
    * Example: consider a game where player have to flip a coin until he comes up with a head, and let $p$ be the probability of obtaining a head from a coin flip
        * The player always has to flip a coin once

        $\hspace{1.0cm} \rightarrow H = H(p)$
        * In case the player failed (with probability $1 - p$), he has to flip the coin once more

        $\hspace{1.0cm} \rightarrow H = H(p, 1-p) + (1 - p) \cdot H(p, 1-p)$
        * In case he failed again, he has to flip the coin once more

        $\hspace{1.0cm} \rightarrow H = H(p, 1-p) + (1 - p) \cdot H(p, 1-p) + (1 - p)^2 \cdot H(p, 1-p)$
        * If he failed for a large number of times, he has to flip the coin again and again

        $\hspace{1.0cm} \rightarrow H = H = H(p, 1-p) + (1 - p) \cdot H(p, 1-p) + (1 - p)^2 \cdot H(p, 1-p) + \dots$

**Theorem**: 
* Statement: the only $H$ satisfying the axioms above is $H = -K \log_{i=1}^n p_i \log p_i$ where $K > 0$ is a constant
    * $K$ amounts to a choice of a unit of measure
* Derivation:
    * For any sample space of possibilities with probabilities $p_1, ..., p_n$

    $\hspace{1.0cm} \rightarrow$ We are always able to divie the sample space into infinitely  many equally likely possibilities 
    * Let $A(n) = H(\frac{1}{n}, ..., \frac{1}{n})$
    * Equally likely possibilities case: direct prove with "hamburger" theorem
        * Consider dividing $s^m$ equally likely possibilities into leaves of a $s$-ary tree with heigh $m$ where $m$ is sufficiently large

        $\hspace{1.0cm} \rightarrow A(s^m) = m A(s)$
        * Consider dividing $t^n$ equally likely possibilities into leaves of a $t$-ary tree with heigh $n$ where $n$ is sufficiently large

        $\hspace{1.0cm} \rightarrow A(t^n) = n A(t)$
        * By axiom (3), if $s^m \leq t^n \leq s^{m+1}$,
        
        $\hspace{1.0cm} \rightarrow m A(s) \leq n A(t) \leq (m + 1) A(s)$
        * Consider the inequality $s^m \leq t^n \leq s^{m+1}$
            * $m \log s \leq n \log t \leq (m + 1) \log s$

            $\hspace{0.3cm} \leftrightarrow \frac{m}{n} \log s \leq \log t \leq \frac{m}{n} \log s + \frac{1}{n} \log s$

            $\hspace{0.3cm} \leftrightarrow \frac{m}{n} \leq \frac{\log t}{\log s} \leq \frac{m}{n} + \frac{1}{n}$

            $\hspace{0.3cm} \leftrightarrow \frac{\log t}{\log s} \approx \frac{m}{n}$
        * Consider $m A(s) \leq n A(t) \leq m A(s) + A(s)$
            * $\frac{m}{n} A(s) \leq A(t) \leq \frac{m}{n} A(s) + \frac{1}{n} A(s)$

            $\hspace{0.3cm} \leftrightarrow \frac{m}{n} \leq \frac{A(t)}{A(s)} \leq \frac{m}{n} + \frac{1}{n}$

            $\hspace{0.3cm} \leftrightarrow \frac{A(t)}{A(s)} \approx \frac{m}{n}$
        * From above, $\frac{A(t)}{A(s)} \approx \frac{\log t}{\log s}$
        * Suppose $A(t) = K \log t$

        $\hspace{1.0cm} \rightarrow$ To ensure that $A(t)$ is monotonically increasing in $n$, $K$ must be positive
    * General case of arbitrary probability $p_1, ..., p_n$: direct prove with the law of large numbers
        * First divide the sample space into infinitely many equally likely possibilities and cluster these possibilities into $n$ clusters
            * Cluster $i$ has $p_i \times 100\%$ percent of elements
        * Let $N$ be the number of divided possibilities
        
        $\hspace{1.0cm} \rightarrow$ The entropy of cluster $i$ is $A(p_i N) = K \log (p_i N)$
        * By axiom (3), the entropy of the whole sample space is $K \log N = H(p_1, ..., p_n) + K \sum_{i=1}^n p_i \log (p_i N)$
            * Explain: decompose the choice into two successive choices
                * Step 1: choose cluster among $n$ clusters
                * Step 2: choose an element in the chosen cluster
            * Consequence: $H(p_1, ..., p_n) = - K \sum_{i=1}^n p_i \log p_i$

>**NOTE**: initially, Shannon defines "entropy" just to lend certain plausibility to some of his later definitions

**Entropy of the set of probabilities $p_1, ..., p_n$**: $H = - \sum_{i=1}^n p_i \log p_i$
* Interpretation: measure information, choice, and uncertainty
* "Entropy": the form of $H$ is recognized as "entropy" in statistical mechanics

**Entropy of a discrete random variable**: $H(X)$ where $X \sim \{p_1, ..., p_n\}$

## Information entropy
**Entropy**: measure the uncertainty of a random variable
* Assumptions:
    * $X$ is a discrete random variable whose set of possible values is $\cal{X}$
    * $p(X)$ is the p.f of $X$
* Entropy of $X$: $H(X) = \sum_{x \in \cal{X}} p(x) \log \frac{1}{p(x)}$
* Intuition: assume that $X$ is represented by a string whose symbols are taken from a $D$-ary alphabet

$\hspace{1.0cm} \rightarrow$ The expected length of the string is $H(X)$ (see intuition of self-information)

**Other intuitions of entropy**:
* The amount of randomness in $X$, measured in bits
* The minimum number of random bits needed to generate a drawn from $X$
* Average number of bits needed to store a draw from $X$ with compression
* Minimum number of bits needed for $A$ to communicate one draw from $X$ to $B$
* Number of yes / no questions needed, on average, to guess a draw from $X$

**Properties of $H$**:
* $H = 0$ if and only if all $p_i$ but one are zero
    * Intuition: only when we are certain of the outcome, $H$ will vanish, otherwise $H$ is positive
* Given $n$, $H$ is a maximum and equal to $\log n$ when $p_i = \frac{1}{n}$ $\forall i$
    * Prove 1: direct prove using first-order optimization method
        * Let $\{p_1, ..., p_{n-1}, p_n\}$ be a set of probabilities where $p_n = 1 - \sum_{i=1}^{n-1} p_i$

        $\hspace{1.0cm} \rightarrow$ Solve $\frac{\partial H(X)}{\partial p} = 0$ for $p = \{p_1, ..., p_{n-1}\}$
        * The solution is $p_1 = ... = p_n = \frac{1}{n}$, combined with the fact that $H(X)$ is concave

        $\hspace{1.0cm} \rightarrow H(X) \leq \log n$
    * Prove 2: direct prove using Jensen inequality (better prove)
        * Recall that a concave function $f$ will satisfy $f(\sum_i c_i x_i) \geq \sum_i c_i f(x_i)$ where $\sum_i c_i = 1$ 
        * $x \log \frac{1}{x}$ is a concave function

        $\hspace{1.0cm} \rightarrow (c_1 x_1 + ... + c_n x_n) \log \frac{1}{c_1 x_1 + ... + c_n x_n} \geq c_1 x_1 \log \frac{1}{x_1} + ... + c_n x_n \log \frac{1}{x_n}$
        * Replace $c_i$ by $\frac{1}{n}$ and $x_i$ by $p_i$

        $\hspace{1.0cm} \rightarrow \frac{1}{n} \log n \geq \frac{1}{n} H(X)$
        * From above, $\log n \geq H(X)$

* If $X$ and $Y$ are two events with $m$ and $n$ possibilities (respectively), then $H(X, Y) \leq H(X) + H(Y)$
    * Intuition: the uncertainty of a joint event is no greater than the sum of individual uncertainties
    * Prove: direct prove using Jensen's inequality
        * $H(X) + H(Y) - H(X, Y) = \sum_{x, y} p(x, y) \log \frac{p(x, y)}{p(x) p(y)}$

        $\hspace{4.9cm} \geq \log \frac{1}{\sum_{x, y} p(x) p(y)}$

        $\hspace{4.9cm} = 0$
* Any change towrad equalization of $p_1, ..., p_n$ increases $H$
    * Formal: 
        * Assumptions:
            * $p'_i = \sum_j a_{ij} p_j$
            * $\sum_i a_{ij} = \sum_j a_{ij} = 1$
            * $a_{ij} \geq 0$
        * Conclusion: $H$ increases if we transform $p$ to $p'$ 
    * Prove: direct prove using Jensen inequality
        * Let $H(p) = \sum_i p_i \log \frac{1}{p_i}$ for any set of probabilities $\{p_1, ..., p_n\}$
        * $H(p') = \sum_i (\sum_j a_{ij} p_j) \log \frac{1}{\sum_j a_{ij} p_j}$

        $\hspace{1.8cm} \geq \sum_{i, j} a_{ij} p_j \log \frac{1}{p_j}$

        $\hspace{1.8cm} = \sum_j p_j \log \frac{1}{p_j}$

        $\hspace{1.8cm} = H(p)$

**Concavity of $H(X)$**: $H(X)$ is a concave function of $p$
* Prove: direct prove
    * Weighted sum of concave functions (i.e. $\log p(x)$) is a concave function

## Information content
**Information content**:
* Assumptions: 
    * $X$ is a random variable with p.m.f $p_X(\cdot)$
* Self-information of measuring $X$ as outcome $x$: $I_X(x) = \log \frac{1}{p_X(x)}$
    * Other names: self-information or surprisal

>**NOTE**: the base of the logarithmic depends on unit of information used

* Interpretation: if we are told that $X$ has occured

$\hspace{1.0cm} \rightarrow$ We have received $I_X(x)$ bits of information
* Intuition:
    * Dividng an unit-length segment into $\frac{1}{p_X(x)}$ sub-segments of length $p_X(x)$
    * From above, if we use a string with symbols from a $D$-ary alphabet
    
    $\hspace{1.0cm} \rightarrow$ We need at least $\log_D \frac{1}{p_X(x)}$ symbols to exactly index each sub-segment
    * For events with probability $p \neq p_X(x)$, we represent such event with different number of symbols

    $\hspace{1.0cm} \rightarrow$ We don't need to seperate them from $x$, since they differ from $x$ (in the number of symbols)

**Self information as a measure of informativeness**: 
* Assumptions:
    * $\cal{X}$ is a set of symbols
    * $\forall x \in \cal{X}$, $I_X(x) = \log_{|\cal{X}|} \frac{1}{p_X(x)}$
* Conclusion: $I_X(x)$ measures the proportion of informative part of $x$

>**NOTE**: this perspective can be applied to entropy, relative entropy, mutual information, etc.

**Properties**:
* Rarer events yield more information
* If $X$ and $Y$ are independent events

$\hspace{1.0cm} \rightarrow I_{XY}(x, y) = I_X(x) + I_Y(y)$

**Information content and entropy**: 
* Difference:
    * Information content: act on events
    * Entropy: act on random variables
* Relationship: $H(X) = I(X; X)$ where $I(X; X)$ is the mutual information of $X$ with itself
    * Intuition: 
        * $I(X; Y)$ measures the shared information of $X$ and $Y$
        * From above, if $Y = f(X)$ where $f$ is deterministic
        
        $\hspace{1.0cm} \rightarrow I(X; Y)$ measures the information (or uncertainty) of $X$ 
    * Consequence: if $Y = f(X)$ where $f$ is deterministic then $H(X, Y) = I(X; Y) = H(X)$ (or equivalently $H(Y|X) = 0$)

# Joint entropy, conditional entropy, and cross entropy
## Joint entropy and conditional entropy
**Joint entropy**: $H(X, Y) = - \sum_{x, y} p(x, y) \log p(x, y)$

**Conditional entropy of $Y$ given $X$**: 
* Formulation: $H(Y|X) = \sum_x p(x) [- \sum_y p(y|x) \log p(y|x)] = - \sum_{x, y} p(x, y) \log p(y|x)$
    * Original notation: $H_X(Y)$
    * Another equivalent formulation: $H(X, Y) = H(X) + H(Y|X)$
* Interpretation: the average of the entropy of $Y$, for each value of $X$, weighted according to $p(x)$
* Intuition: uncertainty of the joint event $X, Y$ is the uncertainty of $Y$ plus the uncertainty of $Y|X$

**Compare conditional entropy and entropy**: $H(Y) \geq H(Y|X)$
* Intuition: 
    * The uncertainty of $Y$ is never increased by knowing $X$
    * The uncertainty of $Y$ will be decreased unless $X$ and $Y$ are independent
* Prove: direct prove
    * $H(Y) + H(X) \geq H(X, Y) \leftrightarrow H(Y) \geq H(Y|X)$

**Chain rule for entropy**: $H(X, Y) = H(X) + H(Y|X)$
* Prove: direct prove
* Corollary: $H(X, Y|Z) = H(X|Y, Z) + H(Y|Z)$

## Cross entropy
**Application**: measure the number of symbols needed to identity an event drawn from a set

**Cross-entropy**:
* Assumptions:
    * $p$ and $q$ are discrete distributions
* Conclusion:
    * Formula: $H(p, q) = \sum_i p(x) \log \frac{1}{q(x)}$

    $\hspace{4.1cm} = E_p(-\log q)$

    $\hspace{4.1cm} = H(p) + D_{KL}(p\|q)$
* Intuition: the cost of represent $X$, in bits, based on $q$ instead of its distribution $p$

>**NOTE**: cross entropy MUST be understood as a cost function rather than a distance since $d(A, B) = d(B, A)$ for any distance function $d$

# Mutual information
**Definition**:
* Assumptions:
    * $X$ and $Y$ are discrete random variables
* Mutual information: $I(X; Y) = \sum_{(x, y) \in \cal{X} \times \cal{Y}} p(x, y) \log \frac{p(x, y)}{p(x) p(y)}$
    * Other name: information gain
    * Equivalant formulations:
        * $I(X; Y) = \sum_{(x, y) \in \cal{X} \times \cal{Y}} p(x, y) \log \frac{p(x|y)}{p(x)}$
        * $I(Y; X) = \sum_{(x, y) \in \cal{X} \times \cal{Y}} p(x, y) \log \frac{p(y|x)}{p(y)}$
* Intuition: $I(X; Y) = H(X) - H(X|Y) = H(X) + H(Y) - H(X, Y)$
    * Intuition 1: $I(X; Y)$ measures the shared information, measured in bits, of $X$ and $Y$
    * Intuition 2: $I(X; Y)$ measures the change, in $H(X)$, from a prior state $X$ to a state $X|Y$ with more information

**Properties**:
* $I(X; Y) \geq 0$
* $I(X; Y|Z) \geq 0$
* $I(X; Y) = I(Y; X)$

**Convexity and concavity of $I(X; Y)$**:
* $I(X; Y)$ is a concave function of $p(x)$ for fixed $p(y|x)$
* $I(X; Y)$ is a convex function of $p(y|x)$ for fixed $p(x)$

**Conditional mutual information**: $I(X; Y|Z) = H(X|Z) - H(X|Y, Z)$
* Interpretation: the mutual information of $X$ and $Y$, given $Z$
* Non-negativity: $I(X; Y|Z) \geq 0$

**Chain rule for mutual information**: $I(X; Y_1, Y_2) = I(X; Y_2|Y_1) + I(X; Y_1)$
* Interpretation: the mutual information of $X$ and $Y_1 \cup Y_2$
* Prove: direct prove
    * $I(X; Y_1, Y_2) = H(X) - H(X|Y_1, Y_2)$ 
    
    $\hspace{2.8cm} = H(X) - H(X|Y_1) + H(X|Y_1) - H(X|Y_1, Y_2)$

    $\hspace{2.8cm} = I(X; Y     _1) + I(X; Y_2|Y_1)$
* Intuition: mutual information of $X$ and $Y_1 \cup Y_2$ is composed of
    * Mutual information of $X$ and $Y_1$ (i.e. explained by $Y_1$)
    * Mutual information of $X|Y_1$ and $Y_2$ (i.e. unexplained by $Y_1$ but explained by $Y_2$)

## Data processing inequality
**Data processing inequality**: if $X \to Y \to Z$ is a Markov chain then $I(X; Y) \geq I(X; Z)$
* Equality: when $X \to Z \to Y$
* Informal: post-processing cannot increase information
* Intuition: given a raw message $X$ and its encoded one $Y$, $I(X; Y)$ gives the amount of information that $Y$ tells us about $X$

$\hspace{1.0cm} \rightarrow$ Any posterior transformation on $Y$, which produces some variable $Z$, may make the information about $X$ lost

**Prove**: direct prove
* $I(X; Y, Z) = I(X; Y) + I(X; Z|Y) = I(X; Y)$ due to Markov assumption

$\hspace{2.5cm} = I(X; Z) + I(X; Y|Z)$
* From above, $I(X; Y) \geq I(X; Z)$ since $I(X; Y|Z) \geq 0$

**Corollaries**: 
* If $Z = g(Y)$ then $I(X; Y) \geq I(X; g(Y))$
    * Explain: $X \to Y \to g(Y)$
* If $X \to Y \to Z$ then $I(X; Y|Z) \leq I(X; Y)$
    * Explain: from the prove
    * Intuition: mutual information of $X$ and $Y$ decreases if we know the the downstream variable $Z$

# Kullback - Leibler divergence (relative entropy)
**Application**: measure how one probability distribution is different from a some reference probability distribution

>**NOTE**: the term "divergence", which is confusing, should be changed into "cost" since relative entropy is a cost, rather than a distance in a Euclidean space, where $d(A, B) = d(B, A)$ for any distance function $d$

**KL-divergence**:
* Assumptions:
    * $P, Q$ are discrete probability distributions defined on the same probability space
* KL divergence: $D_{KL}(P\|Q) = \sum_i P(i) \log \frac{P_i}{Q_i}$
* Intuition: the expeceted amount of cost increased, in number of symbols needed, of represeting $P$ via $Q$ instead of $P$ itself
* Interpretations:
    * Machine learning: the information gain achieved if $Q$ is used instead of $P$
    * Bayesian inference: the information gained when one revises one's beliefs from the prior distribution $Q$ to the posterior distribution $P$
        * Explain: the amount of information lost when $Q$ is used to approximate $P$

**Symmetrized divergence**: $D_{KL}(P\|Q) + D_{KL}(Q\|P)$
* Application: feature selection in classification problems

**Information inequality**: $D_{KL}(P\|Q) \geq 0$
* Prove: direct prove with Jensen's inequality
    * $\log \frac{1}{x}$ is a convex function of $x$
    
    $\hspace{1.0cm} \rightarrow \sum_i P(i) \log \frac{P_i}{Q_i} \geq \log \frac{1}{\sum_i Q_i} = 0$

**Properties**:
* If $P_1, P_2$ are independent distributions with joint distribution $P(x, y) = P_1(x) P_2(y)$ and $Q, Q_1, Q_2$ likewise,

$\hspace{1.0cm} \rightarrow D_{KL}(P\|Q) = D_{KL}(P_1\|Q_1) + D_{KL}(P_2\|Q_2)$
* $D(p\|q)$ is convex in $(p, q)$

## Conditional KL-divergence
**Conditional KL divergence**: $D[p(y|x)\|q(y|x)] = \sum_{x, y} p(x, y) \log \frac{p(y|x)}{q(y|x)}$
* Non-negativity: $D[p(y|x)\|q(y|x)] \geq 0$

**Chain rule for KL divergence**: $D_{KL}(p(x, y)\|q(x, y)) = D_{KL}(p(x)\|q(x)) + D_{KL}(p(y|x)\|q(y|x))$
* Prove: direct prove
* Intuition: use "number of bits" intuition from Information Entropy

**Relation to other quantities of information theory**:
* Self-information: 
    * Formal: $I(m) = D_{KL}(\delta_{im}\|\{p_i\})$
        * $\delta_{im}$ is the Kronecker delta function
        * $\{p_i\}$ is the distribution, from which the event $m$ is taken
    * Intuition: use "number of bits" intuition from Information Entropy
* Mutual information: $I(X; Y) = D_{KL}(p(x, y)\|p(x) p(y))$
    * Intuition: $A \cap B = A + B - A \cup B$
* Entropy: 
    * Formal: $H(X) = \log N - D_{KL}(p_X(x)\|p_U(X))$
        * $N$ is the number of symbols in the $N$-ary alphabet used for encoding
        * $p_X$ is the true distribution of $X$
        * $p_U$ is the uniform distribution over $N$ items
    * Intuition: use "number of bits" intuition from Information Entropy
* Cross entropy: $H(p, q) = H(p) + D_{KL}(p\|q)$

# Sufficient statistics
**Introduction**: show the power of the data-processing inequality

**Sufficient statistic**:
* Assumptions:
    * $\{f_\theta(x)\}$ is a family of probability mass functions indexed by $\theta$
    * $X \sim \{f_\theta(x)\}$
    * $T(X)$ is any statistic
* Observations:
    * $\theta \to X \to T(X)$

    $\hspace{1.0cm} \rightarrow I[\theta; T(X)] \leq I(\theta; X)$ for any $\theta$
    * If $I[\theta; T(X)] = I(\theta; X)$ then no information is lost
* Sufficient statistic: $T(X)$ is sufficient for $\theta$ if it contains all the information in $X$ about $\theta$
    * Formal: $I[\theta; T(X)] = I(\theta; X)$

**Definition of sufficient statistic in statistics and information theory**:
* Sufficient statistic in statistic: $T$ is a sufficient statistic if $P(x|T, \theta) = P(x|T)$
    * Formal: $\theta \to T(X) \to X$ is a Markov chain
* Equivalence to "sufficient statistic" in information theory: we know $\theta \to X \to T(X)$ is a Markov chain also

$\hspace{1.0cm} \rightarrow \theta \to T(X) \to X$ implies $I[\theta; T(X)] = I(\theta; X)$

**Minimal sufficient statistic**: $T(X)$ is a minimal sufficient statistic relative to $\{f_\theta(x)\}$ if it's a function of every other sufficient statitic $U$
* Formal: $\theta \to T(X) \to U(X) \to X$
* Interpretation: 
    * $T$ compresses the information about $\theta$ in the sample
    * $U$ may contain additional irrelevant information

# Fano's inequality
## Fano's inequality
**Other names**: Fano converse or Fano lemma

**Motivation**:
* $H(X|Y) = 0$ if and only if $X = f(Y)$

$\hspace{1.0cm} \rightarrow$ We can estimate $X$ from $Y$ with zero probability of error if any only if $H(X|Y) = 0$ (i.e. $X$ is a function of $Y$)
* From above, we expect to be able to estimate $X$ with a low probability of error if any only if $H(X|Y)$ is small

$\hspace{1.0cm} \rightarrow$ This idea is quantified by Fano's inequality

**Problem of interest**:
* Assumptions:
    * We wish to estimate a random variable $X$ with a distribution $p(x)$
        * $\cal{X}$ is the support of $X$
    * We observe a random variable $Y$ that is related to $X$ by the conditional distribution $p(y|x)$
    * We estimate $X$ by $\tilde{X} = g(Y)$
        * $\tilde{X}$ takes on values in $\tilde{\cal{X}}$
        
        >**NOTE**: we won't restrict $\tilde{\cal{X}}$ to be equal to $\cal{X}$ and we allow $g(Y)$ to be random
    
    * $P_e = \text{Pr}(\tilde{X} \neq X)$
* Problem: bound the probability that $\tilde{X} \neq X$ by bounding $H(X|Y)$ (i.e. how deterministic $X$ is, given $Y$)

**Fano's inequality**:
* Observation: $X \to Y \to \tilde{X}$ forms a Markov chain
* Fano's inequality:
    * Original form: $H(P_e) + P_e \log{|\cal{X}|} \geq H(X|\tilde{X}) \geq H(X|Y)$
    * Weakened form: $1 + P_e \log{|\cal{X}|} \geq H(X|Y)$ (or $P_e \geq \frac{H(X|Y) - 1}{\log |\cal{X}|}$)
* Corollaries:
    * $H(p) + p \log{|\cal{X}|} \geq H(X|Y)$ where $p = \text{Pr}(X \neq Y)$
    * $H(P_e) + P_e \log({|\cal{X}|} - 1) \geq H(X|Y)$

>**NOTE**: $P_e = 0$ implies $H(X|Y) = 0$ as intuition suggests

* Prove: 
    * Original form: direct prove using axiom (3) of information entropy
        * From axiom (3) of information theory
        
        $\hspace{1.0cm} \rightarrow H(X) = H(P_e) + P_e \log |{\cal{X}}| \geq H(X|\tilde{X})$
        * From the data processing inequality

        $\hspace{1.0cm} \rightarrow H(X|\tilde{X}) = H(X) - I(X; \tilde{X}) \geq H(X) - I(X; Y) = H(X|Y)$
    * Weakened form: since $1 \geq \log 2 \geq H(P_e)$ for any logarithm base $D \geq 2$

**Remark**: if there's no knowledge of $Y$, $X$ must be guessed without any information
* Assumptions:
    * $X \in \{1, ..., m\}$
    * $p_1 \geq p_2 \geq ... \geq p_m$
* Best guess of $X$: $\tilde{X} = 1$
    * Error rate: $P_e = 1 - p_1$
* Fano's inequality: $H(P_e) + P_e \log (m-1) \geq H(X)$
    * Equality case: $(p_1, ..., p_m) = (1 - P_e, \frac{P_e}{m-1}, ..., \frac{P_e}{m-1})$
* Prove: direct prove from Fano's inequality

## Another relationship of error rate and entropy
**Theorem**:
* Assumptions:
    * $X$ and $X'$ are two i.i.d random variables
        * $X \sim p(x)$
    * Entropy of $X$ and $X'$ is $H(X)$
* Conclusion: $P(X = X') = \sum_x p^2(x)$
* Prove: $P(X = X') = \sum_x p(X = x) \cdot p(X' = x)$

**Lemma**: $P(X = X') \geq 2^{-H(X)}$
* Prove: direct prove using Jensen's inequality 
    * $\log P(X = X') = \log [\sum_x p^2(x)] \geq \sum_x p(x) \log p(x) = - H(X)$ due to the concavity of $\log x$
* Intuition: use "number of bits" intuition from information entropy

**Corollary**:
* Assumptions:
    * $X, X'$ are independent
    * $X \sim p(x)$
    * $X' \sim r(x)$
    * $x, x' \in {\cal{X}}$
* Conclusion:
    * $P(X = X') \geq 2^{-H(p)-D(p\|r)}$
    * $P(X = X') \geq 2^{-H(r)-D(r\|p)}$

## Intuition
**Original form**: to represent $X$ given $\tilde{X}$, we need to carry out two steps:
* Verify whether $\tilde{X} = X$

$\hspace{1.0cm} \rightarrow$ This takes $H(P_e)$ symbols
* If $\tilde{X} \neq X$, which happens with probability $P_e$, we need to specify $X$ from $\cal{X}$

$\hspace{1.0cm} \rightarrow$ This takes $P_e H(X)$ symbols, which equals $P_e \log {|\cal{X}|}$ in the worst case
* From above, we need $H(P_e) + P_e \log {|\cal{X}|}$ symbols to represent $X$ given $\cal{X}$ in the worst case

$\hspace{1.0cm} \implies H(P_e) + P_e \log {|\cal{X}|} \geq H(X|\tilde{X}) \geq H(X|Y)$

**Weakened form**: $H(P_e) \leq \log_D 2 \leq 1$ where $D \geq 2$

$\hspace{1.0cm} \implies 1 + P_e \log {|\cal{X}|} \geq H(P_e) + P_e \log {|\cal{X}|} \geq H(X|\tilde{X}) \geq H(X|Y)$

**Conclusion**: given $\tilde{X} \in \cal{X}$, we only need to determine the true value of $X$ among $|\cal{X}| - 1$ other values in $\cal{X}$

$\hspace{1.0cm} \implies H(P_e) + P_e \log(|{\cal{X}}| - 1)\geq H(X|\tilde{X}) \geq H(X|Y)$

---

# BONUS
* Kronecker delta function: $\delta_{ij} = \begin{cases} 0 && i \neq j \\ 1 && i = j\end{cases}$
* Another notation of Markov chain: 
  * Assumptions:
    * $X, Y, Z$ form a Markov chain
  * Conclusion:
    * Notation: $X \to Y \to Z$
* Kolmogorov complexity of an object (e.g. a piece of text): the length of the shortest computer program (in a predetermined programming language) that produces the object as output
* Linear homogeneous difference equation: $\sum_{t=1}^n a_t X(t) = 0$
    * Idea:
        * Guess $X(t) = x^t$ and solve $\sum_{t=0}^n a_t x^t = 0$ for solutions $x_1, ..., x_n$
    * General solution: $\sum_{i=1}^n c_i x_i$
        * Explain: due to linearity of the equation and the difference operation
* Linear inhomogeneous difference equation: $\sum_{t=1}^n a_t X(t) = b$
    * Idea:
        * Solve $\sum_{t=1}^n a_t X(t) = 0$ for $x_1, ..., x_n$
        * Find a particular solution $x_0$ for $\sum_{t=1}^n a_t X(t) = b$
    * General solution: $x_0 + \sum_{i=1}^n c_i x_i$