<!-- TOC titleSize:1 tabSpaces:2 depthFrom:1 depthTo:6 withLinks:1 updateOnSave:1 orderedList:0 skip:0 title:1 charForUnorderedList:* -->
# Table of Contents
- [Table of Contents](#table-of-contents)
- [Recall](#recall)
- [Introduction](#introduction)
- [Asymptotic equipartition property theorem](#asymptotic-equipartition-property-theorem)
- [Consequences of the AEP: data compression](#consequences-of-the-aep-data-compression)
- [High probability sets and the typical set](#high-probability-sets-and-the-typical-set)
- [NEW WORD](#new-word)
<!-- /TOC -->

# Recall
**Weak law of large numbers**: 
* Assumptions:
    * $n$ is the sample size
    * $\bar{X}_n$ is the sample mean
    * $\mu$ is the population mean (i.e. the true mean of $X$)
* Conclusion: $\lim_{n \to \infty} \text{Pr}(|\bar{X}_n - \mu| \geq \epsilon) = 0$
* Reference: see Probability and Statistics - DeGroot

# Introduction
**Application**: divide the set of all sequences into two sets
* The typical set: contain the sequences where the sample entropy is close to the true entropy
* The non-typical set: contain other sequences

>**NOTE**: any property that is proved for the typical sequences will be true with high probability and then determine the average behavior of a large sample

$\hspace{1.0cm} \rightarrow$ Most of attention will be on the typical sequences

**Convergence of random variables**:
* Assumptions:
    * $X_1, X_2, ...$ is a sequence of random variables
    * $X$ is a random variable
* Conclusion:
    * Convergence to $X$ in probability: if for every $\epsilon > 0$, $\text{Pr}(|\bar{X}_n - X| > \epsilon) \to 0$
    * Convergence to $X$ in mean square: if $E((\bar{X}_n - X)^2) \to 0$
    * Convergence to $X$ with probability $1$: $\text{Pr}(\lim_{n \to \infty} \bar{X}_n = X) = 1$

# Asymptotic equipartition property theorem
**Asymptotic equipartition property theorem (AEP)**:
* Assumptions:
    * $X_1, X_2, ...$ are i.i.d
    * $X_i \sim p(x)$
* Conclusion:
    * $\frac{1}{n} \log \frac{1}{p(X_1, ..., X_n)} \to H(X_i)$ (in probability)
    * (or) $p(X_1, ..., X_n) \to D^{-n H(X_i)}$ (in probability)
* Informal: the sample mean of information content of the sample drawn from the distribution of $X$ converges in probability to the entropy of a variable $X$
* Intuition:
    * Due to independence, $\log \frac{1}{p(X_1, ..., X_n)} = \sum_{i = 1}^n \log \frac{1}{p(X_i)}$ (i.e. the total number of symbols required to represent $X_1, ..., X_n$ independently)
    * From above, $\frac{1}{n} \log \frac{1}{p(X_1, ..., X_n)} = E(\log \frac{1}{p(X)}) \to H(X)$ (according to the weak law of large numbers)

**Typical set**:
* Typical set:
    * Assumptions:
        * $X_1, ..., X_n$ are i.i.d random variables whose values taken from $\cal{X}$
        * $H(X)$ is the entropy of $X_i$
    * Conclusion:
        * The typical set $A_\epsilon^{(n)}$ w.r.t $p(x)$: 
            * $\{(x_1, ..., x_n) \in {\cal{X}^n} : \frac{1}{D^{n(H(X) + \epsilon)}} \leq p(x_1, ..., x_n) \leq \frac{1}{D^{n(H(X) - \epsilon)}}\}$
            * (or) $\{(x_1, ..., x_n) \in {\cal{X}^n} : H(X) - \epsilon \leq \frac{1}{n} \log \frac{1}{p(x_1, ..., x_n)} \leq H(X) + \epsilon\}$
    * Intuition: a sample is typical if its sample entropy is close to the true entropy $H(X)$ of the distribution, from which is sample is drawn

>**NOTE**: the notion of typicality is only concerned with the probability of a sequence and not the actual sequence itself

* Properties of $A_\epsilon^{(n)}$:
    * For all $\epsilon > 0$ and $\delta > 0$, there exists some $n_0$ that $\text{Pr}(A_\epsilon^{(n)}) > 1 - \delta$ for $n \geq n_0$
        * Explain: due to AEP
        
        >**NOTE**: this property explains why $A_\epsilon^{(n)}$ is said to be "typical"
        
    * $|A_\epsilon^{(n)}| \leq D^{n(H(X) + \epsilon)}$
        * Explain: $\sum_{x^n \in A_\epsilon^{(n)}} p(x^n) \leq 1$
        
        $\hspace{1.0cm} \leftrightarrow |A_\epsilon^{(n)}| \min_{x^n \in A_\epsilon^{(n)}} p(x^n) \leq 1$
        
        $\hspace{1.0cm} \leftrightarrow |A_\epsilon^{(n)}| D^{-n(H(X) + \epsilon)} \leq 1$
        
        $\hspace{1.0cm} \leftrightarrow |A_\epsilon^{(n)}| \leq D^{n(H(X) + \epsilon)}$
    * $|A_\epsilon^{(n)}| \geq (1 - \delta) D^{n(H(X) - \epsilon)}$ for $n$ sufficiently large
        * Explain: $\sum_{x^n \in A_\epsilon^{(n)}} p(x^n) \geq 1 - \delta$
        
        $\hspace{1.0cm} \leftrightarrow |A_\epsilon^{(n)}| \max_{x^n \in A_\epsilon^{(n)}} p(x^n) \geq 1 - \delta$
        
        $\hspace{1.0cm} \leftrightarrow |A_\epsilon^{(n)}| D^{-n(H(X) - \epsilon)} \geq 1 - \delta$
        
        $\hspace{1.0cm} \leftrightarrow |A_\epsilon^{(n)}| \geq (1 - \delta) D^{n(H(X) - \epsilon)}$
* Usage: describe a set with characteristics that belong to the majority of elements in that set

**Historical motivation**: typical set was defined by Shannon since he wanted to determine how efficiently one could possibly encode a stream of symbols from a fixed alphabet
* Assumptions on the stream of symbols: each symbol is an i.i.d random sample from some distribution
* Key insights of Shannon:
    * There is an easily identifiable, relatively small set of "typical" sequences that show up disproportionately often in the stream
    * Assigning this "typical set" of sequences the shortest encodings yields an optimally efficient encoding (asymptotically, as the stream's output grow arbitrarily long)
* Typicality of typical set: sequences in the typical set has their information is about average (not the most probable ones)
    * Explain Shannon's definition of "typicality": Shannon did not want the most "typical" possible typical set; he wanted one that made it easy to prove the result he wanted to prove

>**NOTE**: the typical set defined by Shannon is guaranteed to exist, it is guaranteed to be small (i.e. its size is bounded)

# Consequences of the AEP: data compression
**Problem**:
* Assumptions:
  * $X_1, ..., X_n$ are i.i.d random variables
    * $X_i \sim p_(x)$
* Task: find short descriptions of the sequence $X_1, ..., X_n$

**Idea**: 
* Step 1: divide $\cal{X}$ into $A = A_\epsilon^{(n)}$ and its complement $A^c$
* Step 2: sort $A$ and $A^c$ by some order

$\hspace{1.0cm} \rightarrow$ We can represent each sequence of $A$ by their index

**Proposed coding scheme**:
* Indexing sequences in $A$:
    * $|A| \leq D^{n(H + \epsilon)}$

    $\hspace{1.0cm} \rightarrow$ The indexing requires no more than $n(H + \epsilon) + 1$ symbols
    * For each sequence in $A$, we prefix it by a $0$ symbol

    $\hspace{1.0cm} \rightarrow$ The indexing now requires no more than $n (H + \epsilon) + 2$ symbols
* Indexing sequences in $A^c$:
    * For indexing each sequence in $A^c$, it takes no more than $n \log |\cal{X}| + 1$ bits
    * For each sequence in $A^c$, we prefix it by a $1$ symbol

**Analyze the proposed coding scheme**:
* The code is one-to-one and easily decodable
* We don't take into account the fact  that $|A^c| \leq |\cal{X}^n|$

$\hspace{1.0cm} \rightarrow$ This is good enough to yield an efficient description
* The typical sequences have short description of length $\approx n H$ for sufficient large $n$

**Theorem**:
* Assumptions:
    * $X^n$ is i.i.d $\sim p(x)$
    * $\epsilon > 0$
* Conclusion:
    * There exists a code that maps sequences $x^n$ of length $n$ into $D$-ary strings that:
        * The mapping is one-to-one (i.e. thus invertible)
        * $E(\frac{1}{n} l(X^n)) \leq H(X) + \epsilon$ for $n$ sufficiently large
* Explain: due to AEP
* Corollary: we can represent sequences $X^n$ using $n H(X)$ symbols on average

# High probability sets and the typical set
**Definition**: $a_n = b_n$ means $\lim_{n \to \infty} \frac{1}{n} \log \frac{a_n}{b_n} = 0$
* Intuition: $\frac{a_n}{b_n}$ is finite even when $n \to \infty$ (i.e. they have the same polynomial degree)

**Theorem**
* Assumptions:
    * $X_1, ..., X_n$ are i.i.d
        * $X_i \sim p(x)$
    * For $n = 1, 2, ...$, $B_\delta^{(n)} \subset \cal{X}^n$ is the smallest set with $\text{Pr}(B_\delta^{(n)}) \geq 1 - \delta$
* Conclusion: for $\delta < \frac{1}{2}$ and any $\delta' > 0$, if $\text{Pr}(B_\delta^{(n)}) > 1 - \delta$,

$\hspace{1.0cm} \rightarrow \frac{1}{n} \log |B_\delta^{(n)}| > H - \delta'$ for $n$ sufficiently large
* Another representation: if $\delta_n \to 0$ and $\epsilon_n \to 0$,

$\hspace{1.0cm} \rightarrow |B_{\delta_n}^{(n)}| = |A_{\epsilon_n}^{(n)}| = D^{n H}$
* Prove:
    * $\frac{1}{n} \log |B_\delta^{(n)}| \geq \frac{1}{n} H(X^n|X^n \in B_\delta^{(n)})$

    $\hspace{3.3cm} = \frac{1}{n} H(X^n) - \frac{1}{n} H(A)$ where $A = 1$ if $X^n \in B_\delta^{(n)}$, otherwise $A = 0$
    * Due to AEP, with $n$ sufficiently large, $\log |B_\delta^{(n)}| \geq H(X) - \delta'$

**Corollary**: $B_\delta^{(n)}$ has significant intersection with $A_\epsilon^{(n)}$

---

# NEW WORD
* Disproportionately (adv): không cân xứng