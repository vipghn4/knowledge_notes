<!-- TOC titleSize:1 tabSpaces:2 depthFrom:1 depthTo:6 withLinks:1 updateOnSave:1 orderedList:0 skip:0 title:1 charForUnorderedList:* -->
# Table of Contents
- [Table of Contents](#table-of-contents)
- [Markov chain](#markov-chain)
  - [Definitions and examples](#definitions-and-examples)
    - [Examples](#examples)
  - [Multistep transition probabilities](#multistep-transition-probabilities)
  - [Classification of states](#classification-of-states)
- [Appendix](#appendix)
  - [Concepts](#concepts)
<!-- /TOC -->

# Markov chain
## Definitions and examples
**Importance of Markov chains**. Come from two facts
* There are a large number of physical, biological, economic, and social phenomena, which can be modeled in this way
* There is a well-developed theory that allows us to do computations

**Markov chain**.
* *Assumptions*.
    * $X_n$ is a sequence of random variables
* *Markov property*. Given the current state $X_n$, any other information about the past is irrelevant for predicting $X_{n+1}$
* *Markov chain*. $X_n$ is a discrete time Markov chain with transition matrix $p(i,j)$ if 
    * $X_n$ has Markov property
    * For any $j,i,i_{n-1},\dots,i_0$, we have

        $$P(X_{n+1}=j|X_n=i,X_{n-1}=i_{n-1},\dots,X_0=i_0) = P(X_{n+1}=j|X_n=i) = p(i,j)$$

    >**NOTE**. $p(i,j)$ is the basic information required to describe a Markov chain

* *Requirements for transition probabilities $p(i,j)$*. Any matrix with the following properties gives rise to a Markov chain $X_n$
    * $p(i,j)\geq 0$ since they are probabilities
    * $\sum_j p(i,j) = 1$, since when $X_n=i$, $X_{n+1}$ will be in some state $j$
* *Temporally homogeneous Markov chain*. A Markov chain $X_n$ with transition probability independent of the time $n$

**Absorbing states**. The states $x$ correponding to fixation of the Markov chain, i.e. $p(x,x) = 1$

**Two-stage Markov chains**. A generalization of ordinary Markov chains, when $X_{n+1}$ only depends on $(X_n,X_{n-1})$

### Examples
**Simple examples**.
* *Gambler's ruin*. Let $X_n$ be the amount of money the gambler have, after $n$ plays

    $\to$ $X_n$ has the Markov property
* *Ehrenfest chain*. Originated in physics as a model for two cubical volumes of air, i.e. two urns with $N$ balls in total, connected by a small hole
    
    $\to$ We pick one of the $N$ balls at random and move it to the other urn
    * *Markov chain*. Let $X_n$ be the number of balls in the left urn, after the $n$th draw
        
        $\to$ $X_n$ has the Markov property
* *Social mobility*. Let $X_n$ be a family's social class in the $n$th generation, which is either $1=\text{lower},2=\text{middle},3=\text{upper}$

    $\to$ In a simple version of sociology, $X_n$ has the Markov property
    * *Related question*. Do the fractions of people in the three classes approach a limit?
* *Brand preference*. Let $X_n$ be the brand chosen on the $n$th purchase, i.e. customers trying some brands will have probability of trying out other brands

    $\to$ In a simple version, $X_n$ has the Markov property
    * *Related question*. Do the market shares of the products stabilize?
* *Inventory chain*. Consider the consequences of using an $s,S$ inventory control policy
    * *Inventory control policy*. When the stock on hand, at the end of the day, falls to $s$ or below

        $\to$ We order enough to bring it back up to $S$, at the beginning of the next day
    * *Assumptions*. 
        * $X_n$ is the amount of stock on hand at the end of day $n$
        * $D_{n+1}$ is the demand on day $n+1$, which is a random variable
    * *Conclusion*. With $S$ and $D_{n+1}$, $X_n$ has the Markov property
    * *Related question*. If we make $p'$ profit on each unit sold, but it costs $s'$ a day to store items

        $\to$ What is the long-run profit per day of the policy, and how to choose $s,S$ to maximize profit?

**Branching process**.
* *Assumptions*. Consider a population as following
    * Each male individual in the $n$th generation independently gives birth
        
        $\to$ He produces $k$ children, i.e. generation $n+1$, with probability $p_k$

        >**NOTE**. Only male children count, since only they carry on the family name
    
    * $X_n\geq 0$ is the number of individuals in generation $n$
    * $Y_1,Y_2,\dots$ is the independent variables with $P(Y_m=k)=p_k$

        $\to$ $Y_i$ indicates the number of children produced by the $i$th individual
* *Transition probability*.

    $$p(i,j)=\begin{cases}P(Y_1 + \dots + Y_i = j) & i>0,j\geq 0 \\ 0 & i=j=0\end{cases}$$

* *Question of interest*. What is the probability that the line of a man becomes extinct, i.e. the branching process becomes absorbed at $0$

## Multistep transition probabilities
**Brief**. The goal of this section is to compute the probability of going from $i$ to $j$ in $m>1$ steps

$$p^m(i,j) = P(X_{n+m}=j|X_n=i)$$

**Theorem**. The $m$ step transition probability $P(X_{n+m}=j|X_n=i)$ is the $m$th power of the transition matrix $p$
* *Chapman-Kolmogorov equation*. The key ingredient to prove the theorem

    $$p^{m+n}(i,j) = \sum_k p^m(i,k) p^n(k,j)$$

## Classification of states
**Notations**.
* We are often interested in the behavior of the chain for a fixed initial state, hence we introduce

    $$P_x(A) = P(A|X_0=x),\quad E_x=E[P_x(A)]$$

* The time of the first return to $y$, i.e. being at time $0$ does not count, is denoted as

    $$T_y=\min\{n\geq 1:X_n=y\}$$

* The time of the $k$th return to $y$, where $k\geq 2$, is denoted as

    $$T_y^k = \min\{n> T_y^{k-1}:X_n=y\}$$

* The probability that $X_n$ returns to $y$ when it starts at $y$ is denoted as

    $$\rho_{yy}=P_y(T_y<\infty)$$

**Theorem**. The probability $X_n$ will return to $y$ at least $m$ times is $\rho_{yy}^m$
* *Explain*. After the first return, the chain is at $y$

    $\to$ The probability of the $m$th return is $\rho_{yy}^m$

**Stopping time**. $T$ is a stopping time if the occurrence, or non-occurrence, of the event "we stop at time $n$", i.e. $\{T=n\}$, can be determined by $X_0,\dots,X_n$
* *Othger names*. Markov time, Markov moment, optional stopping time, or optional time
* *Informal definition*. A specific type of random time
    * *Random time*. A random variable, whose value is interpreted as the time, at which a given stochastic process exhibits a certain behavior of interest
    * *Stopping rule*. A stopping time is often defined by a stopping rule, a mechanism for deciding whether to continue or stop a process on the basis of the present position and past events
* *Consequence*. $T_y$ is a stopping time
    * *Explain*. $\{T_y=n\}=\{X_1\neq y,\dots,X_{n-1}\neq y,X_n=y\}$

**Strong Markov property**.
* *Assumptions*. $T$ is a stopping time
* *Conclusion*. Given $T=n$ and $X_T=y$, we have
    * Any information about $X_0,\dots,X_T$ is irrelevant for predicting the future
    * $X_{T+k}$ behaves like the Markov chain with initial state $y$, for any $k\geq 0$
* *Interpretation*. The Markov property holds at stopping time
    * *Explain*.
        * Stopping at time $n$ depends only on $X_0,\dots,X_n$
        * In a Markov chain, the distribution of the future only depends on the past through the current state
* *Consequence*. The conditional probability that we will return one more time, given we have returned $k-1$ times, is $\rho_{yy}$

    $\to$ $P_y(T_y^k<\infty) = \rho^k_{yy}$

**Transient and recurrent states**.
* *Transient state*. When $\rho_{yy}<1$, the probability of returning $k$ times is $\rho^k_{yy}\to 0$ as $k\to\infty$

    $\to$ Eventually the Markov chain does not find its way back to $y$
* *Recurrent state*. When $\rho_{yy}=1$, the probability of returning $k$ times is $\rho_{yy}^k=1$

    $\to$ The chain returns to $y$ infinitely many times

# Appendix
## Concepts
**Positive part of a real number**. $x^+ = \max\{x,0\}$