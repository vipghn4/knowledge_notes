<!-- TOC titleSize:1 tabSpaces:2 depthFrom:1 depthTo:6 withLinks:1 updateOnSave:1 orderedList:0 skip:0 title:1 charForUnorderedList:* -->
# Table of Contents
- [Table of Contents](#table-of-contents)
- [Markov chain](#markov-chain)
  - [Definitions and examples](#definitions-and-examples)
    - [Examples](#examples)
  - [Multistep transition probabilities](#multistep-transition-probabilities)
  - [Classification of states](#classification-of-states)
    - [Strong Markov property](#strong-markov-property)
    - [Transient and recurrent states](#transient-and-recurrent-states)
- [Appendix](#appendix)
  - [Concepts](#concepts)
<!-- /TOC -->

# Markov chain
## Definitions and examples
**Importance of Markov chains**. Come from two facts
* There are a large number of physical, biological, economic, and social phenomena, which can be modeled in this way
* There is a well-developed theory that allows us to do computations

**Markov chain**.
* *Assumptions*.
    * $X_n$ is a sequence of random variables
* *Markov property*. Given the current state $X_n$, any other information about the past is irrelevant for predicting $X_{n+1}$
* *Markov chain*. $X_n$ is a discrete time Markov chain with transition matrix $p(i,j)$ if 
    * $X_n$ has Markov property
    * For any $j,i,i_{n-1},\dots,i_0$, we have

        $$P(X_{n+1}=j|X_n=i,X_{n-1}=i_{n-1},\dots,X_0=i_0) = P(X_{n+1}=j|X_n=i) = p(i,j)$$

    >**NOTE**. $p(i,j)$ is the basic information required to describe a Markov chain

* *Requirements for transition probabilities $p(i,j)$*. Any matrix with the following properties gives rise to a Markov chain $X_n$
    * $p(i,j)\geq 0$ since they are probabilities
    * $\sum_j p(i,j) = 1$, since when $X_n=i$, $X_{n+1}$ will be in some state $j$
* *Temporally homogeneous Markov chain*. A Markov chain $X_n$ with transition probability independent of the time $n$

**Absorbing states**. The states $x$ correponding to fixation of the Markov chain, i.e. $p(x,x) = 1$

**Two-stage Markov chains**. A generalization of ordinary Markov chains, when $X_{n+1}$ only depends on $(X_n,X_{n-1})$

### Examples
**Simple examples**.
* *Gambler's ruin*. Let $X_n$ be the amount of money the gambler have, after $n$ plays

    $\to$ $X_n$ has the Markov property
* *Ehrenfest chain*. Originated in physics as a model for two cubical volumes of air, i.e. two urns with $N$ balls in total, connected by a small hole
    
    $\to$ We pick one of the $N$ balls at random and move it to the other urn
    * *Markov chain*. Let $X_n$ be the number of balls in the left urn, after the $n$th draw
        
        $\to$ $X_n$ has the Markov property
* *Social mobility*. Let $X_n$ be a family's social class in the $n$th generation, which is either $1=\text{lower},2=\text{middle},3=\text{upper}$

    $\to$ In a simple version of sociology, $X_n$ has the Markov property
    * *Related question*. Do the fractions of people in the three classes approach a limit?
* *Brand preference*. Let $X_n$ be the brand chosen on the $n$th purchase, i.e. customers trying some brands will have probability of trying out other brands

    $\to$ In a simple version, $X_n$ has the Markov property
    * *Related question*. Do the market shares of the products stabilize?
* *Inventory chain*. Consider the consequences of using an $s,S$ inventory control policy
    * *Inventory control policy*. When the stock on hand, at the end of the day, falls to $s$ or below

        $\to$ We order enough to bring it back up to $S$, at the beginning of the next day
    * *Assumptions*. 
        * $X_n$ is the amount of stock on hand at the end of day $n$
        * $D_{n+1}$ is the demand on day $n+1$, which is a random variable
    * *Conclusion*. With $S$ and $D_{n+1}$, $X_n$ has the Markov property
    * *Related question*. If we make $p'$ profit on each unit sold, but it costs $s'$ a day to store items

        $\to$ What is the long-run profit per day of the policy, and how to choose $s,S$ to maximize profit?

**Branching process**.
* *Assumptions*. Consider a population as following
    * Each male individual in the $n$th generation independently gives birth
        
        $\to$ He produces $k$ children, i.e. generation $n+1$, with probability $p_k$

        >**NOTE**. Only male children count, since only they carry on the family name
    
    * $X_n\geq 0$ is the number of individuals in generation $n$
    * $Y_1,Y_2,\dots$ is the independent variables with $P(Y_m=k)=p_k$

        $\to$ $Y_i$ indicates the number of children produced by the $i$th individual
* *Transition probability*.

    $$p(i,j)=\begin{cases}P(Y_1 + \dots + Y_i = j) & i>0,j\geq 0 \\ 0 & i=j=0\end{cases}$$

* *Question of interest*. What is the probability that the line of a man becomes extinct, i.e. the branching process becomes absorbed at $0$

## Multistep transition probabilities
**Brief**. The goal of this section is to compute the probability of going from $i$ to $j$ in $m>1$ steps

$$p^m(i,j) = P(X_{n+m}=j|X_n=i)$$

**Theorem**. The $m$ step transition probability $P(X_{n+m}=j|X_n=i)$ is the $m$th power of the transition matrix $p$
* *Chapman-Kolmogorov equation*. The key ingredient to prove the theorem

    $$p^{m+n}(i,j) = \sum_k p^m(i,k) p^n(k,j)$$

## Classification of states
**Notations**.
* We are often interested in the behavior of the chain for a fixed initial state, hence we introduce

    $$P_x(A) = P(A|X_0=x),\quad E_x=E[P_x(A)]$$

* The time of the first return to $y$, i.e. being at time $0$ does not count, is denoted as

    $$T_y=\min\{n\geq 1:X_n=y\}$$

* The time of the $k$th return to $y$, where $k\geq 2$, is denoted as

    $$T_y^k = \min\{n> T_y^{k-1}:X_n=y\}$$

* The probability that $X_n$ returns to $y$ when it starts at $y$ is denoted as

    $$\rho_{yy}=P_y(T_y<\infty)$$

### Strong Markov property
**Filtration**.
* *Assumptions*. 
    * $(\Omega,\mathcal{A},P)$ is a probability space
    * $I$ is an index set, i.e. a set whose members label members of another set, with total order $\leq$
    * $\mathcal{F}_i$ is a sub-$\sigma$-algebra of $\mathcal{A}$, for every $i\in I$
* *The filtration*. $\mathbb{F}=\{\mathcal{F}_i\}_{i\in I}$ is a filtration if

    $$\forall k\leq l,\mathcal{F}_k\subseteq\mathcal{F}_l$$

    $\to$ Filtrations are families of $\sigma$-algebra, which are ordered non-decreasingly
* *Filtered probability space*. $(\Omega,\mathcal{A},\mathbb{F},P)$
* *Example*. 
    * *Assumptions*. 
        * $(X_n)_{n\in\mathbb{N}}$ be a stochastic process on the probability space $(\Omega,\mathcal{A},P)$
        * $\sigma(X_k|k\geq n)$ denotes the $\sigma$-algebra generated by the random variables $X_1,\dots,X_n$
        * $\mathcal{F}_n=\sigma(X_k|k\geq n)$ is a $\sigma$-algebra
    * *Filtration*. $\mathbb{F}=(\mathcal{F}_n)_{n\in\mathbb{N}}$ is a filtration

**Random time**. 
* *Assumptions*. $X_n$ is a stochastic process
* *Random time $\tau$*. A discrete random variable on the same probability space as $X_n$, taking values in time set $\mathbb{N}=\{0,1,\dots\}\cup \{\infty\}$
    * *Interpretation*. $\tau$ denotes the time, at which $X_n$ exhibits a certain behavior of interest
    * *State at random time $\tau$*. $X_\tau$, i.e. if $\tau=n$ then $X_\tau=X_n$

**Stopping time**. A random variable $T$ is a stopping time if the occurrence, or non-occurrence, of the event "we stop at time $n$", i.e. $\{T=n\}$, can be determined by $X_0,\dots,X_n$
* *Other names*. Markov time, Markov moment, optional stopping time, or optional time
* *Motivating example*. Consider a gambling system where $\{X_n\}$ are independent Rademacher random variables with

    $$P(X_n=1)=P(X_n=-1)=1/2$$

    and $Z_n=\sum_{i=1}^n X_n$ is the money won or lost after $n$ games
    * *Stopping time*. $T=\min\{n:Z_n\geq 0\}$, i.e. play until $Z_n\geq 0$
* *Interpretation*. By watching the process, we know at the time when $T occurs

    $\to$ If asked to stop at $T$, we know when to stop
* *Consequence*. $T_y$ is a stopping time
    * *Explain*. $\{T_y=n\}=\{X_1\neq y,\dots,X_{n-1}\neq y,X_n=y\}$
* *Examples*.
    * The first passage time 

        $$T_j=\inf\{n\geq 1:X_n=j\}$$

        is a stopping time, since

        $$\{T_j=n\}=\{X_1\neq j,\dots,X_{n-1}\neq j,X_n=j\}$$
    * The last exit time

        $$L^A = \sup\{n\geq 0:X_n\in A\}$$

        is not a stopping time, since the event $\{L^A=n\}$ depends on whether $\{X_{n+m}\}_{m\geq 1}$ visits $A$ or not

**Strong Markov property**. The Markov property holds at stopping times
* *Assumptions*.
    * $X_n$ is a Markov chain
    * $T$ is a stopping time of $X_n$
* *Conclusion*. Conditional on $T<\infty$ and $X_T=y$, the chain resets at $T$, i.e.
    * Any information about $X_0,\dots,X_T$ is irrelevant for predicting the future
    * $X_{T+k}$ behaves like the Markov chain with initial state $y$, for any $k\geq 0$
* *Proof*. To simplify things, we will only show that

    $$P(X_{T+1}=z|X_T=y,T=n) = p(y,z)$$

    * Let $V_n$ be the set of vectors $(x_0,\dots,x_n)$ so that

        $$\{X_0=x_0,\dots,X_n=x_n\}\implies\{T=n,X_T=y\}$$
    
    * We have

        $$\begin{aligned}
        &P(X_{T+1}=z,X_T=y,T=n)\\
        =&\sum_{x\in V_n} P(X_{n+1}=z,X_n=x_n,\dots,X_0=x_0)\\
        =&\sum_{x\in V_n} P(X_{n+1}=z|X_n=x_n,\dots,X_0=x_0) P(X_n=x_n,\dots,X_0=x_0)\\
        =&p(y,z)\sum_{x\in V_n} P(X_n=x_n,\dots,X_0=x_0)\\
        =&p(y,z) P(T=n,X_T=y)
        \end{aligned}$$

* *Interpretation*. The Markov property holds at stopping time, since
    * Stopping at time $n$ depends only on $X_0,\dots,X_n$
    * In a Markov chain, the distribution of the future only depends on the past through the current state

**Corollary**. The conditional probability that we will return one more time, given we have returned $k-1$ times, is $\rho_{yy}$, hence

$$P_y(T_y^k<\infty) = \rho^k_{yy}$$

### Transient and recurrent states
**Theorem**. The probability $X_n$ will return to $y$ at least $m$ times is $\rho_{yy}^m$
* *Explain*. After the first return, the chain is at $y$

    $\to$ The probability of the $m$th return is $\rho_{yy}^m$

**Transient and recurrent states**.
* *Transient state*. When $\rho_{yy}<1$, the probability of returning $k$ times is $\rho^k_{yy}\to 0$ as $k\to\infty$

    $\to$ Eventually the Markov chain does not find its way back to $y$
* *Recurrent state*. When $\rho_{yy}=1$, the probability of returning $k$ times is $\rho_{yy}^k=1$

    $\to$ The chain returns to $y$ infinitely many times

# Appendix
## Concepts
**Positive part of a real number**. $x^+ = \max\{x,0\}$

**Total (or linear) order**. A partial order, in which any two elements are comparable
* *Formal*. A total order is a binary relation $\leq$ on some set $X$, which satisfies the following for all $a,b,c\in X$
    * *Reflexive*. $a\leq a$
    * *Transitive*. $a\leq b\land b\leq c\implies a\leq c$
    * *Antisymmetric*. $a\leq b\land b\leq a\implies a=b$
    * *Strongly connectivity (total)*. $a\leq b$ or $b\leq a$
* *Analogy*. Total ordering in distributed system

**Filtration (in physics)**. A physical separation process, which separates solid matter and fluid from a mixture using a filter medium, which has a complex structure, through which only the fluid can pass

<div style="text-align:center">
    <img src="https://i.imgur.com/u48c8NM.png">
    <figcaption>Physical filtration</figcaption>
</div>

**Strong Markov property versus Markov property**.
* *Reference*. 
    * https://ece.iisc.ac.in/~parimal/2019/random/lecture-22.pdf
    * http://www.columbia.edu/~ks20/stochastic-I/stochastic-I-ST.pdf
    * https://www.statslab.cam.ac.uk/~james/Markov/s14.pdf