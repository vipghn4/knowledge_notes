<!-- TOC titleSize:1 tabSpaces:2 depthFrom:1 depthTo:6 withLinks:1 updateOnSave:1 orderedList:0 skip:0 title:1 charForUnorderedList:* -->
# Table of Contents
- [Table of Contents](#table-of-contents)
- [Markov chain](#markov-chain)
  - [Definitions and examples](#definitions-and-examples)
  - [Multistep transition probabilities](#multistep-transition-probabilities)
  - [Classification of states](#classification-of-states)
    - [Strong Markov property](#strong-markov-property)
    - [Transient and recurrent states](#transient-and-recurrent-states)
  - [Stationary distributions](#stationary-distributions)
  - [Limit behavior](#limit-behavior)
    - [Periodicity of states](#periodicity-of-states)
    - [Aperiodic Markov chains](#aperiodic-markov-chains)
  - [Special examples](#special-examples)
    - [Doubly stochastic chains](#doubly-stochastic-chains)
    - [Detailed balance condition](#detailed-balance-condition)
    - [Reversibility](#reversibility)
    - [The metropolis-hastings algorithm](#the-metropolis-hastings-algorithm)
  - [Proofs of the main theorems](#proofs-of-the-main-theorems)
  - [Exit distributions](#exit-distributions)
  - [Exit times](#exit-times)
  - [Infinite state spaces](#infinite-state-spaces)
- [Appendix](#appendix)
  - [Concepts](#concepts)
<!-- /TOC -->

# Markov chain
## Definitions and examples
**Importance of Markov chains**. Come from two facts
* There are a large number of physical, biological, economic, and social phenomena, which can be modeled in this way
* There is a well-developed theory that allows us to do computations

**Discrete time Markov chain**.
* *Assumptions*.
    * $X_n$ is a sequence of random variables
* *Markov property*. Given the current state $X_n$, any other information about the past is irrelevant for predicting $X_{n+1}$
* *Markov chain*. $X_n$ is a discrete time Markov chain with transition matrix $p(i,j)$ if 
    * $X_n$ has Markov property
    * For any $j,i,i_{n-1},\dots,i_0$, we have

        $$P(X_{n+1}=j|X_n=i,X_{n-1}=i_{n-1},\dots,X_0=i_0) = P(X_{n+1}=j|X_n=i) = p(i,j)$$

    >**NOTE**. $p(i,j)$ is the basic information required to describe a Markov chain

* *Requirements for transition probabilities $p(i,j)$*. Any matrix with the following properties gives rise to a Markov chain $X_n$
    * $p(i,j)\geq 0$ since they are probabilities
    * $\sum_j p(i,j) = 1$, since when $X_n=i$, $X_{n+1}$ will be in some state $j$
* *Temporally homogeneous Markov chain*. A Markov chain $X_n$ with transition probability independent of the time $n$

**Absorbing states**. The states $x$ correponding to fixation of the Markov chain, i.e. $p(x,x) = 1$

**Two-stage Markov chains**. A generalization of ordinary Markov chains, when $X_{n+1}$ only depends on $(X_n,X_{n-1})$

## Multistep transition probabilities
**Brief**. The goal of this section is to compute the probability of going from $i$ to $j$ in $m>1$ steps

$$p^m(i,j) = P(X_{n+m}=j|X_n=i)$$

**Theorem**. The $m$ step transition probability $P(X_{n+m}=j|X_n=i)$ is the $m$th power of the transition matrix $p$
* *Chapman-Kolmogorov equation*. The key ingredient to prove the theorem

    $$p^{m+n}(i,j) = \sum_k p^m(i,k) p^n(k,j)$$

## Classification of states
**Notations**.
* We are often interested in the behavior of the chain for a fixed initial state, hence we introduce

    $$P_x(A) = P(A|X_0=x),\quad E_x=E[P_x(A)]$$

* The time of the first return to $y$, i.e. being at time $0$ does not count, is denoted as

    $$T_y=\min\{n\geq 1:X_n=y\}$$

* The time of the $k$th return to $y$, where $k\geq 2$, is denoted as

    $$T_y^k = \min\{n> T_y^{k-1}:X_n=y\}$$

* The probability that $X_n$ returns to $y$ when it starts at $y$ is denoted as

    $$\rho_{yy}=P_y(T_y<\infty)$$

### Strong Markov property
**Filtration**.
* *Assumptions*. 
    * $(\Omega,\mathcal{A},P)$ is a probability space
    * $I$ is an index set, i.e. a set whose members label members of another set, with total order $\leq$
    * $\mathcal{F}_i$ is a sub-$\sigma$-algebra of $\mathcal{A}$, for every $i\in I$
* *The filtration*. $\mathbb{F}=\{\mathcal{F}_i\}_{i\in I}$ is a filtration if

    $$\forall k\leq l,\mathcal{F}_k\subseteq\mathcal{F}_l$$

    $\to$ Filtrations are families of $\sigma$-algebra, which are ordered non-decreasingly
* *Filtered probability space*. $(\Omega,\mathcal{A},\mathbb{F},P)$
* *Example*. 
    * *Assumptions*. 
        * $(X_n)_{n\in\mathbb{N}}$ be a stochastic process on the probability space $(\Omega,\mathcal{A},P)$
        * $\sigma(X_k|k\geq n)$ denotes the $\sigma$-algebra generated by the random variables $X_1,\dots,X_n$
        * $\mathcal{F}_n=\sigma(X_k|k\geq n)$ is a $\sigma$-algebra
    * *Filtration*. $\mathbb{F}=(\mathcal{F}_n)_{n\in\mathbb{N}}$ is a filtration

**Random time**. 
* *Assumptions*. $X_n$ is a stochastic process
* *Random time $\tau$*. A discrete random variable on the same probability space as $X_n$, taking values in time set $\mathbb{N}=\{0,1,\dots\}\cup \{\infty\}$
    * *Interpretation*. $\tau$ denotes the time, at which $X_n$ exhibits a certain behavior of interest
    * *State at random time $\tau$*. $X_\tau$, i.e. if $\tau=n$ then $X_\tau=X_n$

**Stopping time**. A random variable $T$ is a stopping time if the occurrence, or non-occurrence, of the event "we stop at time $n$", i.e. $\{T=n\}$, can be determined by $X_0,\dots,X_n$
* *Other names*. Markov time, Markov moment, optional stopping time, or optional time
* *Motivating example*. Consider a gambling system where $\{X_n\}$ are independent Rademacher random variables with

    $$P(X_n=1)=P(X_n=-1)=1/2$$

    and $Z_n=\sum_{i=1}^n X_n$ is the money won or lost after $n$ games
    * *Stopping time*. $T=\min\{n:Z_n\geq 0\}$, i.e. play until $Z_n\geq 0$
* *Interpretation*. By watching the process, we know at the time when $T occurs

    $\to$ If asked to stop at $T$, we know when to stop
* *Consequence*. $T_y$ is a stopping time
    * *Explain*. $\{T_y=n\}=\{X_1\neq y,\dots,X_{n-1}\neq y,X_n=y\}$
* *Examples*.
    * The first passage time 

        $$T_j=\inf\{n\geq 1:X_n=j\}$$

        is a stopping time, since

        $$\{T_j=n\}=\{X_1\neq j,\dots,X_{n-1}\neq j,X_n=j\}$$
    * The last exit time

        $$L^A = \sup\{n\geq 0:X_n\in A\}$$

        is not a stopping time, since the event $\{L^A=n\}$ depends on whether $\{X_{n+m}\}_{m\geq 1}$ visits $A$ or not

**Strong Markov property**. The Markov property holds at stopping times
* *Assumptions*.
    * $X_n$ is a Markov chain
    * $T$ is a stopping time of $X_n$
* *Conclusion*. Conditional on $T<\infty$ and $X_T=y$, the chain resets at $T$, i.e.
    * Any information about $X_0,\dots,X_T$ is irrelevant for predicting the future
    * $X_{T+k}$ behaves like the Markov chain with initial state $y$, for any $k\geq 0$
* *Proof*. To simplify things, we will only show that

    $$P(X_{T+1}=z|X_T=y,T=n) = p(y,z)$$

    * Let $V_n$ be the set of vectors $(x_0,\dots,x_n)$ so that

        $$\{X_0=x_0,\dots,X_n=x_n\}\implies\{T=n,X_T=y\}$$
    
    * We have

        $$\begin{aligned}
        &P(X_{T+1}=z,X_T=y,T=n)\\
        =&\sum_{x\in V_n} P(X_{n+1}=z,X_n=x_n,\dots,X_0=x_0)\\
        =&\sum_{x\in V_n} P(X_{n+1}=z|X_n=x_n,\dots,X_0=x_0) P(X_n=x_n,\dots,X_0=x_0)\\
        =&p(y,z)\sum_{x\in V_n} P(X_n=x_n,\dots,X_0=x_0)\\
        =&p(y,z) P(T=n,X_T=y)
        \end{aligned}$$

* *Interpretation*. The Markov property holds at stopping time, since
    * Stopping at time $n$ depends only on $X_0,\dots,X_n$
    * In a Markov chain, the distribution of the future only depends on the past through the current state

**Example of a weak Markov process**.
* *Process of interest*. Consider a process $\mathbf{X}=(X_0)$
    * If $X_0=0$, then $X_t=0$ for all time $t$
    * If $X_0=x\neq 0$, then $X_t\sim\mathcal{N}(x,t)$, i.e. standard Brownian motion starting from $x$
* *Markov property of $\mathbf{X}$*. $\mathbf{X}$ has Markov property, since the future is independent of the past, given the present
* *Strong Markov property of $\mathbf{X}$*. 
    * *Assumptions*.
        * $T=\inf\{t:X_t=0\}$ is a stopping time
        * $X_0=x\neq 0$
    * *Observations*. 
        * Fixing $t>0$, we have that $P(X_t=0)=0$ since $X_t\sim\mathcal{N}(x,t)$
        * Suppose that $\mathbf{X}$ is a strong Markov process, we have $X_t=0$ on the event $\{T<t\}$
            * *Explain*. The post-$T$ process $\{X_{T+s}:s\geq 0\}$ starts in state $0$, hence stay in state $0$ later on, due to strong Markov property of $\mathbf{X}$
        * Hence, if $\mathbf{X}$ were strong Markov, then

            $$P(X_t=0,T<t)=P(T<t)>0$$

            which is a contradiction
* *References*.
    * https://mathoverflow.net/questions/43833/a-markov-process-which-is-not-a-strong-markov-process
    * https://math.stackexchange.com/questions/1536881/a-markov-process-which-is-not-strong-markov-process-follow-up-2

**Corollary**. The conditional probability that we will return one more time, given we have returned $k-1$ times, is $\rho_{yy}$, hence

$$P_y(T_y^k<\infty) = \rho^k_{yy}$$

### Transient and recurrent states
**Theorem**. The probability $X_n$ will return to $y$ at least $m$ times is $\rho_{yy}^m$
* *Explain*. After the first return, the chain is at $y$

    $\to$ The probability of the $m$th return is $\rho_{yy}^m$

**Transient and recurrent states**.
* *Transient state*. When $\rho_{yy}<1$, the probability of returning $k$ times is $\rho^k_{yy}\to 0$ as $k\to\infty$

    $\to$ Eventually the Markov chain does not find its way back to $y$
* *Recurrent state*. When $\rho_{yy}=1$, the probability of returning $k$ times is $\rho_{yy}^k=1$

    $\to$ The chain returns to $y$ infinitely many times

**Lemma**. If $P_x(T_y\leq k)\geq \alpha > 0$ for all $x$ in the state space $S$, then

$$P_x(T_y>nk)\leq (1-\alpha)^n$$

**State communication**. $x$ communicates with $y$, i.e. $x\to y$, if the following holds

$$\rho_{xy}=P_x(T_y<\infty)>0$$

* *Usage*. Help to identify transient and recurrent states
* *Properties*. If $x\to y$ and $y\to z$ then $x\to z$

**Theorem**.
* If $\rho_{xy}>0$ but $\rho_{yx}<1$, then $x$ is transient
* If $x$ is recurrent and $\rho_{xy}>0$ then $\rho_{yx}=1$

**Closed sets, irreducible sets, and recurrence**.
* *Closed set*. A set $A$ is closed if it is impossible to get out, i.e. if $i\in A$ and $j\notin A$ then $p(i,j)=0$
* *Irreducible set*. A set $B$ is irreducible if whenever $i,j\in B$, $i\to j$ holds
* *Theorem*. If $C$ is a finite closed and irreducible set, then all states in $C$ are recurrent
    * *Proof*. $\TODO$

**State classification in finite-state Markov chain**.
* *Theorem*. If the state space $S$ is finite, then $S$ can be decomposed as

    $$S=T\cup R_1\cup\dots\cup R_k$$

    where $T$ is a set of transient states, and $R_i$, for $1\leq i\leq k$, are closed irreducible sets of recurrent states
    * *Proof*. $\TODO$
* *State classification procedure*.

## Stationary distributions
**Brief**. An irreducible finite state Markov chain converges to a stationary distribution, i.e. $p^n(x,y)\to\pi(y)$
* *Explain*. Since the state transition matrix of such a chain have an eigenvalue of $1$, and other eigenvalues with absolute values less than $1$

**Stationary distribution**. If $qp=q$, where $q$ is some probabilities and $p$ is the state transition matrix

$\to$ $q$ is a stationary distribution
* *Notation*. $\pi$ is used to denote solutions of the equation $\pi p=\pi$

**General two-state transition probability**. Consider a two-state Markov chain with state transition matrix

$$p=\begin{bmatrix}1-a&a\\ b&1-b\end{bmatrix}$$

then $\pi=(\frac{b}{a+b},\frac{a}{a+b})$ is the stationary distribution of the chain

**Theorem**. Consider a $k\times k$ irreducible transition matrix $p$

$\to$ There is a unique solution to $\pi p=\pi$ with $\sum_x \pi_x=1$, and $\pi_x>0$ for all $x$

## Limit behavior
**Brief**. We can restrict our attention to recurrent states when considering the limit behavior of a Markov chain

>**NOTE**. In view of the state classification theorem, we focus on chains consisting of a single irreducible class of recurrent states

### Periodicity of states
**Period of a state $x$**. The greatest common divisor of $I_x=\{n\geq 1:p^n(x,x)>0\}$

$\to$ The chain can return to $x$ only at multiple times of the period of $x$

**Lemma**. $I_x$ is closed under addition, i.e. $i,j\in I_x\implies i+j\in I_x$

$\to$ This lemma is useful for computing state periods

**Lemma**. If $x$ has period $1$, then there exists some $n_0$ so that $n\geq n_0\implies n\in I_x$

$\to$ $I_x$ contains all integers $n$ after some value $n_0$s
* *Proof*. $\TODO$

### Aperiodic Markov chains
**Brief**. While periodicity is a theoretical possibility, it rarely manifests itself in applications, except occasionally as an odd-even parity problem

$\to$ In most cases we will find, or design, our chain to be aperiodic,
i.e. all states have unit period

**Lemma**. If $p(x,x)>0$, then $x$ has unit period

**Lemma**. If $\rho_{xy}>0$ and $\rho_{yx}0$, then $x$ and $y$ have the same period

## Special examples

### Doubly stochastic chains

### Detailed balance condition

### Reversibility

### The metropolis-hastings algorithm

## Proofs of the main theorems

## Exit distributions

## Exit times

## Infinite state spaces

# Appendix
## Concepts
**Positive part of a real number**. $x^+ = \max\{x,0\}$

**Total (or linear) order**. A partial order, in which any two elements are comparable
* *Formal*. A total order is a binary relation $\leq$ on some set $X$, which satisfies the following for all $a,b,c\in X$
    * *Reflexive*. $a\leq a$
    * *Transitive*. $a\leq b\land b\leq c\implies a\leq c$
    * *Antisymmetric*. $a\leq b\land b\leq a\implies a=b$
    * *Strongly connectivity (total)*. $a\leq b$ or $b\leq a$
* *Analogy*. Total ordering in distributed system

**Filtration (in physics)**. A physical separation process, which separates solid matter and fluid from a mixture using a filter medium, which has a complex structure, through which only the fluid can pass

<div style="text-align:center">
    <img src="https://i.imgur.com/u48c8NM.png">
    <figcaption>Physical filtration</figcaption>
</div>

**Strong Markov property versus Markov property**.
* *Reference*. 
    * https://ece.iisc.ac.in/~parimal/2019/random/lecture-22.pdf
    * http://www.columbia.edu/~ks20/stochastic-I/stochastic-I-ST.pdf
    * https://www.statslab.cam.ac.uk/~james/Markov/s14.pdf


**Theorem**. Consider a diagonalizable matrix $A=PDP^T$ with eigenvalues $\lambda_1,\dots,\lambda_n$, where $|\lambda_i|\in(0,1)$, then

$$\lim_{m\to\infty} (I + A + \dots + A^m) = (I - A)^{-1}$$

* *Proof*. 
    * We have that

    $$I + A + \dots + A^m\to P \cdot \text{diag}(\frac{1}{1 - \lambda_1}, \dots,\frac{1}{1 - \lambda_n}) \cdot P^T$$

    * Consider $I-A = P(I - D)P^T$, we have that

        $$(I - A)^{-1} \to P \cdot \text{diag}(\frac{1}{1 - \lambda_1}, \dots,\frac{1}{1 - \lambda_n}) \cdot P^T$$

    * Hence, $I + A + \dots + A^m \to (I - A)^{-1}$