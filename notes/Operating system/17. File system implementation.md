---
title: 17. File system implementation
tags: Operating system
---

<!-- TOC titleSize:1 tabSpaces:2 depthFrom:1 depthTo:6 withLinks:1 updateOnSave:1 orderedList:0 skip:0 title:1 charForUnorderedList:* -->
# Table of Contents
- [Table of Contents](#table-of-contents)
- [File-system implementation](#file-system-implementation)
  - [File system structure](#file-system-structure)
  - [File-system implementation](#file-system-implementation-1)
    - [Overview](#overview)
    - [Partitions and mounting](#partitions-and-mounting)
    - [Virtual file systems](#virtual-file-systems)
  - [Directory implementation](#directory-implementation)
    - [Linear list](#linear-list)
    - [Hash table](#hash-table)
  - [Allocation methods](#allocation-methods)
    - [Contiguous allocation](#contiguous-allocation)
    - [Linked allocation](#linked-allocation)
    - [Indexed allocation](#indexed-allocation)
    - [Performance](#performance)
  - [Free-space management](#free-space-management)
    - [Bit vector](#bit-vector)
    - [Linked list](#linked-list)
    - [Grouping](#grouping)
    - [Counting](#counting)
    - [Space maps](#space-maps)
  - [Efficiency and performance](#efficiency-and-performance)
    - [Efficiency](#efficiency)
    - [Performance](#performance-1)
      - [Caching blocks and pages](#caching-blocks-and-pages)
      - [Synchronization of I/O](#synchronization-of-io)
  - [Recovery](#recovery)
    - [Consistency checking](#consistency-checking)
    - [Log-structured file systems](#log-structured-file-systems)
    - [Other solutions](#other-solutions)
    - [Backup and restore](#backup-and-restore)
- [Appendix](#appendix)
  - [Concepts](#concepts)
<!-- /TOC -->

# File-system implementation
## File system structure
**Characteristics making disks a convenient medium for storing multiple files**.
* A disk can be rewritten in place, i.e. it is possible to read a block from  the disk, modify the block, and write it back into the same place
* A disk can access directly any block of information it contains
    * *Consequences*.
        * It is simple to access any file, either sequentially or randomly
        * Switching from one file to another requires only moving the read-write heads and waiting for the disk to rotate

**Block I/O**. To improve I/O efficiency, I/O transfers between memory and disk are performed in units of blocks, each of which has one or more sectors
* *Sector size*. Vary from 32 bytes to 4096 bytes, depending on the disk drive
    * *Usualy size*. 512 bytes

**File system and disks**. File systems provide efficient and convenient access to the disk by allowing data to be stored, located, and retrieved easily
* *Design problems with file systems*.
    * *Problem 1*. Define how the file system should look to the user

        $\to$ This involves defining a file and its attributes, the operations allowed on a file, and the directory structure for organizing files
    * *Problem 2*. Create algorithms and data structures to map the logical file system onto the physical secondary-storage devices

<div style="text-align:center">
    <img src="/media/wPYCzJ5.png">
    <figcaption>Layered file system</figcaption>
</div>

**Levels of file system**. Each level in the file system uses the features of lower levels to create new features for use by higher levels
* *I/O control*. The lowest level, which consists of device drivers and interrupt handlers to transfer information between the main memory and the disk system
    * *Device driver*. A translator
        * *Inputs*. High-level commands, e.g. `retrive block 123`
        * *Outputs*. Low-level, hardware-specific instructions used by the hardware controller, which interfaces the I/O device to the rest of the system
        * *Working mechanism*. Write specific bit patterns to special locations in the I/O controller's memory

            $\to$ This tells the controller which device location to act on, and what actions to take
* *Basic file system*. Responsible for the following functionalities
    * Need only to issue generic commands to the appropriate device driver to read and write physical blocks on the disk
        * *Identifying physical blocks*. Use block's numeric disk address, e.g. drive 1 - cylinder 73 - track 2 - sector 10
    * Manage the memory buffers and caches which hold various file-system, directory, and data blocks
        * *Buffer block*. Allocated before the transfer of a disk block can occur
            * *Full buffer*. When the buffer is full, the buffer manager must find more buffer memory or free up buffer space to allow a requested I/O to complete
        * *Caches*. Used to hold frequently used file-system metadata to improve performance

            $\to$ Managing their contents is critical for optimum system performance
* *File-organization module*. Know about files and their logical blocks, as well as physical blocks

    $\to$ The file-organization module thus can translate logical block addresses to physical block addresses for the basic file system to transfer
    * *Logical-physical block translation*.
        * *Logical block numbering*. Each file's logical blocks are numbered from $0$ or $1$ through $N$
        * *Block location*. Since the physical blocks containing the data usually do not match the logical numbers

            $\to$ A translation is needed to locate each block
    * *Free-space manager*. The file-organization module also includes the free-space manager, which
        * Track unallocated blocks
        * Provides unallocated blocks to the file-organization module when requested
* *Logic file system*. Manage metadata information and perform protection and security
    * *Metadata management*.
        * *Metadata*. Include all the file-system structure, except the actual data, i.e. contents of the files
        * *Functionality*. Manage the directory structure to provide the file-organization module with the information it needs, given a symbolic file name
        * *File-control blocks*. The logical file system maintains the file structure via file-control blocks
            * *File-control block (FCB)*. Contain information about the file, i.e. ownership, permissions, and location of the file contents
                * *Example*. `inode` in most UNIX file systems
    * *Protection and security*. The logical file system is responsible for protection and security

**Pros and cons of layered file systems**.
* *Pros*. Duplication of code is minimized
    * *Explain*. The I/O control and sometimes the basic file-system code can be used by multiple file systems

        $\to$ Each file system can then have its own logical file-system and file-organization modules
* *Cons*. Introduce more OS overhead, which may result in decreased performance
    * *Consequence*. The use of layering, including the decision about how many layers to use, and what each layer should do, is a major challenge in designing new systems

## File-system implementation
### Overview
**On-disk and in-memory structures**.
* *On-disk structures*. For data storage
* *In-memory structures*. For data access

**On-disk structures to implement a file system**. Contain information about how to boot an OS stored on the disk, the total number of blocks, the number and location of free blocks, the directory structure, and individual files
* *Brief description of on-disk structure components*.
    * *Boot-control block (one per volume)*. Contain information required by the system to boot an OS from that volume

        >**NOTE**. If the disk odes not contain an OS, this block can be empty

        * *Location*. Typically the first block of a volume
        * *Other names*. Boot block (in UFS), and partition boot sector (in NTFS)
    * *Volume-control block (one per volume)*. Contain volume, or partition, details
        * *Example*. The number of blocks in the partition, the size of the blocks, a free-block count and free-block pointers, and a free-FCB and FCB pointers
        * *Other names*. Superblock (in UFS), and master file table (in NTFS)
    * *Directory structure (one per file system)*. Used to organize the files
        * *Contents*.
            * *In UFS*. Include the file names, and associated inode numbers
            * *In NTFS*. Stored in the master file table
    * *Per-file FCB*. Contain details about the file
        * *Identifier number*. It has a unique identifier number to allow association with a directory entry

        >**NOTE**. In NTFS, this information is actually stored within the master file table, which uses a relational database structure, with a row per file

**In-memory structures to implement a file system**. Reflect and extend on-disk structures

$\to$ This is used for both file-system management and performance improvement via caching
* *Data operations*. The data are loaded at mount time, updated during file-system operations, and discarded at dismount
* *Types of structures included*.
    * *In-memory mount table*. Contain information about each mounted volume
    * *In-memory directory-structure cache*. Hold the directory information of recently accessed directories

        >**NOTE**. For directories, at which volumes are mounted, it can contain a pointer to the volume table

    * *System-wide open-file table*. Contain a copy of the FCB of each open file, as well as other information
    * *Per-process open-file table*. Contain a pointer to the appropriate entry in the system-wide open-file table, as well as other information
    * *I/O memory buffers*. Hold the file-system blocks when they are being read from disk, or written to disk

**File operations**.

<div style="text-align:center">
    <img src="/media/cfL7OWm.png">
    <figcaption>File open (a) and file read (b)</figcaption>
</div>

* *File creation procedure*.
    1. An application program calls the logical file system
    2. The logical file system knows the format of the directory structures

        $\to$ To create a new file, it allocates a new FCB
    3. The system reads the appropriate directory into memory, update it with the new file name and FCB, and writes it back to the disk
* *File opening procedure*.
    1. An application call `open()` with a file name as argument to the logical file system
    2. The `open()` system call searches the system-wide open-file table to see if the file is already in use by another process
        * If the file is currently used, a per-process open-file table entry is created, pointing to the existing system-wide open-file table

            $\to$ This can save substantial overhead
        * If the file is not already open, the directory structure is searched for the given file name

            $\to$ Once the file is found, the FCB is copied into a system-wide open-file table in memory

            >**NOTE**. The table not only stores the FCB, but also tracks the number of processes having the file open

            >**NOTE**. Parts of the directory structure are usually cached in memory to speed directory operations

    3. An entry is made in the per-process open-file table, with a pointer to the entry in the system-wide open-file table and some other fields
        * *Fields included in the per-process open-file table along with a FCB*.
            * A pointer to the current location in the file, i.e. for the next `read()` and `write()` operations
            * The access mode, in which the file is open
    3. The `open()` call returns a pointer to the appropriate entry in the per-process open-file table

        $\to$ All file operations are then performed via this pointer

        >**NOTE**. The file name may not be part of the open-file table, since the system has no use for it once the appropriate FCB is located on disk

        >**NOTE**. The file pointer may be cached to save time on subsequent opens of the same file

* *File closing procedure*.
    1. The per-process table entry is removed, and the system-wide entry's open count is decremented
    2. When all users that have opened the file close it, any updated metadata is copied back to the disk-based directory structure

        $\to$ The system-wide open-file table entry is then removed

**Caching of file-system structures**. Most systems keep all information about an open file, except for its actual data blocks, in memory

$\to$ Caching should be used to improve the performance of the file system

### Partitions and mounting
**Disk layout**. Can have many variations, depending on the OS
* *Option 1*. A disk can be sliced into multiple partitions
* *Option 2*. A volume can span multiple partitions on multiple disks

**Raw and cooked partition**.
* *Raw partition*. Contain no file system
    * *Usage*.
        * When no file system is appropriate
            * *Example*. UNIX swap space, or databases
        * Hold information required by disk RAID, i.e. redundant arrays of inexpensive disks, systems, e.g. bit maps indicating which blocks are mirrored, and which have chagned and need to be mirrored
        * Contain a miniature database holding RAID configuration information, e.g. which disks are members of each RAID set
* *Cooked partition*. Contain a file system

**Boot partition**. Boot information can be stored in a separate partition

>**NOTE**. This partition has its own format, since at boot time the system does not have the file-system code loaded
>$\to$ The system cannot interpret the file-system format

* *Partition format*. Boot information is usually a sequential series of blocks, loaded as an image into memory
* *Execution of boot image*.
    1. The execution starts at a predefined location, e.g. the first byte
    2. The boot loader in turn knows enough about the file-system structure to be able to find and load the kernel and start it executing

        $\to$ It can contain more than the instructions for how to boot a specific OS
* *Dual boot*. PCs and other systems can be dual-booted
    * *Problem*. How does the system know which one to boot?
    * *Idea*. A boot loader which understands multiple file systems and multiple OSes can occupy the boot space

        $\to$ Once loaded, it can boot one of the OSes available on the disk
    * *Disk partitioning*. The disk can have multiple partitions, each containing a different type of file system and a different OS

**Root partition**. Contain the OS kernel and sometimes other system files
* *Mounting root partition*. Root partition is mounted at boot time
* *Automatically mounted volumes*. Some volumes can be automatically mounted at boot, or manually mounted later, depending on the OS
* *Procedure to verify valid file system on a device*.
    1. The OS asks the device driver to read the device directory and verifying that the directory has the expected format
    2. If the format is invalid, the partition must have its consistency checked and possibly corrected, either with or without user intervention
    3. The OS notes in its in-memory mount table that a file system is mounted, along with the type of the file system

**Mounting in UNIX**. File systems can be mounted at any directory
* *Implementation*. Mounting is implemented by setting a flag in the in-memory copy of the inode for the directory

    $\to$ The flag indicates that the directory is a mount point
* *Mount table entry*. Contain a pointer to the superblock of the file system on the device

    $\to$ The OS can traverse its directory structure, switching seamlessly among file systems of varying types

### Virtual file systems
**Problem**. Modern OSes must concurrently support multiple types of file systems

$\to$ How does an OS allow multiple types of file systems to be integrated into a directory structure
* *Suboptimal solution*. Write directory and file routines for each type
* *Improved solution*. Most OSes, including UNIX, use OOP techniques to simplify, organize, and modularize the implementation
    * *Pros*. Allow very dissimilar file-system types to be implemented withint the same structure, e.g. network file systems (NFS)

        $\to$ Users can access files conatined within multiple file systems on the local disk, or even on a remote file systems

<div style="text-align:center">
    <img src="/media/lratjNk.png">
    <figcaption>Schematic view of a VFS</figcaption>
</div>

**Implementation details isolation from basic system-call**. Data structures and procedures are used to isolate the basic system-call functionality from the implementation details

$\to$ The file-system implementation consists of three major layers
* *File-system interface*. The first layer, which is based on `open()`, `read()`, `write()`, and `close()` calls and on file descriptors
* *Virtual file system (VFS)*. The second layer
    * *Functions*. The VFS layer serves two important functions
        * The VFS layer separates file-system-generic operations from their implementation by defining a clean VFS interface
            * *Explain*. Several implementations for the VFS interface may coexist on the same machine, allowing transparent access to different types of file systems mounted locally
        * The VFS layer provides a mechanism for uniquely representing a file throughout a network
            * *Explain*.
                * The VFS is based on a file-representation structure, called a `vnode`

                    $\to$ `vnode` contains a numerical designator for network-wide unique file, which is required for support of NFS
                * The kernel maintains one vnode structure for each active node, i.e. file or directory
            * *Consequences*. The VFS distinguishes local files from remote ones, and local files are further distinguished according to their file-system types
    * *Request handling*.
        * *Local requests*. The VFS activates the file-system-specific operations to handle local request according to their file-system types
        * *Remote requests*. The OS then calls the NFS protocol procedures for remote requests
* *Implementation of file-system type or remote-file-system protocol*. The third layer of the architecture

## Directory implementation
### Linear list
**Linear list**. The simplest method of implementing a directory
* *Idea*. Use a linear list of file names with pointers to the data blocks
* *Pros and cons*.
    * *Pros*. Simple to program
    * *Cons*. Time-consuming to execute

**Basic operations**.
* *Create a new file*.
    1. Search the directory to make sure that no existing file has the same name
    2. Add a new entry at the end of the directory
* *Delete a file*. 
    1. Search the directory for the named file
    2. Release the space allocated to it

**Directory entry reusing**.
* *Option 1*. Mark the entry as unused, by assigning it a special name, or with used-unused bit in each entry
* *Option 2*. Attach it to a list of free directory entries (just like free list in `GstAtomicQueue`)
* *Option 3*. Copy the last entry in the directory into the freed location and to decrease the length of the directory

**Crucial drawbacks**. Finding a file requires a linear search

$\to$ Directory information is used frequently, and users will notice if access to it is slow
* *Solution 1*. Use a software cache to store the most recently used directory information
    * *Cache hit*. Avoid the need to constantly reread the information from disk
* *Solution 2*. Use binary search tree to decrease the average search time
    * *Drawback*. The requirement that the list be kept sorted may complicate creating an deleting files
        * *Explain*. We may have to move substantial amounts of directory information to maintain a sorted directory
* *Solution 3*. Use sophisticated data structures like B-tree

### Hash table
**Hash table**. A list list stores the directory entries, but a hash data structure is also used
* *Idea*. The hash table takes a value computed from the file name, and returns a pointer to the file name in the linear list
* *Benefits*.
    * Greatly decrease the directory search time
    * Insertion and deletion are fairly straightforward

**Collisions**. When two file names hash to the same location

**Crucial drawbacks**. Fixed size of the hash table, and the dependence of the hash function on the size
* *Example*. If our hash table has 64 entries, but we want to hash value 65, we must enlarge the hash table size and update the hash function
* *Solution*. Use a chained-overflow hash table, where each hash entry is a linked list 

## Allocation methods
In this section, we learn about commonly used methods of allocating disk space

### Contiguous allocation

<div style="text-align:center">
    <img src="https://i.imgur.com/GTEfBak.png">
    <figcaption>Contiguous allocation of disk space</figcaption>
</div>

**Contiguous allocation**. Require that each file occupy a set of contiguous blocks on the disk
* *Disk address*. Define a linear ordering on the disk
* *Benefits*. If only one job is accessing the disk, accessing block $b+1$ after block $b$ normally requires no head movement
    * *Explain*. Head movement is required only when moving from the last sector of one cylinder to the first sector of the next cylinder

        $\to$ The head need only move from one track to the next
    * *Consequence*. The number of disk seeks required for accessing contiguously allocated files is minimal

        $\to$ This results in good performance

**File addressing**. Contiguous allocation of a file is defined by the disk address and the length (in block units) of the first block
* *Explain*. If the file is $n$ blocks long and starts at location $b$

    $\to$ It occupies blocks $b,\dots,b+n-1$
* *Directory entry for eac file*. Indicate the address of the starting block and the length of the area allocated for the file

**File accessing**. Both sequential and direct access can be supported by contiguous allocation
* *Sequential access*. The file system remembers the disk address of the last block referenced and, when necessary, reads the next block
* *Direct access to block $i$ of a file starting at block $b$*. We can immediately access block $b+i$

**Drawbacks**.
* Finding space of a new file is hard, and is determined by the system chosen to manage free space
    * *Dynamic storage-allocation*. The contiguous-allocation problem can be seen as a particular application of dynamic storage-allocation

        $\to$ First fit and best fit are the most common strategies used to select a free hole from the set of available holes

        >**NOTE**. Simulations have shown that both first fit and best fit are more efficient than worst fit in terms of time and storage utilization

        >**NOTE**. First fit is generally faster
* External fragmentation problem, i.e. as files are allocated and deleted, the free disk space is broken into little pieces

    $\to$ This is a problem when largest contiguous chunk is insufficient for a request
    * *Solution*. The following approach is very costly but compact
        * *Procedure*.
            1. Copy an entire file system onto another disk or tape
            2. The original one is freed completely, creating one large contiguous free space
            3. Copy the files back onto the original disk by allocating contiguous space from this large hole
        * *Drawback*. Some systems require this function to be done off-line, with the system unmounted

            $\to$ During this down time, normal system operation cannot be permitted
        * *Consequence*. This solution is avoided at all costs on production machines
    * *Alternative solution*. Most modern systems which need defragmentation can perform it online, during normal system operations
        
        $\to$ The performance penalty can be substantial
* Determining how much space is required for a file is hard
    * *Explain*. When the file is created, the total amount of space it will need must be found and allocated

        $\to$ How does the creator know the size of the file to be created
    * *Simple solution*. Copy an existing file

        >**NOTE**. The size of an output file may be difficult to estimate in general
    
    * *Too little space for file allocation*. The file cannot be extended, esptially with best-fit allocation strategy, where the space on both sides of the file may be in use

        $\to$ We cannot make the file larger in place
        * *Possibility 1*. The user program can terminated, with an appropriate error message

            $\to$ The user must then allocate more space and run the program again
            * *Drawback*. Repeated runs may be costly
            * *Solution*. The user will normally overestimate the amount of space required, resulting in considerable wasted space
        * *Possibility 2*. Find a larger hold, copy the contents of the file to the new space, and release the previous space
* Even if the total amount of space required for a file is known in advance, preallocation may be inefficient
    * *Explain*. A file which will grow slowly over a long period (months or years) must be allocated enough space for its final size

        $\to$ This space may be unused for a long time
    * *Solution 1*. Some OSes use a modified contiguous-allocation scheme
        * *Idea*.
            1. A contiguous chunk of space is allocated intially
            2. If the allocated space proves not to be large enough, another chunk of contiguous space, i.e. an extent, is added
            3. The location of a file's blocks is then recorded as a location and a block count, plus a link to the first block of the next extent
    * *Solution 2*. The owner of the file can set the extent size
        * *Drawbacks*. 
            * Inefficiencies if the owner is incorrect
            * Internal fragmentation can still be a problem if the extents are too large
            * External fragmentation can be come a problem as extents of varying sizes are allocated and deallocated

### Linked allocation

<div style="text-align:center">
    <img src="https://i.imgur.com/74kDM73.png">
    <figcaption>Linked allocation of disk space</figcaption>
</div>

**Linked allocation**.
* *Motivation*. Linked allocation solves all problems of contiguous allocation
* *Idea*. 
    * *File organization*. Each file is a linked list of disk blocks, where the disk blocks may be scattered anywhere on the disk
    * *Directory organization*. The directory contains a pointer to the first and last blocks of the file
* *Pros*.
    * There is no external fragmentation
    * Any free block on the free-space list can be used to satisfy a request
    * The size of a file need not be declared when that file is created
    * A file can continue to grow as long as free blocks are available
    * It is never necessary to compact disk space
* *Cons*.
    * Linked allocation can be used effectively only for sequential-access files

        $\to$ It is inefficient to support a direct-access capability for a linked-allocation files
    * The space required for the pointers, e.g. if a pointer requires 4 bytes out of a 512-byte block

        $\to$ 0.78 percent of the disk is being used for pointers, rather than for information
        * *Solution*. Collect blocks into multiples, called clusters, and allocate clusters rather than blocks
            
            $\to$ This allows the logical-to-physical block mapping to remain simple but improves disk throughput, since fewer disk-head seeks are required
            * *Cost*. Increase internal fragmentation, since more space is wasted when a clutser is partially full, than when a block is partially full
            * *Usage*. Used to improve the disk-access time for many other algorithms, so that they are used in most file systems
    * The files are linked together by pointers scattered all over the disk

        $\to$ What if a pointer were lost or damaged
        * *Consequence*. A bug in the OS software or a disk hardware failure may result in picking up the wrong pointer

            $\to$ This results in linking into the free-space list or into another file
        * *Solution 1*. Use doubly linked lists
        * *Solution 2*. Store the file name and relative block number in each block

**Basic operations**.
* *Create a new file*. Create a new entry in the directory
    * The pointer to the first disk block of the file is initialized to `nil`, i.e. end-of-list pointer value, to signify an empty file
    * The size field is set to 0
* *Write to a file*. 
    1. The free-space-management system to find a free block
    2. The new block is written to and is linked to the end of the file
* *Read a file*. Read blocks by following the  pointer from block to b lock

**File-allocation table (FAT)**. An important variation on linked allocation

<div style="text-align:center">
    <img src="https://i.imgur.com/I2SeZ9V.png">
    <figcaption>File-allocation table</figcaption>
</div>

* *Idea*. A section of disk at the beginning of each volume is set aside to contain a table
    * *Table structure*. Has one entry for each disk block, and is indexed by block number
        * *Entry structure*. Each entry contain the block number of the next block in the file

            $\to$ The chain continues until it reaches the last block
        * *Unused blocks*. Indicated by a table value of 0
    * *Functionality*. Much similar to a linked list
        * *Directory structure*. Has one entry containinig the block number of the first block of each file
* *Block allocation to a file*. 
    1. Find the fist 0-valued table entry 
    2. Replace the previous EOF value with the address of the new block
    3. The 0 is then replaced with the EOF value
* *Pros*. Random-access time is improved
    * *Explain*. The disk head can find the location of any block by reading the information in the FAT
* *Cons*. 
    * Significant number of disk head seeks, unless the FAT is cached
        * *Explain*. The disk head must move to the start of the volume to read the FAT, then find the location of the block in question, then move to the location of the block itself

            $\to$ In worst case, both moves occur for each of the blocks

### Indexed allocation

<div style="text-align:center">
    <img src="https://i.imgur.com/ZmClDcq.png">
    <figcaption>Indexed allocation of disk space</figcaption>
</div>

**Index allocation**.
* *Motivation*. Linked allocation cannot support efficient direct access without FAT
    * *Explain*. Pointers to the blocks are scattered with the blocks themselves all over the disk, and must be retrieved in order

        $\to$ Indexed allocation solves this problem
* *Idea*. Bring all pointers together into one location, i.e. the index block
    * *Index block of each file*. Each file has its own index block, which is an array of disk-block addresses
        * *Index block entry*. The $i$th entry in the block points to the $i$th block of the file
    * *Directory structure*. Contain the address of the index block
* *Block access*. To find and read the $i$th block

    $\to$ We use the pointer in the $i$th index-block entry
* *Creating a file*. When the file is created, all pointers in the index block are set to `nil`
* *Writing a file*. When the $i$th block is first written, a block is obtained from the free-space manager, and its address is put in the $i$th index-block entry

**Direct access**. Indexed allocation supports direct access without suffering from external fragmentation
* *Explain*. Any free block on the disk can satisfy a request for more space

**Wasted disk space**. Indexed allocation suffers from wasted space
* *Explain*. An entire index block must be allocated, even if only one or two pointers will be non-nil
    
    $\to$ The pointer overhead of the index block is generally greater than the pointer overhead of linked allocation
* *Index block size*. Every file must have an index block, thus the index block should be as small as possible
    * *Too small index block*. Not able to hold enough pointers for a large file

        $\to$ A mechanism will be required to deal with this issue
* *Mechanisms for large files*.
    * *Linked scheme*. 
        * *Motivation*. An index block is normally one disk block

            $\to$ It can be read and written directly by itself
        * *Idea*. To allow large files, we can link together several index blocks
    * *Multilevel index*. Use a first-level index block to point to a set of second-level index blocks, which in turn point to the file blocks
    * *Combined scheme*. Used in the UFS (Universal Flash Storage)
        * *Idea*. 
            * *Direct block*. The first $N$ pointers of the index block in the file's inode point to direct blocks
                * *Direct block*. Contain addresses of blocks which contain data of the file
                * *Consequence*. Data for small files (no more than $N$ blocks) do not need a separate index block
            * *Indirect blocks*. The next $M$ pointers point to indirect blocks
                * *Single indirect blocks*. The first pointer of these points to a single indirect block
                    * *Single indirect blocks*. An index block containing addresses of blocks which do contain data
                * *Double indirect blocks*. The second pointer of these contains the address of a block containing the addresses of blocks containing pointers to the actual data blocks
                * *Next pointers*. Follow the same manner as the first two blocks, e.g. triple indirect blocks, quadra indirect blocks, etc.

**Use-case**. UNIX file system

<div style="text-align:center">
    <img src="https://i.imgur.com/FyXo9Mu.png">
    <figcaption>The UNIX inode</figcaption>
</div>

### Performance
**Important criterias for benchmarking disk allocation methods**. Storage efficiency and data-block access times

**Deciding disk allocation method**. Determine how the systems will be used
* *Explain*. A system with mostly sequential access should not use the same method as a system with mostly random access

## Free-space management
**Problem**. Since disk space is limited, we need to reuse the space from deleted files for new files, if possible

**Free-space list**. Keep track of free disk space
* *Idea*. Record all free disk blocks, i.e. those not allocated to some file or directory
* *File creation*. 
    1. Search the free-space list for the required amount of space
    2. Allocate the space to the new file
    3. Remove the allocated space from the free-space list

>**NOTE**. The free-space list, despite its name, might not be implemented as a list

### Bit vector
**Bit vector**. Frequently, the free-space list is implemented as a bit map or bit vector
* *Idea*. Each block is represented by 1 bit, which is 1 if the block is free, and 0 otherwise
* *Pros*. Relative simple and efficient in finding the first free block for $n$ consecutive free blocks on the disk
    * *Explain*. Many computers supply bit-manipulation instructions for this purpose
* *Finding the first free block*. 
    1. Sequentially check each word in the bit map to see whether that value is not 0
    2. The first non-zero word is scanned for the first 1 bit, which is the location of the first free block
* *Calculation of block number*. $\text{n_bits_per_word} \times \text{n_zero_words} + \text{offset_of_first_1_bit}$

    $\to$ Hardware features drives software functionality

**Effiency fo bit vectors**. Bit vectors are inefficient unless the entire vector is kept in main memory, and is written back to disk occasionally for recovery needs

$\to$ Keeping it in main memory is possible for smaller disks but not necessarily for larger ones

### Linked list
**Linked list**. Link together all the free disk blocks, keeping a pointer to the first free block in a special location on the disk, and caching it in memory

$\to$ This is similar to `free_list` of `GstAtomicQueue`
* *Block structure*. Contain a pointer to the next free disk block
* *Pros*. Not efficient to traverse the list
    * *Explain*. We must read each block, which requires substantial I/O time

        >**NOTE**. Traversing the free list is not a frequent action

### Grouping

<div style="text-align:center">
    <img src="https://i.imgur.com/SxswoVc.png">
    <figcaption>Linked free-space list on disk</figcaption>
</div>

**Grouping**. A modification of the free-list approach stores the addresses of $n$ free blocks in the first free block
* The first $n-1$ of these blocks are actually free
* The last block contains the addresses of another $n$ free blocks

**Consequence**. The addresses of a large number of free blocks can be found quickly

### Counting
**Key observation**. Several contiguous blocks may be allocated or freed simultaneously, particularly when the space is allocated with the contiguous-allocation or through clustering

$\to$ We can keep the address of the first free block, and the number $n$ of free contiguous blocks following the first one

**Free-space list entry structure**. Consist of a disk address and a count

>**NOTE**. This is similar to the extent method of allocating blocks

>**NOTE**. These entries can be stored in a B-tree, rather than a linked list, for efficient lookup, insertion, and deletion

### Space maps
**Problem**. How to encompass huge numbers of files, directories, and even file systems
* *Explain*. The resulting data structures may have large and inefficient if they had not been designed and implemented properly
* *Meta I/O*. Can have large performance impact in case of large number of files and directories
    * *Example*. If the free-space list is implemented as a bit map

        $\to$ Freeing 1GB of data on 1TB disk could cause a thousands of blocks of bit maps to be updated, since the data blocks could be scattered over the entire disk

**Idea of ZFS**. Use a combination in the free-space management algorithm to control the size of data structures and minimize the I/O required to manage those structures
* *Idea*. Create metaslabs to divide the space on the device into chunks of manageable size
    * *Disk volume*. A given volume may contain hundreds of metaslabs
    * *Metaslab*. Each metaslab has an associated space map
        * *Space map*. A log of all block activity (allocating and freeing), in time order, in counting format
    * *Free-list implementation*. Use the counting algorithm to store information about free blocks
        * *Count structures location*. Rather than writing count structures to disk, ZFS uses log-structured file-system techniques to record them, i.e. the space map
* *Allocating and freeing space from a metaslab*. 
    1. Load the associated space map into memory in a balanced-tree structure for very efficient operation, indexed by offset
    2. Replay the log into that structure
    3. The in-memory space map is then an accurate representation of the allocated and free space in the metaslab
       * *Space map compression*. ZFS also condenses the map as much as possible by combining contiguous free blocks into a single entry
    4. The free-space list is updated on disk, as part of the transaction-oriented operations of ZFS

**Free-list representation**. The log, plus the balanced tree

## Efficiency and performance
### Efficiency
**Efficient use of disk space**. Depend heavily on the disk allocation and directory algorithms in use
* *UNIX file system*. UNIX inodes are preallocated on a volume, i.e. even an empty disk has a percentage of its space lost to inodes

    $\to$ However, by preallocatingthe inodes and spreading them across the volume, we improve the file system's performance, i.e. from UNIX allocation and free-space algorithms
* *BSD UNIX*. Consider the clustering scheme, which aids in file-seek and file-transfer performance at the cost of internal fragmentation

    $\to$ To reduce this fragmentation, BSD UNIX varis the cluster size as a file grows
    * *Consequence*. Large clusters are used where they can be filled, and small clusters are used for small files and the laste cluster of a file
* *Types of data kept in a file's directory (inod) entry*. Require consideration also
    * *Last write date and last access date*. Keeping in information results in the fact that whenever a file is read, a field in the directory structure must be written to

        $\to$ The block must be read into memory, a section changed, and the block written back out tot disk, i.e. since operations on disks occur only in block, or cluster, chunks
        * *Consequence*. Keeping this information is inefficient for frequently accessed files, thus we must weigh its benefit against its performance cost when designing a file system
    * *Conclusion*. Every data item associated with a file needs to be considered for its effect on efficiency and performance

**Choosing a pointer size or any fixed allocation size within an OS**.  One of the difficulties is planning for the effects of changing technology
* *Change of technology*. Consider that IBM PC XT had a 0-MB hard drive, and an MS-DOS file system that could support only 32 MB, i.e. each FAT entry was 12 bits, pointing to an 8-KB cluster

    $\to$ As disk capacities increased, larger disks had to be split into 32-MB partitions, since the file system could not track blocks beyond 32 MB
    * *Consequence*. As hard disks with capacities over 100 MB became common, the disk data structures and algorithms in MS-DOS had to be modified to allow larger file systems

### Performance
**Performance improvement**. Even after the basic file-system algorithms have been selected, we can still improve performance in several ways
* *Example*. Most disk controllers include local memory to form an on-board cache, which is large enough to store entire tracks at a time, i.e.
    1. Once a seek is performed, the track is read into the disk cache, starting at the sector under the disk head, reducing latency time
    2. The disk controller then transfers any sector requests to the OS
    3. Once blocks make it from the disk controller into main memory, the OS may cache the blocks there

#### Caching blocks and pages
**Buffer cache and page cache**. 
* *Buffer cache*. Some systems maintain a separate section of main memory for a buffer cache, where blocks are kept under the assumption that they will be used again shortly

    $\to$ The buffer cache caches disk blocks to optimize block I/O
* *Page cache*. Some systems cache file data using a page cache, which uses virtual-memory techniques to cache file data as pages, rather than as file-system-oriented blocks

    $\to$ The paga cache caches pages of files to optimize file I/O
    * *Explain*. Caching file data using virtual addresses is far more efficient than caching via physical disk blocks, as accesses interface with virtual memory rather than the file system
    * *Location*. The operating system keeps a page cache in otherwise unused portions of the main memory (RAM)
        
        $\to$ This results in quicker access to the contents of cached pages and overall performance improvements
    * *Implementation*. A page cache is implemented in kernels with the paging memory management, and is mostly transparent to applications
* *Buffer cache and page cache in Linux*. Prior to Linux kernel version 2.4, the two caches were distinct, i.e. files were in the page cache, disk blocks were in the buffer cache
    
    $\to$ Given that most files are represented by a filesystem on a disk, data was represented twice, once in each of the caches
    * *Pros and cons*. Simple to implement, but inelegant and inefficient
    * *Consequence*. The contents of the two caches were unified
        * *Page cache*. The VM subsystem now drives I/O and it does so out of the page cache
            
            $\to$ If cached data has both a file and a block representation, i.e. as most data does, the buffer cache will simply point into the page cache
            * *Consequence*. Only one instance of the data is cached in memory
            
            >**NOTE**. The page cache is what we picture when you think of a disk cache: It caches file data from a disk to make subsequent I/O faster

        * *Buffer cache*. The buffer cache remains, however, as the kernel still needs to perform block I/O in terms of blocks, not pages
            * *Buffer cache and page cache*. 
                * As most blocks represent file data, most of the buffer cache is represented by the page cache
                * A small amount of block data isn't file backed—metadata and raw block I/O for example—and thus is solely represented by the buffer cache
* *Unified virtual memory*. Several systems, e.g. Solaris, Linux, and Windows, use page caching to cache both process pages and file data, i.e. unified virtual memory

**Unified buffer cache**. Provided by some versions of UNIX and Linux

<div style="text-align:center">
    <img src="https://i.imgur.com/q2XRU5Y.png">
    <figcaption>I/O using a unified buffer cache</figcaption>
</div>

* *Opening and accessing file approaches*. Use memory mapping, or use the standard system calls `read()` and `write()`
* *File access without unified buffer cache*.

    <div style="text-align:center">
        <img src="https://i.imgur.com/xVdnT0m.png">
        <figcaption>I/O without a unified buffer cache</figcaption>
    </div>

    * *`read()` and `write()` system calls*. Go through the buffer cache
    * *Memory-mapping call*. Require using two caches, i.e. the page cache and the buffer cache
        * *Explain*. 
            * A memory mapping proceeds by reading in disk blocks from the file system and storing them in the buffer cache
            * Since the virtual memory system does not interface with the buffer cache, the contents of the file in the buffer cache must be copied into the page cache
        * *Consequence*. 
            * Double caching problem is introduced and requires caching file-system data twice
            * Inconsistencies between the two caches may result in corrupt files
* *File access with unified buffer cache*. Both memory mapping and `read()`-`write()` system calls use the same page cache

    $\to$ We can avoid double caching and allow the virtual-memory system to manage file-system data

**Cache replacement algorithm**. 
* *LRU*. Regardless of whether we are caching disk blocks, or pages, or both, LRU seems a reasonable general-purpose algorithm for block or page replacement
    * *Difficulty in Solaris*. The evolution of the Solaris page-caching algorithms reveals the difficulty in choosing an algorithm
        * *Explain*. Solaris allows processes and the page cache to share unused memory, i.e. there is no distinction between allocating pages to a process and allocating pages to the page cache

            $\to$ A system performing many I/O operations used most of the available memory for caching pages
            * *Consequence*. Due to the high I/O rates, the page scanner reclaimed pages from processes, rather than from the page cache, when free memory ran low
        * *Solution*. 
            * Solaris 2.6 and Solaris 7 optionally implemented priority paging, i.e. the page scanner gives priority to process pages over the page cache
            * Solaris 8 applied a fixed limit to process pages and the file-system page cache, preventing either from forcing the other out of memory
            * Solaris 9 and 10 changed the algorithms to maximize memory use and minimize thrashing
* *Free-behind and read-ahead*. Some systems optimize their page cache by using different replacement algorithms, depending on the access type of the file
    * *Idea*. A file being read or written sequentially should not have its pages replaced in LRU order
        * *Explain*. The most recently used page will be used last, or perhaps nevery again
        * *Consequence*. Sequential access can be optimized by techniques like free-behind and read-ahead
    * *Free-behind*. Remove a page from the buffer, as soon as the next page is requested, i.e. the previous pages are not likely to be used again and waste buffer space
    * *Read-ahead*. A requested page and several subsequent pages are read and cached, i.e. these pages are likely to be requested after the current page is processed
        * *Track cache and read-ahead*. One may think that a track cache on the controller would eliminate the need for read-ahead on a multiprogrammed system

            $\to$ But due to the high latency and overhead involved in making many small transfers from the track cache to main memory, performing a read-ahead remains beneficial

#### Synchronization of I/O
**Problem**. Whether writes to the file system occur synchronously or asynchronously
* *Synchronous writes*. Occur in the order, in which the disk subsystem receives them, and the writes are not buffered

    $\to$ The calling routine must wait for the data to reach the disk drive, before it can proceed
* *Asynchronous writes*. The data are stored in the cache, and control returns to the caller

>**NOTE**. Asynchronous writes are done most of the time, however, metadat writes, among others, can be synchronous

**Page cache, file system, and disk drives**. Have some interesting interactions
* *Writing data to a disk file*. When data are written to a disk file, the pages are buffered in the cache, and the disk driver sorts its output queue according to disk address

    $\to$ This allows the disk driver to minimize disk-head seeks and to write data at times optimized for disk rotation
    * *Consequence*. Unless synchronous writes are required, a process writing to disk simply writes into the cache, and the system asynchronously writes the data to disk when convenient

        $\to$ The user process sees very fast writes
* *Reading data from a disk file*. The block I/O system does some read-ahead
* *Read and write speeds*. Writes are much more nearly asynchronous than reads, hence output to disk through the file system is often faster than output for large transfers

## Recovery
**Problem**. Files and directories are kept both in main memory and disk, and care must be taken to ensure that a system failure does not result in loss of data, or in data in consistency
* *Consequences of a system crash*. Cause inconsistencies among on-disk file-system data structures, e.g. directory structures, free-block pointers, and free-FCB pointers
    * *Explain*. 
        * Many file systems apply changes to these structures in place
        * A typical operation, e.g. creating file, can involve many structural changes within the file system on the disk
            * *Examples*. Directory structure are modified, FCBs are allocated, data blocks are allocated, and the free counts for all of these blocks are decreased
        * Changes in the file system can be interrupted by a crash, and inconsistencies among the structures can result
* *Consequence of implementation bugs*. Bugs in file-system implementation, disk controllers, and even user applications can corrupt a file system

### Consistency checking
**Corruption detection in file system**. 
* *Option 1*. A scan of all the metadata on each file system can confirm or deny the consistency of the system
    * *Drawback*. This can take minutes or hours and should occur everytime the system boots
* *Option 2*. The file system records its state within the file-system metadata
    * *Metadata change*. At the start of any metadata change, a status bit is set to indicate that the metadata is in flux
        * If all updates to the metadata complete sucessfully, the file system can clear the bit
        * If the status bit remains set, a consistency checker is run

**Consistency checker**. A systems program, e.g. `fsck` in UNIX or `chkdsk` in Windows, compares the data in the directory structure, with the data blocks on disk, and tries to fix any inconsistencies it finds
* *Effeciency of consistency checker*. The allocation and free-space-management algorithms dictate what types of problems the checker can find and how successful it will be in fixing them
    * *Link allocation*. If link allocation is used and there is a link from any block to its next block, then the entire file can be reconstructed from the data blocks, and the directory structure can be recreated
    * *Indexed allocation*. The loss of a directory entry on an indexed allocation system can be disastrous, since the data blocks have no knowledge of one another
        * *Solution*. UNIX caches directory entries for reads, but any write resulting in changes in space allocation or other metadata is done synchronously, before the corresponding data blocks are written

### Log-structured file systems
**Motivation**. The database log-based recovery algorithms have been applied successfully to the problem of consistency checking, resulting in log-based transaction-oriented, or journaling, file systems
* *Problem with consistency-checking approach*. In this approach, we allow structures to break and repair them on recovery, however, there are several problems with this approach
    * Inconsistency may be irreparable, i.e. consistency check may not be able to recover the structures, resulting in loss of files and even entire directories
    * Consistency checking can require human intervention to resolve conflicts, hence it is inconvenient if no human is available

        $\to$ The system can remain unavailable until a human tells it how to proceed
    * Consistency checking takes system and clock time, e.g. to check terabytes of data, hours of clock time may be required
* *Solution*. Apply log-based recovery techniques to file-system metadata updates
    * *Application*. This approach is becoming common on many OSes

**Log-based recovery**.
* *Idea*. All metadata changes are written sequentially to a log
    * *Transaction*. Each set of operations for performing a specific task is a transaction
* *Transaction logging*.
    1. Once the changes are written to the log, they are considered to be committed
        
        $\to$ The system can return to user process, allowing it to continue execution
    2. Meanwhile, the log entries are replayed across the actual file system structures
    3. As the changes are made, a pointer is updated to indicate which actions have completed and which are still incomplete
    4. When an entire committed transaction is completed, it is removed from the log file
* *Log file implementation*. A circular buffer, which writes to the end of its space and then continues at the beginning, overwriting older values as it goes
    * *Log location*. May be in a separate section o the file sytems, o even on a separate disk spindle

        >**NOTE**. It is more efficient, but more complex, to have it under separate read and write heads, decreasing head contention and seek times
* *Inconsistency detection*. If the system crashes, the log file will contain zero or more transactions

    $\to$ Any transactions it contains were not completed to the file system, even though they were committed by the OS, hence they must now be complete, so that the file-system structures remain consistent
* *Problem*. When a transaction was aborted, i.e. not committed before the system crashed

    $\to$ Any changes from such a transaction, which were applied to the file system, must be undone, preserving the consistency of the file system
* *Side benefit of using logging on disk metadata updates*. The updates proceed much faster than when they are applied directly to the on-disk data structures
    * *Explain*. Due to the performance advantage of sequential I/O over random I/O, i.e.
        * The costly synchronous random metadata writes are turned into much less costly synchronous sequential writes on the log-structured file system's logging area

            $\to$ Those changes in turn are replayed asynchronously via random writes to the appropriate structures
    * *Consequence*. A significant gain in performance of metadata-oriented operations, e.g. file creation and deletion

### Other solutions
**Network Appliance's WAFL approach**.
* *Idea*. Never overwrite blocks with new data, rather, 
    1. A transaction writes all data and metadata changes to new blocks

        $\to$ When the transaction is complete, the metadata structures, which pointed to the old versions of the blocks, are updated to point to the new blocks
    2. The file system can then remove the old pointers and the old blocks and make them available for reuse
* *Snapshot*. If the old pointers and blocks are kept, a snapshot is created, which is a view of the file system before the last update took place
* *Consequence*. No consistency checking is required if the pointer update is done automatically
* *Consistency checker*. WAFL has a consistency checker, hence some failure scenarios can still cause metadata corruption

**Sun's ZFS approach**. More innovative to handle disk consistency
* *Idea*. Never overwrite blocks, like WAFL, but ZFS goes further and provides check-summing of all metadata and data blocks

    $\to$ This solution, when combined with RAID, assures that data are always correct
* *Consequence*. ZFS has no consistency checker

### Backup and restore
**Backup and restore**. Magnetic disks sometimes fail, and care must be taken to ensure that the data lost in such a failure are not lost forever
* *Back up data*. System programs can be used to back up data from disk to another storage device, e.g. a floppy disk, magnetic tape, optical disk, or other hard disk
* *Restore data*. Recovery from the loss of an individual file, or of an entire disk

**Minimizing the copying required**. We can use information from each file's directory entry
* *Example*. If the backup program knows when the last backup of a file was done, and the file's last write date in the directory indicates that the file has not changed since that date

    $\to$ The file does not need to be copied again
* *Typical backup schedule*. The new cycle can have its backup written over the previous set, or onto a new set of backup media, i.e.
    * *Day 1*. Copy to a backup medium all files from the disk, i.e. full backup
    * *Day 2*. Copy to another medium all files changed since day 1, i.e. incremental backup
    * *Day 3*. Copy to another medium all files changed since day 2, and so on
* *Data restoration*. We can restore an entire disk by starting restores with the full backup and continuing through each of the incremental backups

    $\to$ The larger the value of $N$, the greater the number of media that must be read for a complete restore
* *Pros*. We can restore any file accidentally deleted during the cycle, by retrieving the deleted file from the backup of the previous day
    * *Cycle length*. A compromise between the amount of backup medium required, and the number of days back from which a restore can be done

# Appendix
## Concepts
**Implementation of directories**.
* *UNIX*. Treat a directory in exactly the same way as a file, i.e. one with a `type` field indicating that it is a directory
* *Windows*. Implement separate system calls for files and directories

    $\to$ Directories are treated as entities separate from files

**Naming for file pointer in popular OSes**.
* *UNIX*. File descriptor
* *Windows*. File handle

**File system as an interface to other system aspects**. Some system complicate the file system scheme further by using the file system as an interface to other system aspects, e.g. networking
* *In UFS*. The system-wide open-file table holds
    * The inodes and other information for files and directories, and
    * Similar information for network conections and devices
* *Consequences*. One mechanism can be used for multiple purposes

**Superblock**. A unique data strucutre in a filesystem, which holds metadata about the file system, e.g. which inode is the top-level directory and the type of the file system use

**Buffer cache in file system**. Kernel must copy disk blocks to main memory to access their contents and write them back if modified
* *Buffer cache*. Memory used to cache kernel resources, e.g. disk blocks and nawme translation, i.e. mapping from paths to inodes