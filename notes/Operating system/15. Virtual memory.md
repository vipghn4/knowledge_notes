---
title: 15. Virtual memory
tags: Operating system
---

<!-- TOC titleSize:1 tabSpaces:2 depthFrom:1 depthTo:6 withLinks:1 updateOnSave:1 orderedList:0 skip:0 title:1 charForUnorderedList:* -->
# Table of Contents
- [Table of Contents](#table-of-contents)
- [Virtual memory](#virtual-memory)
  - [Background](#background)
  - [Demand paging](#demand-paging)
    - [Basic concepts](#basic-concepts)
    - [Performance of demand paging](#performance-of-demand-paging)
  - [Copy-on-write](#copy-on-write)
  - [Page replacement](#page-replacement)
    - [Basic page replacement](#basic-page-replacement)
  - [Allocation of frames](#allocation-of-frames)
    - [Minimum number of frames](#minimum-number-of-frames)
    - [Allocation algorithms](#allocation-algorithms)
    - [Global versus allocation](#global-versus-allocation)
    - [Non-uniform memory access](#non-uniform-memory-access)
  - [Thrashing](#thrashing)
    - [Working-set model](#working-set-model)
    - [Page-fault frequency](#page-fault-frequency)
  - [Memory-mapped files](#memory-mapped-files)
    - [Basic mechanism](#basic-mechanism)
    - [Memory-mapped I/O](#memory-mapped-io)
  - [Allocating kernel memory](#allocating-kernel-memory)
    - [Buddy system](#buddy-system)
    - [Slab allocation](#slab-allocation)
  - [Other considerations](#other-considerations)
    - [Prepaging](#prepaging)
    - [I/O interlock](#io-interlock)
- [Appendix](#appendix)
  - [Concepts](#concepts)
<!-- /TOC -->

# Virtual memory
**Problems**. Memory-management strategies are used in computer systems to keep many processes in memory simultaneously to allow multiprogramming

$\to$ However, they tend to require an entire process be in memory before it can execute

**Virtual memory**. A technique which allows the execution of processes which are not completely in memory
* *Pros*.
    * Programs can be larger than physical memory
    * Main memory is abstracted into an extremely large, uniform array of storage, separating logical memory as viewed by the user from physical memory

        $\to$ Programmers are freed from the concerns of memory-storage limitations
    * Allow processes to share files easily and to implement shared memory
    * Provide an efficient mechanism for process creation
* *Cons*.
    * Hard to implement
    * May substantially decrease performance if it is used carelessly

## Background
<div style="text-align:center">
    <img src="/media/QCPiNV6.png">
    <figcaption>Virtual memory and main memory</figcaption>
</div>

**Partially locate the program in main memory**. Confer many benefits
* A program would no longer be constrained by the amount of physical memory available

    $\to$ Users would be able to write programs for an extremely large virtual address space, simplifying the programming task
* Each user program could take less physical memory

    $\to$ More programs could be run at the same time, with a corresponding increase in CPU utilization and throughput without increasing in response time or turnaround time
* Less I/O would be needed to load or swap user programs into memory

    $\to$ Each user program would run faster

**Virtual memory**. Involve the separation of logical memory as perceived by users from physical memory
* *Benefits*.
    * Allow an extremely large virtual memory to be provided for programmers when only a smaller physical memory is available
    * Make the task of programming much easier, since programmer no longer needs to worry about the amount of physical memory available

<div style="text-align:center">
    <img src="/media/aYp8J4X.png">
    <figcaption>Virtual address space</figcaption>
</div>

* *Virtual address space of a process*. Refer to logical (or virtual) view of how a process is stored in memory
    * *Typical view*. A process begins at a certain logical address, and exists in contiguous memory
    * *Organization of virtual address space*. The same as organization of physical memory

<div style="text-align:center">
    <img src="/media/gyOMgF0.png">
    <figcaption>Shared library using virtual memory</figcaption>
</div>

* *Virtual memory allows files and memory to be shared by two or more processes*. This is done via page sharing
    * *Benefits*.
        * System libraries can be shared by several processes through mapping of the shared object into a virtual address space
            * *Explain*. Although each process considers the shared libraries to be part of its virtual address space

                $\to$ The actual pages where the libraries reside in physical memory are shared by all the processes

            >**NOTE**. Typically, a library is mapped read-only into the space of each process linked with it

        * Enable processes to share memory, via the use of shared memory
        * Allow pages to be shared during process creation, thus speeding up process creation

## Demand paging
**Loading executable from disk into memory**.
* *Option 1*. Load the entire program in physical memory at program execution time
    * *Drawback*. We may not initially need the entire program in memory
* *Option 2 (demand paging)*. Load pages only as they are needed

    >**NOTE**. This is commonly used in virtual memory systems

    >**NOTE**. Demand-paging system is simlar to a paging system with swapping, where processes reside in secondary memory, e.g. a disk

**Lazy swapper**. Never swap a page into memory unless that page will be needed
* *Terminology*. Using "swap" is incorrect since we are now viewing a process as a sequence of pages, rather than a large continuous address space
    * *Solution*. We should use "pager" in connection with demand paging
    * *Explain*.
        * *Swapper*. Manipulate entire processes
        * *Pager*. Concern with individual pages of a processes

<div style="text-align:center">
    <img src="/media/HMZIZnM.png">
    <figcaption>Transfer of a paged memory to contiguous disk space</figcaption>
</div>

### Basic concepts
**Idea of swapping page**.
1. When a process is to be swapped in

    $\to$ The pager guesses which pages will be used before the process is swapped out again
2. The pager then brings only those pages into memory

    $\to$ It avoids reading into pages which will not be used anyway, decreasing the swap time and the amount of physical memory needed

>**NOTE**. We need some form of hardware support to distinguish between pages in memory and pages on disk

**Valid-invalid bit scheme**. A scheme to distinguish between pages in memory and pages on disk
* *Valid and invalid pages*.
   * *Valid page*. Pages is both legal and in memory
   * *Invalid page*. Pages not valid, or is valid but is currently on disk
* *Page-table entry*.

    <div style="text-align:center">
        <img src="/media/F3MpcfG.png">
        <figcaption>Page table when some pages are not in main memory</figcaption>
    </div>

    * When the page is brought into memory

        $\to$ Page-table entry is set as usual
    * When the page is not currently in memory

        $\to$ Page-table entry is marked invalid or contains the address of the page on disk

    >**NOTE**. Marking a page invalid will have no effect if the process never attempts to access that page
    >$\to$ If we guess right and only the loaded pages are actually needed, the process will run exactly as if we had brought in all pages

**Page fault**. Access to a page marked invalid causes a page fault
* *Handling page fault*.
    1. Check an internal table, which is usually kept with the process control block, for this process to determine whether the reference was valid or an invalid memory access
    2. If reference was invalid, we terminate the process. Otherwise, if we have not yet brought in that page, we now page it in
    3. We find a free frame, by taking one from the free-frame list
    4. We schedule a disk operation to read the desired page into the newly allocated frame
    5. When the disk read is complete, we modify the internal stable kept with the process and the page table, to indicate that the page is now in memory
    6. We restart the instruction that was interrupted by the trap

        $\to$ The process can now access the page as if it had always been in memory
* *Extreme case*. We can start executing a process with no pages in memory
* *Page faults and process execution*.
    * When the OS sets the instruction pointer to the first instruction of the process, which is on a non-memory-resident page

        $\to$ The process immediately faults for the page
    * After this page is brought into memory, the process continues to execute, faulting as necessary until every page it needs is in memory

        $\to$ At this point, it can execute without page faults

**Pure demand paging**. Never bring a page into memory until it is required

**Locality of reference**.
* *Problem*. Theoretically, some programs could access several new pages of memory with each instruction execution, e.g. one for instruction and many for data

    $\to$ This possibly causes multiple page faults per instruction
    * *Consequence*. The problem can result in unacceptable performance

>**NOTE**. Fortunately, analysis of running processes shows that this behavior is exceedingly unlikely
>* *Explain*. Programs tend to have locality of reference, which results in reasonable performance from demand paging

**Hardware support for demand paging**. The same as hardware for paging and swapping, i.e.
* *Page table*. Has the ability to mark an entry invalid through a valid-invalid bit, or a special value of protection bits
* *Secondary memory*. Usually a high-speed disk holding the pages that are not present in main memory
    * *Other names*. Swap device
    * *Swap space*. The section of disk used for demand paging

**Restarting instruction after a page fault**. A crucial requirement for demand paging
* *Explain*. We save the state, i.e. register, condition code, instruction counter, of the interrupted process when the page fault occurs

    $\to$ We must be able to restart the process in exactly the same place and state, except that the desired page is now in memory and is accessible

**Major difficulty with demand paging**. When one instruction may modify several different location
* *Explain*. If some of the required memory block causes a page fault

    $\to$ A page fault may occur after the instruction is partially done

>**NOTE**. Additionally, if the source and the destination memory blocks overlap, the source block may have been modified
>$\to$ We cannot simply start the instruction

* *Solutions*.
    * *Option 1*.
        1. The microcode computes and attempts to access both ends of each required memory blocks

            $\to$ If a page is going to occur, it will happen at this step, before anything is modified
        2. The instruction can then take place

            $\to$ Since all the relevant pages are in memory, no page fault can occur
    * *Option 2*. Use temporary registers to hold the values of overwritten locations

        $\to$ If there is page fault, all the old values are written back into memory before the trap occurs
        * *Explain*. This action restores memory to its state before the instruction was started

            $\to$ The instruction can be repeated

### Performance of demand paging
>**NOTE**. Demand paging can significantly affect the performance of a computer system

**Effective access time**.
* *No-page-fault case*. The effective access time is equal to the memory access time

    >**NOTE**. For most computer systems, the memory-access time, denoted $ma$, ranges from $10$ to $200$ $\mu s$

* *Page-fault case*. We must first read the relevant page from disk, then access the desired word
    * *Assumptions*.
        * $p\in[0,1]$ is the probability of a page fault
    * *Effective access time*. $\text{effective_access_time}=(1-p)\times ma + p \times \text{page_fault_time}$
    * *Page fault time*.
        * *Page fault instruction sequence*.
            1. Trap to the OS
            2. Save the user registers and process state
            3. Determine that the interrupt was a page fault
            4. Check that the page reference was legal and determine the location of the page on the disk
            5. Issue a read from the disk to a free frame:
                * Wait in a queue for this device until the read request is serviced
                * Wait for the device seek and / or latency time
                * Begin the transfer of the page to a free frame
            6. While waiting, allocate the CPU to some other user
            7. Receive an interrupt from the disk I/O subsystem
            8. Save the registered and process state for other user
            9. Determine that the interrupt was from the disk
            10. Correct the page table and other tables to show that the desired page is now in memory
            11. Wait for the CPU to be allocated to this process again
            12. Restore the user registers, process state, and new page table, then resume the interrupted instruction
        * *Major components of the page-fault service time*.
            * Service the page-fault interrupt (can be reduced with careful coding)
            * Read in the page
            * Restart the process (can be reduced with careful coding)
* *Consequence*. It is important to keep the page-fault rate low in a demand-paging system

    $\to$ Otherwise, the effective access time increases, slowing process execution dramatically

**Handling and using the swap space**.
* *Disk I/O to swap space speed*. Disk I/O to swap space is generally faster than that to the file system
    * *Explain*. The swap space is located in much larger blocks, and file lookups and indirect allocation methods are not used
    * *Consequence*. The system can gain better paging throughput by copying an entire file image into the swap space at process startup

        $\to$ The system then performs demand paging from the swap space

>**NOTE**. Some systems attempt to limit the amount of swap space used through demand paging of binary files

## Copy-on-write
**Idea**.
* *Quick start of a process with demand paging*. We can demand-paging in the page containing the first instruction
* *Improvement*. Process creation using `fork()` system call may initially bypass the need for demand paging by using a technique similar to page sharing

    $\to$ This technique provides for rapid process creation and minimizes the number of new pages, which must be allocated to the newly created process

**Copy-on-write**. A common technique used in several OSes, e.g. Windows XP, Linux, etc.
* *Recall `fork()` system call*. Create a child process which is a duplicate of its parent
    * *Traditional workflow*.
        1. Create a copy of the parent's address space for the child
        2. Duplicate the pages belonging to the parent
    * *Drawback of traditional workflow*. If many child processes invoke the `exec()` system call immediately after creation

        $\to$ The copying of the parent's address space may be unnecessary
* *Idea of copy-on-write*. Allow the parent and child processes initially to share the same pages
* *Copy-on-write pages*. The shared pages between the parent and child processes
    * *Explain*. A copy of the shared page is created if either process writes to a shared page

**Pool of free pages**. When it it determined that a page is going to be duplicated using copy-on-write

$\to$ It is important to note the location, from which the free page will be allocated
* *Pool of free pages*. Many OSes provide a pool of free pages for such requests
    * *Allocation time*. These pages are typically allocated when
        * The stack or heap for a process must expand, or
        * There are copy-on-write pages to be managed
* *Zero-fill-on-demand*. A technique used by Oses to allocate free pages
    * *Idea*. Zero-fill-on-demand pages have been zeroed-out before being allocated, thus erasing the previous contents

**Variations**.
* *Virtual memory fork (`vfork()`)*. The parent process is suspended, and the child process uses the address space of the parent

    $\to$ If the child process changes any pages of the parent's address space, the altered pages will be visible to the parent once it resumes
    * *Consequence*. `vfork()` must be used with caution to ensure that the child process does not modify the address space of the parent
    * *Usage*. When the child process calls `exec()` immediately after creation
    * *Benefits*. Extremely efficient method of process creation, and is sometimes used to implement UNIX command-line shell interfaces
        * *Explain*. There is no page copying

## Page replacement
**Revisiting page-fault rate**. In our earlier discussion of page-fault rate, we assumed that each page faults at most once, i.e. when it is first referenced
* *Page-fault with demand paging*. If a process of 10 pages actually uses only half of them

    $\to$ Demand paging saves the I/O necessary to load the 5 pages that are never used
    * *Consequence*. We can increase our degree of multiprogramming by running twice as many processes
* *Memory over-allocation*. If we increase our degree of multiprogramming, we are over-allocating memory
    * *Definition*. Situations where resources, e.g. processing power, memory, data management, bandwidth, or other specifications, are allocated at excessive levels
    * *Benefits* We have higher CPU utilization and throughput, with 10 frames to spare
    * *Drawbacks*. It is possible that each of these processes, for a particular data set, may suddenly try to use all of its pages, rather than just half of them

        $\to$ We will be out-of-memory

**I/O buffers**. System memory is not used only for holding program pages, but also buffers for I/O

$\to$ I/O buffers also consume a considerable amount of memory, increasing the strain on memory-placement algorithms
* *Problem*. Decide how much memory to allocate to I/O, and how much to program pages, is a significant challenge
* *Solutions*.
    * *Option 1*. Allocate a fixed percentage of memory for I/O buffers
    * *Option 2*. Allow both user processes and the I/O subsystem to compete for all system memory
* *Over-allocation's behavior*.
    1. While a user process is executing, a page fault occurs

        $\to$ The OS determines where the desired page is residing on the disk
    2. The OS then finds that there are no free frames on the free-frame list
    3. The OS terminate the user process (not the best choice), or swap out a process, freeing all its frames and reducing the level of multiprogramming

### Basic page replacement
**Page replacement**.
* *Procedure*. If no free is free
    1. Find one that is not currently being used and free it, i.e.
        * We first write its contents to swap space
        * We then change the page table and other tables to indicate that the page is no longer in memory
    2. Use the freed frame to hold the page, for which the process faulted
* *Modified page-fault service routine to include page replacement*.
    1. Find the location of the desired page on the disk
    2. Find a free frame:
        * If there is a free frame, use it
        * If there is no free frame, use a page-replacement algorithm to select a victim frame
        * Write the victim frame to the disk, change the page and frame tables accordingly
    3. Read the desired page into the newly freed frame, change the page and frame tables
    4. Restart the user process

    >**NOTE**. If no frames are free, 2 page transfers are required. This effectively doubles the page-fault service time and increases the effective access time accordingly

* *Modify bit (or dirty bit)*. Used to reduce the overhead of 2 page transfers
    * *Modify bit*. A bit, which is set by the hardware whenever any word or byte in the page is written into, to indicate that the page has been modifed
    * *Idea*. Each page or frame has a modify bit associated with it in the hardware
        * When we select a page for replacement, we examine its modify bit
            * If the bit is set, i.e. the page has been modified since it was read in from the disk

                $\to$ We must write the page to the disk
            * If the bit is not set, i.e. the page has not been modified since it was read into memory

                $\to$ We need not write the memory page to the disk
    * *Benefits*. Significantly reduce the time required to service a page fault
        * *Explain*. It reduces I/O time by one-half if the page has not been modified

**Page replacement and demand paging**. Page replacement is basic to demand paging
* *Explain*.
    * It completes the separation between logical memory and physical memory
        * *Explain*. An enormous virtual memory can be provided for programmers on a smaller physical memory

            $\to$ Without demand paging, developers must care about physical memory size
    * Enable over-allocation to improve CPU utilization and throughput

**Implementation of demand paging**.
* *Free-allocation algorithm*. If we have multiple processes in memory, we must decide how many frames to allocate to each process
* *Page-replacement algorithm*. When page replacement is required, we must select the victim frame to be replaced
    * *Objective*. Minimize the page-fault rate
    * *Evaluation*. Run the algorithm on a particular string of memory references, and compute the number of page faults
        * *Reference string*. The string of memory references
        * *Generation of reference string*.
            * *Option 1*. Generate the string artifically, using a random-number generator
            * *Option 2*. Trace a given system and record the address of each memory reference

>**NOTE**. Designing appropriate algorithms is an important task, since disk I/O is so expensive

**Belady's anomaly**. For some page-replacement algorithms

$\to$ The page-fault rate may increase as the number of allocated frames increases

**Page replacement algorithms**.
* FIFO page replacement
* Optimal page replacement, i.e. replace the page that will not be used for the longest period of time
* Least-recently-used (LRU) page replacement
* LRU-approximation page replacement
* Counting-based page replacement
    * Least-frequently-used (LFU) page-replacement algorithm
    * Most-frequently-used (MFU) page-replacement algorithm
* Page-buffering algorithms

## Allocation of frames
**Problem**. How do we allocate the fixed amount of free memory among the various processes

**Simplest case**. Single-user system
* *Assumptions*.
    * A single-user system with $N_M$ KB of memory composed of pages $p$ KB in size

        $\to$ This system has $N_M / p$ frames
    * The OS takes $N_O$ KB, leaving $N_U = N_M - N_O$ KB for the user process
* *Simple frame allocation algorithm*.
    * *Initialization*. All frames are in the free-frame list
    * *Iteration*.
        * The first $N_U / p$ page faults would all get free frames from the free-frame list
        * When the free-frame list was exhausted

            $\to$ A page-replacement algorithm would be used to select one of the in-memory pages to be replaced
    * *Termination*. When the process terminated

        $\to$ All frames would be placed on the free-frame list

### Minimum number of frames
**Problem**. Our strategies for the allocation of frames are constrained in various ways
* *Example*.
    * We cannot allocate more than the total number of available frames, unless there is page sharing
    * We must allocate at least a minimum number of frames

**Reason for minimum number of frames**. Due to  reasons
* *Explain*.
    * As the number of frames allocated to each process decreases

        $\to$ The page-fault rate increases, slowing process execution
    * When a page fault occurs before an executing instruction is complete

        $\to$ The instruction must be restarted (recall from previous sections)
* *Consequence*. We must have enough frames to hold all the different pages that any single instruction can reference
    * *Explain*. If we never have enough frames to all all different pages that any instruction can reference

        $\to$ The instruction will keep restarting forever (344)

### Allocation algorithms
**Equal allocation**. Split $m$ frames among $n$ processes to give everyone an equal share of $m/n$ frames

**Proportional allocation**. Allocate available memory to each process according to its size

**Allocation and multiprogramming level**.
* *Increasing multiprogramming level*. Each process will lose some frames to provide the memory required for the new process
* *Decreaseing multiprogramming level*. The frames allocated to the departed process can spread over the remaining processes

>**NOTE**. Both equal and proportional allocation methods do not consider priorities of processes
>$\to$ We may want to give high-priority process more memory to speed its execution

**Proportional allocation with priority**. Use proportional allocation scheme wherein the ratio of frames depends on the priorities of processes also

### Global versus allocation
**Page replacement**. Another important factor in the way frames are allocated to the various processes
* *Classes of page-replacement algorithms*.
    * *Global replacement*. Allow a process to select a replacement frame from the set of all frames, even if that frame is currently allocated to some other process

        $\to$ One process can take a frame from another
        * *Example*. Allow high-priority processes to select frames from low-priority processes for replacement
    * *Local replacement*. Each process select from only its own set of allocated frames

**Drawback of approaches**.
* *Global replacement*. A process cannot control its own page-fault rate
    * *Explain*. The set of pages in memory for a process depends not only on the paging behavior of that process, but also the paging behavior of other processes

        $\to$ The same process may perform quite different due to totally external circumstances
* *Local replacement*. May hinder a process, by not making available to it other, less used pages of memory

**Conclusion**. Global replacement generally results in greater system throughput and is therefore the more common method

### Non-uniform memory access
**Uniform memory access (UMA)**. All main memory are created equal, or at least accessed equally

**Non-uniform memory access (NUMA)**.
* *Motivation*. In systems with multiple CPUs, a given CPU can access some sections of main memory faster than it can access others
    * *Explain*. Due to the interconnectivity of CPUs and memory in the system
        * Frequently, such a system is made up of several system boards, each of which contains multiple CPUs and some memory
        * The system boards are interconnected in various ways, ranging from system buses to high-speed network connections
    * *Consequence*. CPUs on a particular board can access memory on that board with less delay than they can access memory on other boards in the system
* *Non-uniform memory access*. Memory access times vary significantly, and without exception, they are slower than systems in which memory and CPUs are located on the same board

**Frame allocation for NUMA systems**. Manage which page frames are stored at which locations can significantly affect performance in NUMA systems
* *Objective*. Have memory frames allocated "as close as possible" to the CPU, on which the process is running
    * *Definition of "close"*. Refer to "with minimum latency"
* *Idea*. Make change to the scheduling system
    * Have a scheduler tracking the last CPU on which each process ran
    * The scheduler tries to schedule each process onto its previous CPU
    * The memory-management system tries to allocate frames for the process close to the CPU, on which it is being scheduled
* *Consequence*. Improve cache hits and decrease memory access times

**Multithread NUMA systems**.
* *Example problem*. A process with many runnig threads may end up with those threads scheduled on many different system boards

    $\to$ How is the memory to be allocated?
* *lgroup entity in the kernel*. Solaris' solution
    * *Idea*. Gather together close CPUs and memory

        >**NOTE**. In fact, there is a hierachy of lgroups based on the amount of latency between groups
    * *Frame allocation*.
        * Solaris tries to schedule all threads of a process and allocate all memory of a process within an lgroup
        * If not possible, it picks nearby lgroups for the rest of the resources needed

## Thrashing
**Problem**.
* If the number of frames allocated to a low-priority process falls below the minimum number required by the architecture
    1. We must suspend that process' execution
    2. We should then page out its remaining pages, freeing all its allocated frames
* If the process does not have the number of frames it needs to support pages in active use

    $\to$ It will quickly page-fault, and thus it must replace some page
    * Since all its pages are in active use

        $\to$ It must replace a page, which will be needed again right away
    * Consequently, it quickly faults again, and again, replacing pages that it must brign back immediately

**Thrashing**. The high paging activity described above
* *Consequence*. Severe performance problems
* *Another cause of thrashing*.
    * A process enters a new phase in its execution and needs more frames

        $\to$ It starts faulting and taking frames away from other processes
    * The faulting processes must use the paging device to swap pages in and out

        $\to$ As they queue up for the paging device, the ready queue empties, and the CPU utilization decreases
    * The CPU scheduler sees the decreasing CPU utilization and increase the degree of multiprogramming as a result

        $\to$ The new process tries to get started by taking frames from running processes, causing more page faults and a longer queue for the paging device

**Local replacement algorithm (or priority replacement algorithm)**. A method to limit the effects of thrashing
* *Naive idea*. If one process starts thrashing, it cannot steal frames from another process and cause the later to thrash as well
* *Problem*. If processes are thrashing, they will be in queue for the paging device most of the time

    $\to$ The average service time for a page fault will increase due to the longer average queue for the paging device
* *Idea*. To prevent thrashing, we must provide a process with as many frames as it needs

    $\to$ How do we know how many frames it needs?
* *Locality model of process execution*. As a process executes, it moves from locality to locality
    * *Locality*. A set of pages, which are actively used together
    * *Consequence*. A program is generally composed of several different localities which may overlap
* *Locality-based idea*.
    * If we allocate enough frames to a process to accommodate its current locality

        $\to$ It will fault for pages in its locality untill all these pages are in memory, then it will not fault again until changes localities
    * If we do not allocate enough frames to accommodate the size of the current locality

        $\to$ The process will thrash, since it cannot keep in memory all the pages, which it is actively using

### Working-set model
**Working-set model**. Based on the assumption of locality
* *Idea*.
    1. Examine the most recent $\Delta$, i.e. called work-set window, page references

        $\to$ The set of pages in the most recent $\Delta$ page references is the working set
    2. If a page is in active use, it will be in the working set

        $\to$ If it is no longer being used, it will drop from the working set $\Delta$ time units after its last references
* *Working set and program's locality*. The working set is an approximation of program's locality

**Choosing $\Delta$**.
* *Small $\Delta$*. The work set will not encompass the entire locality
* *Large $\Delta$*. The work set may overlap several localities

**Total demand for frames at a time**. $D = \sum_i \text{WSS}_i$ where $\text{WSS}_i$ is the $i$-th working-set size at the time

$\to$ If $D$ is greater than the total number of available frames, thrashing will occur, since some processes will not not have enough frames

**The use of working-set model**.
* *Strategy*. The OS monitors the working set of each process and allocates to that working set enough frames to provide it with its working-set size
    * If there are enough extra frames

        $\to$ Another process can be initiated
    * If the sum of the working-set sizes increases, excedding the total number of available frames

        $\to$ The OS selects a process to suspend. The process' pages are swapped out, and its frames are reallocated to other processes

        >**NOTE**. The suspended process can be restarted later

* *Pros*. Keep the degree of multiprogramming as high as possible, thus optimize the CPU utilization
* *Cons*. It is hard to keep track of the working set
    * *Explain*. The working-set window is a moving window
        * At each memory reference, a new reference appears at one end and the oldest reference drop the other end
        * A page is in the working set if it is referenced anywhere in the working-set window
    * *Solution*. Approximate the working-set model with a fixed-interval timer interrupt and a reference bit
        * *Idea*. After a certain amount a time, the timer interrupts us, we then copy and clear the reference-bit values for each page

### Page-fault frequency
**Page-fault frequency (PFF)**. A more direct way to control thrashing
* *Motivation*. Thrashing has a high page-fault rate

    $\to$ We want to control the page-fault rate
    * When it is too high, we know that the process needs more frames
    * If it is too low, then the process may have too many frames
* *Idea*. Establish a upper and lower bounds on the desired page-fault rate
    * If the actual page-fault rate exceeds the upper limit, we allocate the process another frame
    * If the page-fault rate falls below the lowe limit, we remove a frame from the process
* *Consequence*. We can directly measure and control the page-fault rate to prevent thrashing

## Memory-mapped files
**Motivation**. Consider a sequential read of a file on disk using the standard system calls `open()`, `read()`, and `write()`

$\to$ Each file access requires a system call and disk access
* *Alternative solution*. Use the virtual-memory techniques to treat the I/O as routine memory accesses

**Memory mapping a file**. Allow a part of the virtual address space to be logically associated with the file

$\to$ This can lead to significant performance increase when performing I/O

### Basic mechanism
**Basic mechanism**.
* *Idea*. Map a disk block to a page, or pages, in memory
    * Initial access to the file proceeds through ordinary demand paging, resulting a page fault

        $\to$ A page-sized portion of the file is then read from the file system into a physical page
    * Subsequent reads and writes to the file are handled as routine memory accesses
* *Benefits*.
    * This simplifies the file access and usage by allowing the system to manipulate files through memory, rather than incurring the overhead of `read()` and `write()`
    * File access is much faster

**Memory-disk synchronization**. Writes to the file mapped in memory are not necessarily immediate (synchronous) writes to the file on disk
* *Idea*.
    * The physical is updated when the OS periodically checks whether the page in memory has been modified
    * When the file is closed, all the memory-mapped data are written back to disk, and revmoed from the virtual memory of the process

**Concurrent file mapping by multiple processes**.
* *Operations*.
    * *Writing*. Writes by any of the processes modify the data in virtual memory, and can be seen by all others that map the same section of the file

        >**NOTE**. It should be clear how the sharing of memory-mapped sections of memory is implemented

* *Implementation of mapped memory sharing*. The virtual memory map of each sharing process points to the same page of physical memory, i.e. the page holding a copy of the disk block

    $\to$ The memory-mapping system calls can support copy-on-write functionality, allowing processes to share a file in read-only mode, but to have their own copies of any data they modify

    <div style="text-align:center">
        <img src="/media/1kg3v7a.png">
        <figcaption>Memory-mapped files</figcaption>
    </div>

### Memory-mapped I/O
**Recall**. Each I/O controller includes registers to hold commands and the data being transferred

$\to$ Usually, special I/O instructions allow data transfers between registers and system memory

**Memory-mapped I/O**. Provided by many computer architectures to allow more convenient access to I/O devices
* *Idea*. Ranges of memory addresses are set aside and are mapped to the device registers

    $\to$ Reads and writes to these memory addresses cause the data to be transferred to and from the device registers
* *Applications*.
    * When the devices have fast response times, e.g. video controllers
    * Applied to serial and parallel ports used to connect modems and printers to a computer

        $\to$ The CPU transfers data through these kinds of devices by reading and writing a few device registers, called an I/O port

## Allocating kernel memory
**Acquisition of free pages by a user process**. When a process running in user mode requests additional memory

$\to$ Pages are allocated from the list of free page frames maintained by the kernel

>**NOTE**. If a user process requests a single byte of memory
>$\to$ Internal fragmentation will result, as the process will be granted an entire page frame

**Kernel memory**. Allocated from a free-memory pool different from the list used to satisfy ordinary user-mode processes
* *Motivations*.
    * The kernel requests memory for data structures of varying sizes, some of which are less than a page size

        $\to$ The kernel must use memory conservatively and attempt to minimize waste due to fragmentation
    * Pages allocated to user-mode processes do not necessarily have to be in contiguous physical memory. However, certain hardware devices interact directly with physical memory

        $\to$ They may require memory residing in physically contiguous pages
* *Strategies for managing free memory assigned to kernel processes*. Buddy system, and slab allocation

### Buddy system
**Buddy system**. Allocate memory from a fixed-size segment consisting of physically contiguous pages

<div style="text-align:center">
    <img src="/media/Qixwgb8.png">
    <figcaption>Buddy system allocation mechanism</figcaption>
</div>

* *Allocation method*. Use a power-of-2 allocator, i.e. satisfy requests in units sized as a power of 2

    $\to$ A request in units not appropriately sized is rounded up to the next highest power of 2
* *Advantages*. Adjacent buddies can quickly be combined to form larger segments using a technique known as coealescing
*  *Drawback*. Rounding up to the next highest power of 2 is very likely to cause fragmentation within allocated segments

### Slab allocation
**Terminlogies**.
* *Slab*. Made up of one or more physically contiguous pages
* *Cache*. Consist of one or more slabs

    >**NOTE**. There is a single cache for each unique kernel data structure

* *Objects*. Each cache is populated with objects, which are instantiations of the kernel data structure represented by the cache

**Slab-allocation**. Use cache to store kernel objects

<div style="text-align:center">
    <img src="/media/poPTyqS.png">
    <figcaption>Slab allocation</figcaption>
</div>

* *Cache creation*. When a cache is created

    $\to$ A number of objects, which are initially marked as free, are allocated to the cache
* *Object allocation*.
    * Initially, all objects in the cache are marked as free
    * When a new object for a kernel data structure is needed

        $\to$ Th allocator can assign any free object from the cache to satisfy the request
    * The object assigned from the cache is marked as used
* *Number of objects in the cache*. Depend on the size of the associated slab

**Possible states of a slab**.
* *Full*. Al objects in the slab are marked as used
* *Empty*. All objects in the slab are marked as free
* *Partial*. The slab consists of both used and free objects

**Slab allocator mechanism**.
1. Attempt to satisfy the request with free object in a partial slab
2. If no partial slab exist, a free object is assigned from an empty slab
3. If no empty slab are available, a new slab is allocated from contiguous physical pages and assigned to a cache

    $\to$ Memory for the object is allocated from this slab

**Benefits**.
* No memory is wasted due to fragmentation
    * *Explain*.
        * Each unique kernel data structure has an associated cache
        * Each cache is made up of one or more slabs, which are divided into chunks the size of the objects being represented
        * Therefore, when the kernel requests memory for an object

            $\to$ The slab allocator returns the exact amount of memory required to represent the object
* Memory requests can be satisfied quickly

    $\to$ Slab-allocation scheme is thus particularly effective for managing memory when objects are frequently allocated and deallocated
    * *Explain*.
        * Objects are created in advance and thus can be quickly allocated from the cache
        * When the kernel has finished with an object and releases it

            $\to$ It is marked as free and returned to its cache, making it immediately available for subsequeent requests from the kernel

## Other considerations
### Prepaging
**Problem**. An obvious property of demand paging is the large number of page faults that occur when a process is started
* *Root of problem*.
    * When trying to get the initial locality into memory
    * When swapped-out process is restarted, i.e. all its pages are on the disk

        $\to$ Each must be brought in by its own page fault
* *Solution*. Prepaging

**Prepaging**. Bring into memory at one time all the pages that will be needed
* *Prepaging with working-set model*.
    * If we must suspend a process due to an I/O wait or a lack of free frames

        $\to$ We remember the working set for that process
    * When the process is to be resumed, since the I/O has finished or enough free frames have become available

        $\to$ We automatically bring back into memory its entire working set  before restarting the process
* *Cost of prepaging*. Should be less than the cost of servicing the corresponding page faults, for prepaging to be advantageous

### I/O interlock
**Page locking**. When demand paging is used, we sometimes need to allow some of the pages to be locked in memory
* *Use cases*. When I/O is done to or from user virtual memory

**Problem**. We must be sure the following sequence of events do not occur
1. A process issues an I/O request and is put in a queue for that I/O device
2. Meanwhile, the CPU is given to other processes
3. These processes cause page faults. One of them, using a global replacement algorithm, replaces the page containing the memory buffer for the waiting process

    $\to$ The pages are paged out
4. Some time later, when the I/O request advances to the head of the device queue, the I/O occurs to the specified address

    $\to$ However, this frame is now being used for a different page belonging to another process

**Potential solutions**.
* *Approach 1*. Never execute I/O to user memory
    * *Explain*.
        * Data are always copied between system memory and user memory
        * I/O taks place only between system memory and the I/O device
    * *Drawback*. This extra copying may result in unacceptably high overload
* *Approach 2*. Allow pages to be locked into memory
    * *Implementation*. A lock bit is associated with every frame

        $\to$ If the frame is locked, it cannot be selected for replacement
    * *Procedure to write a block on a tape*.
        1. We lock into memory the pages containing the block
        2. The system then continues as usualy, with locked pages cannot be replaced
        3. When the I/O is complete, the pages are unlocked

**Usages of lock bits**.
* *Usage 1*. Frequently, some or all of the OS kernel is locked into memory, as many OSes cannot tolerate a page fault caused by the kernel
* *Usage 2*. Related to normal page replacement
    * *Problem*. Consider the following sequence of events
        1. A low-priority process faults

            $\to$ The system selects a replacement frame, the paging system reads the necessary page into memory
        2. Ready to continue, the low-priority process entires the ready queue and waits for the CPU

            $\to$ Since it is a low-priority process, it may not be selected by the CPU scheduler for a time
        3. While the low-priority process waiting, a high-priority process faults

            $\to$ The system looks for a replacement, the paging system sees a page in memory but has not referenced or modified, i.e. it is the page brought in by the low-priority process
        4. The page of the low-priority process is then cleaned and will not need to be written out

            $\to$ It apparently has not been used for a long time
        5. After all, we are simply delaying the low-priority process for the benefit of the high-priority process

            $\to$ We are wasting the effort spent to bring in the page for the low-priority process
    * *Usage of lock bit*. If we decide the prevent replacement of a newly brought-in page until it can be used at least once

        $\to$ We can use the lock bit to implement this mechanism
    * *Implementation*. When a page is selected for replacement

        $\to$ Its lock bit turned on, and remains on until the faulting process is again dispatched

**Danger lock bit**. Lock bit may get turned on but never turned off

$\to$ The locked frame becomes unusable

# Appendix
## Concepts
**Sparse address space**. Virtual address spaces which include holes (blank segment betweeen the heap and the stack of the process)
* *Benefits*.
    * Holes can be filled as the stack or heap segments grow
    * When we wish to dynamically link libraries, or possibly shared objects, during program execution

**One-level indirect addressing**. A `load` instruction on page $p_1$ can refer to an address on page $p_2$, which is an indirect reference to page $p_3
