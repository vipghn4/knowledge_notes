---
title: 11. CPU scheduling
tags: Operating system
---

<!-- TOC titleSize:1 tabSpaces:2 depthFrom:1 depthTo:6 withLinks:1 updateOnSave:1 orderedList:0 skip:0 title:1 charForUnorderedList:* -->
# Table of Contents
- [Table of Contents](#table-of-contents)
- [11. CPU scheduling](#11-cpu-scheduling)
  - [Basic concepts](#basic-concepts)
    - [CPU-I/O burst cycle](#cpu-io-burst-cycle)
    - [CPU scheduler](#cpu-scheduler)
    - [Preemptive scheduling](#preemptive-scheduling)
    - [Dispatcher](#dispatcher)
  - [Scheduling criteria](#scheduling-criteria)
  - [Scheduling algorithms](#scheduling-algorithms)
    - [First-come, first-served scheduling](#first-come-first-served-scheduling)
    - [Shortest-job-first scheduling](#shortest-job-first-scheduling)
    - [Priority scheduling](#priority-scheduling)
    - [Round-Robin (RR) scheduling](#round-robin-rr-scheduling)
    - [Multilevel queue scheduling](#multilevel-queue-scheduling)
    - [Multilevel feedback queue scheduling](#multilevel-feedback-queue-scheduling)
  - [Thread scheduling](#thread-scheduling)
    - [Content scope](#content-scope)
    - [Pthread scheduling](#pthread-scheduling)
- [Appendix](#appendix)
  - [Concepts](#concepts)
<!-- /TOC -->

# 11. CPU scheduling
## Basic concepts
**Single-processor system**. Only one process can run at a time, any others must wait until the CPU is free and can be scheduled
* *Multiprogramming*. The objective is to have some process running at all times, to maximize CPU utilization
    * *Problem to solve*. A process is executed until it must wait, typically for the completion of some I/O request

        $\to$ The CPU then just sits idle, thus all the waiting time is wasted and no useful work is accomplished
    * *Idea*. Try to use the time productively by keeping several processes in memory at one time

        $\to$ When one process has to wait, the OS takes the CPU away from that process and gives the CPU to another process

**Scheduling**. Scheduling processes in a multiprogramming system is a fundamental OS function

### CPU-I/O burst cycle
**Key property of processes**. Process execution consists of a cycle of CPU execution and I/O wait

$\to$ Processes alternate between these two states
* *Terminology*. Process execution begins with a CPU burst and is followed by an I/O burst, which is followed by another CPU burst, then another I/O burst, and so one

    $\to$ Eventually, the final CPU burst ends with a system request to terminate execution

**CPU burst duration**. Vary greatly from process to process, and from computer to computer, but they tend to have a frequency curve similar to the following figure

$\to$ The curve is generally characterized as exponential or hyperexponential, with a large number of short CPU bursts and a small number of long CPU bursts

<div style="text-align:center">
    <img src="https://i.imgur.com/G3UpxXu.png">
    <figcaption>Burst duration (in milliseconds) distribution</figcaption>
</div>

>**NOTE**. This distribution can be important in the selection of an appropriate CPU-scheduling algorithm

* *I/O bound program*. Typically has many short CPU bursts
* *CPU bound program*. Might have a few long CPU burst

### CPU scheduler
**Short-term scheduler**. Whenever the CPU become idle, the OS must select one of the processes in the ready queue to be executed
* *Idea*. The scheduler selects a process from the processes in memory, which are ready to execute and allocates the CPU to that process
* *Ready queue*. Not necessarily a FIFO queue, it can be a priority queue, a tree, or simply an unordered linked list
    * *Queue entries*. Generally process control blocks (PCBs) of the processes

### Preemptive scheduling
**Cases for CPU-scheduling decisions**. CPU-scheduling decisions may take place under the following cases
1. When a process switches from the running state to waiting state, e.g. as the result of an I/O request or an invocation of `wait` for the termination of one of the child processes
2. When a process switches from the running state to the ready state, e.g. when an interrupt occurs
3. Whewn a process switches from the waiting state to the ready state, e.g. at completion of I/O
4. When a process terminates

>**NOTE**. For cases 1 and 4, there is no choice in terms of scheduling, since a new process (if one exists in the ready queue) must be selected for execution

>**NOTE**. There is a choice for cases 2 and 3

**Nonpreemptive (or cooperative) scheduling scheme**. When scheduling takes place only under cases 1 and 4, then the scheduling scheme is non-preemptive, otherwise, it is preemptive
* *Non-preemptive scheduling*. Once the CPU has been allocated to a process, the process keeps the CPU until it releases the CPU either by terminating or by switching to the waiting state

    $\to$ No process is interrupted until the process is completed, and after that the processor switches to another process 
    * *Use cases*. Microsoft Windows 3.x
* *Preemptive scheduling*. There is interruptions during the execution of the process, with the intention of resuming the process at a later time
    * *Use cases*. Microsoft Windows 95 and subsequent versions, Mac OS X
    * *Drawback*. Incur a cost associated with access to shared data
        * *Explain*. If two processes share some data
            * While one is updating the data, it is preempted, i.e. interrupted, so that the second process can run
            * The second process then tries to read the data, but they are in an inconsistent state

                $\to$ We need mechanisms to coordinate access to shared data

>**NOTE**. Cooperative scheduling is the only method which can be used on certain hardware platforms, since it does not require the special hardware, e.g. a timer, required for preemptive scheduling

**Preemption and the design of OS kernel**. Preemption affects the design of the OS kernel
* *Problem*. During the processing of a system call, the kernel may be busy with an activity on behalf of a process (as part of the system call), e.g. changing important kernel data, etc.

    $\to$ What if the process is preempted, i.e. stopped or paused, in the middle of these changes and the kernel, or the device driver, needs to read or modify the same structure
* *Solution*. Certain OSes, including most versions of UNIX, waits either for a system call to complete, or for an I/O block to take place, before doing a context switch

    $\to$ This ensures that the kernel structure is simple, since the kernel will not preempt a process while the kernel data structures are in an inconsistent state
    * *Drawback*. This kernel-execution model is a poor one for supporting real-time conputing and multiprocessing

**Critical section and interrupts**. Interrupts can occur at any time, and they cannot always be ignored by the kernel

$\to$ The sections of code affected by interrupts must be guarded from simultaneous use

### Dispatcher
**Dispatcher**. A component involved in the CPU-scheduling function
* *Dispatcher*. The module giving control of the CPU to the process selected by the short-term scheduler
* *Functionality*.
    * Switching context
    * Switching to user mode
    * Jumping to the proper location in the user program to restart the program
* *Requirements*. The dispatcher should be as fast as possible, since it is invoked during every process switch
    * *Dispatch latency*. The time for the dispatcher to stop one process and start another running

## Scheduling criteria
**CPU utilization**. We want to keep the CPU as busy as possible

**Throughput**. If the CPU is busy executing processes, then work is being done

$\to$ One measure of work is the number of processes completed per time unit, i.e. throguhput

**Turnaround time**. From the point of view of a particular process, the important criterion is how long it takes to execute that process
* *Turnaround time*. The time of submission of a process to the time of completion, including the periods spent waiting to get into memory, waiting in the ready queue, executing on CPU, and doing I/O

**Waiting time**. The sum of periods spent waiting in the ready queue

**Response time**. The time it takes to start responding, i.e. the time from the submission of a request until the first response is produced

**Performance optimization**. 
* *Option 1*. Optimize the average measure of the metric of interest
* *Option 2*. Optimize the minimum or maximum metric values
* *Option 3*. For interactive system, it is more important to minimize the variance in repsonse time, rather than to minimize the average response time

## Scheduling algorithms
### First-come, first-served scheduling
**Idea**. When  a process enters the ready queue, its PCB is linked onto the tail of the quieue

$\to$ When the CPU is free, it is allocated to the process at the head of the queue, and the running process is removed from the queue

**Performance of FCFS scheduling in a dynamic situation**.
* *Assumptions*.
    * There are one CPU-bound process and many I/O-bound processes
* *Convey effect*. All the other processes wait for the one big process to get off the CPU
    * *Example*.
        * As the processes flow around the system, the CPU-bound process will get and hold the CPU

            $\to$ During this time, all the other processes will finish their I/O and will move into the ready queue, waiting the CPU
        * While the processes wait in the ready queue, the I/O devices are idle
        * Eventually, the CPU-bound process finishes its CPU burst and moves to an I/O device

            $\to$ All the I/O-bound processes, which have short CPU bursts, execute quickly and move back to the I/O queues
        * At this point, the CPU sits idle, the CPU-bound process will then move back to the ready queue and be allocated the CPU

            $\to$ The I/O processes again end up waiting in the ready queue until the CPU-bound process is done
    * *Consequence*. Lower CPU and device utilization

**Preemption of FCFS**. FCFS scheduling algorithm is nonpreemptive

$\to$ The FCFS algorithm is thus particularly troublesome for time-sharing systems, where it is important that each user get a share of the CPU at regular intervals

**Pros ans cons**.
* *Pros*. Simple and easily managed, simple to write and understand
* *Cons*.
    * The average waiting time is often quite long

### Shortest-job-first scheduling
**Idea**. Associate with each process the length of the process' next CPU burst

$\to$ When the CPU is available, it is assigned to the process having the smallest next CPU burst

>**NOTE**. If the next CPU bursts of two processes are the same, FCFS scheduling is used

**Pros and cons**.
* *Pros*. SJF scheduling is provably optimal, in that it gives the minimum average waiting time for a given set of processes
    * *Explain*. Moving a short proces before a long one decreases the waiting time of the short process more than increasing the waiting time of the long proces
* *Cons*.
    * It is difficult to know the length of the next CPU request
        * *Option 1*. For long-term job scheduling in a batch system, we can use the length of the process time limit which a user specifies when he submits the job

            $\to$ Users are motivated to estimate the process time limit accurately
    * This algorithm cannot be implemented at the level of short-term CPU scheduling
        * *Explain*. In short-term scheduling, there is no way to know the length of the CPU burst
            * *Solution 1*. Try to approximate SJF scheduling by predicting the value of the length of the CPU burst
                * *Idea*. Based on execution time of previous similar CPU bursts
                * *Formal*. The next CPU burst is generally predicted as an exponential average of the measured lengths of previous CPU bursts, i.e.

                    $$\tau_{n+1}=\alpha t_n + (1 - \alpha) \tau_n$$

                    where $t_n$ contains our most recent information, $\tau_n$ stores the past history
                * *Common value of $\alpha$*. $1/2$

**Preemption of SJF scheduling**. SJF algorithm can be either preemptive or nonpreemptive

$\to$ The choice arises when a new process arrives at the ready queue while a previous process is still executing
* *Explain*. The newly arrived CPU burst may be shorter than what is left of the currently executing process
    * *Preemptive SJF algorithm*. Preempt the currently executing process

        $\to$ This is also called *shortest-remaining-time-first scheduling*
    * *Nonpreemptive SJF algorithm*. Allow the currently running process to finish its CPU burst

**Usage**. Frequently in long-term scheduling

### Priority scheduling
**Priority scheduling algorithm**. A priority is associated with each process, and the CPU is allocated to the process with the highest priority

>**NOTE**. Equal-priority processes are scheduled in FCFS order

**Definition of priority**. Priorities can be defined either internally or externally
* *Internally defined priority*. Use some measurable quantity or quantities to compute the priority of a process
    * *Example*. Time limits, memory requirements, the number of open files, etc.
* *Externally defined priority*. Set by criteria outside the OS, e.g. the importance of the process, the type and amount of funds being paid for computer use, etc.

**Preemption of priority scheduling**. Just as SJF, priority scheduling can be either preemptive or nonpreemptive

**Indefinite blocking (or starvation)**. A process is ready to run but waiting for the CPU can be considered blocked

$\to$ A priority scheduling algorithm can leave some low-priority processes waiting indefinitely
* *Possible endings*. 
    * *Case 1*. The process will eventually be run
    * *Case 2*. The computer system will crash and lose all unfinished low-priority processes
* *Solution*. Use aging, i.e. gradually increasing the priority of processes waiting in the system for a long time

### Round-Robin (RR) scheduling
**RR scheduling algorithm**. Designed specifically for time-sharing systems
* *Idea*. Similar to FCFS scheduling, but preemption is added to enable the system to switch between processes
    * *Time quantime (or time slice)*. A small unit of time defined by the RR algorithm
    * *Ready queue*. TReated as a circular queue, i.e. the CPU scheduler goes around the rewady queue, allocating the CPU to each process for a time interval of up to 1 time quantum

        $\to$ If the job is not completed within that time interval, it is interrupted

**Implementation**. We keep the ready queue as a FIFO queue of processes

$\to$ New processes are added to the tail of the ready queue
* *CPU allocation*. The CPU scheduler pickes the first process from the ready queue, sets a timer to interrupt after 1 time quantum, and dispatches the process
* *Possible cases*.
    * *Case 1*. The process may have a CPU burst of less than 1 time quantum

        $\to$ The process itself will release the CPU voluntarily, and the scheduler will proceed to the next process in the ready queue
    * *Case 2*. The process may have a CPU burst of more than 1 time quantum

        $\to$ The timer will go off and cause an interrupt to the OS, then a context switching will be executed, and the process will be put at the tail of the ready queue

**Preemption of RR scheduling**. If a process' CPU burst exceeds 1 time quantum, it is preempted and is put back in the ready queue

$\to$ RR scheduling is preemptive

**Time quantum**. The average waiting time under the RR policy is often long        
* *Explain*. If there are $n$ processes in the ready queue, and the time quantim is $q$, then each process gets $1/n$ of the CPU time in chunks of at most $q$ time units

    $\to$ Each process must wait no longer than $(n-1)\times q$ time units until its next time quantum
* *Performance of RR algorithm*. Depend heavily on the size of the time quantum
    * *Large time quantum*. The RR policy is the same as FCFS policy
    * *Small time quantum*. The RR is called processor sharing, and, in theory, creates the appearance that each of $n$ processes has its own processor running at $1/n$ the speed of the real processor
        * *Usage*. Used in Control Data Corporation (CDC) hardware to implement ten peripheral processors with only one set of hardware, and ten sets of registers
* *RR scheduling from software view*. We need to consider the effect of context switching on the performance of RR scheduling
    * *Explain*. If the time quantum is too short, then there will be many context switches
* *Turnaround time and the time quantum*. The average turnaround time can be improved if most processes finish their next CPU burst in a single time quantum
* *Rule-of-thumb to set time quantum*. 80 percent of the CPU bursts should be shorter than the time quantum

### Multilevel queue scheduling
**Processes classification**. A class of scheduling algorithms has been created for cases, in which processes are easily classified into different groups
* *Example*. A common distribution is made between foreground (interactive) processes and background (batch) processes
    * *Explain*. These two types of processes have different response-time requirements and thus may have different scheduling needs

**Multilevel queue scheduling algorithm**. Partition the ready queue into several separate queues

$\to$ Processes are permanently assigned to one queue, generally based on some property of the process, e.g. memory size, process priority, process type, etc.

<div style="text-align:center">
    <img src="https://i.imgur.com/rUkJKG1.png">
    <figcaption>Multilevel queue scheduling</figcaption>
</div>

* *Scheduling algorithm*. 
    * *Intra-queue scheduling*. Each queue has its own scheduling algorithm, e.g. foreground queue scheduled by RR algorithm, while background queue is scheduled by FCFS algorithm
    * *Inter-queue scheduling*. There must be scheduling among the queues, which is commonly implemented as fixed-priority preemptive scheduling

**Time slicing among the queues**. Each queue gets a certain portion of the CPU time, which it can then schedule among its various proceses

### Multilevel feedback queue scheduling
**Problem**. When the multilevel queue scheduling algorithm is used, processes are permanently assigned to a queue when they enter the system

$\to$ The process do not move from one queue to another, making the algorithm inflexible

**Multilevel feedback queue scheduling**. Allow a process to move between queues
* *Idea*. Separate processes according to the characteristics of their CPU bursts
    * If a process uses too much CPU time, it will be moved to a lower-priority queue

        $\to$ This leaves I/O-bound and interactive processes in the higher-priority queues
    * A process waiting too long in a lower-priority queue may be moved to a higher-priority queues

        $\to$ This kind of aging prevents starvation

**Algorithm parameters**.
* The number of queues
* The scheduling algorithm for each queue
* The method used to determine when to upgrade a process to a higher-priority queue
* The method used to determine when to demote a process to a lower-priority queue
* The method used to determine which queue a process will enter when that process needs service

## Thread scheduling
### Content scope
**Scheduling algortihm as a distinction between user-level and kernel-level threads**.
* *Process-contention scope (PCS)*. On systems implementing the many-to-one and many-to-many user-level-threads-to-kernel-level-threads mapping
    
    $\to$ The thread library schedules user-level threads to run on an available lightweight process (LWP)
    * *Terminologies*.
        * *"Process-contention scope"*. Competition for the CPU takes place among threads belong to the same process
        * *"Schedules user-level threads to run on an available lightweight process (LWP)"*. In this, we do not mean that the thread is actually running on a CPU

            $\to$ This would require the OS to schedule the kernel thread onto a physical CPU
    * *Implementation*. PCS is done according to priority, i.e. the scheduler selects the runnable thread with the highest priority to run
* *System-contention scope (SCS)*. Decide which kernel thread to schedule onto a CPU

    $\to$ Competition for the CPU with SCS scheduling takes place among all threads in the system
    * *Usage*. In systems using one-to-one model, i.e. every SCS thread is associated to each LWP by the thread library and are scheduled by the system scheduler to access the kernel resources

### Pthread scheduling
**POSIX Pthread API allowing specifying PCS or SCS**.
* *Contention scope values of pthreads*.
    * *`PTHREAD_SCOPE_PROCESS`*. Schedule threads using PCS scheduling
    * *`PTHREAD_SCOPE_SYSTEM`*. Schedule threads using SCS scheduling
* *Functions for getting and setting the contention scope policy*.
    * `pthread attr setscope(pthread attr t *attr, int scope)`
    * `pthread attr getscope(pthread attr t *attr, int *scope)`

# Appendix
## Concepts
**Long-term scheduler (job scheduler)**. Determine which programs are admitted to the system for processing, i.e. it selects processes from the queue and loads them into memory for execution

<div style="text-align:center">
    <img src="https://i.imgur.com/6F5Jw6k.png">
    <figcaption>Long-term and short-term scheduling</figcaption>
</div>

* *Objectives*. 
    * Provide a balanced mix of jobs, e.g. I/O bound and CPU bound
    * Control the degree of multiprogramming

**Interactive and batch sessions**. 
* *Batch job*. Consist of a predefined grou of processing actions which require little or no interaction between us and the system

    $\to$ We submit the job, it enters a job queue, where it waits until the system is ready to process the job
* *Interactive job*. Start when we sign on to the system and ends when we sign off the system

    $\to$ We enter requests and the system acts on each request