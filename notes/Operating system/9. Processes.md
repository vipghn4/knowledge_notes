---
title: 9. Processes
tags: Operating system
---

# Table of Contents
[toc]

# 9. Processes
**Process**. 
* *Process as program in execution*. A program in execution
    * *Required resources for a process*. CPU time, memory, files, and I/O devices

        $\to$ These resources are allocated to the process when it is created or while it is executing
* *Process as the unit of work in a system*. System consists of a collection of processes
    * *Types of processes*. OS processes, i.e. execute system code, and user processes, i.e. execute user code

        >**NOTE**. All these processes may execute concurrently

**Threads**. Most modern OSes now support processes having multiple threads

>**NOTE**. Traditionally, a process contained only a single thread of control as it ran

**OS' role**. Responsible for following activities in connection with process and thread management
* The creation and deletion of both user and system processes
* The scheduling of processes
* The provision of mechanisms for synchronization, communication, and deadlock handling for processes

## Process concept
**Motivation**. What to call all the CPU activities
* *Examples*.
    * *Batch system*. Execute jobs
    * *Time-shared system*. Execute user programs, or tasks
* *Conclusion*. In many respects, all these activities are similar, thus we call all of them *processes*

### The processes
**Process in memory**.

<div style="text-align:center">
    <img src="/media/016TmaN.png">
    <figcaption>Process in memory</figcaption>
</div>

* *Text section*. The program code
* *Program counter*. The value of PC represents the current activity
* *Contents of processor's register*
* *Process stack*. Contain temporary data, e.g. function parameters, return addresses, and local variables
* *Data section*. Contain global variables
* *Process heap*. Contain dynamically allocated memory during process runtime

**Process and program**. A program, by itself, is not a process
* *Differences*.
    * *Program*. A passive entity, e.g. a file containing a list of instructions stored on disk
    * *Process*. An active entity, with a program counter specifying the next instruction to execute and a set of associated resources
* *Program becomes process*. A program becomes a process when an executable file is loaded into memory
* *Executable file loading techniques*. Double-click an icon representing the executable file, or enter the name of the executable file on the command-line
* *Multi-process program*. Two processes may be associated with the same program, but they are considered two separate execution sequences

### Process state

<div style="text-align:center">
    <img src="/media/0I0fRmM.png">
    <figcaption>Diagram of process state</figcaption>
</div>

**Process state**. Each process may be in one of the following state
* *New*. The process is being created
* *Running*. Instructions are being executed
* *Waiting*. The process is waiting for some event to occur
* *Ready*. The process is waiting to be assigned to a processor
* *Terminated*. The process has finished execution

### Process control block
**Process control block**. Also called task control block. Each process is represented in the OS by a process control block

<div style="text-align:center">
    <img src="/media/NfUFbqx.png">
    <figcaption>Oricess control block (PCB)</figcaption>
</div>

* *PCB structure*. 
    * *Process state*
    * *Process counter*. The counter indicates the address of the next instruction to be executed for the process
    * *CPU registers*. Include accumulators, index registers, stack pointers, and general-purpose registers, plus any condition-code information

        >**NOTE**. CPU registers and process state must be saved when an interrupt occurs, for backup purpose
    
    * *CPU-scheduling information*. Include a process priority, pointers to scheduling queues, and any other scheduling parameters
    * *Memory management information*. Include the value of the base and limit registers, the page tables, or the segment tables, depending on the memory system used by the OS
    * *Accounting information*. Include the amount of CPU and real time used, time limits, account numbers, job, or process numbers, etc.
    * *I/O status information*. Include the list of I/O devices allocated to the process, a list of open files, etc.
* *Conclusion*. PCB serves as the repository for any information, which may vary from process to process

## Process scheduling
**Objectives**.
* *Objective of multiprogramming*. Have some process running at all times, to maximize CPU utilization
* *Objective of time sharing*. Switch the CPU among processes so frequently that users can interact with each program while it is running

**Key idea**. Have a process scheduler to select an available process, e.g. from a set of several available processes, for program execution on the CPU

>**NOTE**. For a single-processor system, there will never be more than one running process

>**NOTE**. For UNIX, we can use command `cat /proc/cpuinfo | grep processor | wc -l` to find the number of processors

### Scheduling queues
**Scheduling queues**.
* *Job queue*. As processes enter the system, they are put into a job queue consisting of all processes in the system
* *Ready queue*. Consist of processes which are residing in main memory and are ready and waiting to execute
    * *Implementation*. Ready queue is generally stored as a linked list, i.e.
        * Ready-queue header contains pointers to the first and final PCBs in the list
        * Each PCB includes a pointer field pointing to the next PCB in the ready queue
* *Device queue*. The list of processes waiting for a particular I/O device
    * *Explain*. When a process is located the CPU, it executes for a while and eventually quits, is interrupted, or waits for the occurrence of a particular event, e.g. the completion of an I/O request

        $\to$ If the proces makes an I/O request to a shared device, it may have to wait since the device is busy with the I/O request of some other process

**Queueing diagram**. A common representation of process scheduling

<div style="text-align:center">
    <img src="/media/0JbipDb.png">
    <figcaption>Queueing-diagram representation of process scheduling</figcaption>
</div>

### Schedulers
**Scheduler**. A process migrates among the various scheduling queues throughout its lifetime

$\to$ The OS must select, for scheduling purpose, processes from these queses in some fashion. This is done by schedulers

**Schedulers in batch system**. In a batch system, more processes are submitted than can be executed immediately

$\to$ These processes are spooled to a mass-storage device, e.g. a disk, where they are kept for later execution
* *Long-term and short-term schedulers*.
    * *Long-term scheduler*. Also called job scheduler. Select processes from the pool and loads them into memory for execution
    * *Short-term scheduler*. Also called CPU scheduler. Select from processes, which are ready for execution, and allocate the CPU to one of them
* *The primary distinction between long-term and short-term scheduler*. The two schedulers differ in frequency of execution
    * *Short-term scheduler*. Must select a new process for the CPU frequently
        * *Explain*. A process may execute for only a few miliseconds before waiting for an I/O request
        * *Expected execution frequency*. Once every 100 miliseconds
        * *Consequences*. The short-term scheduler must be fast to minimize the work dedicated for scheduling purpose
    * *Long-term scheduler*. Minutes may separate the creation of one new process and the next

        >**NOTE**. Long-term scheduler controls the degree of multiprogramming
        >$\to$ To keep the degree of multiprogramming stable, long-term scheduler may need to be invoked only when a process leaves the system
    
        * *Consequences*. The long-term scheduler must make careful selection, i.e. it should select a good process mix of I/O-bound and CPU-bound processes
            
        >**NOTE**. In some systems, e.g. UNIX and Microsoft Windows, the long-term scheduler may be absent or minimal
        >$\to$ The stability of the processes in these systems depends either on a physical limitation, or on the self-adjusting nature of human users

**Medium-term scheduler**. Some OSes, e.g. time-sharing systems, may introduce an additional, intermediate level of scheduling

<div style="text-align:center">
    <img src="/media/IPv0yux.png">
    <figcaption>Addition of medium-term scheduling to the queueing diagram</figcaption>
</div>

* *Key idea*. Swapping processes
    * *Swapping*. Sometimes, it can be advantageoues to remove processes from memory, and from active contention for the CPU, and thus reduce the degree of multiprogramming (swapped out)

        $\to$ Later, the process can be reintroduced into memory, and its execution can be continued where it left off (swapped in)
    * *Purposes*. 
        * Improve process mix, or
        * A change in memory requirements has overcommitted available memory

            $\to$ Memomry must be freed up
            
### Context switch
**Interrupting a process**. When the OS wants to interrupt a process

$\to$ It needs to save the current *context* of the process running on the CPU, i.e. state save, so that it can restore the context when its processing is done, i.e. state restore

**Context representation**. The process context is represented in the PCB of the process
* *Context structure*.
    * CPU registers
    * Process state
    * Memory-management information

**Context switch**. The task of performing a state save of the current process and a state restore of a different process, i.e. when switching the CPU to another process
* *Procedure*.
    1. The kernel saves the context of the old process in its PCB
    2. The kernel loads the saved context of the new process scheduled to run
* *Context-switch time*. A pure overhead, i.e. the system does no useful work while switching
    * *Elements deciding context-switch time*.
        * Memory speed
        * Number of registers to be copied
        * The existence of special instructions, e.g. an instruction to load or store all registers
    * *Expected speed*. A few miliseconds per switch

## Operations on processes
### Process creation
**Process tree**. Each of the newly created processes may in turn create other processes

$\to$ This forms a tree of processes

**Process identifier (pid)**. Most OSes identify processes according to a unique process identifier, which is typically an integer number

>**NOTE**. In Linux, we can obtain a list of processes by using `ps` command

**Resources for process creation**.
* *Physical and logical resources*. CPU time, memory, files, I/O devices
    * *Resource aquirement* A subprocess may be able to obtain its resources directly from the OS, or it may be constrained to a subset of the resources of the parent process

        $\to$ The parent process have to partition its resources among its children, or it may be able to share some resources, e.g. memory or files, among several of its children

    >**NOTE**. Restrciting a child process to a subset of the parent's resources prevents any process from overloading the system by creating too many subprocesses

* *Initialization data (input)*. Inputs may be passed along by the parent process to the child process

**Possibilities for execution when a process creates a new one**.
1. The parent continues to execute concurrently with its children
2. The parent waits until some or all of its children have terminated

**Possibilities for the address space of the new process**.
1. The child process is a duplicate of the parent process, e.g. having the same program and data as the parent
2. The child process has a new program loaded into it

## Interprocess communication
**Indepedent and cooperating processes**.
* *Independent process*. A process which cannot affect or be affected by the other processes executing in the system

    $\to$ Any process having no shared data with any other process is independent
* *Cooperating process*. A process which can affect or be affected by the other processes executing in the system

    $\to$ Any process sharing data with other processses is a cooperating process

**Why process cooperation is allowed**.
* *Information sharing*. Several users may be interested in the same piece of information, e.g. a shared file
* *Computation speedup*. If we want to finish some task faster, we must break it into subtasks, each of which will be executing in parallel with the others

    >**NOTE**. Sppedup can be achieved only if the computer has multiple processing elements, e.g. CPUs or I/O channels

* *Modularity*. We may want to construct the system in a modular fashion, i.e. divide the system functions into separate processes or threads
* *Convenience*. An individual user may work on many tasks at the same time

**Interprocess communication (IPC) mechanism**. Allow cooperating processes to exchange data and information

<div style="text-align:center">
    <img src="/media/m0rPEd1.png">
    <figcaption>Message passing and shared memory communication models</figcaption>
</div>

* *Fundamental models of interprocess communication*.
    * *Shared memory*. A region of memory is shared by cooperating processes

        $\to$ Processes can exchange information by reading and writing data to the shared region
        * *Usage*. Useful for exchangin larger amounts of data, since
            * Allow maximum speed and convenience of communication
            * Faster than message passing, i.e. since message passing requires system calls, thus require more time-consuming task of kernel intervention

                $\to$ In shared-memory method, system calls are required only to establish shared-memory regions
    * *Message passing*. Communication takes place by means of message exchanged between cooperating processes
        * *Usage*. Useful for exchanging smaller amounts of data, since 
            * No conflicts need to be avoided
            * Easier implementation

### Shared-memory systems
**Shared-memory region**.
* *Location*. Typically reside in the address space of the process creating the shared-memory segment

    $\to$ Other processes wishing to communicate using this shared-memory segment must attach it to their address space

    >**NOTE**. Normally, the OS tries to prevent one process from accessing another process's memory
    >$\to$ Shared memory requires that two or more processes agree to remove this restriction

* *Shared data format*. Determined by the processes using the shared memory

    >**NOTE**. Shared data format are not under OS' control

>**NOTE**. Processes are responsible for ensuring that they are not writing to the same location simultaneously

**Producer-consumer problem**. A common paradigm for cooperating processes
* *Description*. A producer process produces information, which is consumed by a consumer process
* *Shared-memory solution*. Have available a buffer of items, which can be filled by the producer and emptied by the consumer
    * *Buffer location*. In a region of memory, which is shared by the producer and the consumer processes
    * *Synchronization*. The producer and the consumer must be synchronized so that the consumer does not try to consume an item, which has not been produced
* *Shared buffers*.
    * *Buffer types*.
        * *Unbounded buffer*. Place no practical limit on the size of the buffer

            $\to$ The consumer may have to wait for new items, but the producer can always produce new items
        * *Bounded buffer*. Assume a fixed buffer size

            $\to$ THe consumer must wait if the buffer is empty, and the producer must wait if the buffer is full
    * *Implementation*. Buffers are implemented as a circular array with two logical pointers `in` and `out`
        * The variable `in` points to the next free position in the buffer
        * The variable `out` points to the first full position in the buffer

        >**NOTE**. The buffer is empty when `in == out`, and is full when `(in+1)%BUFFER_SIZE == out`

### Message-passing systems
**Motivation**. We want a communication scheme which works for distributed environment

**Message-passing facility**.
* *Must-have operations*. `send(message)` and `receive(message)`
* *Message size*. There is a common tradeoff in OS desin
    * *Fixed-sized messages*. Make the system-level implementation straightforward, but also make the task of programming more difficult
    * *Variable-sized messages*. Require a more complex system-level implementatiohn, but the programming task is simpler

**Communication link**. Assume that two processes P and Q want to communicate

$\to$ There must be a communication link between them
* *Implementation*. We are concerned here not with the link's physical implementation, e.g. shared memory, hardware bus, etc., but rather its logical implementation, i.e.
    * Direct or indirect communication
    * Synchronous or asynchronous communication
    * Automatic or explicit buffering

#### Naming
**Direct communication**. Each process wants to communicate must explicitly name the recipient or sender of the communication
* *Operation implementation*.
    * *`send(P, message)`*. Send a message to process P
    * *`receive(Q, message)`*. Receive a message from process Q
* *Communication link's properties*.
    * A lnk is established automatically between every pair of processes, which want to communicate

        $\to$ The processes need to know only each other's identity to communicate
    * A link is associated with exactly two processes
    * Between each pair of processes, there exists only one link
* *Symmetry addressing*. Direct communication exhibits symmetry in addressing

**Indirect communication**. The messages are sent to and received from mailboxes, or ports
* *Mailbox*. An object, into which messages can be placed by processes, and from which messaged can be removed
    * *Identification*. Each mailbox has a unique identification
    * *Communication scheme*. A process can communicate with some other process via a number of different mailboxes

        >**NOTE**. Two processes can communicate only if they share a mailbox

* *Operation implementation*.
    * *`send(A, message)`*. Send a message to mailbox A
    * *`receive(A, message)`*. Receive a message from mailbox A
* *Communication link's properties*.
    * A link is established between a pair of processes only if both members of the pair have a shared mailbox
    * A link may be associated with more than two processes
    * Between each pair of communicating processes, there can be a number of different links, with each link corresponding to one mailbox
* *Receiving messages from a mailbox*. Suppose that process $P_1$ sends a message to mailbox $A$, then both processes $P_2$ and $P_3$ execute a `receive()` from $A$

    $\to$ Which process will receive the message from $P_1$ depends on which of the following methods we choose
    * Allow a link to be associated with two processes at most
    * Allow at most one process at a time to execute a `receive()` operation
    * Allow the system to select arbitrarily which process will receive the message

        >**NOTE**. The system also may define an algorithm for selecting which process will receive the message, or it may identify the receiver to the sender
* *Mailbox's owner*.
    * *Mailbox owned by a process*. We have to distinguish between the owner, i.e. receive only, and the user, i.e. send only
        * *Pros*. There is no confusion about which process should receive a message sent to the mailbox, since there is only one receiver
        * *Cons*. When a process owning a mailbox terminates, the mailbox disappears

            $\to$ Any process, which subsequently sends a message to this mailbox, must be notified that the mailbox no longer exists
    * *Mailbox owned by the OS*. 
        * *Pros*. The mailbox is independent and is not attached to any particular process
        * *Cons*. The OS must provide a mechanism allowing a process to
            * Create a new mailbox
            * Send and receive messages through the mailbox
            * Delete a mailbox

        >**NOTE**. The process creating a new mailbox is the mailbox's owner by default

#### Synchronization
**Synchronization**. Message passing may be blocking or nonblocking, i.e. synchronous or asynchronous
* *Blocking send*. The sending process is blocked until the message is received by the receiving process or by the mailbox
* *Nonblocking send*. The sending process sends the message and resumes operation
* *Blocking receive*. The receiver blocks until a message is available
* *Nonblocking receive*. The receiver retrieves either a valid message or a null

>**NOTE**. Different combinations of `send()` and `receive()` are possible

**Rendezvous**. When both `send()` and `recevie()` are blocking
* *Solution*. 
    * The producer merely invokes the blocking `send()` call, and waits until the message is deliviered to either the receiver or the mailbox
    * When the consumer invokes `receive()`, it blocks until a message is available

#### Buffering
**Message queue**. Messages exchanged by communicating processes reside in a temporary queue, before being received or sent

**Queue implementations**.
* *Zero capacity*. The queue has a maximum length of zero, i.e. the link cannot have any messages waiting in it

    $\to$ The sender must block until the recipient receives the message
* *Bounded capacity*. The queue has finite length $n$
    * If the queue is not full when a message is sent, the message is placed in the queue

        $\to$ The sender can continue execution without waiting
    * If the link is full
        
        $\to$ The sender must block until space is available in the queue
* *Unbounded capacity*. The queue's length is infinite

    $\to$ Any number of messages can wait in it, and the sender never blocks

## Communication in client-server systems
### Sockets
**Socket**. An endpoint for communication, i.e. a pair of processes communicating over a network employ a pair of sockets, one for each process
* *Identification*. A socket is identified by an IP address concatenated with a port number
* *Communicating mechanism*. Client-server architecture, i.e.
    1. The server waits for incoming client requests by listening to a specified port
    2. Once a request is received, the server accepts a connection from the client socket to complete the connection

    >**NOTE**. All connections must be unique, i.e. all connections consist of a unique pair of sockets

* *Socket types*.
    * *Connection-oriented (TCP) sockets*. Include the connection establishment and connection termination in the communication procedure
        * *Example*. Handshake method is commonly used to establish the connection between the sender and the receiver
    * *Connectionless (UDP) sockets*. Do not include any connection establishment and connection termination

        $\to$ It does not give the guarantee of reliability
    * *Multicast sockets*. Allow data to be sent to multiple recipients
* *Pros and cons*.
    * *Pros*. Common and efficient
    * *Cons*. Socket communication is a low-level form of communication between distributed processes, i.e.
        * Sockets allow only an unstructured stream of bytes to be exchanged between the communicating threads
        
            $\to$ The client or server application must impose a structure on the data

**Ports**. Servers implementing specific services, e.g. FTP, HTTP, etc., listen to well-known ports
* *Example*. telnet server listens to port 23, FTP server listens to port 21, HTTP server listens to port 80
* *Well-known ports*. All ports below 1024 are considered well known
    * *Explain*. We can use them to implement standard services
* *Port assignment*. When a client process initiates a request for a connection

    $\to$ It is assigned a port by its host computer

    >**NOTE**. This port is some arbitrary number greater than 1024

### Remote procedure calls (RPCs)
**RPC**. A way to abstract the procedure-call mechanism for use between systems with network connections
* *RPC and IPC*. 
    * In RPC, we must use a message-based communication scheme to provide remote service, since the two processes are on different machines
    * In RPC, message are well structured and are thus no longer just packets of data
* *Communication mechanism*.
    1. Each message is addressed to an RPC daemon listening to a port on the remote system
        * *Message structure*. Contain an identifier to the function to execute, and the parameters to pass to the function
    2. The function is then executed as requested, and any output is sent back to the requester in a separate message
* *Semantics of RPCs*. Allow a client to invoke a procedure on a remote host, as it would invoke a procedure locally

**Stub**. The RPC system hides the details, which allow communication to take place, by providing a stub on the client side

>**NOTE**. Typically, a separate stub exists for each separate remote procedure

* *Mechanism*. 
    1. The client invokes a remote procedure

        $\to$ The RPC system calls the appropriate stub, passing it the parameters provided to the remote procedure
    2. The stub locates the port on the server and marshals the parameters, i.e. packaging the parameters into a form, which can be transmitted over a network
    3. The stub transmits a message to the server using message passing
    4. A similar stub on the server side receives the message and invokes the procedure on the server
    5. If necessary, return values are passed back to the client using the same technique

**Problems**.
* *Differences in data representation on the client and server machines*.
    * *Example*. Different machines may represent a 32-bit interger differently
    * *Solution*. Define a machine-independent representation of data, i.e. external data representation (XDR)
    * *Consequences*.
        * On the client side, parameter marshalling involves converting the machine-dependent data into XDR before communication
        * On the server side, the XDR data are unmarshalled and converted to the machine-dependent representation for the server
* *Semantics of a call*. RPCs can fail, or be duplicated and executed more than once, due to common network errors
    * *Solution*. The OS must ensure that the messages are acted on exactly once, rather than at most once. However, "exactly once" functionality is hard to implement
        * *"At most once" functionality implementation*. Attach a timestamp to each message
            * *Explain*. The server must keep a history of all the timestamps of messages it has already processed, or a history large enough to ensure that repeated messages are detected

                $\to$ Incoming messages having a timestamp already in the history are ignored
        * *"Exactly once" functionality implementation*. The server must implement the "at most once" protocol, but must also acknowledge to the client that the RPC call was received and executed
            * *Purpose*. Remove the risk that the server will never receive the request
            * *Implementation*. Use ACK messages

                $\to$ The client must resend each RPC call periodically until it receives the ACK for the call
                
<div style="text-align:center">
    <img src="/media/vqXEpf4.png">
    <figcaption>Execution of a RPC</figcaption>
</div>

* *Communication between a server and a client*. 
    * *Problem description*.
        * *Standard procedure calls*. Some form of binding takes place during link, load, or execution time to replace the procedure call's name by the memory address of the procedure call
        * *RPC scheme*. Require a similar binding of the client and the server port

            $\to$ How does a client know the port numbers on the server
    * *Approach 1*. The binding information may be predetermined, in the form of fixed port addresses
        * *Cons*. Once a program is compiled, the server cannot change the port number of the requested service
    * *Approach 2*. Binding can be done dynamically by a rendezvous mechanism
        * *Explain*. Typically, an OS provides a rendezvous, i.e. a matchmaker, daemon on a fixed RPC port

            $\to$ The client sends a message with the name of the RPC to the rendezvous deamon requesting the port address of the RPC it needs to execute
        * *Cons*. Introduce extra overhead of the initial request

**Usage**. Useful in implementing a distributed file system

# Appendix
## Case study
**Process representation in Linux**. PCB in Linux is represented by the C structure `task_struct`

<div style="text-align:center">
    <img src="/media/ERBTlYm.png">
    <figcaption>Active processes in Linux</figcaption>
</div>

* *`taskstruct` structure*. Contains all the necessary information for representing a process, i.e. 
    * Process state
    * Scheduling and memory-management information
    * List of open files
    * Pointers to the process' parent and any of its children
* *Example code*.

    ```c=
    pid_t pid; /* process identifier */
    long state; /* state of the process */
    unsigned int time_slice; /* scheduling information */
    struct task_struct *parent; /* process' parent */
    struct list_head children; /* process' children */
    struct files_struct *files; /* list of open files */
    struct mm_struct *mm; /* address space of the process */
    ```

## Concepts
**Process' parent and children**.
* *Process' parent*. The process which created the underlying proces
* *Process' children*. Any process created by the underlying process

**Dispatching**. A process is dispatched when it is selected for execution

**Degree of multiprogramming**. The number of processes in memory

>**NOTE**. If the degree of multiprogramming is stable
>$\to$ The average rate of process creation must be equal to the average departure rate of processes leaving the system

**I/O-bound and CPU-bound processes**.
* *I/O-bound processes*. Processes which spend more of its time doing I/O than doing computation
* *CPU-bound processes*. Processes which spend more of its time doing computation and generate I/O request infrequently

**Symmetry addressing communication**. 
* *Definitions*.
    * *Symmetry addressing*. Both the sender process and the receiver process must name the other to communicate
    * *Asymmetry addressing*. Only the sender names the recipient, the recipient is not required to name the sender, i.e. `send(P, message)` and `receive(id, message)`
* *Limitations*. Limited modularity of the process definitions, i.e. changing the identifier of a process may necessitate examining all other process definitions
    * *Explain*. All references to the old identifier must be found, so that they can be modified to the new one
    * *Conclusion*. Any such hard-coding techniques are less desirable than techniques involving indirection

**Ports and sockets**. 
* *Similarity*. Both Socket and Port are the terms used in Transport Layer
* *Difference*.
    * A port is a logical construct assigned to network processes so that they can be identified within the system
    * A socket is a combination of port and IP address

**Loopback**. The IP address `127.0.0.1` is a special IP address known as loopback
* *Explain*. When a computer refers to `127.0.0.1`, it is referring to itself
* *Consequence*. A client and server on the same host can communicate using the TCP/IP protocol

**Handshake method**. A method commonly used to establish connections between sockets
* *Procedure*.
    1. Host $A$ sends host $B$ a `synchronize` (SYN) message with its own sequence number $x$
    2. Host $B$ receives the message and replies with a `synchronize-acknowledgement` (SYN-ACK) message with its own sequence number $y$ and acknowledgement number $x+1$
    3. Host $A$ receives the message and replies with an acknowledgement number $y+1$
    4. Bob receives the message and does not reply
* *Brief description*.
    * The synchronize message acts as service request from one server to another
    * The acknowledgement message returns to the requesting server to let it know the message was receive