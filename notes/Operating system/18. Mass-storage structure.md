---
title: 17. File system implementation
tags: Operating system
---

<!-- TOC titleSize:1 tabSpaces:2 depthFrom:1 depthTo:6 withLinks:1 updateOnSave:1 orderedList:0 skip:0 title:1 charForUnorderedList:* -->
# Table of Contents
- [Table of Contents](#table-of-contents)
- [Mass-storage structure](#mass-storage-structure)
  - [Overview of mass-storage structure](#overview-of-mass-storage-structure)
    - [Magnetic disks](#magnetic-disks)
    - [Magnetic tapes](#magnetic-tapes)
  - [Disk structure](#disk-structure)
  - [Disk attachment](#disk-attachment)
    - [Host-attached storage](#host-attached-storage)
    - [Network-attached storage](#network-attached-storage)
    - [Storage-area network](#storage-area-network)
  - [Disk scheduling](#disk-scheduling)
    - [FCFS scheduling](#fcfs-scheduling)
    - [SSTF scheduling](#sstf-scheduling)
    - [SCAN scheduling](#scan-scheduling)
    - [C-SAN scheduling](#c-san-scheduling)
    - [LOOK scheduling](#look-scheduling)
    - [Selection of a disk scheduling algorithm](#selection-of-a-disk-scheduling-algorithm)
  - [Disk management](#disk-management)
    - [Disk formatting](#disk-formatting)
    - [Boot block](#boot-block)
    - [Bad blocks](#bad-blocks)
  - [Swap-space management](#swap-space-management)
    - [Swap-space use](#swap-space-use)
    - [Swap-space location](#swap-space-location)
  - [RAID structure](#raid-structure)
- [Appendix](#appendix)
  - [Concepts](#concepts)
<!-- /TOC -->

# Mass-storage structure
## Overview of mass-storage structure
### Magnetic disks
**Magnetic disks**. Provide the bulk of mass storage for modern computer systems

<div style="text-align:center">
    <img src="https://i.imgur.com/NiTA1zn.png">
    <figcaption>Moving-head disk mechanism</figcaption>
</div>

* *Disk*. Conceptually, each disk platter has a flat circular shape, like a CD
    * *Common platter diameters*. Range from 1.8 to 5.25 inches
    * *Platter surfaces*. The two surfaces of a platter are covered with a magnetic material
* *Read-write heads*. A read-write head flies just above each surface of every platter
    * *Disk arm*. The heads are attached to a disk arm moving all the heads as a unit
* *Tracks*. The surface of a platter is logically divided into circular tracks, which are subdivided into sectors

    $\to$ Each track may contain hundreds of sectors
* *Cylinder*. The set of tracks, which are at one arm position, makes up a cylinder

    $\to$ There may be thousands of concentric cylinders in a disk drive
* *Storage capacity of common disk drives*. Measured in gigabytes

**Performance measures**. When the disk is in use, a drive motor spins it at high speed

$\to$ Most drives rotate 60 to 200 times per second
* *Disk speed*. Have two parts, i.e. the transfer rate, and the positioning time
* *Transfer rate*. The rate, at which data flow between the drive and the computer
    * *Typical transfer rate*. Several megabytes of data per second
* *Positioning time*. Sometimes called the random-access time, consist of two parts
    * *Seek time*. The time necessary to move the disk arm to the desired cylinder
    * *Rotational latency*. The time necessary for the desired sector to rotate the disk head
    * *Typical seek times and rotaitonal latencies*. Several milliseconds

**Head crash**. Since the disk head flies on an extremely thin cushion of air, measured in microns

$\to$ There is a danger that the head will make contact with the disk surface
* *Head crash*. Although the disk platters are coated with a thin protective layer, the head will sometimes damage the magnetic surface

    $\to$ This accident is called a head crash
* *Consequence*. A head crash normally cannot be repaired

    $\to$ The entire disk must be replaced

**Removable disks**. A disk can be removable, allowing different disks to be mounted as needed
* *Disk structure*. Removable magnetic disks generally consist of one platter, held in a plastic case to prevent damage while not in the disk drive
* *Floppy disks*. Inexpensive removable magnetic disks, which have a soft plastic case containing a flexible platter
    * *Disk head*. The head of a floppy-disk drive generally sits directly on the disk surface

        $\to$ The drive is designed to rotate more slowly than a hard-disk drive, to reduce the wear on the disk surface
    * *Storage capacity*. Typically only 1.44 MB or so
* *Availability and capacity*. Removable disks are available working much like normal hard disks, and have capacities measured in gigabytes

**I/O bus**. A disk drive is attached to a computer by a set of wires, called an I/O bus
* *Types of buses*.
    * Enhanced integrated drive electronic (EIDE)
    * Advanced technology attachment (ATA)
    * Serial ATA (SATA)
    * Universal serial bus (USB)
    * Fiber channel (FB)
    * Small computer-systems interface (SCSI)

**Controller**. Data transfers on a bus are carried out by special electronic processors, called controllers
* *Host controller*. The controller at the computer end of the bus
* *Disk controller*. Built into each disk drive

**Data transfer procedure**. To perform a disk I/O operation

<div style="text-align:center">
    <img src="https://i.imgur.com/mHm2DJO.png">
    <figcaption>Hybrid HDD</figcaption>
</div>

1. The computer places a command into the host controller, typically using memory-mapped I/O ports
2. The host controller sends the command via messages to the disk controller
3. The controller operates the disk-drive hardware to carry out the command
4. Disk controllers usually have a built-in cache
    * *Data transfer at the disk drive*. Happen between the cache and the disk surface
    * *Data transfer to the host*. Occur between the cache and the host controller, at fast electronic speed

### Magnetic tapes
**Magtenic tape**. Used as an early mass-storage medium

<div style="text-align:center">
    <img src="https://i.imgur.com/6mjeSt2.png">
    <figcaption>Magnetic tapes</figcaption>
</div>

* *Pros and cons*.
   * *Pros*. Relatively permanent and can hold large quantities of data
   * *Cons*. Access time is slow, compared with that of main memory and magnetic disk
* *Random access to magnetic tape*. Thousand times slower than that of magnetic disk

$\to$ Tapes are not very useful for mass storage
* *Usage*. Mainly for backup, storage of infrequently used information, and as a medium for transferring information from one system to another

**Tape structure**. A tape is kept in a spool, and is wound or rewound past a read-write head

<div style="text-align:center">
    <img src="https://i.imgur.com/HoLmPSl.png">
    <figcaption>Magnetic tape structure</figcaption>
</div>

* *Write speed*. Moving to the correct spot on a tape can take minutes

    $\to$ But once positioned, tape drives can write data at speeds comparable to disk drives
* *Tape capacity*. Vary greatly, depending on the particular type of tape drive
    * *Range*. Typically from 20 GB to 200 GB
* *Data compression*. Some tape has built-in compression, which can more than double the effective storage

## Disk structure
**Logical block (cluster)**. Modern disk drives are addressed as large 1D arrays of logical blocks, where the logical block is the smallest unit of transfer
* *Logical block as unit of transfer*. The OS will logically read a whole block at a time

    $\to$ The OS cannot read part of a block
* *Typical logical block sizes*. Usually 512 bytes, although some disks can be low-level formatted to have different logical block size, e.g. 1024 bytes
* *Implementation*. The 1D array of logical blocks is mapped onto the sectors of the disk sequentially
    * *Sector order*. Sector 0 is the first sector of the first track on the outermost cylinder

        $\to$ The mapping proceeds in order through that track, then through the rest of the tracks in that cylinder, then through the rest of the cylinders from the outermost to the innermost
* *Logical block number to old-style disk address translation*. We can, in principle, convert a logical block number into an old-style disk address
    * *Old-style disk address*. Consist of a cylinder number, a track number within that cylinder, and a sector number within that track
    * *Difficulties*. In practice, it is difficult to perform this translation, i.e.
        * Most disks have some defective sectors, but the mapping hides this by substituting spare sectors from elsewhere on the disk
        * The number of sectors per track is not constant on some drives

**Constant linear velocity (CLV) and constant angular velocity (CAV)**.
* *Constant linear velocity (CLV)*. On media using CLV, the density of bits per track is uniform
    * *Explain*. The farther a track is from the center of the disk, the greater its length, thus the more sector in can hold

        $\to$ As we move from outer zones to inner zones, the number of sectors per track decreases
        * *Consequence*. Tracks in the outermost zone typically hold 40% more sectors than do tracks in the innermost zone
    * *Consequence*. The drive increases its rotation speed as the head moves from the outer to the inner tracks to keep the same rate of data moving under the head
    * *Usage*. Used in CD-ROM, and DVD-ROM drives
* *Constant angular velocity (CAV)*. The disk rotation speed can stay constant, i.e. the density of bits decreases from inner tracks to outer tracks to keep the data rate constant
    * *Usage*. In hard disks

## Disk attachment
**Disk storage access**. Computers access disk storage in two ways
* *Host-attached storage*. Disk storage is accessed via I/O ports
* *Network-attached storage*. Disk storage is accessed via a remote host in a distributed file system

### Host-attached storage
**Host-attached storage**. Storage accessed through a local I/O ports
* *I/O port technologies*.
    * *ATA*. Typical desktop PC uses an I/O bus architecture, called IDE or ATA

        $\to$ This architecture supports a maximum of two drives per I/O bus
    * *SATA*. A newer, similar protocol having simplified cabling
    * *SCSI and FC*. High-end workstations and servers generally use more sophisticated I/O architectures, e.g. SCSI and fiber channel (FC)

**SCSI**. A bus architecture, whose physical medium is usually a ribbon cable with a large number of conductors, e.g. 50 or 68

<div style="text-align:center">
    <img src="https://i.imgur.com/Y77g7SH.png">
    <figcaption>SCSI bus architecture</figcaption>
</div>

* *Supported number of devices*. Maximum of 16 devices per bus

    <div style="text-align:center">
        <img src="https://i.imgur.com/cU60HUO.png">
        <figcaption>SCSI bus</figcaption>
    </div>

    <div style="text-align:center">
        <img src="https://i.imgur.com/qz9nGFa.png">
        <figcaption>SCSI bus chaining</figcaption>
    </div>

    * *SCSI initiator and targets*. The devices include one controller card in the host, i.e. the SCSI initiator, and up to 15 storage devices, i.e. SCSI targets
* *Logical units*. A SCSI disk is a common SCSI target, but the protocol provides the ability to address up to 8 logical units in each SCSI target
    * *Logical-unit addressing*. Used to direct commands to components of a RAID array, or components of a removable media library
    * *Logical unit versus targets*. Generally speaking, a target is a physical device, and a LUN is a logical subdivision of the storage it contains
        * *Explain*. A LUN represents an logical individually addressable SCSI device, which is part of a physical physical SCSI device

**FC**. A high-speed serial architecture, which can operate over optical filber or over a four-conductor copper cable

$\to$ This variant is expected to dominate in the future, and is the basis of storage-area networks (SANs)
* *Pros*. Due to the large address space, and the switched nature of the communication

    $\to$ Multiple hosts and storage devices can attach to the fabric, allowing great flexibility in I/O communication
* *Arbitrated loop (FC-AL)*. An FC variant, which can address 126 devices, including drives and controllers

**I/O commands initiating data transfers to a host-attached storage device**. Reads and writes of logical data blocks directed to specifically identified storage unit, e.g. bus ID, SCSI ID, and target logical unit

### Network-attached storage
**Network-attached storage (NAS) device**. A special-purpose storage system, which is accessed remotely over a data network

<div style="text-align:center">
    <img src="https://i.imgur.com/62ByCB5.png">
    <figcaption>Network-attached storage</figcaption>
</div>

* *Storage access*. Clients access network-attached storage via a RPC interface, e.g. NFS for UNIX system or CIFS for Windows machines
    * *RPCs*. Carried via TCP or UDP over an IP network, usually the same LAN carrying all data traffic to the clients
* *Storage unit*. Usually implemented as a RAID array, with software implementing the RPC interface
* *NAS as a storage protocol*. It is easiest to think of NAS as simply another storage-access protocol

    $\to$ Rather than using a SCSI device driver and SCSI protocols to access storage, a system using NAS would use RPC over TCP/IP
* *Pros and cons*.
    * *Pros*. Provide a convenient way for all the computers on a LAN to share a pool of storage with the same ease of naming and access enjoyed with local host-attached storage
    * *Cons*. It is less efficient and have lower performance than some direct-attached storage options

**iSCSI**. The latest NAS protocol
* *Idea*. Use the IP network protocol to carry the SCSI protocol

    $\to$ Networks, rather than SCSI cables, can be used as interconnects between hosts and their storage
* *Consequence*. Hosts and treat their storage as if it were directly attached, even if the storage is distant from the host

### Storage-area network
**Drawback of NAS**. The storage I/O operations consume bandwidth on the data network, thus increase the latency of network communication

$\to$ This problem can be acute in large client-server installations
* *Explain*. The communication between servers and clients competes for bandwidth, with the communication among servers and storage devices

**Storage-area network (SAN)**. A private network, using storage protocols rather than networking protocols, connecting servers and storage units

<div style="text-align:center">
    <img src="https://i.imgur.com/RX3tEMb.png">
    <figcaption>Storage-area network</figcaption>
</div>

* *Flexibility of SAN*. The power of a SAN lies in its flexibility, i.e. multiple hosts and storage arrays can attach to the same SAN, and storage can be dynamically allocated to hosts
* *SAN switch*. Allow or prohibit access between the hosts and the storage
    * *Example*. If a host is runnig low on disk space, the SAN can be configured to allocated more storage to that host
* *Benefits*. 
    * Allow clusters of servers to share the same storage
    * Allow storage arrays to include multiple direct host connections

**FC**. The most common SAN interconnect

## Disk scheduling
**Disk scheduling**. For disk drives, using hardware efficiently means havling fast access time and large disk bandwidth
* *Disk drive performance measures*.
    * *Disk access time*. Have two major components, i.e. seek time and rotational latency as given previously
    * *Disk bandwidth*. The total number of bytes transferred, divided by the total time between the first request for the service and the completion of the last transfer
* *Improving disk drive performance*. By managing the order, in which disk I/O requests are serviced

**I/O request procedure**. Whenever a process needs I/O to or from the disk
1. It issues a system call to the OS
    * *Request format*. Specify several pieces of information
        * Whether this operation is input or output
        * What the disk address for the transfer is
        * What the memory address for the transfer is
        * What the number of sectors to be transferred is
2. If the desired disk drive and controller are available, the request can be serviced immediately

    $\to$ Otherwise, any new requests for service will be placed in the queue of pending requests for the drive

**Disk scheduling and multiprogramming system**. For a multiprogramming system with many processes, the disk queue may often have several pending requests

$\to$ When one request is completed, the OS chooses which pending request to service next

### FCFS scheduling
**FCFS scheduling**. The simplest form of disk scheduling is first-come first-served (FCFS)

$\to$ This is intrinsically fair, but generally does not provide the fastest service
* *Explain*.

    <div style="text-align:center">
        <img src="https://i.imgur.com/8Pt5QbY.png">
        <figcaption>FCFS disk scheduling</figcaption>
    </div>

### SSTF scheduling
**SSTF scheduling**. It seems reasonable to service all the requests close to the current head position, before moving the head away to service other requests

$\to$ This assumption is the basis for the shortest-seek-time-first (SSTF) algorithm
* *SSTF and SJF scheduling*. SSTF scheduling is essentially a form of shortest-job-first (SJF) scheduling

    $\to$ It may cause starvation of some requests
    * *Explain*. Requests may arrive at anytime, i.e. while a request is being serviced, and there is a coming continual stream of requests near one another and close to the request being serviced

        $\to$ The previously pending requests will never be served

        >**NOTE**. This scenario becomes increasingly likely as the pending-request queue grows longer

### SCAN scheduling
**SCAN algorithm**. The disk arm starts at one end of the disk and moves toward the other end, servicing requests as it reaches each cylinder, until it gets to the other end of the disk

$\to$ At the other end, the direction of head movement is reversed, and servicing continues
* *Elevator algorithm*. The SCAN algorithm is sometimes called the elevator algorithm, since
    * *Explain*. The disk arm behaves just like an elavator in a building, first servicing all the requests going up, then reversing to service requests the other way
* *Consequence*. 
    * If a request arrives in the queue, just in front of the head, it will be serviced almost immediately
    * A request arriving just behind the head will have to wait until the arm moves to the end of the disk, reverses direction, and comes back

**Problem**. Assuming a uniform distribution of requests for cylinders, consider the density of requests when the head reaches one end and reverses direction
* At this point, relatively few requests are immediately in front of the head, since these cylinders have recently been serviced
* The largest density of requests is at the other end of the disk

    $\to$ These requests have also waited the longest

### C-SAN scheduling
**Circular SCAN (C-SCAN) scheduling**. A variant of SCAN designed to provide a more uniform wait time
* *Idea*. Treat the cylinders as a circular list wrapping around from the final cylinder to the first one 
    * Like SCAN, C-SCAN moves the head from one end of the disk to the other, servicing requets along the way
    * When the head reaches the other end, it immediately returns to the beginning of the disk, without servicing any requests from the return trip

### LOOK scheduling
**Practical implementation of SCAN and C-SCAN**. Btoh algorithms move the disk arm across the full width of the disk

$\to$ In practice, neither algorithm is often implemented this way
* *Idea*. More commonly, the arm goes only as far as the final request in each direction

    $\to$ It then reverses direction immediately, without going all the way to the end of the disk

**LOOK and C-LOOK scheduling**. The practical implementations of SCAN and C-SCAN

### Selection of a disk scheduling algorithm
**Usage of different scheduling algorithms**.
* *SSTF*. Common and have a natural appeal, since it increases performance over FCFS
* *SCAN andC-SCAN*. Perform better for systems placing a heavy load on the disk
    * *Explin*. These algorithms are less likely to cause a starvation problem
* *Optimal scheduling*. For any particular list of requests, we can define an optimal order of retrieval, but the computation required to find an optimal schedule may not justify the savings over SSTF or SCAN

**Selection of a disk scheduling algorithm**. There are several aspects deciding the performance of a scheduling algorithm
* *Number and types of requests*. With any scheduling algorithm, performance depends heavily on the number and types of requests
    * *Example*. If the queue usually has just one outstanding request

        $\to$ All scheduling algorithms behave the same, since they have only one choice of where to move the disk head
* *Disk requests and file allocation method*. Requests for disk service can be greatly affected by the file-allocation method
    * *Examples*. 
        * A program reading a contiguously allocated file will generate several requets, which are close together on the disk

            $\to$ This results in limited head movement
        * A linked or indexed file may include blocks widely scattered on the disk

            $\to$ This results in greater head movement
* *Location of directories and index blocks*. Since every file must be opened to be used, and opening a file requires searching the directory structure

    $\to$ The directories will be accessed frequently
    * *Examples*. 
        * If a directory entry is on the first cylinder, and a file's data are on the final cylinder

            $\to$ The disk head has to move the entire width of the disk
        * If the directory entry were on the middle cylinder

            $\to$ The head would have to move only one-half the width
    * *Improvement idea*. Caching the directories and index blocks in main memory can help to reduce disk-arm movement

**Disk-scheduling module**. Due to the complexity for disk scheduling algorithm selection, the disk scheduling algorithm should be written as a separate module of the OS

$\to$ The disk scheduling algorithm can be replaced with a different algorithm if required
* *Default algorithm*. SSTF or LOOK

**Other disk scheduling performance measures**.
* *Rotational latency*. For modern disks, the rotational latency can be nearly as large as the average seek time
    * *Problem*. It is difficult for the OS to schedule for improved rotational latency
        * *Explain*. Modern disks do not disclose the physical location of logical blocks
    * *Solution*. Disk manufacturers have been alleviating this problem by implementing disk-scheduling algorithms in the controller built into the disk drive
        * *Consequence*. If the OS sends a batch of requests to the controller, the controller can queue them and then schedule them to improve both seek time and rotational latency

>**NOTE**. If I/O performance were the only consideration, the OS would gladly rturn over the responsibility of disk scheduling to the disk hardware

**Other constraints on disk scheduling**. The OS may have other constraints on the service order for requests
* *Example*. 
    * Demand paging may take priority over application I/O, and writes are more urgent than reads if the cache is running out of pages
    * It may be desirable to guarantee the order of a set of disk writes to make the file system robust in the face of system crashes
        * *Example*. If the OS allocated a disk page to a file, and the application wrote data into the page before the OS had a chance to flush the modified inode and free-space list back to disk
* *Consequence*. The OS may choose to do its own disk scheduling, and to spoon-feed the requests to the disk controller, one by one, for some types of I/O

## Disk management
**Disk management**. The OS is responsible for several other aspects of disk management, e.g. disk initialization, booting from disk, and bad-block recovery

### Disk formatting
**Low-level formatting (or physicall formatting)**. A new magnetic disk is a blank slate, i.e. it is just a platter of a magnetic recording material
* *Low-level formatting*. Before a disk can store data, it must be divided into sectors, which the disk controller can read and write
    * *Idea*. Fill the disk with a special data structure for each sector
    * *Data structure within each sector*. Typically consist of a header, a data area of usually 512 bytes in size, and a trailer
        * *Header and trailer*. Contain information used by the disk controller, e.g. a sector number and an error correcting code (ECC)
* *Error correcting code*.
    * *Error correcting function*.
        * When the controller writes a sector of data during normal I/O

            $\to$ The ECC is updated with a value calculated from all the bytes in the data area
        * When the sector is read, the ECC is recalculated and compared with the stored value

            $\to$ If the stored value and calculated numbers are different, this mismatch indicates that the data area of the sector has become corrupted and the disk sector may be bad
    * *Error correcting code*. The ECC is an error-correcting code since it contains enough information, if only a few bits of data have been corrupted, to enable the controller to identify which bits have changed, and calculate what their correct values should be

        $\to$ The ECC then reports a recoverable soft error
    * *ECC processing*. The controller automatically does the ECC processing whenever a sector is read or written
* *Low-level formatting hard disks*. 
    * *Low-level formatting at factory*. Most hard disks are low-level-formatted at the factory, as part of the manufacturing process

    $\to$ This enables the manufacturers to test the disk, and to initialize the mapping from logical block numbers to defect-free sectors on the disk
    * *Low-level formatting by disk controller*. The disk controller is instructed to low-level-format the disk

        $\to$ It can also be told how many bytes of data space to leave between the header and trailer of all sectors
        * *Typical data space size*. 256, 512, or 1024 bytes
        * *Sector size's effects*. Formatting a disk with a larger sector size means that fewer sectors can fit on each track

            $\to$ But it also means that fewer headers and trailers are written on each track and more space is available for user data

**Partitioning and logical formatting**. Before we can use a disk to hold files, the OS needs to record its own data structures on the disk

$\to$ It does so in two steps, i.e. partition and logical formatting
* *Partition*. Partition the disk into one or more groups of cylinders, then treat each partition as though it were a separate disk
    * *Example*. One partition can hold a copy of the OS's executable code, while another holds user files
    * *Why partitioning by cylinder*. Recall that hard disks use CAV
* *Logical formatting (File system creation)*. The OS stores the initial file-system data structures onto the disk
    * *File system data structure*. May include maps of free and allocated space, i.e. a FAT or inodes, and an initial empty directory

**Clustering**. To increase efficiency, most file systems group blocks into larger chunks, frequently called clusters

$\to$ Disk I/O is done via blocks, but file system I/O is done via clusters, effectively assuring that I/O has more sequential-access and fewer random-access characteristics

**Disk partitioning as large sequential array of logical blocks**. Some OSes give special programs the ability to use a disk partition as a large sequential array of logical blocks, without any file-system data structures
* *Raw disk and raw I/O*. The array is sometimes called raw disk, and I/O to this array is termed raw I/O
    * *Example*. Some database systems prefer raw I/O, since it enables them to control the exact disk location, where each database record is stored
* *Raw I/O and file-system services*. Raw I/O bypasses all the file-system services, e.g. buffer cache, file locking, prefetching, space allocation, file names, and directories
* *Usage*. We can make certain applications more efficient by allowing them to implement their own special-purpose storage services on a raw partition

    $\to$ However, most applications perform better when they use the regular file-system services

### Boot block
**Booting**. For a computer to start running, e.g. when it is powered up or rebooted, it must have an initial program to run
* *Bootstrap*. The initial program run by the computer, which tends to be simple
    * *Functionality*. Initialize all aspects of the system, from CPU registers to device controllers and the contents of main memory, then start the OS
    * *OS starting procedure*.
        1. The bootstrap program finds the OS kernel on disk
        2. The bootstrap program loads the kernel into memory
        3. The bootstrap program jumps to an initial address to begin the OS execution

**Bootstrap storage**. 
* *Bootstrap in ROM*. 
    * *Idea*. For most computers
        * The bootstrap is stored within ROM, which is a convenient location
            * *Explain*. ROM needs no initialization
        * Within the ROM, the bootstrap is at a fixed location, at which the processor can start executing whenever powered up or reset
    * *Pros and cons*.
        * *Pros*. Since ROM is read-only, it cannot be infected by a computer virus
        * *Cons*. Changing the bootstrap code requires changing the ROM hardware chips
* *Bootstrap in disk*. Most systems store a tiny bootstrap loader program in the boot ROM
    * *Bootstrap loader*. Bring a full bootstrap program from disk

        $\to$ The full bootstrap program can be changed easily
    * *Boot blocks*. The full bootstrap program is stored in the boot blocks, at a fixed location on the disk
    * *Boot disk (system disk)*. A disk having a boot partition
    * *Booting procedure*. The code in the boot ROM instructs the disk controller to read the boot blocks into memory, and starts executing that code

        >**NOTE**. No device drivers are loaded at this point
        
    * *Full bootstrap program*. Sophisticated than the bootstrap loader in the boot ROM, i.e.
        * The full bootstrap program can load the entire OS from a non-fixed location on disk
        * The full bootstrap program can then start the OS running

**Example - Windows' boot process**. The Windows system places its boot code in the first sector on the hard disk

$\to$ This is called the master boot record (MBR)
* *MBR*. Contain boot code, a table listing the partitions for the hard disk, and a flag indicating which partition the system is to be booted from
* *Hard disk partitioning*. Windows allows a hard disk to be divided into one or more partitions, one of which is identified as the boot partition
    * *Boot partition*. Contain the OS and device drivers
* *Booting procedure*.
    1. Booting begins in a Windows system by running code, which is resident in the system's ROM memory

        $\to$ This code directs the system to read the boot code from the MBR
    2. Once the system identifies the boot partition, it reads the first sector from that partition, i.e. the boot sector`
    3. The system then loads the various subsystems and system services

### Bad blocks
**Disk failures**. Since disks have the moving parts and small tolerances, i.e. the disk head flies just above the disk surface

$\to$ They are prone to failure
* *Complete failures*. The disk needs to be replaced and its contents restored from backup media to the new disk
* *Sector defection*. More frequently, one or more sectors become defective
* *Bad blocks*. Most disks even come from the factory with bad blocks

    $\to$ Depending on the disk and the controller in use, these blocks are handled in a variety of ways

**Bad block manually handling**. On simple disks, e.g. disks with Integrated drive electronics (IDE) controllers

$\to$ Bad blocks are handled manually
* *Example 1*. 
    1. The MS-DOS `format` command performs logical formatting and scans the disk to ffind bad blocks
    2. If `format` finds a bad block, it writes a special value into the corresponding FAT entry, to tell the allocation routines not to use that block
* *Example 2*. If blocks go bad during normal operation, a special program, e.g. `chkdsk`, must be run manually to search for the bad blocks, and to lock them away

    $\to$ Data resided on the bad blocks usually are lost

**Bad block automatically handling**. More sophisticated disks, e.g. the SCSI disks used in high-end PCs and most workstations and servers, are smarter about bad-block recovery
* *Idea*. The controller maintains a list of bad blocks on the disk
    * *Bad block list initialization*. The list is initialized during the low-level formatting at the factory
    * *Bad block list update*. The list is updated over the life of the disk
* *Sector sparing (forwarding)*. Low-level formatting also sets aside spare sectors not visible to the OS

    $\to$ The controller can be told to replace each bad sector logically with one of the spare sectors
* *Example of bad sector transaction*.
    1. The OS tries to read logical block 87
    2. The controller calculates the ECC, and finds that the sector is bad

        $\to$ It reports this finding to the OS
    3. The next time the system is rebooted, a special command is run to tell the SCSI controller to replace the bad sector with a spare sector
    4. After that, whenever the system requests logical block 87, the request is translated into the replacement sector's address by the controller
* *Bad sector handling and disk scheduling*. Such a redirection by the controller above could invalidate any optimization by the OS' disk-scheduling algorithm
  
    $\to$ Most disks are formatted to provide a few spare sectors in each cylinder, and a spare cylinder as well
    * *Consequence*. When a bad block is remapped, the controller uses a spare sector from the same cylinder, if possible
* *Sector slipping*. An alternative to sector sparing, i.e. some controllers can be instructed to replace a bad block by sector slipping
    * *Procedure*. Suppose logical block 17 becomes defective, and the first available spare follows sector 202
        
        $\to$ The sector slipping remaps all the sectors from 17 to 202, moving them all down one spot
        * *Explain*. Sector 202 is copied into the spare, then 201 into 202, then 200 into 201, etc.
    * *Consequence*. Slipping the sectors this way frees up the space of sector 18, thus sector 17 can be mapped to it

**Hard error**. The replacement of a bad block generally is not totally automatic, since the data in the bad block are usually lost
* *Soft error*. Trigger a process, in which a copy of the block data is made, and the block is spared or slipped
* *Hard error (unrecoverable error)*. Result in lost data

    $\to$ Whatever file was using that block must be repaired, and requires manual intervention

## Swap-space management
**Swapping (recall)**. Moving entire processes between disk and main memory
* *Traditional implementation of swapping*. Swapping occurs when the amount of physical memory reaches a critical low point, and processes are moved from memory to swap space to free available memory
* *Practical implementation of swapping*. In practice, systems combine swapping with virtual-memory techniques and swap pages, not necessarily entire process

    >**NOTE**. Some systems use the term swapping and paging interchangeably, reflecting the merging of these two concepts

**Swap-space mangement**. Another low-level task of the OS
* *Motivation*. Virtual memory uses disk space as an extension of main memory, and disk access is much slower than memory access

    $\to$ Using swap space significantly decreases system performance
* *Objective of the design and implementation of swap space*. Provide the best throughput of the virtual memory syustem

### Swap-space use
**Swap space**. Used in various ways by different OSes, depending on the memory-management algorithms in use
* *Example*. 
    * System implementing swapping may use swap space to hold an entire process image, including the code and data segments
    * Paging systems simply store pages, which have been pushed out of main memory
* *Required amount of swap space*. Vary from a few megabytes of disk space to gigabytes, depending on the amount of physical memory, the amount of virtual memory it is backing, and the way virtual memory is used

**Overestimation of swap space**. It may be safer to overestimate than to underestimate the amount of swap space required
* *Explain*. 
    * If a system runs out of swap space, it may be forced to abort processes or may crash entirely
    * Overestimation wastes disk space that could otherwise be used for files, but it does no other harm
* *Recommended swap space size*. Some systems recommand the amount to be set aside for swap space
    * *Solaris*. Suggest setting swap space equal to the amount, by which virtual memory exceeds pageable physical memory
    * *Linux's old suggestion*. Suggest setting swap space to double the amount of physical memory
    * *Linux's current suggestion*. Most Linux systems now use considerably less swap space

        >**NOTE**. There is currently much debate in the Linux community about whether to set aside swap space at all

**Multiple swap spaces**. Some SOes, including Linux, allow the use of multiple swap spaces

$\to$ These spaces are usually put on separate disks, so that the load placed on the I/O system by paging and swapping can be spread over the system's I/O devices

### Swap-space location
**Swap-space location**. A swap space can reside in one of two places
* It can be carved out of the normal file system, i.e. the swap space is a large file within the file system
    * *Pros and cons*.
        * *Pros*. Normal file-system routines can be used to create, name, and allocate space for the swap space

            $\to$ Easy to implement
        * *Cons*. Inefficient
            * Navigating the directory structure and the disk-allocation data structures take time and, possibly, extra disk accesses
            * External fragmentation can greatly increase swapping times by forcing multiple seeks during reading or writing of a process image
    * *Improvement ideas*. Cache the block-lcoation information in physical memory, and use special tools to allocate physically contiguous blocks for the swap file
        * *Drawback*. The cost of traversing the file-system data structures remains
* It can be in separate disk partition, i.e. swap space is created in a separate raw partition

    $\to$ No file system or directory structure is placed in this space
    * *Swap-space storage manager*. Used to allocate and deallocate the blocks from the raw partition
        
        $\to$ This manager uses algorithms optimized for speed, rather than for storage efficiency
        * *Explain*. Swap space is accessed much more frequently than the file systems, when it is used
    * *Internal fragmentation*. Internal fragmentation may increase, but this trade-off is acceptable
        * *Explain*. 
            * The life of data in the swap space generally is much shorter than that of files in the file system
            * Since swap space is reinitialized at boot time, any fragmentation is short-lived
    * *Idea*. Create a fixed amount of swap space during disk partitioning

        $\to$ Adding more swap space requires either repartitioning the disk, which involves moving the other file-system partitions or destroying them and restoring them from backup, or adding another swap space elsewhere

**Combined method for swap-space location**. Some OSes are flexible and can swap both in raw partitions and in file-system space
* *Example*. Linux, i.e. the policy and implementation are separate, allowing the machine's administrator to decide which type of swapping to use

## RAID structure
**Motivation**. Disk drives have continued to get smaller and cheaper, hence it is economically feasible to attach many disks to a computer system
* *Benefits*.
    * Having a large number of disks in a system presents chances for improving the rate, at which data can be read of written, if the disks are operated in parallel
    * This setup offers the potential for improving the reliability of data storage, since redundant information can be stored on multiple disks

        $\to$ Failure of one disk does not lead to loss of data

**Redundant arrays of independent disks (RAIDs)**. Used to address the performance and reliability issues
* *Past implementation*. RAIDs composed of small, cheap disks were viewed as a cost-effective alternative to large, expensive disks
* *Today implementation*. RAIDs are used for their higher reliability, and higher data-transfer rate, rather than for economic reasons

**More details notes**. Read the notes about computer architecture instead!

# Appendix
## Concepts
**Cluster (file systems)**. Also called allocation unit, or block, i.e. in many disk drives, sectors usually are grouped into clusters, or logical blocks, which function as the smallest data unit permitted
* *Definition*. A unit of disk space allocation for files and directories
    * *Explain*. To reduce the overhead of managing on-disk data structures, the file system does not allocate individual disk sectors by default

        $\to$ It allocates contiguous groups of sectors, called clusters
* *Cluster allocation*. A cluster is the smallest logical amount of disk space, which can be allocated to a file

    $\to$ Storing small files on a file system with large clusters will waste disk space
    * *Slack space*. The wasted disk space within a cluster
* *Cluster size effects*. Typically range from 1 sector (512 bytes) to 128 sectors (64 KiB)
    * *Small cluster sizes*. Cluster sizes small relative to the average file size will make the wasted space per file statistically about half of the cluster
    * *Large cluster sizes*. 
        * The wasted space will become greater
        * Reduce book-keeping overhead and fragmentation, leading to faster reading and writing speed overall
* *Physical continuity of a cluster*. A cluster need not be physically contiguous on the disk
    * *Explain*.
        * A cluster may span more than one track
        * If sector interleaving is used, maybe discontiguous within a track
    * *Clustering and fragmentation*. Clustering should not be confused with fragmentation
        * *Explain*. Sectors are still logically contiguous, while fragmentation does not
* *Lost cluster*. Occur when a file is removed form the directory listing, but the file allocation table (FAT) still shows the clusters allocated to the file

**More about logical block**.
* *From "Federal information processing standards publication (FIPS PUB 97)"*. 
    * *Logical block*. A block of data, which is the normal unit of information read or recorded during READ or WRITE operations
    * *Logical address*. Each logical block is uniquely identified by a 32-bit logical address

        $\to$ Logically successive blocks are identified by successive logical address
    * *Physical block*. The smallest unit of data, which may be physically independently written on a disk drive
        * *Block structure*. Contain an ID area, and a data area
            * *Data area*. May be less than, equal to, or more than one logical block
        * *Logical-to-physical-block mapping*. Defined in the vendor's documentation
    * *Physical address*. The addess of a block, i.e. logical or physical, which is meaningful in terms of the physical structure of a disk drive
* *From alphaurax-computer*.
    * *Why are there logical blocks*. The OS cannot point straight to the sectors, since there are limits to the number of blocks, or drive addresses, which an OS can address

        $\to$ By defining a block as several sectors, an OS can work with bigger hard drives without increasing the number of block addresses
* *From reddit*.
    * *Difference between sectors, blocks, and clusters*.
        * *Sectors*. The smallest logical unit a hard drive cares about, e.g. 512b
            * *"Care about" meaning*. A sector should contain a header, the actual data, and any ECC data
        * *Physical blocks*. Like sectors, but are handled by file systems, or sometimes by RDBMS
            * *Usage*. Useful for buffering and aligning writes / reads with the disk's sector size
        * *Logical block (cluster)*. A logical group defined by a filesystem, as the smallest contiguous block / sector to use to store files and directories
            * *Typical size*. Several blocks / sectors large, leading to some minor space wastage
* *From stackoverflow*.
    * Using logical blocks, disk reads are sequential, up to the block size, resulting in less seeks and higher read throughput
    * Blocks provide a simple implementation which can be mapped to pages, which results in higher write throughput as well

**M.2 disk drives**. M.2, formerly known as the Next generation form factor (NGFF), is a specification for internally mounted computer expansion cards and associated connectors

**Logical unit number (LUN)**. A number used to identify a logical unit, which is a device addressed by the SCSI protocol, or by Storage Area Network protocols encapsulating SCSI

**Why hard disks use CAV rather than CLV**.
* *Random access and data transfer rates of CLV*. In CLV, the seek performance would be greatly affected during random access
    * *Explain*. Due to the requirement to continually modulate the disc's rotation speed to be appropriate for the read head's position
    * *Conclusion*. To accommodate this requirements of modern CD-ROM drives, CAV systems are used
* *Noise level of CLV and CAV*. CAV discs should produce less noise during playback
* *Pros and cons of CAV*.
    * *Pros*.
        * Individual data blocks can be addressed by sector and track directly, since the number of sectors per track does not change for different tracks
        * The design and manufacture of CAV disks are simpler and easier
        * Moving the read/write head from its present location to a particular address requires only a short movement
        * There is only a minimal waiting time for the proper sector to spin under the read-write head
        * Lower average noise level than CLV
    * *Cons*. The amount of data can be stored on the outer tracks is limited by the storage capacity of the inner tracks
* *Pros and cons of CLV*.
    * *Pros*. Guarantee a constant rate of data access
    * *Cons*.
        * Random access of data is slow and difficult
        * Noticeable performance issues with modern CD-ROMs
        * Higher average noise level than CAV 

**Rawdisk**. Often referred to as raw, used to refer to hard disk access at a raw, binary level, beneath the file system level, and using partition data at the Master boot record (MBR)