<!-- TOC titleSize:1 tabSpaces:2 depthFrom:1 depthTo:6 withLinks:1 updateOnSave:1 orderedList:0 skip:0 title:1 charForUnorderedList:* -->
# Table of Contents
- [Table of Contents](#table-of-contents)
- [Introduction](#introduction)
- [Probabilistic model](#probabilistic-model)
  - [Idea](#idea)
  - [Constructing a classifier from the probability model](#constructing-a-classifier-from-the-probability-model)
- [Parameter estimation and event models](#parameter-estimation-and-event-models)
- [Discussion](#discussion)
<!-- /TOC -->

# Introduction
**Naive Bayes classifiers**: a family of simple probabilistic classifiers based on applying Bayes's theorem with strong (naive) independence assumptions between the features
* Other name: simple Bayes, independence Bayes

**History**: studied extensively since 1960s to apply in text retrieval and remain a popular (baseline) method for text categorization with word frequencies as features

**Performance**: with appropriate pre-processing, naive Bayes is competitive in text classification domain with more advanced methods like SVM

**Advantages**:
* Highly scalable (i.e. the number of paramters linear in the number of variables) in a learning problem
* Maximum-likelihood training can be done by evaluating a closed-form expression, rather than expensive iterative approximation

**Application**:
* Text retrieval
* Text categorization
* Medical diagnosis

# Probabilistic model
## Idea
**Abstract idea**: given a problem instance to be classified, represented by a vector of $n$ independent features $x = (x_1, ..., x_n)$

$\hspace{1.0cm} \rightarrow$ Naive Bayes assigns to $x$ probabilities $P(C_k|x_1, ..., x_n)$ for each of $K$ possible outcomes (or classes) $C_k$
* Problem: if $n$ is large or if some $x_i$ can take on a large number of values

$\hspace{1.0cm} \rightarrow$ Basing the model on probability tables is infeasible

**Bayesian theorem**: $P(C_k|x) = \frac{P(C_k) P(x|C_k)}{P(x)}$

**Compute $P(C_k, x) = P(C_k) P(x|C_k)$**:
* $P(C_k, x) = P(C_k) \prod_i P(x_i|x_{i+1}, ..., x_n, C_k)$
* For simplicity, we assume that $x_1, ..., x_n$ are independent

$\hspace{1.0cm} \rightarrow P(C_k|x) = P(C_k) \prod_i P(x_i|C_k)$

**Evidence**: $Z = P(x) = \sum_k P(C_k) P(x|C_k)$

$\hspace{1.0cm} \rightarrow Z$ is a constant

## Constructing a classifier from the probability model
**Idea**: a Bayes classifier, is the function that assigns a class label $\hat{y} = C_k$ for some $k$ as follow $\hat{y} = \arg \max_k P(C_k) \prod_i P(x_i|C_k)$

# Parameter estimation and event models
**Estimate class's prior probability $P(C_k)$**:
* Equiprobable class: $P(C_k) = \frac{1}{K}$ where $K$ is the number of classes
* Inferred form training set: $P(C_k) = \frac{n_k}{N}$ 
    * $n_k$ is the number of examples in class $k$
    * $N$ is the size of the training set

**Estimate feature's distribution $P(x_i|C_k)$**:
* Idea: assume a distribution or generate non-parametric models for the features from the training set
* Some common distributions for $P(x_i|C_k)$:
    * Gaussian
    * Multinomial
    * Bernoulli
* Semi-supervised parameter estimation:
    * Assumptions:
        * $D = L \cup U$ is the dataset
            * $L$ is the set of labeled examples
            * $U$ is the set of unlabeled examples
        * $\theta$ is the paramters of the naive Bayes model
    * Training procedure: train the classifier on $L$ based on EM algorithm for MLE
        * E step: predict $P(C_k|x)$ for all examples $x$ in $D$ and each class $k$
        * M step: re-train the model parameters based on the probabilities (not the labels) predicted in the previous step (see GMM HMM system note for more detail)
        * Step 3: stop if convergence
    * Convergence: determined based on improvement to the model likelihood $D(D|\theta)$

# Discussion
**Drawback of naive Bayes**: the independence assumptions are far-reaching and inaccurate

**Advantages of naive Bayes**: