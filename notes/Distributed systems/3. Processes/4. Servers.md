<!-- TOC titleSize:1 tabSpaces:2 depthFrom:1 depthTo:6 withLinks:1 updateOnSave:1 orderedList:0 skip:0 title:1 charForUnorderedList:* -->
# Table of Contents
- [Table of Contents](#table-of-contents)
- [Servers](#servers)
  - [General design issues](#general-design-issues)
    - [Concurrent versus iterative servers](#concurrent-versus-iterative-servers)
    - [Contacting a server - end points](#contacting-a-server---end-points)
    - [Interrupting a server](#interrupting-a-server)
    - [Stateless versus stateful servers](#stateless-versus-stateful-servers)
  - [Object servers](#object-servers)
  - [Example - The Apache Web server](#example---the-apache-web-server)
  - [Server clusters](#server-clusters)
    - [Local-area clusters](#local-area-clusters)
    - [Wide-area clusters](#wide-area-clusters)
    - [Case study - PlanetLab](#case-study---planetlab)
- [Appendix](#appendix)
  - [Discussion](#discussion)
<!-- /TOC -->

# Servers
## General design issues
**Server**. A process implementing a specific service on behalf of a collection of clients

$\to$ In essence, each server is organized in the same way, i.e. it waits for an incoming request from a client and subsequently ensures that the request is served

### Concurrent versus iterative servers
**Types of servers**.
* *Iterative server*. The server handles the request and, if required, returns a response to the requesting client
* *Concurrent server*. Do not handle the request, but pass it to a separate thread or another process, after which it immediately wait for the next incoming request
    * *Examples*.
        * Multithreaded server
        * A server, which forks a new process for each new incoming request

            $\to$ This is followed in many Unix systems
    * *Consequence*. The thread or process handling the request is responsible for returning a response to the requesting client

### Contacting a server - end points
**Contact points of a server**. 
* *Idea*.
    * Clients send request to an end point, i.e. a port, at the matching hosting the server
    * Each server listens to a specific end point
* *Clients getting to know service end points*.
    * *Option 1*. Globally assign end points for well-known services

        $\to$ The client needs to find only the network address of the server machine, using name services
        * *Examples*. Port 21 for FTP requiests, and port 80 for HTTP requests
    * *Option 2*. No global end points for the service, i.e. a client needs to look up the end point

        <div style="text-align:center">
            <img src="https://i.imgur.com/lPKB2F3.png">
            <figcaption>Client-to-server binding using a daemon (a), and client-to-server binding using a superserver (b)</figcaption>
        </div>

        * *Solution*. Have a special daemon running on each machine, which runs the server, to keep track oif the current end point of each service implemented by a co-located server
            * *Daemon end point*. The daemon listens to a well-known end point

                $\to$ The client contacts the daemon, requests the end point, then contacts the specific server
    * *Option 3*. Use a superserver listening to each end point associated with a specific service
        * *Motivation*. It is common to associate an end point with a specific service, but implementing each service by means of a separate server may waste resources
            * *Example*. In a typical Unix system, it is common to have lots of servers running simultaneously, with most of them passively waiting for a client request

                $\to$ Using a superserver removes the burden of keeping track so many passive processes
        * *Example*. The `inetd` daemon in Unix listens to a number of well-known ports for Internet services
            * *Request processing*. When a request comes in, the daemon forks a process to handle it, which will exit when finished

### Interrupting a server
**Question**. Whether and how a server can be interrupted

**Option 1**. Work only too well in the current Internet, and is sometimes the only alternative
* *Idea*. The user abruptly exit the client application, which will automatically break the connection to the server, immediately restart it, and pretend nothing happened

    $\to$ The server will eventually tear down the old connection, thinking the client has probably crashed

**Option 2**. Develop the client and server, so that it is possible to send out-of-band data
* *Out-of-band data*. Data which is to be processed by the server, before any other data from the client
* *Implementation approaches*.
    * *Option 1*. Let the server listen to a separate control end point, to which the client sends out-of-band data

        $\to$ At the same time, the server listens, with a lower priority, to the end point, through which the normal data passes
    * *Option 2*. Send out-of-band data across the same connection, through which the client is sending the original request

        $\to$ When urgen data are received at the server, the original data is interrupted, through a signal in Unix system, after which it can inspect the data and handle them accordingly

### Stateless versus stateful servers
**Question**. Whether or not the server is stateless

**Stateless server**. Do not keep information on the state of the clients, and can change its own state without having to inform any client

$\to$ It merely responds to incoming requests, handle them, and forgets the client completely
* *Maintaining client information*. In many stateless designs, the server actually does maintain information on its clients

    $\to$ However, if this information is lost, it will not lead to a disruption of the server offered by the server
* *Soft state*. A particular form of a stateless design
    * *Idea*. The server promises to maintain state on behalf of the client, but only for a limited time

        $\to$ After the time has expired, the server falls back to default behavior, discarding any information it kept on account of the associated client
    * *History*. Originate from protocol design in computer networks

**Stateful server**. Generally maintain persistent information on its clients

$\to$ The information needs to be explicitly deleted by the server
* *Benefits*. Improve the performance of read and write operations as perceived by the client
* *Drawback*. If the server crashes, it has to recover its entire state, as it was just before the crash

    $\to$ Enabling recovery can introduce considerable compplexity
* *Session (temporary) state and permanent state*.
    * *Session state*. Associated with a series of operations by a single user, and should be maintained for some time, but not infinitely

        $\to$ Session state is often maintained in three-tiered client-server architectures
        * *Three-tiered client-server architectures*. The application server actually needs to access a database server, through a series of queries, before being able to respond to the requesting client
        * *State lost*. There is no real harm if session state is lost, provided that the client can simply re-issue the original request

            $\to$ This allows for simpler and less reliable storage of state
    * *Permanent state*. Information is maintained in databases

**Client state and provided service**. The choice of a stateless or stateful design should not affect the services provided by the server

**Cookie**. A server may want to keep a record on a client's behavior, so that it can more effectively respond to its requests
* *Common solution*. When the server cannot maintain state, the client sends along additional information on its previous accesses
    * *Example*. In Web, this information is often transparently stored by the client's browser in a cookie
* *Cookie*. A small piece of data containing client-specific information of interest to the server

    >**NOTE**. Cookies are never executed by a browser, they are merely stored

* *Idea*. The first time a client accesses a server, the server sends a cookie along with the requested Web pages back to the browser

    $\to$ The browser then safely tucks the cookie away
    * *Consequence*. Each subsequent time the client accesses the server, its cookie for the server is sent along with the request

## Object servers
**Object server**. Required for distributed objects
* *Differences from traditional servers*. An object server by itself does not provide a specific service

    $\to$ Specific services are implemented by the objects residing in the server
    * *Explain*. The server provides only the means to invoke local objects, based on requests from remote clients
    * *Consequence*. It is relatively easy to change services by simply adding and removing objects
* *Object server role*. A place where objects live
    * *Object structure*. Consist of two parts, i.e. data presenting its state, and the code for executing its methods

**Object invocation**. There are differences in the way an object server invokes its object
* *Examples*.
    * In a multithreaded server, each object may be assigned a separate thread
    * A separated thread may be used for each invocation request
* *Invocation procedure*. For an object to be invoked, the object server needs to know which code to execute, on which data it should operate, whether it should start a separate thread to take care of the invocation, etc.
    * *Approach 1*. Assume that all objects look alike, and there is only one way to invoke an object
        * *Drawback*. Generally inflexible and often unnecessarily constrains developers of distributed objects
    * *Approach 2*. The server supports different policies

**Different activation policies**.
* *Example 1*. A transient object, i.e. an object which exists only as long as its server exists, but possibly for a shorter period of time
    * *Usage*. Used as an in-memory, read-only copy of a file, or used as a calculator
    * *Time to live*. Created at the first invocation request, and destroyed once no clients are bound to the object
    * *Pros and cons of transient objects*. 
        * *Pros*. A transient object needs a server's resources only as long as the object is really required
        * *Cons*. An invocation may take some time to complete, since the object needs to be created first
    * *Alternative approach*. Create all transient objects at the same the server is initialized

        $\to$ The cost is resource consumption even when no client is making use of the object
* *Example 2*. Each of the objects is placed in a memory segment of its own, i.e. objects share neither code nor data
    * *Usage*. When an object implementation does not separate code and data, or for security purposes
* *Example 3*. Objects at least share their code
    * *Example*. A database containing objects belonging to the same class, which can be efficiently implemented by loading the class implementation only once into the server

        $\to$ When a request for an object invocation comes in, the server only have to fetch the object's state and execute the requested method
* *Example 4*. Threading policies, e.g.
    * *Option 1*. Implement the server with only a single thread of contorl
    * *Option 2*. The server may have several threads, one for each of its object

        $\to$ When an invocation request comes in for an object, the server passes the request to the corresponding thread's queue
        * *Benefits*. All invocations are serialized through a single thread associated with the object, automatically protecting objects against concurrent access
    * *Option 3*. Use a separate thread for each invocation request, requiring that objects should have already been protected against concurrent access

>**NOTE**. Which policy to use depends on whether threads are available, how much performance matter, and similar factors

**Activation policies**. Decisions on how to invoke an object, i.e. to emphasize that in many cases, the object must first be brought into the server's address space, i.e. activated, before it can be invoked

<div style="text-align:center">
    <img src="https://i.imgur.com/g8kdVHV.png">
    <figcaption>An object server supporting different activation policies</figcaption>
</div>

* *Object adapter, or object wrapper*. After object activation,. we need to group objects per activation policy

    $\to$ Such a mechanism is referred as object adapter, or object wrapper
    * *Object adapter*. Software implementing a specific activation policy
    * *Problem*. Object adapters come as generic components to assist developers of distributed objects, and which need only to be configured for a specific policy
* *Object adapter in object server*. 
    * An object adapter has one or more objects under its control
    * Several object adapters may reside in the same server, since a server should be able to simultaneously support objects that require different activation policies

        $\to$ When an invocation request is delivered to the server, the request is dispatched to the appropriate object adapter
* *Object adapter and object interface*. Object adapters are unware of the specific interfaces of the objects they control

    $\to$ Otherwise, they could never be generic
    * *Central issue of object adapter and object interface*. The object adapter should be able to extract an object reference from an invocation request, and subsequently dispatch the request to the referenced object, following a specific activation policy
    * *Idea*. The object adapter hands an invocation request to the server-side stub (skeleton) of the object
        * *Server-side stub of obejct*. Normally generated from the interface definitions of the object
            * *Functionality*. Unmarshall the request and invoke the appropriate method
* *Object adapter and activation policies*. 
    * An object adapter can support different activation policies by simly configuring it at runtime
    * An adapter can be configured to generate object identifiers, or to let the application provide one
    * An adapter can be configured to operate in single-threaded or multi-threaded mode
* *Object implementation*. Hidden for the object adapter, who communicates only with a skeleton

    $\to$ The actual implementation may have nothing to do with what we often see with language-level objects
    * *Servant*. The general term for a piece of code, which forms the implementation of an object

## Example - The Apache Web server
**Apache Web server**. An extremely popular server, estimated to be used to host approximately 50% of all Web sites

**Platform-independent server**. Realized by providing the server its own basic runtime environment, which is implemented for different OSes
* *Apache portable runtime (APR)*. The runtime environment, which is a library providing a platform-independent interface for file handling, networking, locking, threads, etc.
* *Consequence*. When extending Apache, portability is largely guaranteed, provided that only calls to the APR are made, and that calls to platform-specific libraries are avoided

**Apache server**. Apache can be considered as a completely general server tailored to produce a response to an incoming request

>**NOTE**. There are all kinds of hidden dependencies and assumptions, by which Apache turns out to be primarily suited for handling requests for Web documents

* *Assumptions on how coming requests should be handled*.

    <div style="text-align:center">
        <img src="https://i.imgur.com/oqoGL0a.png">
        <figcaption>The general organization of the Apache Web server</figcaption>
    </div>

* *Hook*. A placeholder for a specific group of functions
    * *Usage*. The Apache core assumes that requests are processed in a number of phases, each phase consisting of a few hooks

        $\to$ Each hook represents a group of similar actions, which need to be executed as part of processing a request
    * *Examples*.
        * A hook to translate URL to a local file name
        * A hook for writing information to a log
        * A hook of checking a client's identification
        * A hook for checking guess rights
        * A hook for checking which MIME type the request is related to
    * *Hook invocation*. The hooks are processed in a predetermined order

          $\to$ Apache enforces a specific flow of control concerning the processing of requests
* *Modules*. The functions associated with a hook are all provided by separate modules
    * *Hook modification*. In principle, a developer could change the set of hooks, which will be processed by Apache
        
        $\to$ But it is far more common to write modules containing the functions, which need to be called as part of processing the standard hook provided by unmodified Apache
        * *Explain*. Every hook can contain a set of functions, which each should match a specific function prototype
            * A module developer will write functions for specific hooks
            * When compiling Apache, the developer specifies which function should be added to which hook
    * *Relationship between modules*. There may b e tens of modules, and each hook will generally contain several functions
        * *Dependencies between modules*. Modules are considered to be mutual independent, so functions in the same hook will be executed in some arbitrary order
            * *Handling module dependencies*. Let a developer specify an ordering, in which functions from different modules should be processed

## Server clusters
### Local-area clusters
**Server cluster**. A collection of machines connected through a network, where each machine runs one or more servers

**General organization**.
* *Three-tier logical organization*. In many cases, a server cluster is logically organized into three tiers

    <div style="text-align:center">
        <img src="https://i.imgur.com/EjWjcPe.png">
        <figcaption>The genreal organization of three-tiered server cluster</figcaption>
    </div>

    * *Tier 1*. Consist of a logical switch, through which client requests are routed
        * *Examples*. 
            * Transport-layer switches accept incoming TCP connection requests, and pass requests on to one of servers in the cluster
            * A Web server accepting incoming HTTP requests, but partly passes requests to application servers for further processing, then collect results from those servers and return an HTTP response
    * *Tier 2*. Contain servers dedicated to application processing, which are typically servers running on high-performance hardware dedicated to delivering compute power
        * *Storage bottleneck*. In case of enterprise server clusters, applications need only run on relatively low-end machines, as required compute power is not the bottleneck, but access to storage is
    * *Tier 3*. Consist of data-processing servers, notably file and database servers
        * *Machine configuration*. Depending on the usage of the server cluster, these servers may be running on specialized machines, configured for high-speed disk access and having large server-side data caches
* *Two-tier logical organization*. It is frequently the case that each machine is equipped with its own local storage, often integrating application and data processing in a single server leading to a two-tiered architecture
* *Request dispatching*. When a server cluster offers multiple services, different machiens may run different application servers

    $\to$ The switch will have to be able to distinguish services, otherwise it cannot forward requests to the proper machines
    * *Consequence*. Certain machines may be temporarily idle, while others are receiving an overload of requests

        $\to$ We need to temporarily migrate services to idle machines
        * *Solution*. Use virtual machines allowing a relatively easy migration of code to real machines

**Request dispatching at the first tier**. The first tier consists of the switch, also known as the front end
* *Access transparency*. Hide the fact that there are multiple servers
    * *Explain*. Client applications running on remote machines should have no need to know anything about the internal organization of the cluster
    * *Realization*. Invariably offered by means of a single access point implemented through some kind of hardware switch, e.g. a dedicated machine
* *Server switch*. Form the entry point for the server cluster, offering a single network address
    * *Multiple access points*. For scalability and availability, a server cluster may have multiple access points, where each access point is realized by a separate dedicated machine
* *Stanard way to access a server cluster*. Set up a TCP connection, over which application-level requets are sent as part of a session

    $\to$ A session ends by tearing down the connection
    * *Transport-layer switches*. The switch accepts incoming TCP connection requests, and hands off such connection to one of the servers
    * *Switch operation cases*.
        * *Case 1*. The client sets up a TCP connection, so that all requests and responses pass through the switch

            $\to$ The switch will set up a TCP connection with a selected server and pass client requests to that server, and also accept server responses, which it will pass on to the client
            * *Consequence*. The switch sits in the middle of a TCP connection between the client and a selected server, rewriting the source and destination addresses when passing TCP segments

            >**NOTE**. This approach is a form of NAT
        * *Case 2*. The switch can actually hand off the connection to a selected server, such that all responses are directly communicated to the client, without passing through the server
            
            <div style="text-align:center">
                <img src="https://i.imgur.com/mxb1aBd.png">
                <figcaption>The principle of TCP handoff</figcaption>
            </div>

            * *Procedure*.
                1. When the switch receives a TCP connection request, it identifies the best server for handling the request, and forwards the request packet to the server
                2. The server will send an ACK back to the requesting client, inserting the switch's IP address as the source field of the header of the IP packet carrying the TCP segment
                    * *Address rewriting*. Required for the client to continue executing the TCP protocol
                        * *Explain*. The client expects an answer back from the switch, not from some arbitrary server it has never heard of before
            * *Pros and cons*.
                * *Pros*. Effective when responses are much larger than requests
                * *Cons*. Require OS-level modifications
* *Load balancing*. The switch can play an improtant role in distributing the load among the various servers
    * *Simplest load-balancing policy*. The switch can follow round robin, i.e. each time it picks the next server from its list to forward a request to

        $\to$ The switch needs to keep track to which server it handed off a TCP connection, at least until that connection is torn down
        * *Consequence*. Maintaining connection state and handing off subsequent TCP segments belonging to the same TCP connection may actually slow down the switch
    * *Advanced load-balancing policy*. If the switch can distinguish the services when a request comes in, it can take informed decisions on where to forward the request to
        * *Transport-level switches*. This server selection can take place at the transport level, provided services are distinguished by means of a port number
        * *Content-aware request distribution*. The switch inspects the payload of the incoming request

### Wide-area clusters
**WAN's problems over LAN**. Deploying clusters across a WAN is cumbersome as one had to deal with multiple adminstrative organizations, e.g. ISPs
* *Solution*. Use facilities of a single cloud provider

**Rasons for WAN clustesrs**. Provide locality, i.e. offering data and services which are close to clients

>**NOTE**. If wide-area locality is not critical, it may be suffice, or even be better, to place virtual machines in a single data center
>$\to$ Interprocess communication can benefit from low-latency local networks

**Request dispatching**. Important for wide-area locality, i.e. if a client accesses a service, its request should be forwarded to a nearby server
* *Redirection policy*. Decide which server should handle the client's request
    * *Idea*. 
        1. A client initially contact a request dispatcher, e.g. like the switch in LAN clusters, the dispatcher then estimate the latency between the cliet and several servers
        2. The dispatcher then infrom the client about the selected server
    * *Redirection mechanism*.
        * *Option 1*. The dispatcher is actually a DNS server, i.e. the client provides a domain name to a local DNS server

            $\to$ The DNS server returns an IP address of the associated services, possibly after having contacted other DNS servers
            * *Drawbacks*.
                * Rather than sending the client's IP address, the local DNS contacted by the client acts as a proxy for the client

                    $\to$ The DNS server's IP address is used to identify the location of the client, leading to a huge additional communication cost, as the local DNS server is often not that local
                * The address of the local DNS server may not even being used, and the DNS server may be fooled by the fact that the requester is another DNS server acting as an intermediate between the original client and the deciding DNS server

                    $\to$ Locality awareness has been completely loss
            * *Reasons for using DNS-based redirection*. 
                * It is relatively easy to implement and transparent to the client
                * There is no need to rely on location-aware client-side software

### Case study - PlanetLab
**PlanetLab**. A collaborative distributed system, in which different organizations each donate one or more computers, adding up to a total of hundreds of nodes

$\to$ These computers form a 1-tier server cluster, where access, procesing, and storage can take place on each node individually

**General organization**.
* *General organization*. A participating organization donates one or more nodes, i.e. computers, which are subsequently shared among all PlanetLab users

    <div style="text-align:center">
        <img src="https://i.imgur.com/e5M2AMU.png">
        <figcaption>The basic organization of a PlanetLab node</figcaption>
    </div>

    * *Important components*.
        * *Virtual machine monitor (VMM)*. An enhanced Linux OS, which mainly comprise adjustments for supporting the second component, i.e. Linux Vservers
            * *Explain*. Linux VMM ensures that Vservers are separated, i.e. processes in different Vservers are executed concurrently and independently, each making use only of the software packages and programs available in their own environment
            * *Process isolation*. The isolation between processes in different Vservers is strict

                $\to$ This ease supporting users from different organizations wanting to use PlatnetLab
        * *Vserver*. A separate environment, in which a group of processes run
* *Slices*. Support experimentations with completely different distributed systems and applications

    <div style="text-align:center">
        <img src="https://i.imgur.com/UcbGWl9.png">
        <figcaption>The principle of a PlanetLab slice</figcaption>
    </div>

    * *Slice*. A set of Vservers, each Vserver running on a different node
    
        $\to$ A slice can be thought of as a virtual server cluster, implemented by means of collection of virtual machines
    * *Node manager*. Central to managing PlanetLab resources, i.e. each node has a manager, implemented by means of a separate Vserver, whose only task is to create other Vservers on the node it manages and to control resource allocation
    * *Slice creation sevicese (SCS)*. To create a new slice, each node run a SCS, which can contact the node manager requesting it to create a Vserver and to allocate resources
        * *Remote connection to node manager*. Impossible, allowing the manager to concentrate only on local resource management

            $\to$ The SCS will not accept slice-creation requests from anybody, only specific slice authorities are eligible for requesting the creation of a slice
        * *Slice authority*. Have access rights to a collection o nodes
            * *Simplest model*. There is a single, centralized slice authority allowed to request slice creation on all nodes
            * *Practice model*. The slice authority is the one used to get a user up-and-running on PlanetLab
* *Resource specification (rspec)*. Used to keep track of resources
    * *Idea*. rsec specifies a time interval, during which certain resources have been allocated
    * *rspec identification*. Identified through a globally unique 128-bit identifier known as a resource capability (rcap)

        $\to$ Given an rcap, the node manager can look up the associated rspec in a local table
    * *Resource-slice bound*. To make use of resources, it is necessary to create a slice
    * *Service provider*. Each slice is associated with a service provider, which is an entity having an account on PlanetLab

**Vservers**. A Vserver is organized according to a contained-based approach
* *Container versus virtual machines*. Containers rely on a single, shared OS kernel

    $\to$ Resource management is mostly done only by the underlying OS, not by any of the Vservers

    <div style="text-align:center">
        <img src="https://i.imgur.com/BpKWnC7.png">
        <figcaption>Containers vs VMs</figcaption>
    </div>

    * *Primary task of Vserver*. Support a group of processes and keep that group isolated from processes running under the jurisdiction of another Vserver

        $\to$ A Vserver forms an isolated container for a group of processes and their allocated resources
    * *Other drawbacks of VMs*.
        * Each VM includes a separate OS image, which adds overhead in memory and storage footprint
        * VMs severely limits the portability of applications when public clouds, private clouds, and traditional data centers
* *Container isolation*. Technically established by the VMM, for which purpose the Linux OS has been adapted
    * *The separation of independent name spaces*. To create the illusion that a Vserver is really a single machine, the Unix process `init` traditionally always gets process ID 1, with its parent having ID 0
        * *Consequence*. If the Linux VMM will already have an `init` process running, thus it needs to create another `init` process for every Vserver, and those processes will each also get process ID 1

            $\to$ The kernel will keep track of a mapping between such a virtual process ID and the real, actually assigned process ID
    * *The separation of libraries*. Each Vserver is provided its own set of libraries, using the directory structure that those libraries expect, e.g. `/dev`, `/home`, `/proc`, etc.
        * *Implementation*. Use `chroot` command to effectively give each Vserver its own root directory

        >**NOTE**. Measures are required at kernel level to prevent unauthorized access of one Vserver to the directory tree of another Vserver
* *Container in running separate guest OSes*. Resource allocation can generally be much simpler than using VMs
    * *Explain*. It is possible to overbook resources by allowing for dynamic resource allocation, just as is done with allocating resources to normal processes
        * *Traditional VMs*. When using a guest OS, the guest will have to be allocated a fixed amount of resources in advance

            $\to$ If the nodes are required to have only few GB of main memory, then memory may be a scarce resource
            * *Consequence*. It is a must to dynamically allocate memory to VMs to be running at the same time, on a single node
    * *Memory overflow*. The Vserver hogging memory when swap space is almost filled, is reset

# Appendix
## Discussion
**The Ice runtime system**.

**Enterprise Java Beans**.

**Efficient content-ware request distribution**.

**An alternative for organizing wide-area server clusters**.