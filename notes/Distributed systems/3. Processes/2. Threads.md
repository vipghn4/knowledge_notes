<!-- TOC titleSize:1 tabSpaces:2 depthFrom:1 depthTo:6 withLinks:1 updateOnSave:1 orderedList:0 skip:0 title:1 charForUnorderedList:* -->
# Table of Contents
- [Table of Contents](#table-of-contents)
- [Threads](#threads)
  - [Introduction to threads](#introduction-to-threads)
  - [Thread usage in nondistributed systems](#thread-usage-in-nondistributed-systems)
  - [Threads in distributed systems](#threads-in-distributed-systems)
- [Appendix](#appendix)
  - [Discussion](#discussion)
<!-- /TOC -->

# Threads
**Threads**. The granularity of processes are provided by the OSes, on which distributed systems are built, is not sufficient

$\to$ Having a finer granularity in the form of multiple threads of control per process makes it much easier to build distributed applications and get better performance

## Introduction to threads
**Process**. 
* *Virtual processor*. To execute a program, an OS creates a number of virtual processors, each one for running a different program
* *Process table*. Used by the OS to keep track of virtual processors
    * *Structure*. Contain process context
    * *Process context*. A set of entries in process table, i.e. to store CPU register values, memory maps, open files, accounting information, priviledges, etc.
* *Process context*. The software analog of the hardware's processor context
    * *Processor's context*. Consist of the minimal information automatically stored by the hardware to handle interrupt, and to later return to where the CPU left off
        * *Structure*. Contain at least the program counter, and sometimes other register values, e.g. stack pointer
* *Process*. Often defined as a program in execution, i.e. a program which is currently being executed on one of the OS' virtual processors
    * *Process consistency*. The OS takes great care to ensure that independent processes cannot maliciously or inadvertently affect the correctness of each other's behavior

        $\to$ The fact that multiple processes may be concurrently sharing the same CPU and other hardware resources is made transparent

        >**NOTE**. Hardware support is usually required by the OS to enforce this separation
    
    * *Cost of concurrency transparency*. 
        * Each time a process is created, the OS must create a complete independent address space
            * *Allocation procedure*. Mean initializing memory segments by, e.g. 
                * Zeroing a data segment
                * Copying the associated program into a text segment
                * Setting up a stack for temporary data
        * Switching CPU between two processes may require some effort
        * If the OS supports more processes than it can simultaneously hold in main memory

            $\to$ Processes may have to be swapped between main memory and disk, before the actual switch can take place

**Thread**. 
* *Comparison to process*.
    * *Similarity*. A thread executes its own piece of code, independently from other threads
    * *Difference*. No attempt is made to achieve a high degree of concurrency transparency if this would result in performance degradation

        $\to$ A thread system generally maintains only the minimum information to allow a CPU to be shared by several threads
* *Thread context*. Often consist of the processor context, along with some other information for thread management

    $\to$ Information which is not strictly necessary to manage multiple threads is generally ignoted
    * *Consequence*. Protecting data against inappropriate access by threads within a process is left to developers
* *Implications of deploying threads*.
    * The performance of a multi-threaded application need hardly ever be worse than that of its single-threaded counterpart

        $\to$ In many cases, multi-threading even leads to performance gain
    * Development of multithreaded applications requires additional intellectual effort
        * *Explain*. Threads are not automatically protected against each other
    * Proper design and keeping things simple, as usual, help a lot

## Thread usage in nondistributed systems
**Benefits to multithreaded processes**.
* In a single-threaded process, whenever a blocking system call is executed, the process as a whole is blocked

    $\to$ If there is only one thread of control, computation cannot proceed while the program is waiting of input
    * *Solution*. Have at least two threads of control, i.e. one for handling interaction with the user, and one for processing other things
* It becomes posible to exploit parallelism when executing the program on a multiprocessor or multicore system
    * *Explain*. Each thread is assigned to a different CPU, or core, while shared data are stored in shared main memory

        $\to$ When properly designed, such parallelism can be transparent, i.e. the process will run equally well on a uniprocessor system
* Large applications are often developed as a collection of cooperating programs, each to be executed by a separate process
    * *Interprocess communication (IPC)*. A means to implement cooperation between programs
        * *Drawback*. In all IPC mechanisms, communication often requires relatively extensive context switching
    * *Solution*. An application can be constructed so that different parts are executed by separate threads

        $\to$ Communication are implemented by using shared data
        * *Consequence*. Thread switching can sometimes be done entirely in user space

            $\to$ The effect can be dramatic improvement in performance
* Many applications are simply easier to structure as a collection of cooperating threads
    * *Example*. Applications required to perform several tasks, which are more or less independent

**Thread implementation**. Threads are provided in the form of a thread package
* *Thread package*. Contain operations to create and destroy threads, as well as operations on synchronization variables, e.g. mutexes and condition variables
    * *Approaches of implementation*.
        * *Option 1*. Construct a thread library, which is executed entirely in user space
        * *Option 2*. Have the kernel be aware of threads and schedule them
* *User-level thread library*.
    * *Pros*.
        * It is cheap to create and destroy threads, since all thread administration is kept in the user's address space, i.e.
            * The price of creating a thread is primarily determined by the cost for allocating memory to set up a thread stack
            * Destroying a thread mainly involves freeing memory for the stack, which is cheap in this case
        * Switching thread context can often be done in a few instructions
            * *Explain*. Only the values of the CPU registers need to be stored and subsequently reloaded with the previously stored values of the thread, to which it is being switched

                $\to$ There is no need to change memory maps, flush the TLB, do CPU accounting, etc.
    * *Cons*. Suck when deploying many-to-one threading model, i.e. multiple threads are mapped to a single schedulable entity
        
        $\to$ The invocation of a blocking system call will immediately block the entire process, to which the thread belongs
        * *Many-to-one threading model*. Implementations of the many-to-one model, i.e. many user threads to one kernel thread, allow the application to create any number of threads that can execute concurrently
            * *Explain*. All threads activity is restricted to user space 
            * *Consequence*. Only one thread at a time can access the kernel, so only one schedulable entity is known to the operating system
                
                $\to$ This model provides limited concurrency and does not exploit multiprocessors
* *One-to-one threading model*. Solve the problem of user-level thread library with many-to-one threading model
    * *Idea*. Every thread is a scheduable entity
    * *Cost of implementation*. Every thread operation, e.g. creation, deletion, synchronization, etc. will have to be carried out by the kernel, requiring a system call

        $\to$ Switching thread contexts may be as expensive as switching process contexts
    * *Motivation for popularity*. The performance of context switching is generally dictated by ineffective use of memory caches, not by the distinction between the many-to-one or one-to-one threading model

        $\to$ Many OSes offer this model, if only for its simplicity

**Multiprocess versus multithread**. Using processes rather than threads has several advantages
* *Example*. The data space is separated, i.e. each process works on its own part of data, and is protected from interference from others through the OS

    $\to$ This makes programming easier, since developers do not have to worry about shared data concurrent access

## Threads in distributed systems
**Important property of threads**. Threads can provide a convenient means of allowing blocking system calls without blocking the entire process, in which the thread is running

$\to$ It is attractive of use threads in distributed systems
* *Explain*. It is much easier to express communication in the form of maintaining multiple logical connections at the same time

**Multithreaded clients**. 
* *Motivation*. To establish a high degree of distribution transparency, distributed systems operating in WAN need to conceal long interprocess messsage propagation times
    * *Explain*. The RTT in a WAN can easily be in the order of hundreds of milliseconds
    * *Solution*. The usual way to hide communication latencies is to initiate communication and immediately proceed with something else
* *Web browser example*. To fetch each element of a Web document, the browser has to set up a TCP/IP connection, read the coming data, and pass it to a display component
    
    $\to$ Setting up conncetion and reading coming data are inherently blocking operations
    * *Consequence*.Some browsers start displaying data while it is still coming in

        $\to$ The user need not wait until all the components of the entire page are fetched before the page is made available
    * *Conclusion*. The Web browser is doing a number of tasks simultaneously

        $\to$ Developing the browser as a multithreaded client simplifies matters considerably
* *Another motivation*. Several connections can be opened simultaneously for a Web browser
* *Server replicas*. When using a multithreaded client, connections may be set up to different replicas, allowing data to be transferred in parallel

    $\to$ This is possible only if the client can handle truly parallel streams of coming data

**Multithreaded servers**. The main use of multithreading in distributed systems is found at the server side
* *Benefits*. Considerably simplify the server code, and make it much easier to develop servers exploiting parallelism to attain high performance, even on uniprocessor systems

    >**NOTE**. With modern multicore processors, multithreading for parallelism is obvious

* *Scenario*. Consider the organization of a file server which occasionally has to block waiting for the disk

    $\to$ The file server normally waits for a coming request for a file operation, carries out the request, then sends back the response
* *Three ways to construct a server*.
    * *Multithreading*. Characterized by parallelism and blocking system calls
    * *Single-threaded process*. Characterized by no parallelism and blocking system calls
    * *Finite-state machine*. Characterized by parallelism and nonblocking system calls
* *Multithreading server organization*. Use dispatcher - worker model

    <div style="text-align:center">
        <img src="https://i.imgur.com/6m98dLx.png">
        <figcaption>A multithreaded server organized in dispatcher - worker model</figcaption>
    </div>

    * *Dispatcher*. Read coming requests for a file operation

        $\to$ The requests are sent by clients to a well-known end point for the server
    * *Worker thread*. After examining the request, the server chooses an idle, i.e. blocked, worker thread and hands it the request

        $\to$ The worker proceeds by performing a blocking read on the local file system, causing the thread to be suspended until the data is ready
            * *Consequence*. Another thread is selected to be executed
    * *Conclusion*. This model is great since it combines blocking system calls and parallelism
        * Blocking system calls make programming easier
        * Multiple threads allow for parallelism, thus performance improvement
* *Single threading server organization*. The main loop of the file server gets a request, examines it, and carries it out to completion before getting the next one
    * *Drawbacks*. 
        * While waiting for the disk, the server is idle and does not process any other requests
        * If the file server is running on a dedicated machine, the CPU is simply idle while the file server is waiting for the disk
    * *Consequence*. Many fewer requests per time unit can be processed

        $\to$ Threads gain considerable performance
    * *Conclusion*. Retain the ease and simplicity of blocking system calls, but may severely hinder performance
* *Single-threaded finite-state machine organization*. 
    * *Data flow*.
        1. When a request comes in, the only thread examines it
            * If it can be satisfied from the in-memory cache, then fine
            * If not, the thread must access the disk
        2. Instead of issuing a blocking disk operation, the thread schedules an asynchronous, i.e. nonblocking, disk operation, for which it will be later interrupted by the OS
            * *Explain*. The thread records the status of the request, i.e. it has a pending disk operation, and continues to see if there were any other coming requests requiring its attention
        3. Once a pending disk operation has been completed, the OS notifies the thread

            $\to$ The thread looks up the status of the associated request and continue processing it
        4. A response is then sent to the originating client, using a nonblocking call to send a message over the network
    * *Consequence*. We are simulating the behavior of multiple threads and their respective stacks the hard way
    * *Conclusion*. Achieve high performance through parallelism, but uses nonblocking calls, which is generally hard to program and maintain

**Multiprocess server**. The OS can offer more protection against accidental access to shared data
* *Drawback*. If processes need to commute a lot, there will be a noticeable adverse affect on performance

# Appendix
## Discussion
**The cost of a context switch**.

**Lightweight processes**.

**Exploiting client-side threads for performance**.