<!-- TOC titleSize:1 tabSpaces:2 depthFrom:1 depthTo:6 withLinks:1 updateOnSave:1 orderedList:0 skip:0 title:1 charForUnorderedList:* -->
# Table of Contents
- [Table of Contents](#table-of-contents)
- [Distributed event matching](#distributed-event-matching)
  - [Centralized implementations](#centralized-implementations)
- [Appendix](#appendix)
  - [Concepts](#concepts)
<!-- /TOC -->

# Distributed event matching
**Distributed event matching (or notification filtering)**. The heart of publish-subscribe systems
* *Procedure of publication and subscription*.
    1. A process specifies through a subscription $S$, in which events it is interested
    2. When a process publishes a notification $N$ on the occurrence of an event, the system needs to see if $S$ matches $N$
    3. In case of a match, the system should send the notification $N$, possibly including the data associated with the event, which took place, to the subscriber
* *Problems*. We need to facilitate at least two things, which can be separated, but this need not always be the case
    * Matching subscriptions against events
    * Notifying a subscriber in case of a match
* *Assumption*. There exists a function $\text{match}(S,N)$, which returns `true` when subscription $S$ matches the notification $N$, and `false` otherwise

## Centralized implementations
**Simple naive implementation**. Have a fully centralized server, which handles all subscription and notifications
* *Publication and subscription scheme*.
    1. A subscriber simply submits a subscription, which is subsequently stored
    2. When a publisher submits a notification, which is checked against each and every subscription
    3. When a match is found, the notification is copied and forwarded to the associated subscriber
* *Drawback*. This not not scalable, unless the server has enough processing power, and the matching can be done efficiently
* *Applications*. Used in cases, where the matching function can be implemented efficiently

    $\to$ This is often the case when dealing with topic-based filtering, i.e. matching then resort to checking for equality of attribute values
    * Linda tuple spaces, or Java Spaces
    * Used in many publish-subscribe systems running within a single department or organization
* *Simple scaling up method*. Deterministically divide the network across multiple servers
    * *Idea*. Use two functions, as explained by Baldoni et. al. (2009)
        * *`sub2node(S)`*. Take a subscription $S$ and map it to a nonempty subset of servers
        * *`not2node(N)`* . Take a notification $N$ and map it to a nonempty subset of servers
    * *Rendezvous nodes for $S$*. The servers, to which `sub2node(S)` is mapped
    * *Constraint*. For any subscription $S$ and matching notification $N$

        $$\text{sub2node}(S) \cap \text{not2node}(N)\neq \emptyset$$

        * *Explain*. There must be at least one server, which can handle the subscription when there is a matching notification
        * *Practical implementation*. Satisfied by topic-based publish-subscribe systems by using a hashing function on the names of the topics
    * *Consequence*. The idea of having a central server can be extended by distributing the matching across mutiple servers and dividing the work
        
        $\to$ The servers, generally referred to as brokers, are organized into an overlay network
* *Remaining problem*. How to route notifications to the appropriate set of subscribers
    * *Classes of algorithm*. Flooding, selective routing, and gossip-based dissemination

**Flooding solution**. A simple way to make sure that notifications reach their subscribers is to deploy broadcasting
* *Approach 1*. Store each subscription at every broker, while publishing notifications only a single broker

    $\to$ The latter will handle identifying the matching subscriptions and subsequently copy and forward the notification
* *Approach 2*. Store a subscription only at one broker, while broadcasting notifications to all brokers

    $\to$ Matching is distributed across the brokers, which may lead to a more balanced workload among the brokers

**Name-based routing**. When systems become big, flooding is not the best way to go, if even possible, instead, routing notifications across the overlay network of brokers may be necessary

$\to$ This is typically the way to go in information-centric networking, which makes use of name-based routing
* *Name-based routing*. A special case of selective notification routing
    * *Requirement*. Brokers can take routing decisions by considering the content of a notification message
        
        $\to$ It is assumed that each notification carries enough information, which can be used to cut-off routes, for which it is known that they do not lead to its subscribers
* *Assumptions*.
    * $N$ is the number of brokers, to which clients, i.e. applications, can send subscriptions and retrieve notifications, in a publish-subscribe system
* *Carzaniga et. al. 2004 proposed routing scheme*. A two-layered routing scheme, in which the lowest layer consists of a shared broadcast tree connecting the $N$ brokers

    <div style="text-align:center">
        <img src="https://i.imgur.com/c3XES11.png">
        <figcaption>Naive content-based routing</figcaption>
    </div>

    * *Tree setup*. There are various approaches, ranging from network-level multicast support, to application-level multicast trees as discussed previously
        * *Tree nodes*. The tree is set up with the $N$ brokers as end nodes, with a collection of intermediate nodes forming routers
        * *Distinction between a server and a router*. Only logical distinction, i.e. a single machine may host both kinds of processes
    * *Message broadcasting*. Every broker broadcasts its subscriptions to all other brokers

        $\to$ Every broker will be able to compile a list of `(subject, destination)` pairs
        * *Consequence*. When a process publishes a notification N, its associated broker prepends the destination brokers to that message

            $\to$ When the message reaches a router, the latter can use the list to decide on the paths, which the message should follow
* *Routing filter*. Refine the capabilities of routers for deciding where to forward notifications to
    * *Routing filters*. Each broker broadcasts its subscription across the network, so that routers can compose routing filters
    * *Example*. Consider the figure above, where node 3 subscribes to notifications, for which an attribute lies in range $[0,3]$, but node 4 wants messages with $a\in[2,5]$

        $\to$ Router $R_2$ will create a routing filter as a table, with an entry for each of its outgoing links, e.g.

        | Interface | Filter |
        | --- | --- |
        | To node 3 | $a\in[0,3]$ |
        | To node 4 | $a\in[2,5]$ |
        | To $R_1$ | Unspecified |

        * *Consequence*. The subscriptions from nodes 3 and 4 dictate that any notification with $a$ lying in $[0,3]]\cup[2,5]=[0,5]$ should be forwarded along the path to router $R_2$

            $\to$ This is precisely the information that $R_1$ will store in its table
* *Leaving nodes*. When a node leaves the system, or when it is no longer interested in specific notifications

    $\to$ It should cancel its subscription and essentially broadcast this information to all routers
    * *Routing filter adjustment*. This cancellation may lead to adjusting various routing filters
        * *Consequence*. Late adjustments will at worst lead to unnecessary traffic, as notifications may be forwarded along paths, for which there are no longer subscribers
            
            $\to$ Timely adjustments are required to keep performance at an acceptable level

**Gossiping-based event matching**. Subscribers interested in the same notifications form their own overlay network, which is constructed through gossiping
* *Publication and subscription procedures*. When a notification is published, it merely needs to be routed to the appropriate overlay

    $\to$ For the later, a random walk can be deployed
* *Alternative solution*. A publisher first joins the overlay of subscribers, before flooding its notification
* *Subscriber overlay network*. Built per topic and constitute a ring with shortcuts to facilitate efficient dissemination of a notification

# Appendix
## Concepts
**TIB / Rendezvous**. Flooding notifications is used in TIB/Rendezvous, of which the basic architecture is as following

<div style="text-align:center">
    <img src="https://i.imgur.com/wgKuaXY.png">
    <figcaption>The principle of a publish/subscribe system as implemented in TIB/Rendezvous</figcaption>
</div>

* *Notification*. A message tagged with a compound keyword describing its content, e.g. `news.comp.os.books`
* *Subscription*. A subscriber provides (part of) a keyword, or indicating the messages it wants to receive, e.g. `new.comp.*.books`

    $\to$ These keywords are said to indicate the subject of a message
* *Fundamental of implementation*. The use of broadcasting common in LANs, although it also uses more efficient communication facilities when possible
    * *Examples*. If it is known exactly where a subscriber resides, point-to-point messages will generally be used
* *Rendezvous daemon*. Each host on the network will run a rendezvous daemon, which takes care that messages are sent and delivered according to their subject
    * *Message publication*. When a message is published, it is multicast to each host on the network running a rendezvous daemon
        * *Multicast implementation*. Use facilities offered by the underlying network, e.g. IP-multicasting or hardware broadcasting
    * *Message subscription*. Process subscribing to a subject pass their subscription to their local daemon, who then constructs a table of `(process, subject)` entries
        * *Message forwarding*. When a message on subject S arrives, the daemon checks in its table for local subscribers, and forwards the message to each one

            $\to$ If there are no subscribers for S, the message is discarded immediately
* *Drawback*. Since messages are forwarded to every node anyway, potentially complex matching of published data against subscriptions can be done entirely locally without further network communication needed

**Gossiping for content-based event matching**.