<!-- TOC titleSize:1 tabSpaces:2 depthFrom:1 depthTo:6 withLinks:1 updateOnSave:1 orderedList:0 skip:0 title:1 charForUnorderedList:* -->
# Table of Contents
- [Table of Contents](#table-of-contents)
- [Distributed event matching](#distributed-event-matching)
  - [Centralized implementations](#centralized-implementations)
- [Appendix](#appendix)
  - [Concepts](#concepts)
<!-- /TOC -->

# Distributed event matching
**Distributed event matching (or notification filtering)**. The heart of publish-subscribe systems
* *Procedure of publication and subscription*.
    1. A process specifies through a subscription $S$, in which events it is interested
    2. When a process publishes a notification $N$ on the occurrence of an event, the system needs to see if $S$ matches $N$
    3. In case of a match, the system should send the notification $N$, possibly including the data associated with the event, which took place, to the subscriber
* *Problems*. We need to facilitate at least two things, which can be separated, but this need not always be the case
    * Matching subscriptions against events
    * Notifying a subscriber in case of a match
* *Assumption*. There exists a function $\text{match}(S,N)$, which returns `true` when subscription $S$ matches the notification $N$, and `false` otherwise

## Centralized implementations
**Simple naive implementation**. Have a fully centralized server, which handles all subscription and notifications
* *Publication and subscription scheme*.
    1. A subscriber simply submits a subscription, which is subsequently stored
    2. When a publisher submits a notification, which is checked against each and every subscription
    3. When a match is found, the notification is copied and forwarded to the associated subscriber
* *Drawback*. This not not scalable, unless the server has enough processing power, and the matching can be done efficiently
* *Applications*. Used in cases, where the matching function can be implemented efficiently

    $\to$ This is often the case when dealing with topic-based filtering, i.e. matching then resort to checking for equality of attribute values
    * Linda tuple spaces, or Java Spaces
    * Used in many publish-subscribe systems running within a single department or organization
* *Simple scaling up method*. Deterministically divide the network across multiple servers
    * *Idea*. Use two functions, as explained by Baldoni et. al. (2009)
        * *`sub2node(S)`*. Take a subscription $S$ and map it to a nonempty subset of servers
        * *`not2node(N)`* . Take a notification $N$ and map it to a nonempty subset of servers
    * *Rendezvous nodes for $S$*. The servers, to which `sub2node(S)` is mapped
    * *Constraint*. For any subscription $S$ and matching notification $N$

        $$\text{sub2node}(S) \cap \text{not2node}(N)\neq \emptyset$$

        * *Explain*. There must be at least one server, which can handle the subscription when there is a matching notification
        * *Practical implementation*. Satisfied by topic-based publish-subscribe systems by using a hashing function on the names of the topics
    * *Consequence*. The idea of having a central server can be extended by distributing the matching across mutiple servers and dividing the work
        
        $\to$ The servers, generally referred to as brokers, are organized into an overlay network
* *Remaining problem*. How to route notifications to the appropriate set of subscribers
    * *Classes of algorithm*. Flooding, selective routing, and gossip-based dissemination

**Flooding solution**. A simple way to make sure that notifications reach their subscribers is to deploy broadcasting
* *Approach 1*. Store each subscription at every broker, while publishing notifications only a single broker

    $\to$ The latter will handle identifying the matching subscriptions and subsequently copy and forward the notification
* *Approach 2*. Store a subscription only at one broker, while broadcasting notifications to all brokers

    $\to$ Matching is distributed across the brokers, which may lead to a more balanced workload among the brokers

# Appendix
## Concepts
**TIB / Rendezvous**. Flooding notifications is used in TIB/Rendezvous, of which the basic architecture is as following

<div style="text-align:center">
    <img src="https://i.imgur.com/wgKuaXY.png">
    <figcaption>The principle of a publish/subscribe system as implemented in TIB/Rendezvous</figcaption>
</div>

* *Notification*. A message tagged with a compound keyword describing its content, e.g. `news.comp.os.books`
* *Subscription*. A subscriber provides (part of) a keyword, or indicating the messages it wants to receive, e.g. `new.comp.*.books`

    $\to$ These keywords are said to indicate the subject of a message
* *Fundamental of implementation*. The use of broadcasting common in LANs, although it also uses more efficient communication facilities when possible
    * *Examples*. If it is known exactly where a subscriber resides, point-to-point messages will generally be used
* *Rendezvous daemon*. Each host on the network will run a rendezvous daemon, which takes care that messages are sent and delivered according to their subject
    * *Message publication*. When a message is published, it is multicast to each host on the network running a rendezvous daemon
        * *Multicast implementation*. Use facilities offered by the underlying network, e.g. IP-multicasting or hardware broadcasting
    * *Message subscription*. Process subscribing to a subject pass their subscription to their local daemon, who then constructs a table of `(process, subject)` entries
        * *Message forwarding*. When a message on subject S arrives, the daemon checks in its table for local subscribers, and forwards the message to each one

            $\to$ If there are no subscribers for S, the message is discarded immediately
* *Drawback*. Since messages are forwarded to every node anyway, potentially complex matching of published data against subscriptions can be done entirely locally without further network communication needed

**Gossiping for content-based event matching**.