<!-- TOC titleSize:1 tabSpaces:2 depthFrom:1 depthTo:6 withLinks:1 updateOnSave:1 orderedList:0 skip:0 title:1 charForUnorderedList:* -->
# Table of Contents
- [Table of Contents](#table-of-contents)
- [Process resilience](#process-resilience)
  - [Resilience by process groups](#resilience-by-process-groups)
    - [Group organization](#group-organization)
    - [Membership management](#membership-management)
  - [Failure masking and replication](#failure-masking-and-replication)
  - [Consensus in faulty systems with crash failures](#consensus-in-faulty-systems-with-crash-failures)
  - [Example - Paxos](#example---paxos)
    - [Essential Paxos](#essential-paxos)
      - [Phase 1](#phase-1)
      - [Phase 2](#phase-2)
    - [Understanding Paxos](#understanding-paxos)
      - [System overview](#system-overview)
      - [System crash handling](#system-crash-handling)
  - [Consensus in faulty systems with arbitrary failures](#consensus-in-faulty-systems-with-arbitrary-failures)
    - [Why having $3k$ processes is not enough](#why-having-3k-processes-is-not-enough)
    - [Why having $3k+1$ processes is enough](#why-having-3k1-processes-is-enough)
    - [Example - Practical Byzantine fault tolerance (PBFT)](#example---practical-byzantine-fault-tolerance-pbft)
  - [Some limitations on realizing fault tolerance](#some-limitations-on-realizing-fault-tolerance)
    - [On reaching consensus](#on-reaching-consensus)
    - [Consistency, availability, and partitioning](#consistency-availability-and-partitioning)
  - [Failure detection](#failure-detection)
<!-- /TOC -->

# Process resilience
**Brief**. This section concentrates on the protection against process failures, which is achieved by replicating processes into groups

## Resilience by process groups
**Key approach to tolerating a faulty process**. Organize several identical
processes into a group
* *Key property of groups*. When a message is sent to the group itself, all members of the group receive it
    * *Consequence*. If one process in a group fails, hopefully some other process can take over for it
* *Dynamic process group*. Process groups may be dynamic, i.e. new groups can be created and old groups can be destroyed
    
    $\to$ A process can join a group or leave one during system operation
* *Group membership*. A process can be a member of several groups at the same time
  
  $\to$ Mechanisms are needed for managing groups and group membership
* *Purposes of groups*. Allow a process to deal with collections of other processes as a single abstraction
    * *Explain*. A process $P$ can send a message to a group $\mathbf{Q} = \{Q_1,\dots,Q_n\}$ of servers without having to know who they are, how many there are, or where they are, which may change from one call to the next
      
        $\to$ To $P$, the group $Q$ appears to be a single, logical process

### Group organization
**Brief**. An important distinction between different groups has to do with their internal structure

**Types of group structure**.
* *Flat group*. All processes are equal, i.e. here is no distinctive leader and all decisions are made collectively
    * *Pros*. The flat group is symmetrical and has no single point of failure
        
        $\to$ If one of the processes crashes, the group simply becomes smaller, but can continue
    * *Cons*. Decision making is more complicated
        * *Example*. To decide anything, a vote often has to be taken, incurring some delay and overhead
* *Hierarchical group*. Some kind of hierarchy exists, e.g. one process is the coordinator and all the others are workers
    * *Request handling*. 
        1. When a request for work is generated, either by an external client or by one of the workers
            
            $\to$ It is sent to the coordinator
        2. The coordinator decides which worker is best suited to carry it out, and forwards it there
   
   >**NOTE**. More complex hierarchies are also possible

    * *Pros*. As long as the coordinator is running, it can make decisions without bothering everyone else
    * *Cons*. Loss of the coordinator brings the entire group to a grinding halt
        * *Solution*. When the coordinator in a hierarchical group fails
            
            $\to$ Its role will need to be taken over and one of the workers is elected as new coordinator

### Membership management
**Problem of interest**. When group communication is present, some method is needed for
* Creating and deleting groups, and
* Allowing processes to join and leave groups

**Centralized approach**. Have a group server to which all these requests can be sent
* *Group server*. Maintain a complete database of all the groups and their exact membership
* *Pros*. Straightforward, efficient, and fairly easy to implement
* *Cons*. There is only a single point of failure, i.e. if the group server crashes
    
    $\to$ Group management ceases to exist
    * *Consequence*. Probably most or all groups will have to be reconstructed from scratch, possibly terminating existing work

**Decentralized approach**. Manage group membership in a distributed way
* *Example*. If reliable multicasting is available
    * *Joining group*. An outsider can send a message to all group members announcing its wish to join the group
    * *Leaving group*. A member sends a goodbye message to everyone
* *Discrimination between process crash and process leaving*. If fail-stop failure semantics is generally not appropriate
    
    $\to$ There is no polite announcement that a process crashes as there is when a process leaves voluntarily
    * *Solution*. The other members have to discover this experimentally by noticing that the crashed member no longer responds to anything
        
        $\to$ Once it is certain that the crashed member is really down, it can be removed from the group
* *Leaving and joining synchronization*. Leaving and joining have to be synchronous with data messages being sent
    * *Explain*. 
        * Starting at the instant that a process has joined a group, 
            
            $\to$ It must receive all messages sent to that group
        * As soon as a process has left a group
            
            $\to$ It must not receive any more messages from the group, and the other members must not receive any more messages from it
    * *Solution*. Convert the joining/leaving operation into a sequence of messages sent to the whole group
* *Group reconstruction after massive failures*. What to do if so many processes go down that the group can no longer function at all
    
    $\to$ Some protocol is needed to rebuild the group
    * *Consequence*. Invariably, some process will have to take the initiative to start the ball rolling
        
        $\to$ What happens if two or three try at the same time
    * *Solution*. The protocol must to be able to withstand this, e.g. a leader-election algorithm may be needed

## Failure masking and replication
**Process groups**. Part of the solution for building fault-tolerant systems
* *Idea*. Having a group of identical processes allows us to mask one or more faulty processes in that group
    
    $\to$ We can replicate processes and organize them into a group to replace a single vulnerable process with a fault tolerant group
    * *Approaches for replication*. Use primary-based protocols, or replicated-write protocols
* *Primary-based replication*. Generally appears in the form of a primary-backup protocol
    * *Idea*. A group of processes is organized in a hierarchical fashion, in which a primary coordinates all write operations
    * *Primary node*. In practice, the primary is fixed, although its role can be taken over by one of the backups, if need be
        
        $\to$ When the primary crashes, the backups execute some election algorithm to choose a new primary
* *Replicated-write protocols*. Used in the form of active replication, as well as by means of quorum-based protocols
    * *Idea*. Organize a collection of identical processes into a flat group
    * *Pros*. Such groups have no single point of failure
    * *Cons*. The cost of distributed coordination

**Degree of replication**. An important issue with using process groups to tolerate faults is how much replication is needed

>**NOTE**. To simplify our discussion, we consider only replicated-write systems

* *$k$-fault tolerant system*. A system, which can survive faults in $k$ components and still meet its specifications
    * If the components fail silently, then having $k + 1$ of them is enough to provide $k$-fault tolerance
        * *Explain*. If $k$ components stop, then the answer from the other one can be used
    * If components exhibit arbitrary failures, continuing to run when faulty and sending out erroneous or random replies
        
        $\to$ A minimum of $2k + 1$ processes are needed to achieve $k$-fault tolerance, i.e. for major voting
        * *Explain*. Even when the $k$ failing processes could accidentally, or even intentionally, generate the same reply
            
            $\to$ The remaining k + 1 will also produce the same answer, so the client or voter can just believe the majority

**Problem**. Consider a $k$-fault tolerant group a single process fails, if more than $k$ members fail

$\to$ All bets are off and whatever the group does, its results, if any, cannot be trusted
* *Conclusion*. The process group, in its appearance of mimicking the behavior of a single, robust process, has failed

## Consensus in faulty systems with crash failures
**Consensus in faulty systems with crash failures**. Consider a potentially very large collection of clients sending commands to a group of processes, which jointly behave as a single, highly robust process

$\to$ To make this work, we need to make an important assumption
* *Assumption of interest*. In a fault-tolerant process group, each nonfaulty process executes the same commands, in the same order, as every other nonfaulty process
    * *Explain*. After the execution of all users' requests, the state of all processes are the same
* *Consequence*. The group members need to reach consensus on which command to execute
    * If failures cannot happen, reaching consensus is easy
        * *Example*. We can
            * Use Lamport's totally ordered multicasting, or
            * Use a centralized sequencer handing out a sequence number to each command, which needs to be executed
    * If failures can happen, reaching consensus among a group of processes is tricky

**Flooding consensus**. An algorithm for reaching consensus between processes
* *Assumptions*. 
    * $P = \{P_1,\dots,P_n\}$ is a group of processes operating under fail-stop failure semantics
        
        $\to$ Crash failures can be reliably detected among the group members
    * Every group member maintains a list of proposed commands received directly from clients, or from its fellow group members
    * A client contacts a group member requesting it to execute a command
* *Idea*. Conceptually the algorithm operates in rounds
    * *In each round, a process $P_j$ sends its list of proposed commands it has seen so far to every other process in $P$
    * At the end of a round, each process merges all received proposed commands into a new list
        
        $\to$ Given the new list, it deterministically selects the command to execute, if possible

        >**NOTE**. The selection algorithm must be the same for all processes

* *Correctness*. This approach works as long as processes do not fail
    * *Explain*. At the end of each round, all process have the same set of proposed commands

        $\to$ By applying the same selection algorithm, the command selected for execution will be the same across processes

**Failure handling in flooding consensus**. Assume that some $P_j$ detects, during round $r$, say $P_k$ has crashed 

<div style="text-align:center">
    <img src="https://i.imgur.com/EoW4vYk.png">
    <figcaption>Reaching consensus through flooding in the presence of crash failures</figcaption>
</div>

* *Assumption*. To make this concrete, we simplify the problem by assuming that
    * $P$ has four processes $\{P_1,\dots,P_4\}$
    * $P_1$ crashes during round $r$
    * $P_2$ receives the list of proposed commands from $P_1$ before it crashes
    * $P_3$ and $P_4$ do not receive the list of proposed commands from $P_1$ before it crashes
    * All processes knew who was group member at the beginning of round $r$
* *Round increment*. A process will decide to move to a next round when it has received a message from every nonfaulty process
    * *Consequence*. This assumes that each process can reliably detect the crashing of another process
        
        $\to$ Otherwise the process would not be able to decide who the nonfaulty processes are
* *Failure handling flow*.
    * *Round $r$*. 
        * $P_2$ is ready to make a decision on which command to execute when it receives the lists of the other members
            
            $\to$ It has all commands proposed so far
        * From $P_3$'s perspective, if there is another process that did receive $P_1$'s proposed commands
            
            $\to$ That process may then make a different decision than itself
            * *Solution*. $P_3$ postpones its decision until the next round
            
            >**NOTE**. The same holds for $P_4$
    
    * *Round $r+1$*. 
        1. Since $P_2$ received all commands, it can indeed make a decision and can subsequently broadcast that decision to the others
        2. $P_3$ and $P_4$ will then be able to make a decision, i.e. they decide to execute the same command selected by $P_2$

**Correctness of flooding consensus**.
* *Correctness under reliable failure detection*. A process will move to a next round without having made a decision, only when it detects that another process has failed
    
    $\to$ In the worst case at most one nonfaulty process remains, and this process can simply decide whatever proposed command to execute
    
    >**NOTE**. Reliable failure detection is assumed in this case

* *Correctness with packet loss*. If the decision by $P_2$ sent to $P_3$ was lost
    
    $\to$ $P_3$ can still not make a decision, hence, we need to make sure that it makes the same decision as $P_2$ and $P_4$
    * If $P_2$ did not crash, a retransmission of its decision will save the day
    * If $P_2$ did crash, this will be also detected by $P_4$, who will subsequently rebroadcast its decision
        
        $\to$ Meanwhile, $P_3$ has moved to a next round, and after receiving the decision by $P_4$, will terminate its execution of the algorithm

## Example - Paxos
**Realistic assumption of failure detection**. The flooding-based consensus algorithm is not very realistic, since it relies on a fail-stop failure model
* *More realistic assumption*. A fail-noisy failure model, in which a process will eventually reliably detect that another process has crashed

**Paxos**. A widely adopted consensus algorithm originally published in 1989 as a technical report by Leslie Lamport

>**NOTE**. It took about a decade before someone decided that it may not be such a bad idea to disseminate it through a regular scientific channel

### Essential Paxos
**Paxos' assumptions**. Weaker than the assumptions of flooding consensus
* The distributed system is partially synchronous, or it may even be asynchronous
* Communication between processes may be unreliable, meaning that messages may be lost, duplicated, or reordered
* Corrupted messages can be detected, hence  subsequently ignored
* All operations are deterministic, i.e. once an execution is started, it is known exactly what it will do
* Processes may exhibit crash failures, but not arbitrary failures, nor do processes collude

>**NOTE**. By-and-large, these are realistic assumptions for many practical distributed systems

**Modeling**.
* *Logical processes*. The algorithm operates as a network of logical processes, of which there are different types

    <div style="text-align:center">
        <img src="https://i.imgur.com/bkG20h3.png">
        <figcaption>The organization of Paxos into different logical processes</figcaption>
    </div>

    * *Clients*. There are clients requesting a specific operation to be executed
    * *Proposers*. At the server side, each client is represented by a proposer, which attempts to have a client's request accepted
        * *Leader*. A proposer is designated as being the leader, and drives the protocol toward reaching consensus
    * *Acceptors*. A proposed operation is accepted by an acceptor
        
        $\to$ If a majority of acceptors accepts the same proposal, the proposal is said to be chosen
    * *Learners*. What is chosen still needs to be learned, i.e. informed, hence we will have a number of learner processes
        
        $\to$ Each learner process will execute a chosen proposal once it has been informed by a majority of acceptors
* *Physical processes*. A single proposer, acceptor, and learner form a physical process, running on a machine, with which the client communicates
    * *Fault tolerance*. If a proposer crashes, then the physical process that it is part of will have crashed
        
        $\to$ By replicating this server we aim at obtaining fault tolerance in the presence of crash failures
* *Basic operating model*.
    1. The leading proposer receives requests from clients, one at a time
    2. A nonleading proposer forwards any client request to the leader
    3. The leading proposer sends its proposal to all acceptors, telling each to accept the requested operation
    4. Each acceptor will subsequently broadcast a learn message
    5. If a learner receives the same learn message from a majority of acceptors
        
        $\to$ It knows that consensus has been reached on which operation to execute, and will execute it

**Issues of the basic operating model**.
* *Execution order problem*. Not only do the servers need to reach consensus on which operation to execute
    
    $\to$ Each of the servers must actually executes it
    * *Question of interest*. How to ensure that a majority of the nonfaulty servers will carry out the operation?
    * *Solution*. Have learn messages be retransmitted
        * *Drawback*. An acceptor have to log its decisions, in turn requiring a mechanism for purging logs
    * *Correctness*. Since we are assuming globally ordered proposal timestamps
        
        $\to$ Missing messages can be easily detected
        * *Consequence*. Accepted operations will always be executed in the same order by all learners
* *Failing leader problem*.
    * *Response to clients*. This client repsonding mechanism leads to the problem of interest
        * The server hosting the leading proposer will inform the client when its requested operation has been executed 
        * If another process had taken over the lead, it will also handle the response to the client
    * *Failing leader problem*. If the failure of a leader can be reliably detected
        1. A new leader would be elected
        2. The recovering leader would instantly notice that the world around it had changed
    * *Solution*. Paxos has been designed to tolerate proposers who still believe they are in the lead, i.e. solve the second problem
        * *Idea*. Proposals may be sent out concurrently by different proposers, each believing to be the leader
            
            $\to$ These proposals can be distinguished from one another 
            * *Explain*. To ensure that the acceptors handle only the proposals from the current leader
        * *Leader-election algorithm*. Relying on a leading proposer, given that proposals can be sent from any proposer
            
            $\to$ There must be a leader-election algorithm
            
            >**NOTE**. In principle, that algorithm can operate independently from Paxos, but is normally part of it

**Distinguishing concurrent proposals from different proposers**.
* *Assumptions*.
    * Each proposal $p$ has a uniquely associated logical timestamp $\text{ts}(p)$

        >**NOTE**. How uniqueness is achieved is left to an implementation
        >
        >$\to$ We will describe some of the details shortly

    * $\text{oper}(p)$ is the operation associated with proposal $p$
* *Objective*. Only proposals from the current leader should be accepted by the acceptors
* *Key idea*. Allow multiple proposals to be accepted, but each of these accepted proposals has the same associated operation
    * *Explain*. This can be achieved by guaranteeing that if a proposal $p$ is chosen
        
        $\to$ Any proposal with a higher timestamp will also have the same associated operation
    * *Formal*. We require that
        
        $$p \text{ is chosen } \implies \forall p'\in\{p':\text{ts}(p') > \text{ts}(p)\}, \text{oper}(p') = \text{oper}(p)$$

    * *Observation*. For $p$ to be chosen, it needs to be accepted, hence we must guarantee that if $p$ is chosen
        
        $\to$ Any higher-timestamped proposal accepted by any acceptor has the same associated operation as $p$
* *Problem*. At a certain moment, a proposer may send a new proposal $p'$, with the highest timestamp so far, to an acceptor $A$, which had not received any proposal before

    $\to$ In absence of any other proposals, $A$ will simply accept $p'$
    * *Explain*. This may happen due to message loss and multiple proposers each believing to be in the lead
    * *Solution*. We need to guarantee that if proposal $p$ is chosen
        
        $\to$ Any higher-timestamped proposal issued by a proposer, has the same associated operation as $p$
    * *Consequence*. A proposer may need to adopt an operation coming from acceptors in favor of its own
        * *Explain*. This will happen after a leading proposer had failed, but its proposed operation had already made it to a majority of the acceptors
* *Safety and liveness*.
    * *Safety*. The processes collectively formally ensure safety
        * *Safety property*. 
            * Only proposed operations will be learned, i.e. the chosen operation was proposed by some node
            * At most one operation will be learned at a time, i.e. no two distinct learners can learn different operations
        * *Conclusion*. A safety property asserts that nothing bad will happen
    * *Liveness*. Paxos ensures conditional liveness, i.e. if enough processes remain up-and-running
        
        $\to$ A proposed operation will eventually be learned, and executed
        * *Conclusion*. Liveness ensures that eventually something good will happen
        
        >**NOTE**. Liveness is not guaranteed in Paxos, unless some adaptations are made

**Brief procedure of Paxos**. There are two phases, each in turn consisting of two subphases
* *First phase*. The leading proposer interacts with acceptors to get a requested operation accepted for execution
    
    $\to$ This is needed to rule out any trouble caused by different proposers, each believing they are the leader
    * *Best scenario*. An individual acceptor promises to consider the proposer's operation and ignore other requests
    * *Worst scenario*. The proposer was too late and it will be asked to adopt some other proposer's request instead
        
        $\to$ A leadership change had taken place and there may be former requests that need to handled first
* *Second phase*. The acceptors will have informed proposers about the promises they have made
    
    $\to$ The leading proposer promotes a single operation to the one to be executed, and subsequently tells the acceptors

#### Phase 1
**Phase 1a - Prepare**. 
* *Idea*. 
    1. A proposer $P$, who believes it is leader and is proposing operation $o$, tries to get its proposal timestamp anchored
    2. $P$ then broadcasts its proposal to the acceptors
* *Procedure*. 
    1. $P$ selects a proposal number $m$ higher than any of its previously selected numbers
        
        $\to$ This leads to a proposal timestamp $t=(m,i)$, where $i$ is the numerical process identifier of $P$, so that
        
        $$(m,i) < (n,j) \Leftrightarrow (m < n) \lor (m = n \land i < j)$$

        >**NOTE**. This timestamp for a proposal $p$ is an implementation of the previously mentioned timestamp $\text{ts}(p)$
        
    2. Proposer $P$ sends $\text{prepare}(t)$ to all acceptors, noting that messages may be lost
        
        $\to$ By sending the message, $P$ is asking the acceptors to 
        * Promise not to accept any future proposals with a lower proposal timestamp
        * Inform $P$ about an accepted proposal, if any, with the highest timestamp less than $t$
          
            $\to$ If such a proposal exists, $P$ will adopt it

**Phase 1b - Promise**. An acceptor A may receive multiple proposals.

$\to$ Assume it receives $\text{prepare}(t)$ from $P$, there are three cases to consider
* *Case 1*. $t$ is the highest proposal timestamp received from any proposer so far
    
    $\to$ $A$ will return $\text{promise}(t)$ to $P$ stating that $A$ will ignore any future proposals with a lower timestamp
* *Case 2*. If $t$ is the highest timestamp so far, but another proposal $(t', o')$ had already been accepted
    
    $\to$ $A$ also returns $(t',o')$ to P, allowing $P$ to decide on the final operation to be accepted
* *Case 3*. In all other cases, do nothing, i.e. there is apparently another proposal with a higher timestamp that is being processed

**Termination of phase 1**. Once the first phase has been completed, the leading proposer $P$ knows what the acceptors have promised

$\to$ Essentially, the leading proposer knows that all acceptors have agreed on the same operation
* *Consequence*. This will put $P$ into a position to tell the acceptors that they can go ahead
    
    $\to$ This is needed, since although the leading proposer knows on which operation consensus has been reached, this consensus is not known to the others
    
>**NOTE**. We assume that P received a response from a majority of acceptors, whose respective responses may be different

#### Phase 2
**Phase 2a - Accept**. There are two cases to consider
* *Case 1*. $P$ does not receive any accepted operation from any of the acceptors
    
    $\to$ It will forward its own proposal for acceptance by sending $\text{accept}(t,o)$ to all acceptors
    * *Explain*. 
        * By not receiving any accepted operation from any acceptors

            $\to$ $P$ believes that its proposed operation is valid and should be executed, by sending $\text{accept}(t,o)$
        * In case $(t,o)$ is not valid, i.e. some $(t',o')$ with $t'>t$ has been proposed previously

            $\to$ $\text{accept(t,o)}$ will be ignored in step 2b, causing no conflict
* *Case 2*. $P$ was informed about another operation $o'$, which it will adopt and forward for acceptance
    
    $\to$ It does so by sending $\text{accept}(t,o')$, where 
    * $t$ is $P$'s proposal timestamp
    * $o'$ is the operation with proposal timestamp highest among all accepted operations returned by the acceptors in Phase 1b

**Phase 2b - Learn**. If an acceptor receives $\text{accept}(t,o')$, but did not previously send a promise with a higher proposal timestamp
1. It will accept operation $o'$, and tell all learners to execute $o'$ by sending $\text{learn}(o')$
    
    $\to$ At that point, the acceptor can forget about $o'$
2. A learner $L$ receiving $\text{learn}(o')$ from a majority of acceptors, will execute the operation $o'$

    $\to$ We now also know that a majority of learners share the same idea on which operation to execute

**Paxos essence**. This description of Paxos indeed captures only its
essence, i.e. using a leading proposer to drive the acceptors toward the execution of the same operation

$\to$ When it comes to practical implementations, much more needs to be done

### Understanding Paxos
**Brief**. To properly understand Paxos, but also many other consensus algorithms

$\to$ It is useful to see how its design could have evolved
* *Explain*. "Could have" means that the evolution of the algorithm has never been documented

>**NOTE**. The following description is largely based on work described by Meling and Jehl [2013]

#### System overview
**System overview**. Consider a server that we wish to make more robust
* *Idea*. Use replication and making sure that all commands submitted by clients are executed by all servers in the same order
* *Simplest situation*. Add one server, creating a group of two processes $S_1$ and $S_2$.  
    * *Synchronization of execution order*. To make sure that all commands are executed in the same order
        
        $\to$ One process is appointed to be a sequencer, which increments and associates a unique timestamp with every submitted command
    * *Consequence*. Servers are required to execute commands according to their timestamp
* *Primary and backup server*. 
    * *Primary server (or leader)*. The process appointed to be a sequencer
    * *Backup server*. The other server
* *Client request submission*. A client broadcasts its requested command to all servers

    $\to$ If a server notices it is missing a command, it can rely on the other server to forward it when necessary

    >**NOTE**. All commands are assumed to be stored at the servers, and we only need to make sure that the servers agree on which command to execute next

    * *Consequence*. All remaining communication between servers consists of control messages

**Example**. Consider the situation sketched in the figure below

<div style="text-align:center">
    <img src="https://i.imgur.com/2chh8Ou.png">
    <figcaption>Two clients communicating with a two-server process group</figcaption>
</div>

* *Scenario*.
    * Server $S_1$ is the leader handing out time-stamps to submitted requests
    * Client $C_1$ has submitted command $o_1$, while $C_2$ submitted $o_2$
* *Command execution*.
    1. $S_1$ instructs $S_2$ to execute operation $o_2$ with timestamp $1$, and later operation $o_1$ with timestamp $2$
    2. After processing a command, a server will return the result to the associated client
* *Notation*. $\langle\sigma_i^j\rangle$, where $i$ is the index of the reporting server, and $j$ the state it was in, expressed as the sequence of operations it has carried out
    * *Example*. Client $C_1$ will thus see the result $\langle\sigma_k^{21}\rangle$
        
        $\to$ Each server has executed $o_1$ after executing $o_2$

**Packet loss handling**. When a leader associates a timestamp with an operation
    
$\to$ It does so by sending an accept message to the other servers
* *Packet loss handling*. As we assume that messages may be lost
    1. A server accepting an operation $o$ does so by telling the leader it has learned the operation by returning a $\text{learn}(o)$ message

        $\to$ $\text{learn}(o)$ acts as an ACK message returned to the leader
    2. When the leader does not notice that operation $o$ has been learned
        
        $\to$ It simply retransmits an $\text{accept}(o, t)$ message, with $t$ being the original timestamp

>**NOTE**. In our description, we are skipping the phase of coming to agreement on the operation to be carried out
>
>$\to$ We assume the leader has decided and now needs to reach consensus on executing that operation

#### System crash handling
**System crash handling with reliable failure detection**. Assuming that a crash can be reliably detected

<div style="text-align:center">
    <img src="https://i.imgur.com/TYAUM03.png">
    <figcaption>Consequences of leader crashes in combination with a lost accept, and the solution</figcaption>
</div>

* *Issue*. Server $S_2$ will never have learned about operation $o_1$
* *Solution*. Demand that a server may execute an operation only if it knows that the other server has learned the operation

    $\to$ A server can only execute an operation only if it knows for sure that all other servers will execute the same operation 
* *Conclusion*. A server $S$ cannot execute an operation $o$ until it has received a $\text{learn}(o)$ from all other nonfaulty servers

**System crash detection in practice**.
* *A standard approach toward failure detection*. Set a timeout on expected messages
    * *Example*. 
        * Each server is required to send a message declaring it is still alive
        * At the same time the other servers set timeouts on the expected receipt of such messages
    * *Idea*. If a timeout expires, the sender is suspected to have failed
    
    >**NOTE**. In a partially synchronous or fully asynchronous system, there is essentially no other solution

* *Consequence*. A failure may be falsely detected, as the delivery of such `I'm alive` messages may have simply been delayed or lost

**Overview of system crash handling with unreliable failure detection**.
* *Scenario*. 
    * Paxos has realized a failure detection mechanism
    * $S_1$ and $S_2$ falsely conclude that the other has failed
* *Issue*. Each server may independently decide to execute their operation of choice, leading to divergent behavior
* *Solution*. Introduce an extra server, and demand that a server can execute an operation only if it is certain that a majority will execute that operation
    * *Example*. In case of three servers, execution of operation $o$ by server $S$ can take place as soon as $S$ has received at least one other $\text{learn}(o)$ message
        
        $\to$ Together with the sender of that message, $S$ will form a majority
* *Conclusion*. It is clear that Paxos requires at least three replicated servers $\{S_1, S_2, S_3\}$ to operate correctly

**System crash handling with unreliable failure detection and three servers**. Consider when one of the three servers crashes
* *Assumptions*.
    * $S_1$ is the initial leader
    * A server can reliably detect it has missed a message
    * When a new leader needs to be elected, the remaining servers follow a strictly deterministic algorithm
        * *Example*. If $S_{k}$ crashes, then $S_{k+1}$ will become leader
    * Clients may receive duplicate responses, and besides being required to recognize duplicates, form no further part of the Paxos protocol
        * *Explain*. A client cannot be asked to help the servers to resolve a situation
* *Missing message detection*. Use timestamps in the form of strictly increasing sequence numbers
    * *Idea*. Whenever a server notices it has missed a message
        
        $\to$ It can then simply request a retransmission and catch up before continuing
* *Correctness of Paxos*. It is fairly easy to see that no matter when one of $S_2$ or $S_3$ crashes
    
    $\to$ Paxos will behave correctly

    >**NOTE**. We are still demanding that execution of an operation can take place only if a server knows that a majority will execute that operation

* *Leader failure after operation execution*. Suppose $S_1$ crashes after the execution of operation $o_1$
    * *Scenario 1*. $S_3$ is completely ignorant of the situation until the new leader $S_2$ tells it to accept operation $o_2$ via an $\text{accept}(o_2,2)$ message
        
        $\to$ The timestamp $t = 2$ will alert $S_3$ that it missed a previous accept message
        1. $S_3$ will inform the missing of the accept message to $S_2$
        2. $S_2$ can then retransmit $\text{accept}(o_1,1)$, allowing $S_3$ to catch up
    * *Scenario 2*. $S_2$ missed $\text{accept}(o_1, 1)$ and detected that $S_1$ crashed
        1. $S_2$ eventually either send $\text{accept}(o_1, 1)$ or $\text{accept}(o_2, 1)$, i.e. step 2a of Paxos
            
            $\to$ In both cases using timestamp $t = 1$, which was previously used by $S_1$
        2. $S_3$ has enough information to get $S_2$ on the right track again, i.e.
            * If S2 had sent $\text{accept}(o_1,1)$, $S_3$ can tell $S_2$ that it already learned $o_1$
            * If S2 had sent $\text{accept}(o_2,1)$, S3 will inform $S_2$ that it apparently missed operation $o_1$
    * *Conclusion*. When $S_1$ crashes after executing an operation, Paxos behaves correctly
* *Leader failure after operation acceptance*. Suppose $S_1$ crashes immediately after having sent $\text{accept}(o_1, 1)$ to the other two servers
    * *Scenario 1*. $S_3$ is completely ignorant of the situation, since messages are lost, until $S_2$ has taken over leadership and announces that $o_2$ should be accepted
        
        $\to$ $S_3$ can tell $S_2$ that it missed $o_1$, then $S_2$ can help $S_3$ to catch up
    * *Scenario 2*. $S_2$ misses messages, but detects that $S_1$ crashed
        
        $\to$ As soon as it takes over leadership and proposes an operation, it will be using a stale timestamp
        * *Consequence*. This will trigger $S_3$ to inform $S_2$ that it missed $o_1$, which saves the day
    * *Conclusion*. Paxos is seen to behave correctly
* *Leader failure with false detections of crashes*. Problems may arise with false detections of crashes
    * *Scenario*. Consider the scenario sketched in the following figure

        <div style="text-align:center">
            <img src="https://i.imgur.com/fdPIkHn.png">
            <figcaption>Why incorporating the ID of the current leader is required</figcaption>
        </div>

        1. The accept messages from $S_1$ are considerably delayed
            
            $\to$ $S_2$ falsely detects $S_1$ having crashed
        2. $S_2$ takes over leadership and sends $\text{accept}(o_2, 1)$, i.e., with a timestamp $t = 1$, due to step 2a of Paxos
        3. When finally $\text{accept}(o_1,1)$ arrives, $S_3$ cannot do anything, i.e. this is not a message it is expecting
    * *Solution*. If $S_3$ knows who the current leader is, in combination with a deterministic leader election
        
        $\to$ It could safely reject $\text{accept}(o_1,1)$, knowing that by now $S_2$ has taken over
    * *Consequence*. The leader should include its ID in an accept message
* *Failure cases of Paxos*. Paxos behaves correctly in most cases
    
    $\to$ Unfortunately, although being correct, the algorithm can still come to a grinding halt
    
    <div style="text-align:center">
        <img src="https://i.imgur.com/wJEftno.png">
        <figcaption>When Paxos can make no further progress</figcaption>
    </div>
    
    * *Example*. When learn messages returned by $S_3$ are lost, neither $S_1$ nor $S_2$ will ever be able to know what S3 actually executed
        
        $\to$ Did it learn, and execute, $\text{accept}(o_1, 1)$ before or after learning $\text{accept}(o_2, 1)$, or perhaps it learned neither operation

**Making progress in Paxos**.
* *Safety and liveness of Paxos*.
    * *Safety of Paxos*. Up to this point we have discussed the development of Paxos such that safety is ensured
        * *Safety*. Nothing bad will happen, or, put differently, that the behavior of the algorithm is correct
    * *Liveness*. To also ensure that eventually something good will happen, we need to do a bit more
* *Issue*. The servers may have no consensus on who the leader is
* *Solution*. Once $S_2$ decides it should take over leadership, it needs to ensure that any outstanding operations initiated by $S_1$ have been properly dealt with
    
    $\to$ It needs to ensure that its own leadership is not hindered by operations, which have not yet been completed by all nonfaulty processes
    * *Explain*. If leadership is taken over too quickly and a new operation is proposed
        
        $\to$ A previous operation that has been executed by at least one server may not get a chance to be executed by all servers first
    * *Consequence*. Paxos enforces an explicit leadership takeover
        
        $\to$ This is where the role of proposers come from
* *Idea*. When a server crashes, the next one in line will need to take over, but also ensure that any outstanding operations are dealt with
* *Explicit takeover implementation*. 
    1. Broadcasting a proposal message $\text{propose}(S_i)$, where $S_i$ is the next server to be leader
    2. When server $S_j$ receives this message, it replies with a $\text{promise}(o^j,t_j)$ message
        * *$\text{promise}(o^j,t_j)$ message*. Contain the most recently executed operation $o^j$ and its corresponding timestamp $t_j$
    3. $S_i$ is interested in the most recent operation $o^*$ executed by a majority of servers
    4. By adopting this operation from the apparently crashed server $S^*$, which had originally proposed its acceptance
        
        $\to$ $S_i$ can effectively complete what $S^*$ could not, due to its failure
    5. When $S_i$ receives a majority of promises for operation $o^*$, and the highest returned timestamp is $t^*$
        
        $\to$ It broadcasts $\text{accept}(S_i,o^*,t^*)$, which is essentially a retransmission of the last operation proposed before $S_i$ took over leadership
        
        >**NOTE**. If no such $o^*$ exists, $S_i$ will propose to accept its own operation $o^i$

* *Optimization*. There are two optimizations to this procedure
    * *Optimization 1*. Servers do not return the most recently executed operation, but the most recently learned operation, which is still waiting to be executed, if any 
    * *Optimization 2*. Since it may be that the collection of servers have no more pending operations
        
        $\to$ $S_i$ can also suggest a next operation $o^i$ when initially proposing to take over leadership, giving rise to a $\text{propose}(S_i,o^i)$ message
        
        >**NOTE**. In essence, this is the situation we described earlier, yet now it should be clear where the idea of proposals actually comes from

## Consensus in faulty systems with arbitrary failures
**Brief**. So far, we assumed that replicas were only subject to crash failures

$\to$ A process group needs to consist of $2k + 1$ servers to survive $k$ crashed members
* *Underlying assumption*. A process does not collude with another process, or, more specifically, is consistent in its messages to others
* *Violation of assumptions*.
    * *Case 1*. Process $P_2$ is forwarding a different value or operation than it is supposed to
        * *Consequence*. Referring to Paxos, a primary may tell the backups that not operation $o$ had been accepted
            
            $\to$ It propagates a different operation $o'$
    * *Case 2*. $P_1$ is telling different things to different processes
        * *Consequence*. Referring to Paxos, a leader may send operation $o$ to some backups, and at the same time operation $o'$ to others

        >**NOTE**. This need not be malicious actions, but simply omission or commission

* *Problem of interest*. Reaching consensus in a fault-tolerant process group, in which $k$ members can fail assuming arbitrary failures
    
    $\to$ It is shown that we need at least $3k + 1$ members to reach consensus under these failure assumptions

**Scenario**. 
* *Servers organization*. Consider a process group consisting of $n$ members
    * A member has been designated to be the primary $P$
    * The remaining $n â€” 1$ to be the backups $B_1,\dots,B_{n-1}$
* *Assumptions*.
    * A client sends a value $v \in \{T, F\}$ to $P$, where $v$ stands for either true or false
    * Messages may be lost, but this can be detected
    * Messages cannot be corrupted without that being detected, and thus subsequently ignored
    * A receiver of a message can reliably detect its sender
* *Byzantine agreement*. A set of parties in a distributed environment must agree on a value, even if some of the parties are corrupted
    * *Requirements*. In order to achieve what is known as Byzantine agreement, we need to satisfy the following two requirements
        * *BA1*. Every nonfaulty backup process stores the same value
        * *BA2*. If the primary is nonfaulty, then every nonfaulty backup process stores exactly what the primary had sent
    * *Consequence*. 
        * If the primary is faulty, the backups may store the same, but different, and thus wrong, value than the one initially sent by the client
        * If the primary is not faulty, satisfying BA2 implies that BA1 is also satisfied

### Why having $3k$ processes is not enough
**Simplest case**. Consider the case, in which we want to tolerate the failure of a single
process, i.e. $k = 1$
* *Scenario 1*. The faulty primary $P$ is sending two different values to the backups $B_1$ and $B_2$
    * *Reaching consensus*. In order to reach consensus, both backup processes forward the received value to the other

        $\to$ This leads to a second round of message exchanges
        * *Consequence*. $B_1$ and $B_2$ each have received the set of values $\{T, F\}$, from which it is impossible to draw a conclusion
* *Scenario 2*. The primary $P$ and backup $B_2$ operate correctly, but $B_1$ is not
    
    $\to$ Instead of forwarding the value $T$ to process $B_2$, it sends the incorrect value $F$
    * *Consequence*. $B_2$ sees the set of values $\{T, F\}$, from which it cannot draw any conclusions, hence
        * $P$ and $B_2$ cannot reach consensus
        * $B_2$ is not able to decide what to store, violating requirement BA2

**The case where $k> 1$ and $n\leq 3k$**. We will prove by contradiction that a consensus solution for this case does not exxist
* *Assumptions*. Consider a simple reduction scheme
    * There is a solution for the case where $k\geq1$ and $n \leq 3k$
    * $Q_1,\dots,Q_n$ are $n$ processes
* *Servers organization*. Partition $Q_1,\dots, Q_n$ into three disjoint sets $S_1, S_2, S_3$, together containing all processes, so that
    * $S_1 \cap S_2 = S_1 \cap S_3 = S_2 \cap S_3 = \emptyset$
    * $S_1 \cup S_2 \cup S_3 = \{Q_1,...,Q_n\}$
    * $\forall i, |S_i|\leq n/3$ (Giang's comment: Should be $|S_i|\leq \lceil n/3\rceil$)
* *Simplification*. Consider the case, in which three processes $Q^*_1, Q^*_2, Q^*_3$ simulate the actions taking place in and between the processes of $S_1, S_2, S_3$
    * *Scenario*. $Q^*_1$ is faulty, yet $Q^*_2$ and $Q^*_3$ are not, i.e.
        * All processes simulated by $Q^*_1$ are assumed to be faulty, leading to incorrect messages being sent to $Q_2^*$ and $Q_3^*$
        * All messages coming from processes in $S_2$ and $S_3$ are assumed to be correct
    * *Observation*.
        * Since $n \leq 3k$ and $|S_i| \leq n/3$, at most $n/3$ of the simulated processes $Q_1,\dots, Q_n$ are faulty
            
            $\to$ We are satisfying the condition, for which we assumed that there would be a general solution
        * If there would exist a solution for the general case
            
            $\to$ $Q^*_1, Q^*_2, Q^*_3$ could simulate this solution, which would then also be a solution for the special case that $n = 3$ and $k = 1$
        * Yet we just proved that this cannot be so, leading to a contradiction
    * *Conclusion*. The assumption that there is a general solution for $k \geq 1$ and $n \leq 3k$ is false

### Why having $3k+1$ processes is enough
**Brief**. Consider the case in which we have a group of $3k + 1$ processes

$\to$ There exists a solution, in which $k$ group members may suffer from fail-arbitrary failures, yet the remaining nonfaulty processes will still reach consensus

**Simple case**. Consider the case where $n = 4$ and $k = 1$

<div style="text-align:center">
    <img src="https://i.imgur.com/eeudrPI.png">
    <figcaption>Reaching consensus with four processes of which one may fail arbitrarily</figcaption>
</div>

* *Server organization*. A primary $P$ and three backup processes $B_1$, $B_2$, and $B_3$
* *Scenario 1*. The primary $P$ is faulty and is providing inconsistent information to its backups
    * *Solution*. The processes will forward what they receive to the others, i.e.
        1. During the first round, $P$ sends $T$ to $B_1$, $F$ to $B_2$, and $T$ to $B_3$
        2. Each of the backups sends what they have to the others
        3. After two rounds, each of the backups will have received the set of values $\{T,T, F\}$
            
            $\to$ They can reach consensus on the value $T$
* *Scenario 2*. One of the backups fails, i.e. the nonfaulty primary sends $T$ to all the backups, yet $B_2$ is faulty
    
    $\to$ $B_1$ and $B_3$ will send out $T$ to the other backups in a second round
    * *Worst case*. $B_2$ sends out $F$, while $B_1$ and $B_3$ come to the same conclusion, i.e. $P$ had sent out $T$
        
        $\to$ The requirement BA2 is met

**The case where $k>1$ and $n=3k+1$**. Consider the case where $n = 7$ and $k = 2$, and the primary $P$ send out a value $v_0$

>**NOTE**. We effectively use the index 0 to denote the primary $P$, i.e. think of $P$ being equal to a special backup process $B_0$

* *Data flow of backup $B_i$*. All backups $B_i$ will recursively carry out the following steps 
    1. $P$ send $v_0$ to the six backups, and $B_i$ stores the received value as $v_{i,0}\langle\rangle$
        * *$v_{i,0}\langle\rangle$ notation*. Indicate that 
            * The value was received by $B_i$
            * The value was sent by $P = B_0$
            * The value of $v_0$ was directly sent to $B_i$, not through another process, i.e. using the notation $\langle\rangle$
    2. Each backup $B_i$ sends $v_{i,0}\langle\rangle$ to every one of the other five
    backups, which is stored by $B_j$ as $v_{j,i}\langle 0\rangle$
        * *$v_{j,i}\langle 0\rangle$ notation*. Indicate that
            * The value is stored at $B_j$
            * The value was sent by $B_i$
            * The value originated from $P = B_0$, i.e. through the notation $\langle 0\rangle$
    2. Suppose that $B_i$ now has the value $v_{i,j}\langle 0\rangle$
        
        $\to$ It will send out this value to all processes except $P = B_0$, $B_j$, and $B_i$
    3. If $B_k$ receives $v_{i,j}\langle 0\rangle$ from $B_i$, it stores this received value in $v_{k,i}\langle j,0\rangle$
        
        $\to$ $v_0$ will have traveled the path $P\to B_j\to B_i\to B_k$
    4. Assuming that $B_i$ has value $v_{i,j}\langle k,0\rangle$, which it sends out to the three remaining processes not equal to $P = B_0,B_k,B_j,B_i$
* *Notation*. $v_{i_r,i_s}\langle i_n,\dots,i_0\rangle$ indicates that
    * The value is stored at $B_{i_r}$, i.e. receiver
    * The value was sent by $B_{i_s}$, i.e. sender
    * The value has traveled $B_{i_0}\to\dots\to B_{i_n}\to B_{i_s}\to B_{i_r}$
* *Estimation of $v_0$*. Once all these messages have been sent, each backup can start moving out of the recursion again
    * *Round-ending state*. From the last round, $B_4$, for example, will have stored a total of five messages
        
        $$v_{4,1}(6,5,3,2,0), v_{4,2}(6,5,3,1,0), v_{4,3}(6,5,2,1,0), v_{4,5}(6,3,2,l,0), v_{4,6}(5,3,2,1,0)$$

        $\to$ With these five values, it can start computing estimates of $v_0$, i.e. a value that it believes $v_0$ should be
    * *Major voting*. Assuming that each nonfaulty process executes the same procedure $\text{majority}()$ selecting a unique value from a given set of inputs
        
        >**NOTE**. In practice, this will be the majority among the input set
        
        >**NOTE**. If there is no majority, a default value is chosen

* *Why this scheme works*. Let $\text{BAP}(n,k)$ be the above sketched protocol to reach consensus, where $n$ is the number of processes, and $k\geq n$ is some integer indicating the degree of consensus

    <div style="text-align:center">
        <img src="https://i.imgur.com/OzJZ9JH.png">
        <figcaption>Local tree at P3 for solving for Byzantine agreement protocol</figcaption>
    </div>

    * *Initialization*. $\text{BAP}(n,k)$ starts by having $P$ send out its value $v_0$ to the $n â€” 1$ backups
        * If $P$ operates correctly, each of the backups will indeed receive $v_0$
        * If $P$ is faulty, some backups receive $v_0$ while others receive $\bar{v}_0$
    * *Iteration*.
        * Since $B_i$ cannot know whether or not $P$ is working correctly
            
            $\to$ It will have to check with the other backups
        * Hence, $B_i$ runs the protocol again, with value $v_{i,0}\langle\rangle$ and a smaller process group $\{B_1,\dots, B_{i-1}, B_{i+1},\dots, B_n\}$
            
            $\to$ $B_i$ executes $\text{BAP}(n-1,k-1)$ with a total of $n â€” 2$ other processes
            
            >**NOTE**. At this point there are $n â€” 1$ instances of $\text{BAP}(n-1,k-1)$ being executed in parallel

        * In the end, these executions result in each backup $B_i$ taking the majority of $n â€” 1$ values, i.e.
            * One value comes from the primary, i.e. $v_{i,0}\langle\rangle$
            * $n â€” 2$ values come from the other backups, i.e. $B_i$ is dealing with the values $v_{i,1}\langle\rangle,\dots,v_{i,i-1}\langle\rangle,v_{i,i+1}\langle\rangle,\dots,v_{i,n-1}\langle\rangle$
        * Since $B_i$ cannot trust a received value $v_{i,j}\langle\rangle$
            
            $\to$ It will have to check that value with the other $n â€” 2$ backups $B_1,\dots, B_{i-1}, B_{i+1},\dots, B_{j-1}, B_{j+1},\dots, B_{n-1}$
            * *Consequence*. This leads to the execution of $\text{BAP}(n-2,k-2)$, of which a total of $n â€” 2$ instances will be running in parallel
        * The procedure continues, and eventually, a backup process will need to run $\text{BAP}(n-k,0)$
            
            $\to$ This function simply returns the value sent by the primary, after which we can move up the recursion as described above

**Correctness of the Byzantine agreement protocol**. Use induction on $k$

### Example - Practical Byzantine fault tolerance (PBFT)
**Brief**. 
* *History of Byzantine fault tolerance*. Byzantine fault tolerance was for long more or less an exotic topic
    * *Explain*. Since it turned out that combining safety, liveness, and practical performance was difficult to achieve
* *Barbara Liskov's work*. It was around 2000 that Barbara Liskov and her student at that time, Miguel Castro, managed to come up with a practical implementation of a protocol for replicating servers that could handle arbitrary failures

    $\to$ Their solution is known as Practical Byzantine Fault Tolerance (PBFT)

**Overview of PBFT**.
* *Assumptions of PBFT*. Like Paxos, PBFT makes only a few assumptions about its environment, i.e.
    * A faulty server is assumed to exhibit arbitrary behavior
    * Messages may be lost, delayed, and received out of order
    * A message's sender is assumed to be identifiable, i.e. achieved by having messages signed
* *Correctness of PBFT*. 
    * Under the given assumptions, and as long as no more than $k$ servers fail
        
        $\to$ It can be proven that PBFT is safe, i.e. a client will always receive a correct answer
    * If we can additionally assume synchrony, i.e. message delays and response times are bounded
        
        $\to$ It also provides liveness

>**NOTE**. In practice, PBFT assumes a partially synchronous model, in which unbounded delays are an exception

**Essential issue of establishing a $k$-fault-tolerant process group**. Such a group behaves as a single, central server

$\to$ Under the assumption of having only crash failures, when a client sends a request, it should expect $k + 1$ identical answers
* *Explain*. If a server had crashed, fewer responses would be returned, but they would be the same

**Ensuring consistent handling order of concurrent requests**. PBFT adopts a primary-backup model with a total of $3k + 1$ replica servers
* *Assumptions*.
    * The primary $P$ is nonfaulty, i.e. for simplicity
    * $P$ has a notion of the current collection of nonfaulty replica servers, expressed as a view $v$
* *Data flow for reaching consensus*.
    1. A client $C$ sends a request to execute operation $o$ to the primary $P$
    2. $P$ assigns a timestamp $t$ to $o$, which is then incremented to be used for a subsequent request
    3. The primary sends a signed pre-prepare message $\text{pre-prepare}(t,v,o)$ to the backups
    4. A nonfaulty backup will accept to pre-prepare if it is in $v$, and has never accepted an operation with timestamp $t$ in $v$ before
    5. Each backup, which accepts to pre-prepare, sends a signed message $\text{prepare}(t,v,o)$ to the others, including the $P$
    6. When a nonfaulty replica server $S$ has received $2k$ messages $\text{prepare}(t, v, o)$, which all match the pre-prepare message $S$ received by $P$
        
        $\to$ There is consensus among the nonfaulty servers on the order of which operation goes first
* *Correctness of the data flow*.
    * *Assumption*.
        * $\text{PC}(t, v, o)$ is a prepare certificate based on a set of $2k + 1$ messages
        * $\text{PC}(t, v, o')$ is another prepare certificate with the same values for $t$ and $v$, and a different operation $o'$
    * *Observation*. Since each prepare certificate is based on $2k + 1$ values from a total of $3k + 1$ replica servers
        
        $\to$ The intersection of two certificates will necessarily be based on messages from a subset of at least $2\cdot (2k+1) - 3k+1 = k + 1$ servers
    * *Conclusion*. $o=o'$ must hold
* *Data flow for commit*. After a replica server has a prepare certificate
    1. It commits to the operation by broadcasting $\text{commit}(t,v,o)$ to the other members in $v$
    2. Each server $S$ collects $2k$ of such commit messages from other servers, leading to commit certificate to execute operation $o$
        
        $\to$ With its own message and the $2k$ other messages, $S$ knows that there is consensus among the nonfaulty servers, on which operation to actually execute now
    3. Each server $S$ executes $o$ and sends a response to the client
    4. The client collects all the results and takes as the answer the response returned by at least $k + 1$ replicas, of which it knows that there is at least one nonfaulty replica server contributing to that answer

**Dealing with primary server failure**. If a backup detects that the primary fails, it broadcasts a view-change message for view $v + 1$
* *Objective*. A request, which was still being processed at the time the primary failed, will eventually get executed once and only once by all nonfaulty servers
    
    $\to$ There must be no two commit certificates with the same timestamp, which have different associated operations, regardless the view that each of them is associated with
    * *Solution*. Have a quorum of $2k + 1$ commit certificates based on prepare certificates
        * *Explain*. We want to regenerate commit certificates for the new view, and only to make sure that a nonfaulty server is not missing any operation
        
        >**NOTE**. We may be generating a certificate for an operation, which a server $S$ had already executed, which can be observed by looking at timestamps
        >f
        >$\to$ That certificate will be ignored by $S$, as long as it keeps an account of its execution history

* *View-changing procedure*.
    1. A backup server broadcasts a signed message $\text{view-change}(v + 1, P)$, where $P$ is the set of its prepare certificates
        
        >**NOTE**. We ignore garbage collecting issues

        >**NOTE**. PBFT includes a deterministic function $\text{primary}(w)$ known to all backups, which returns who the next primary will be given a view $w$

    2. This backup will wait until it has a total of $2k + 1$ view-change messages
        
        $\to$ This leads to a view-change certificate $X$ of prepare certificates
    3. The new primary broadcasts $\text{new-view}(v + 1,X, O)$, where $O$ consists of pre-prepare messages such that
        * $\text{pre-prepare}(t,v + l,o) \in O$ if the prepare certificate $\text{PC}(t,v',o) \in X$, so that there is no prepare certificate $\text{PC}(t,v'',o')$ with $v'' > v'$, or
            * *Explain*. The pre-prepare message must correspond to the newly changed view, with no operation $o'$ submitted with the same timestamp $t$
        * $\text{pre-prepare}(t,v + l,\text{none}) \in O$ if there is no prepare certificate $\text{PC}(t,v',o')\in X$
    4. Any outstanding, pre-prepared operation from a previous view is moved to the new view, but considering only the most recent view leading to the installment of the current new view, i.e.
        1. Each backup will check $O$ and $X$ to make sure that all operations are indeed authentic
        2. Each backup broadcasts prepare messages for all pre-prepare messages in $O$

**Skipped elements of PBFT**. We have skipped many elements of PBFT that deal with its correctness and above all its efficiency
* *Example*. Garbage collecting logs, or efficient ways of authenticating messages

**Performance of Byzantine fault tolerance**. Have been subject to much research, leading to many new protocols 

$\to$ These new proposals often rely on the original PBFT implementation
* *Improvements*. There is still room for improvement when actually using PBFT for developing robust applications
    * *Examples*. PBFT assumes static membership, i.e., clients and servers are known to each other in advance, but also assumes that a replica server's memory acts as a stable, persistent storage
        
        $\to$ These and other shortcomings along with the inherent complexity of Byzantine fault tolerance have formed a hurdle for its widespread usage

## Some limitations on realizing fault tolerance
**Brief**. The prioce of organizing replicated processes into a group helps to increase fault tolerance is a potential loss of performance
* *Explain*. Processes in a fault-tolerant group may need to exchange numerous messages before reaching a decision
* *Question of interest*. Whether realizing specific forms of fault tolerance, e.g. being able to withstand arbitrary failures, is always possible?

### On reaching consensus
**Brief**. If a client can base its decisions through a voting mechanism

$\to$ We can tolerate that $k$ out of $2k + 1$ processes are lying about their result
* *Underlying assumption*. Processes do not team up to produce a wrong result

    $\to$ Otherwise, wrong result may dominate true results
* *Conclusion*. Matters become more intricate if we demand that a process group reaches consensus, which is needed in many cases 
* *Requirements for reaching consensus*.
    * Processes produce the same output value
    * Every output value must be valid
    * Every process must eventually provide output
* *Ideal case*. When the communication and processes are all perfect, reaching consensus is often straightforward
    
    $\to$ When they are not, problems arise

**Distributed consensus algorithms**.
* *General goal of distributed consensus algorithms*. Have all the nonfaulty processes reach consensus on some issue, and to establish that consensus within a finite number of steps
* *Existence of universal solution*. Different assumptions about the underlying system require different solutions, assuming solutions even exist
* *Possible scenarios by Turek and Shasha*.
    1. *Synchronous versus asynchronous systems*. A system is synchronous if and only if the processes are known to operate in a lock-step mode
        * *Explain*. There is some constant $c > 1$, such that if any process has taken $c + 1$ steps, every other process has taken at least $1$ step
    2. *Communication delay is bounded or not*. Delay is bounded if and only if we know that every message is delivered with a globally and predetermined maximum time
    3. *Message delivery is ordered, in real time, or not*. Messages from the different senders may not be delivered in the order that they were sent in real global time
    4. *Message transmission is done through unicasting or multicasting*

**Scenarios where distributed consensus can be reached**.

<div style="text-align:center">
    <img src="https://i.imgur.com/616kbNH.png">
    <figcaption>Siutations under which distributed consensus can be reached</figcaption>
</div>

>**NOTE**. In all other cases, it can be shown that no solution exists

* *Practical scenario*. Most distributed systems in practice assume that 
    * Processes behave asynchronously
    * Message transmission is unicast
    * Communication delays are unbounded
    * Synchronous behavior to be the default, but there may be unbounded delays
* *Consequence*. We need to make use of ordered reliable message delivery, e.g. provided by TCP

**Fischer et. al. 1985's work**. Proved that if messages cannot be guaranteed to be delivered within a known, finite time

$\to$ No consensus is possible if even one process is faulty
* *Explain*. Arbitrarily slow processes are indistinguishable from crashed ones, i.e. you cannot tell the dead from the living

### Consistency, availability, and partitioning
**Consistency and availability**. Strongly related to the conditions under which consensus can (not) be reached, is when consistency can be reached
* *Consistency*. A process group, to which a client is sending requests, returns
the correct responses to the client
    
    $\to$ We are dealing with a safety property, i.e. nothing bad will happen
* *Operation execution*. Operations are executed by a process group to withstand the failures of $k$ group members
    
    $\to$ Process groups help improve fault tolerance and availability
    * *Availability*. A liveness property, i.e. eventually, something good will happen
* *Conclusion*. In terms of our process groups, we aim to eventually get a correct response to every request issued by a client
    
    $\to$ Being consistent in responses while also being highly available is not an unreasonable requirement for services that are part of a distributed system

**Practical scenarios**. The assumption that the processes in a group can communicate with each other may be false
* *Causes of communication failure*. Messages may be lost
    
    $\to$ A group may be partitioned due to a faulty network
* *CAP Theorem*. 
    * *Statement*. Any networked system providing shared data can provide only two of the following three properties
        * *C*. Consistency, i.e. a shared and replicated data item appears as a single, up-to-date copy
        * *A*. Availability, i.e. updates will always be eventually executed
        * *P*. Tolerant to the partitioning of process group, e.g. because of a failing network
    * *Conclusion*. In a network subject to communication failures, it is  impossible to realize an atomic read/write shared memory that guarantees a response to every request
* *Example*. Consider two processes unable to communicate due to a failing network
    * *Case 1*. When choosing consistency over availability
        
        $\to$ The system will return an error or a time out if particular information cannot be guaranteed to be up to date due to network partitioning
    * *Case 2*. When choosing availability over consistency
        
        $\to$ The system will always process the query and try to return the most recent available version of the information, even if it cannot guarantee it is up to date due to network partitioning
    * *Case 3*. If the two processes can communicate, it may not be possible to maintain both consistency
* *Consensus and consistency*. Consensus requires proving that processes produce the same output, providing consistency is weaker
    
    $\to$ If achieving CAP is impossible, than so is consensus
* *Trade-off between safety and liveness*. The CAP theorem is all about reaching a trade-off between safety and liveness, based on the observation that obtaining both in an inherently unreliable system cannot be achieved

## Failure detection
**Failure detection**. One of the cornerstones of fault tolerance in distributed systems
* *Objective*. For a group of processes, nonfaulty members should be able to decide who is still a member, and who is not
    
    $\to$ We need to be able to detect when a member has failed
* *Two mechanisms for detecting process failures*.
    * *Option 1*. Processes actively send "are you alive?" messages to each other, for which they expect an answer
    * *Option 2*. Processes passively wait until messages come in from different processes
        
        $\to$ This makes sense only when it can be guaranteed that there is enough communication
* *Theoretical support for failure detectors*. There is a huge body of theoretical work on failure detectors
    
    $\to$ What it all boils down to is that a timeout mechanism is used to check whether a process has failed
    * *Idea*. If a process $P$ probes another process $Q$ to see if has failed
        
        $\to$ $P$ is said to suspect $Q$ to have crashed if $Q$ has not responded within some time

**On perfect failure detectors**.
* *Failure detectors in synchronous distributed system*. A suspected crash corresponds to a known crash
* *Failure detectors in partially synchronous systems*. It makes more sense to assume eventually perfect failure detectors
    * *Idea*. A process $P$ will suspect another process $Q$ to have crashed after $t$ time units have elapsed and still $Q$ did not respond to $P$'s probe
        * If $Q$ later does send a message, which is received by $P$, $P$ will
            1. Stop suspecting $Q$
            2. Increase the timeout value $t$
        * If $Q$ does crash, and does not recover, $P$ will continue to suspect $Q$


**Practical problems with using probes and timeouts**.
* Due to unreliable networks, concluding that a process has failed, because it does not return an answer to a probe message, may be wrong
    * *Explain*. It is easy to generate false positives, which may result in a perfectly healthy process removed from a membership list
* Timeouts are just plain crude
    * *Explain*. There is hardly any work on building proper failure detection subsystems that take more into account than only the lack of a reply to a single message
        
        $\to$ This is even more evident when looking at industry-deployed distributed systems
    * *Designing factors of a failure detection subsystem*. There are various issues that need to be taken into account when designing a failure detection subsystem
        * *Example*. Failure detection can take place through 
            * Gossiping, in which each node regularly announces to its neighbors that it is still up and running, or
            * Letting nodes actively probe each other, or
            * A side-effect of regularly exchanging information with neighbors
    * *Obduro's approach (Vogels, 2003)*. Processes periodically gossip their service availability
        
        $\to$ This information is gradually disseminated through the network
        * *Consequence*. 
            * Eventually, every process will know about every other process
            * Eventually, every process will have enough information locally available to decide whether a process has failed or not
        * *Failure detection*. A member, for which the availability information is old, will presumably have failed
* A failure detection subsystem should ideally be able to distinguish network failures from node failures
    * *Solution*. Do not let a single node decide whether one of its neighbors has crashed
        
        $\to$ When noticing a timeout on a probe message, a node requests other neighbors to see whether they can reach the presumed failing node
        * *Consequence*. Positive information can also be shared, i.e. if a node is still alive
            
            $\to$ That information can be forwarded to other interested parties
* When a member failure is detected, how should other nonfaulty processes be informed
    * *A simple and radical approach*. Processes can be joined in a group, which spans a wide-area network
        
        $\to$ The group members create a spanning tree used for monitoring member failures
    * *Detected failure propagation*. 
        1. Members send ping messages to their neighbors
        2. When a neighbor does not respond, the pinging node immediately switches to a state in which it will also no longer respond to pings from other nodes
    * *Consequence*. By recursion, a single node failure is rapidly promoted to a group failure notification