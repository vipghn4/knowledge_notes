<!-- TOC titleSize:1 tabSpaces:2 depthFrom:1 depthTo:6 withLinks:1 updateOnSave:1 orderedList:0 skip:0 title:1 charForUnorderedList:* -->
# Table of Contents
- [Table of Contents](#table-of-contents)
- [Process resilience](#process-resilience)
  - [Resilience by process groups](#resilience-by-process-groups)
    - [Group organization](#group-organization)
    - [Membership management](#membership-management)
  - [Failure masking and replication](#failure-masking-and-replication)
  - [Consensus in faulty systems with crash failures](#consensus-in-faulty-systems-with-crash-failures)
  - [Example - Paxos](#example---paxos)
    - [Essential Paxos](#essential-paxos)
    - [Understanding Paxos](#understanding-paxos)
  - [Consensus in faulty systems with arbitrary failures](#consensus-in-faulty-systems-with-arbitrary-failures)
    - [Why having $3k$ processes is not enough](#why-having-3k-processes-is-not-enough)
    - [Why having $3k+1$ processes is enough](#why-having-3k1-processes-is-enough)
    - [Example - Practical Byzantine fault tolerance](#example---practical-byzantine-fault-tolerance)
  - [Some limitations on realizing fault tolerance](#some-limitations-on-realizing-fault-tolerance)
    - [On reaching consensus](#on-reaching-consensus)
    - [Consistency, availability, and partitioning](#consistency-availability-and-partitioning)
  - [Failure detection](#failure-detection)
<!-- /TOC -->

# Process resilience
**Brief**. This section concentrates on the protection against process failures, which is achieved by replicating processes into groups

## Resilience by process groups
**Key approach to tolerating a faulty process**. Organize several identical
processes into a group
* *Key property of groups*. When a message is sent to the group itself, all members of the group receive it
    * *Consequence*. If one process in a group fails, hopefully some other process can take over for it
* *Dynamic process group*. Process groups may be dynamic, i.e. new groups can be created and old groups can be destroyed
    
    $\to$ A process can join a group or leave one during system operation
* *Group membership*. A process can be a member of several groups at the same time
  
  $\to$ Mechanisms are needed for managing groups and group membership
* *Purposes of groups*. Allow a process to deal with collections of other processes as a single abstraction
    * *Explain*. A process $P$ can send a message to a group $\mathbf{Q} = \{Q_1,\dots,Q_n\}$ of servers without having to know who they are, how many there are, or where they are, which may change from one call to the next
      
        $\to$ To $P$, the group $Q$ appears to be a single, logical process

### Group organization
**Brief**. An important distinction between different groups has to do with their internal
structure

**Types of group structure**.
* *Flat group*. All processes are equal, i.e. here is no distinctive leader and all decisions are made collectively
    * *Pros*. The flat group is symmetrical and has no single point of failure
        
        $\to$ If one of the processes crashes, the group simply becomes smaller, but can continue
    * *Cons*. Decision making is more complicated
        * *Example*. To decide anything, a vote often has to be taken, incurring some delay and overhead
* *Hierarchical group*. Some kind of hierarchy exists, e.g. one process is the coordinator and all the others are workers
    * *Request handling*. 
        1. When a request for work is generated, either by an external client or by one of the workers
            
            $\to$ It is sent to the coordinator
        2. The coordinator decides which worker is best suited to carry it out, and forwards it there
   
   >**NOTE**. More complex hierarchies are also possible

    * *Pros*. As long as the coordinator is running, it can make decisions without bothering everyone else
    * *Cons*. Loss of the coordinator brings the entire group to a grinding halt
        * *Solution*. When the coordinator in a hierarchical group fails
            
            $\to$ Its role will need to be taken over and one of the workers is elected as new coordinator

### Membership management
**Problem of interest**. When group communication is present, some method is needed for
* Creating and deleting groups, and
* Allowing processes to join and leave groups

**Centralized approach**. Have a group server to which all these requests can be sent
* *Group server*. Maintain a complete database of all the groups and their exact membership
* *Pros*. Straightforward, efficient, and fairly easy to implement
* *Cons*. There is only a single point of failure, i.e. if the group server crashes
    
    $\to$ Group management ceases to exist
    * *Consequence*. Probably most or all groups will have to be reconstructed from scratch, possibly terminating existing work

**Decentralized approach**. Manage group membership in a distributed way
* *Example*. If reliable multicasting is available
    * *Joining group*. An outsider can send a message to all group members announcing its wish to join the group
    * *Leaving group*. A member sends a goodbye message to everyone
* *Drawback*. 
    * If fail-stop failure semantics is generally not appropriate
        
        $\to$ There is no polite announcement that a process crashes as there is when a process leaves voluntarily
        * *Solution*. The other members have to discover this experimentally by noticing that the crashed member no longer responds to anything
            
            $\to$ Once it is certain that the crashed member is really down, it can be removed from the group
    * Leaving and joining have to be synchronous with data messages being sent
        * *Explain*. 
            * Starting at the instant that a process has joined a group, 
                
                $\to$ It must receive all messages sent to that group
            * As soon as a process has left a group
                
                $\to$ It must not receive any more messages from the group, and the other members must not receive any more messages from it
        * *Solution*. Convert the joining/leaving operation into a sequence of messages sent to the whole group
    * What to do if so many processes go down that the group can no longer function at all
        
        $\to$ Some protocol is needed to rebuild the group
        * *Consequence*. Invariably, some process will have to take the initiative to start the ball rolling
            
            $\to$ What happens if two or three try at the same time
        * *Solution*. The protocol must to be able to withstand this, e.g. a leader-election algorithm may be needed

## Failure masking and replication
**Process groups**. Part of the solution for building fault-tolerant systems
* *Idea*. Having a group of identical processes allows us to mask one or more faulty processes in that group
    
    $\to$ We can replicate processes and organize them into a group to replace a single vulnerable process with a fault tolerant group
* *Approachs for replication*. Use primary-based protocols, or replicated-write protocols

* *Primary-based replication*. Generally appears in the form of a primary-backup protocol
    * *Idea*. A group of processes is organized in a hierarchical fashion, in which a primary coordinates all write operations
    * *Primary node*. In practice, the primary is fixed, although its role can be taken over by one of the backups, if need be
        
        $\to$ When the primary crashes, the backups execute some election algorithm to choose a new primary
* *Replicated-write protocols*. Used in the form of active replication, as well as by means of quorum-based protocols
    * *Idea*. Organize a collection of identical processes into a flat group
    * *Pros*. Such groups have no single point of failure
    * *Cons*. The cost of distributed coordination

**Degree of replication**. An important issue with using process groups to tolerate faults is how much replication is needed

>**NOTE**. To simplify our discussion, we consider only replicated-write systems

* *$k$-fault tolerant system*. A system, which can survive faults in $k$ components and still meet its specifications
    * If the components fail silently, then having $k + 1$ of them is enough to provide $k$-fault tolerance
        * *Explain*. If $k$ components stop, then the answer from the other one can be used
    * If components exhibit arbitrary failures, continuing to run when faulty and sending out erroneous or random replies
        
        $\to$ A minimum of $2k + 1$ processes are needed to achieve $k$-fault tolerance
        * *Explain*. Even when the $k$ failing processes could accidentally, or even intentionally, generate the same reply
            
            $\to$ The remaining k + 1 will also produce the same answer, so the client or voter can just believe the majority

**Problem**. Consider a $k$-fault tolerant group a single process fails, if more than $k$ members fail

$\to$ All bets are off and whatever the group does, its results, if any, cannot be trusted
* *Conclusion*. The process group, in its appearance of mimicking the behavior of a single, robust process, has failed

## Consensus in faulty systems with crash failures
**Consensus in faulty systems with crash failures**. Consider a potentially very large collection of clients sending commands to a group of processes, which jointly behave as a single, highly robust process

$\to$ To make this work, we need to make an important assumption
* *Assumption of interest*. In a fault-tolerant process group, each nonfaulty process executes the same commands, in the same order, as every other nonfaulty process
* *Consequence*. The group members need to reach consensus on which command to execute
    * If failures cannot happen, reaching consensus is easy
        * *Example*. We can
            * Use Lamport's totally ordered multicasting, or
            * Use a centralized sequencer handing out a sequence number to each command, which needs to be executed
    * If failures can happen, reaching consensus among a group of processes is tricky

**Flooding consensus**. 
* *Assumptions*. 
    * $P = \{P_1,\dots,P_n\}$ is a group of processes operating under fail-stop failure semantics
        
        $\to$ Crash failures can be reliably detected among the group members
    * Every group member maintains a list of proposed commands
        * Some, which it received directly from clients
        * Others, which it received from its fellow group members
    * A client contacts a group member requesting it to execute a command
* *Flooding consensus*. We can reach consensus using  flooding consensus
* *Idea*. Conceptually the algorithm operates in rounds
    * In each round, a process $P_j$ sends its list of proposed commands it has seen so far to every other process in $P$
    * At the end of a round, each process merges all received proposed commands into a new list
        
        $\to$ Given the new list, it deterministically selects the command to execute, if possible

        >**NOTE**. The selection algorithm must be the same for all processes

* *Correctness*. This approach works as long as processes do not fail

**Failure handling in flooding consensus**. Assume that some $P_j$ detects, during round $r$, say $P_k$ has crashed 

<div style="text-align:center">
    <img src="https://i.imgur.com/EoW4vYk.png">
    <figcaption>Reaching consensus through flooding in the presence of crash failures</figcaption>
</div>

* *Assumption*. To make this concrete, we simplify the problem by assuming that
    * $P$ has four processes $\{P_1,\dots,P_4\}$
    * $P_1$ crashes during round $r$
    * $P_2$ receives the list of proposed commands from $P_1$ before it crashes
    * $P_3$ and $P_4$ do not receive the list of proposed commands from $P_1$ before it crashes
    * All processes knew who was group member at the beginning of round $r$
* *Round increment*. A process will decide to move to a next round when it has received a message from every nonfaulty process
    
    >**NOTE**. This assumes that each process can reliably detect the crashing of another process
    >
    >* *Explain*. Otherwise the process would not be able to decide who the nonfaulty processes are

* *Failure handling flow*.
    * *Round $r$*. 
        * $P_2$ is ready to make a decision on which command to execute when it receives the lists of the other members
            
            $\to$ It has all commands proposed so far
        * From $P_3$'s perspective, if there is another process that did receive $P_1$'s proposed commands
            
            $\to$ That process may then make a different decision than itself
            * *Solution*. $P_3$ postpones its decision until the next round
            
            >**NOTE**. The same holds for $P_4$
    
    * *Round $r+1$*. 
        1. Since $P_2$ received all commands, it can indeed make a decision and can subsequently broadcast that decision to the others
        2. $P_3$ and $P_4$ will then be able to make a decision, i.e. they decide to execute the same command selected by $P_2$

**Correctness of flooding consensus**. 
* *Correctness under reliable failure detection*. A process will move to a next round without having made a decision, only when it detects that another process has failed
    
    $\to$ In the worst case at most one nonfaulty process remains, and this process can simply decide whatever proposed command to execute
    
    >**NOTE**. Reliable failure detection is assumed in this case

* *Correctness with packet loss*. If the decision by $P_2$ sent to $P_3$ was lost
    
    $\to$ $P_3$ can still not make a decision, hence, we need to make sure that it makes the same decision as $P_2$ and $P_4$
    * If $P_2$ did not crash, a retransmission of its decision will save the day
    * If $P_2$ did crash, this will be also detected by $P_4$, who will subsequently rebroadcast its decision
        
        $\to$ Meanwhile, $P_3$ has moved to a next round, and after receiving the decision by $P_4$, will terminate its execution of the algorithm

## Example - Paxos
**Realistic assumption of failure detection**. The flooding-based consensus algorithm is not very realistic, since it relies on a fail-stop failure model
* *More realistic assumption*. A fail-noisy failure model, in which a process will eventually reliably detect that another process has crashed

**Paxos**. A widely adopted consensus algorithm originally published in 1989 as a technical report by Leslie Lamport

>**NOTE**. It took about a decade before someone decided that it may not be such a bad idea to disseminate it through a regular scientific channel

### Essential Paxos
**Paxos' assumptions**. Weaker than the assumptions of flooding consensus
* The distributed system is partially synchronous, or it may even be asynchronous
* Communication between processes may be unreliable, meaning that messages may be lost, duplicated, or reordered
* Corrupted messages can be detected, hence  subsequently ignored
* All operations are deterministic, i.e. once an execution is started, it is known exactly what it will do
* Processes may exhibit crash failures, but not arbitrary failures, nor do processes collude

>**NOTE**. By-and-large, these are realistic assumptions for many practical distributed systems

**Key idea**.
* *Logical processes*. The algorithm operates as a network of logical processes, of which there are different types

    <div style="text-align:center">
        <img src="https://i.imgur.com/bkG20h3.png">
        <figcaption>The organization of Paxos into different logical processes</figcaption>
    </div>

    * *Clients*. There are clients requesting a specific operation to be executed
    * *Proposers*. At the server side, each client is represented by a proposer, which attempts to have a client's request accepted
    * *Leader*. A proposer is designated as being the leader, and drives the protocol toward reaching consensus
    * *Acceptors*. A proposed operation is accepted by an acceptor
        
        $\to$ If a majority of acceptors accepts the same proposal, the proposal is said to be chosen
    * *Learners*. What is chosen still needs to be learned, hence we will have a number of learner processes
        
        $\to$ Each learner process will execute a chosen proposal once it has been informed by a majority of acceptors
* *Physical processes*. A single proposer, acceptor, and learner form a physical process, running on a machine, with which the client communicates
    * *Fault tolerance*. If a proposer crashes, then the physical process that it is part of will have crashed
        
        $\to$ By replicating this server we aim at obtaining fault tolerance in the presence of crash failures
* *Basic operating model*.
    1. The leading proposer receives requests from clients, one at a time
    2. A nonleading proposer forwards any client request to the leader
    3. The leading proposer sends its proposal to all acceptors, telling each to accept the requested operation
    4. Each acceptor will subsequently broadcast a learn message
    5. If a learner receives the same learn message from a majority of acceptors
        
        $\to$ It knows that consensus has been reached on which operation to execute, and will execute it

**Issues to address**.
* *Execution order problem*. Not only do the servers need to reach consensus on which operation to execute
    
    $\to$ Each of the servers must actually executes it
    * *Question of interest*. How to ensure that a majority of the nonfaulty servers will carry out the operation?
    * *Solution*. Have learn messages be retransmitted
        * *Drawback*. An acceptor have to log its decisions, in turn requiring a mechanism for purging logs
    * *Correctness*. Since we are assuming globally ordered proposal timestamps
        * Missing messages can be easily detected
        * Accepted operations will always be executed in the same order by all learners
* *Failing leader problem*.
    * *Response to clients*. 
        * The server hosting the leading proposer will inform the client when its requested operation has been executed 
        * If another process had taken over the lead, it will also handle the response to the client
    * *Failing leader problem*.
        * *Ideal scenario*. If the failure of a leader can be reliably detected
            1. A new leader would be elected
            2. The recovering leader would instantly notice that the world around it had changed
        * *Practical scenario*. Life is not so easy
    * *Solution*. Paxos has been designed to tolerate proposers who still believe they are in the lead
        
        $\to$ Proposals may be sent out concurrently by different proposers, each believing to be the leader
        * *Consequence*. We need to make sure that these proposals can be distinguished from one another to ensure that the acceptors handle only the proposals from the current leader

            $\to$ There must be a leader-election algorithm
            
            >**NOTE**. In principle, that algorithm can operate independently from Paxos, but is normally part of it

**Distinguishing concurrent proposals from different proposers**. How uniqueness is achieved is left to an implementation, but we will describe some of the details shortly
* *Assumptions*.
    * Each proposal $p$ has a uniquely associated logical timestamp $\text{ts}(p)$
    * $\text{oper}(p)$ is the operation associated with proposal $p$
* *Objective*. Only proposals from the current leader should be accepted by the acceptors
* *Key idea*. Allow multiple proposals to be accepted, but each of these accepted proposals has the same associated operation
    * *Explain*. This can be achieved by guaranteeing that if a proposal $p$ is chosen
        
        $\to$ Any proposal with a higher timestamp will also have the same associated operation
    * *Formal*. We require that
        
        $$p \text{ is chosen } \implies \forall p'\in\{p':\text{ts}(p') > \text{ts}(p)\}, \text{oper}(p') = \text{oper}(p)$$

    * *Implementation*. For $p$ to be chosen, it needs to be accepted, hence we must guarantee that if $p$ is chosen
        
        $\to$ Any higher-timestamped proposal accepted by any acceptor has the same associated operation as $p$
* *Problem*. At a certain moment, a proposer may send a new proposal $p'$, with the highest timestamp so far, to an acceptor $A$, which had not received any proposal before
    * *Explain*. This may happen according to our assumptions concerning message loss and multiple proposers each believing to be in the lead
    * *Effects*. In absence of any other proposals, $A$ will simply accept $p'$
    * *Solution*. We need to guarantee that if proposal $p$ is chosen
        
        $\to$ Any higher-timestamped proposal issued by a proposer, has the same associated operation as $p$
    * *Consequence*. A proposer may need to adopt an operation coming from acceptors in favor of its own
        
        $\to$ This will happen after a leading proposer had failed, but its proposed operation had already made it to a majority of the acceptors

**Safety and liveness**.
* *Safety*. The processes collectively formally ensure safety
    * *Safety property*. 
        * Only proposed operations will be learned
        * At most one operation will be learned at a time
    * *Conclusion*. A safety property asserts that nothing bad will happen
* *Liveness*. Paxos ensures conditional liveness, i.e. if enough processes remain up-and-running
    
    $\to$ A proposed operation will eventually be learned, and executed
    * *Liveness*. Ensure that eventually something good will happen
        
        $\to$ This is not guaranteed in Paxos, unless some adaptations are made

**Brief procedure of Paxos**. There are two phases, each in turn consisting of two subphases
* *First phase*. The leading proposer interacts with acceptors to get a requested operation accepted for execution
    
    $\to$ This is needed to rule out any trouble caused by different proposers, each believing they are the leader
    * *Best scenario*. An individual acceptor promises to consider the proposer's operation and ignore other requests
    * *Worst scenario*. The proposer was too late and it will be asked to adopt some other proposer's request instead
        
        $\to$ A leadership change had taken place and there may be former requests that need to handled first
* *Second phase*. The acceptors will have informed proposers about the promises they have made
    
    $\to$ The leading proposer essentially takes up a slightly different role by promoting a single operation to the one to be executed, and subsequently telling the acceptors


Phase la (prepare): The goal of this phase is that a proposer P who believes it
is leader and is proposing operation o, tries to get its proposal timestamp
anchored, in the sense that any lower timestamp failed, or that o had also
been previously proposed (i.e., with some lower proposal timestamp).
To this end, P broadcasts its proposal to the acceptors. For the operation
o, the proposer selects a proposal number m higher than any of its
previously selected numbers. This leads to a proposal timestamp t =
(m,i) where i is the (numerical) process identifier of P. Note that
(m,i) < (n,j) <^ [m < n) or (m = n and / < /)
This timestamp for a proposal p is an implementation of the previously
mentioned timestamp ts(p). Proposer P sends prepare(t) to all 
acceptors (but note that messages may be lost). In doing so, it is (1) asking the
acceptors to promise not to accept any proposals with a lower proposal
timestamp, and (2) to inform it about an accepted proposal, if any, with
the highest timestamp less than t. If such a proposal exists, the proposer
will adopt it.
Phase lb (promise): An acceptor A may receive multiple proposals. Assume
it receives prepare(t) from P. There are three cases to consider:
(1) t is the highest proposal timestamp received from any proposer so
far. In that case, A will return promise(t) to P stating that A will
ignore any future proposals with a lower timestamp.
(2) If t is the highest timestamp so far, but another proposal (t', o') had
already been accepted, A also returns (t',of) to P. This will allow P
to decide on the final operation that needs to be accepted.
(3) In all other cases, do nothing: there is apparently another proposal
with a higher timestamp that is being processed.
Once the first phase has been completed, the leading proposer P knows what
the acceptors have promised. Essentially, the leading proposer knows that all
acceptors have agreed on the same operation. This will put P into a position
to tell the acceptors that they can go ahead. This is needed, because although
the leading proposer knows on which operation consensus has been reached,
this consensus is not known to the others. Again, we assume that P received
a response from a majority of acceptors (whose respective responses may be
different).
Phase 2a (accept): There are two cases to consider:
(1) If P does not receive any accepted operation from any of the 
acceptors, it will forward its own proposal for acceptance by sending
accept(£,o) to all acceptors.
(2) Otherwise, it was informed about another operation o', which it will
adopt and forward for acceptance. It does so by sending accept(t,
o'), where t is P's proposal timestamp and o1 is the operation with
proposal timestamp highest among all accepted operations that
were returned by the acceptors in Phase lb.
Phase 2b (learn): Finally, if an acceptor receives accept(£,c/), but did not
previously send a promise with a higher proposal timestamp, it will 
accept operation c/, and tell all learners to execute c/ by sending learn(c/).
At that point, the acceptor can forget about c/. A learner L receiving
learn(o') from a majority of acceptors, will execute the operation c/.
We now also know that a majority of learners share the same idea on
which operation to execute.
It is important to realize that this description of Paxos indeed captures only its
essence: using a leading proposer to drive the acceptors toward the execution
of the same operation. When it comes to practical implementations, much
more needs to be done (and more than we are willing to describe here). An
excellent description of what it means to realize Paxos has been written by
Kirsch and Amir [2008]. Another write-up on its practical implications can be
found in [Chandra et al., 2007].

### Understanding Paxos
To properly understand Paxos, but also many other consensus algorithms, it
is useful to see how its design could have evolved. We say "could have," as
the evolution of the algorithm has never been documented. The following
description is largely based on work described by Meling and Jehl [2013]1. As
our starting point, we consider a server that we wish to make more robust. By
now, we know that this can be achieved through replication and making sure
that all commands submitted by clients are executed by all servers in the same
order. The simplest situation is to add one server, thus creating a group of two
processes, say Si and S2. Also, to make sure that all commands are executed
in the same order, we appoint one process to be a sequencer, which increments
and associates a unique timestamp with every submitted command. Servers
are required to execute commands according to their timestamp. In Paxos,
such a server is referred to as the leader. We can also consider it to be a
primary server, with the other acting as a backup server.
We assume that a client broadcasts its requested command to all servers.
If a server notices it is missing a command, it can rely on the other server
to forward it when necessary. We will not describe how this happens, but
silently assume that all commands are stored at the servers and that we merely
need to make sure that the servers agree on which command to execute next.
As a consequence, all remaining communication between servers consists of
control messages. To make this point clear, consider the situation sketched in
Figure 8.7. (In what follows, we use subscripts to designate processes, and
superscripts to designate operations and states.)
In this example, server Si is the leader and as such will hand out time-
stamps to submitted requests. Client Ci has submitted command o1 while C2
submitted o2. Si instructs S2 to execute operation o2 with timestamp 1, and
later operation o1 with timestamp 2. After processing a command, a server
will return the result to the associated client. We designate this using the
notation (cj), where / is the index of the reporting server, and ; the state it
was in, expressed as the sequence of operations it has carried out. In our
example, client Ci will thus see the result (c21), meaning that each server has
executed o1 after executing o2.
In Paxos, when a leader associates a timestamp with an operation, it does
so by sending an accept message to the other server(s). As we assume that
messages may be lost, a server accepting an operation o does so by telling
the leader it has learned the operation by returning a learn(o) message.
When the leader does not notice that operation o has been learned, it simply
retransmits an accept(o, t) message, with t being the original timestamp. Note
that in our description we are skipping the phase of coming to agreement on
the operation to be carried out: we assume the leader has decided and now
needs to reach consensus on executing that operation.
Compensating for a lost message is relatively easy, but what happens when
also a server crashes? Let us first assume that a crash can be reliably detected.
Consider the situation sketched in Figure 8.8(a). The issue, of course, is that
server S2 will never have learned (about) operation o1. This situation can be
prevented by demanding that a server may execute an operation only if it
knows that the other server has learned the operation as well, as illustrated in
Figure 8.8(b).
It is not difficult to see that with a larger process group, we can get into the
same situation as in Figure 8.8(a). Simply consider a group of three servers
{Si, S2, S3} with Si being the leader. If its accept(o1, t) message to S3 is lost,
yet it knows that S2 has learned o1, then it should still not execute o1 until
it has also received a learn(o1) message from S3. This situation is shown in
Figure 8.9.
Now imagine what would happen if learn(o1) returned by S2 would not
make it to Si. Of course, Si would not execute o1, but otherwise we would
still be in trouble: S2 will execute o1 while S3 would take over leadership and
execute o2 without ever knowing that o1 had already been processed. In other
words, also S2 must wait with the execution of o1 until it knows that S3 has
learned that operation. This brings us to the following:
In Paxos, a server S cannot execute an operation o until it has received
a learn(o) from all other nonfaulty servers.
Up to this point we have assumed that a process can reliably detect that
another process has crashed. In practice, this is not the case. As we will
discuss more extensively shortly, a standard approach toward failure detection
is to set a timeout on expected messages. For example, each server is required
to send a message declaring it is still alive, and at the same time the other
servers set timeouts on the expected receipt of such messages. If a timeout
expires, the sender is suspected to have failed. In a partially synchronous or
fully asynchronous system, there is essentially no other solution. However,
the consequence is that a failure may be falsely detected, as the delivery of
such "I'm alive" messages may have simply been delayed or lost.
Let us assume that Paxos has realized a failure detection mechanism, but
that the two servers falsely conclude that the other has failed, as shown in
Figure 8.10. The problem is clear: each may now independently decide to
execute their operation of choice, leading to divergent behavior. It is at this
point that we need to introduce an extra server, and demand that a server
can execute an operation only if it is certain that a majority will execute that
operation. Note, that in the three-server case, execution of operation o by
server S can take place as soon as S has received at least one (other) learn(o)
message. Together with the sender of that message, S will form a majority.
We have now come to a point where it should be clear that Paxos requires
at least three replicated servers {Si, 82,83} in order to operate correctly. Let
us concentrate on the situation that one of these servers crashes. We make the
following assumptions.
• Initially, Si is the leader.
• A server can reliably detect it has missed a message. The latter can
be realized, for example, through timestamps in the form of strictly
increasing sequence numbers. Whenever a server notices it has missed
a message, it can then simply request a retransmission and catch up
before continuing.
• When a new leader needs to be elected, the remaining servers follow a
strictly deterministic algorithm. For example, we can safely assume that
if Si crashes, then S2 will become leader. Likewise, if S2 crashes, S3 will
take over, and so on.
• Clients may receive duplicate responses, but besides being required to
recognize duplicates, form no further part of the Paxos protocol. In
other words, a client cannot be asked to help the servers to resolve a
situation.
Under these circumstances, it is fairly easy to see that no matter when one of
S2 or S3 crashes, Paxos will behave correctly. Of course, we are still demanding
that execution of an operation can take place only if a server knows that a
majority will execute that operation.
Suppose now that Si crashes after the execution of operation o1. The
worst that can happen in this case is that S3 is completely ignorant of the
situation until the new leader, S2 tells it to accept operation o2. Note that
this is announced through an accept (o2,2) message such that the timestamp
t = 2 will alert S3 that it missed a previous accept message. S3 will tell so to
S2, who can then retransmit accept(o1,1), allowing S3 to catch up.
Likewise, if S2 missed accept(o1, 1), but did detect that Si crashed, it will
eventually either send accept(o1, 1) or accept(o2, 1) (i.e., in both cases using
timestamp t = 1, which was previously used by Si). Again, S3 has enough
information to get S2 on the right track again. If S2 had sent accept(o1,1), S3
can simply tell S2 that it already learned o1. In the other case, when S2 sends
accept(o2,1), S3 will inform S2 that it apparently missed operation o1. We
conclude that when Si crashes after executing an operation, Paxos behaves
correctly.
So what can happen if Si crashes immediately after having sent accept(o1,
1) to the other two servers? Suppose again that S3 is completely ignorant of
the situation because messages are lost, until S2 has taken over leadership
and announces that o2 should be accepted. Like before, S3 can tell S2 that
it (i.e., S3) missed operation o1, so that S2 can help S3 to catch up. If S2
misses messages, but does detect that Si crashed, then as soon as it takes over
leadership and proposes an operation, it will be using a stale timestamp. This
will trigger S3 to inform S2 that it missed operation o1, which saves the day.
Again, Paxos is seen to behave correctly.
Problems may arise with false detections of crashes. Consider the 
situation sketched in Figure 8.11. We see that the accept messages from Si
are considerably delayed and that S2 falsely detects Si having crashed. S2
takes over leadership and sends accept(o2, 1), i.e., with a timestamp t = 1.
However, when finally accept(o1,1) arrives, S3 cannot do anything: this is
not a message it is expecting. Fortunately, if it knows who the current leader
is, in combination with a deterministic leader election, it could safely reject
accept(o1,1), knowing that by now S2 has taken over. We conclude that the
leader should include its ID in an accept message.
We have covered almost all cases and have thus far shown that Paxos
behaves correctly. Unfortunately, although being correct, the algorithm can
still come to a grinding halt. Consider the situation illustrated in Figure 8.12.
What we are seeing here is that, because the learn messages returned by S3 are
lost, neither Si nor S2 will ever be able to know what S3 actually executed: did
it learn (and execute) accept(o1, 1) before or after learning accept(o2, 1), or
perhaps it learned neither operation? A solution to this problem is discussed
in Note 8.4.

**Making progress in Paxos**.
Up to this point we have discussed the development of Paxos such that safety is
ensured. Safety essentially means that nothing bad will happen, or, put differently,
that the behavior of the algorithm is correct. To also ensure that eventually
something good will happen, generally referred to as liveness of an algorithm, we
need to do a bit more. In particular, we need to get out of the situation sketched
in Figure 8.12.
The real problem with this situation is that the servers have no consensus on
who the leader is. Once S2 decides it should take over leadership, it needs to
ensure that any outstanding operations initiated by Si have been properly dealt
with. In other words, it needs to ensure that its own leadership is not hindered
by operations that have not yet been completed by all nonfaulty processes. If
leadership is taken over too quickly and a new operation is proposed, a previous
operation that has been executed by at least one server may not get a chance to be
executed by all servers first.
To this end, Paxos enforces an explicit leadership takeover, and this is where
the role of proposers come from. When a server crashes, the next one in line
will need to take over (recall that Paxos assumes a deterministic leader-election
algorithm), but also ensure that any outstanding operations are dealt with. This 
explicit takeover is implemented by broadcasting a proposal message: propose(Sj),
where Sj is the next server to be leader. When server Sj receives this message,
it replies with a promise(oJ,^) message, containing the most recently executed
operation o* and its corresponding timestamp tj. Note that Sj is particularly
interested in the most recent operation o* executed by a majority of servers. By
"adopting" this operation from the (apparently crashed) server S* that had 
originally proposed its acceptance, Sj can effectively complete what S* could not due
to its failure.
There are two obvious optimizations to this procedure. The first one is not
that servers return the most recently executed operation, but the most recently
learned operation that is still waiting to be executed, if any. Furthermore, because
it may be that the collection of servers have no more pending operations, Sj can
also suggest a next operation o1 when initially proposing to take over leadership,
giving rise to a propose(Sj,o') message. In essence, this is the situation we
described earlier, yet now it should be clear where the idea of proposals actually
comes from.
When Sj receives a majority of promises for operation o*, and the highest
returned timestamp is £*, it broadcasts accept(Sj,o*,£*), which is essentially a
retransmission of the last operation proposed before Sj took over leadership. If no
such o* exists, Sj will propose to accept its own operation o1.

## Consensus in faulty systems with arbitrary failures
So far, we assumed that replicas were only subject to crash failures, in which
case a process group needs to consist of 2k + 1 servers to survive k crashed
members. An important assumption in these cases is that a process does
not collude with another process, or, more specifically, is consistent in its
messages to others. The situations shown in Figure 8.13 should not happen.
In the first case, we see that process P2 is forwarding a different value or
operation than it is supposed to. Referring to Paxos, this could mean that a
primary tells the backups that not operation o had been accepted, but instead
propagates a different operation o'. In the second case, Pi is telling different
things to different processes, such as having a leader sending operation o to
some backups, and at the same time operation or to others. Again, we note
that this need not be malicious actions, but simply omission or commission
In this section, we take a look at reaching consensus in a fault-tolerant
process group in which k members can fail assuming arbitrary failures. In
particular, we will show that we need at least 3k + 1 members to reach
consensus under these failure assumptions.
Consider a process group consisting of n members, of which one has been
designated to be the primary, P, and the remaining n — 1 to be the backups
Bi,...,Bn_i. We make the following assumptions:
• A client sends a value v G {T, F} to the primary, where v stands for
either true or false.
• Messages may be lost, but this can be detected.
• Messages cannot be corrupted without that being detected (and thus
subsequently ignored).
• A receiver of a message can reliably detect its sender.
In order to achieve what is known as Byzantine agreement, we need to satisfy
the following two requirements:
BA1: Every nonfaulty backup process stores the same value.
BA2: If the primary is nonfaulty then every nonfaulty backup process stores
exactly what the primary had sent.
Note that if the primary is faulty, BA1 tells us that the backups may store
the same, but different (and thus wrong) value than the one initially sent by
the client. Furthermore, it should be clear that if the primary is not faulty,
satisfying BA2 implies that BA1 is also satisfied.

### Why having $3k$ processes is not enough
To see why having only 3k processes is not enough to reach consensus, let
us consider the situation in which we want to tolerate the failure of a single
process, that is, k = 1. Consider Figure 8.14, which is essentially an extension
of Figure 8.13.
In Figure 8.14(a), we see that the faulty primary P is sending two different
values to the backups Bi and B2, respectively. In order to reach consensus,
both backup processes forward the received value to the other, leading to
a second round of message exchanges. At that point, Bi and B2 each have
received the set of values {T, F}, from which it is impossible to draw a
conclusion.
Likewise, we cannot reach consensus when wrong values are forwarded.
In Figure 8.14(b), the primary P and backup B2 operate correctly, but Bi is
not. Instead of forwarding the value T to process B2, it sends the incorrect
value F. The result is that B2 will now have seen the set of values {T, F} from
which it cannot draw any conclusions. In other words, P and B2 cannot reach
consensus. More specifically, B2 is not able to decide what to store, so that we
cannot satisfy requirement BA2.

**The case where $k>1$ and $n\leq 3k$**.
Generalizing this situation for other values of k is not that difficult. As explained
by Kshemkalyani and Singhal [2008], we can use a simple reduction scheme.
Assume that there is a solution for the case where k>l and n < 3k. Partition the
n processes Qi,..., Qn into three disjoint sets Si, S2, and S3, together containing
all processes. Moreover, let each set S^ have less or equal than n/3 members.
Formally, this means that
• Si n s2 = Si n s3 = s2 n s3 = 0
• SiUS2US3 = {Qi,...,Qn}
• for each Si, \S{\<n/3
Now consider a situation in which three processes Q^, Q2, and Q3 simulate the
actions that take place in and between the processes of Si, S2, and S3, respectively.
In other words, if a process in Si sends a message to another process in S2, then
Q^ will send a same message to Q2. The same holds for process communication
within a group. Assume that Q^ is faulty, yet Q;> and Q3 are not. All processes
simulated by Q^ are now assumed to be faulty, and will thus lead to incorrect
messages being sent to Q2 and Q3, respectively. Not so for Q;> (and Q3): all
messages coming from processes in S2 (and S3, respectively) are assumed to be
correct. Because n <3k and for each set Si we have that |Sj| < n/3, at most n/3
of the simulated processes Qi,..., Qn are faulty. In other words, we are satisfying
the condition for which we assumed that there would be a general solution.
We can now come to a contradiction, for if there would exist a solution for
the general case, then the processes Q^, Q;>, and Q3 could simulate this solution,
which would then also be a solution for the special case that n = 3 and k = 1. Yet
we just proved that this cannot be so, leading to a contradiction. We conclude that
our assumption that there is a general solution for k > 1 and n < 3k is false.

### Why having $3k+1$ processes is enough
Let us now focus on the case in which we have a group of 3k + 1 processes.
Our goal is to show that we can establish a solution in which k group members
may suffer from fail-arbitrary failures, yet the remaining nonfaulty processes
will still reach consensus. Again, we first concentrate on the case n = 4, k = 1.
Consider Figure 8.15, which shows a situation with one primary P and three
backup processes Bi, B2, and B3.
In Figure 8.15(a) we have sketched the situation in which the primary P is
faulty and is providing inconsistent information to its backups. In our solution,
the processes will forward what they receive to the others. During the first
round, P sends T to Bi, F to B2, and T to B3, respectively. Each of the backups
then sends what they have to the others. With only the primary failing, this
means that after two rounds, each of the backups will have received the set of
values {T,T, F}, meaning that they can reach consensus on the value T.
When we consider the case that one of the backups fails, we get the
situation sketched in Figure 8.15(b). Assume that the (nonfaulty) primary
sends T to all the backups, yet B2 is faulty. Where Bi and B3 will send out T
to the other backups in a second round, the worst that B2 may do is send out
F, as shown in the figure. Despite this failure, Bi and B3 will come to the same
conclusion, namely that P had sent out T, thereby meeting our requirement
BA2 as stated before.

**The case where $k>1$ and $n=3k+1$**.
As a sketch toward a general solution, consider the more intricate case in which
n = 7 and k = 2. We let the primary P send out a value Vq. Using a similar
notation as found in [Kshemkalyani and Singhal, 2008], we proceed as follows.
Note that we effectively use the index 0 to denote the primary P (think of P being
equal to a special backup process Bq).
1. We let P send Vq to the six backups. Backup Bj stores the received value
as vi,o( )• This notation indicates that the value was received by process Bj,
that it was sent by P = Bq, and that the value of Vq was directly sent to Bj
and not through another process (using the notation ( )). To make matters
concrete, consider B4, which will store Vq in v^q ( ).
2. Each backup Bj, in turn, will send Vi$( ) to every one of the other five
backups, which is stored by Bj as Vj^ (0). This notation indicates that the
value is stored at Bj, was sent by Bj, but that it originated from P = Bq
(through the notation (0)). Concentrating on B4, it eventually stores 04,5(0),
which is the value v$$( ) sent by B5 to B4.
3. Suppose that Bj now has the value 0/^(0). Again, it will send out this value
to all processes except P = Bq, Bj, and Bj (i.e., itself). If B^ receives ^Z/y(0)
from Bj, it stores this received value in z^/Z-(/,0). Indeed, by then vq will
have traveled the path P—»Bj—»Bj—»B|<. For example, B2 may send out
z?2,i (0) to B4, who will store it as ^4,2 (1/0). Note at this point, that B4 can
send out this value only to processes B3, B5, and Bg. There is no use in
sending it out to other processes.
4. Continuing this line of thought, assume that Bj has value Vjj(k,0), which
it sends out to the three remaining processes not equal to P = Bq, B^, Bj,
and Bj (itself). Returning to B4, eventually, B4 will also receive a similar
message from, say, B2, leading perhaps to a value 04,2(3,1,0). That value
can be sent only to B5 and Bg, after which only a single round is left.
Once all these messages have been sent, each backup can start moving out of the
recursion again. Let us look at process B4 again. From the last round, it will have
stored a total of five messages:
• ^44(6,5,3,2,0)
• 1^(6,5,3,1,0)
• ^3(6,5,2,1,0)
• z;45(6,3,2,l,0)
• i^6 (5,3,2,1,0)
With these five values, it can start computing estimates of Vq, that is, a value that
it believes Vq should be. To this end, we assume that each (nonfaulty) process
executes the same procedure majority () that selects a unique value from a given
set of inputs. In practice, this will be the majority among the input set. If there is
no majority, a default value is chosen. To give a few examples:
w4/1 (5,3,2,0) <- majority (v4/1 (5,3,2,0), v4/1 (6,5,3,2,0))
W44 (6,3,2,0) <- majority>(v4/1 (6,3,2,0), v4/2(6,5,3,1,0))
w4/5 (3,2,1,0) <- majority{v^ (3,2,1,0), z;4/5 (6,3,2,1,0))
^4,5 (6,3,2,0) <- majority{v^ (6,3,2,0), z;4/5 (6,3,2,1,0))
z^4/6 (3,2,1,0) <- majority{v^ (3,2,1,0), z;4/6 (5,3,2,1,0))
z^4/6 (5,3,2,0) <- majority{v^ (5,3,2,0), z;4/1 (6,5,3,2,0))
In turn, allowing it to compute estimates like:
wA/1 (3,2,0) 4r- majority {v^ (3,2,0), wA/5(3,2,1,0), wA/6(3,2,1,0)
wA/5(3,2,0) 4r- majority(vA/5(3,2,0), wA/1 (5,3,2,0), wA/6(5,3,2,0)
wA/6(3,2,0) ^- majority(vA/6(3,2,0), w4/1 (6,3,2,0), z^4/5(6,3,2,0)
And from there, for example
w4/3 (2,0) <- majority(v4/3 (2,0), w4/1 (3,2,0), w4/5 (3,2,0), z^4/6 (3,2,0))
This process continues until, eventually, B4 will be able to execute
^4,0( ) <- majority(v4/Q( ),w4/i(0),w4/2(0),w4/3(0),^4/5(0),w4/6(0))
and reach the final outcome.
Let us now see why this scheme actually works. We denote by BAP(n,k)
the above sketched protocol to reach consensus. BAP(n,k) starts by having the
primary send out its value Vq to the n — 1 backups. In the case the primary
operates correctly, each of the backups will indeed receive Vq. If the primary is
nonfaulty, some backups receive Vq while others receive Vq (i.e., the opposite of
Because we assume a backup Bj cannot know whether or not the primary is
working correctly, it will have to check with the other backups. We therefore let
Bj run the protocol again, but in this case with value Vj/q( ) and with a smaller
process group, namely {Bi,..., Bj_i, Bj+i,..., Bn}. In other words, Bj executes
BAP(n-l,k-l) with a total of n — 2 other processes. Note that at this point there
are n — 1 instances of BAP(n-l,k-l) being executed in parallel.
In the end, we see that these executions result in each backup Bj taking the
majority of n — 1 values:
• One value comes from the primary: v^q ( )
• n — 2 values come from the other backups, in particular, Bj is dealing with
the values vifl (),..., ^,/-i ( ), vu+1 (),..., ^>-i ( ).
However, because Bj cannot trust a received value V{fj{ ), it will have to check that
value with the other n — 2 backups: Bi,..., Bj_i, Bj+i,..., Bj_i, Bj+i,..., Bn-i-
This leads to the execution of BAP(n-2,k-2), of which a total of n — 2 instances
will be running in parallel. This story continues, and eventually, a backup process
will need to run BAP(n-k,0), which simply returns the value sent by the primary,
after which we can move up the recursion as described above.

**Correctness of the Byzantine agreement protocol**.
With the general scheme given in Note 8.6, it is not that difficult to see why the
protocol is correct. Following Koren and Krishna [2007], we use induction on k to
prove that BAP(n,k) meets the requirements BA1 and BA2 for n > 3k + 1 and for
all k > 0.
First, consider the case k = 0. In other words, we assume that there are no
faulty processes. In that case, whatever the primary sends to the backups, that
value will be consistently propagated throughout the system, and no other value
will ever pop up. In other words, for any n, BAP(n,0) is correct. Now consider
the case k > 0.
First, consider the case that the primary is operating correctly. Without loss of
generality, we can assume the primary sends out T. All the backups receive the
same value, namely T. Each backup will then run BAP(n-l,k-l). By induction,
we know that each of these instances will be executed correctly. This means that
for any nonfaulty backup B, all the other nonfaulty backups will store the value
that was sent by B, namely T. Each nonfaulty backup receives, in total, n — 1
values, of which n — 2 come from other backups. Of those n — 2, at most k values
may be wrong (i.e., F). With k < [n — l)/3, this means that every nonfaulty
backup receives at least 1 + (n — 2) — (n — l)/3 = (2n — 2)/3 values T. Because
(2n — 2)/3 > n/3 for all n > 2, this means that every nonfaulty backup can take
a correct majority vote on the total number of received values, thus satisfying
requirement BA2.
Let us now consider the case that the primary is faulty, meaning that at most
k — 1 backups may operate incorrectly as well. The primary is assumed to send
out any value it likes. There are a total of n — 1 backups, of which at most k — 1
are faulty. Each backup runs BAP(n-l,k-l) and by, induction, each one of these
instances is executed correctly. In particular, for every nonfaulty backup B, all
the other nonfaulty backups will vote for the value sent by B. This means that
all nonfaulty backups will have the same vector of n — 2 results from their fellow
backups. Any difference between two nonfaulty backups can be caused only by
the fact that the primary sent something else to each of them. As a result, when
applying majority() to those complete vectors, the result for each backup will be
the same, so that requirement BA1 is met.

### Example - Practical Byzantine fault tolerance
Byzantine fault tolerance was for long more or less an exotic topic, partly 
because it turned out that combining safety, liveness, and practical performance
was difficult to achieve. It was around 2000 that Barbara Liskov and her
student at that time, Miguel Castro, managed to come up with a practical 
implementation of a protocol for replicating servers that could handle arbitrary
failures. Let us briefly take a look at their solution, which has been coined
Practical Byzantine Fault Tolerance, or simply PBFT [Castro and Liskov,
2002].
Like Paxos, PBFT makes only a few assumptions about its environment. It
makes no assumptions about the behavior of replica servers: a faulty server
is assumed to exhibit arbitrary behavior. Likewise, messages may be lost,
delayed, and received out of order. However, a message's sender is assumed
to be identifiable (which is achieved by having messages signed, as we will
discuss in Chapter 9). Under these assumptions, and as long as no more than
k servers fail, it can be proven that PBFT is safe, meaning that a client will
always receive a correct answer. If we can additionally assume synchrony,
meaning that message delays and response times are bounded, it also provides
liveness. In practice, this means that PBFT assumes a partially synchronous
model, in which unbounded delays are an exception, for example caused by
an attack.
To understand the algorithm, let us take a step back and partly review
what we have discussed so far on establishing a fc-fault-tolerant process group.
An essential issue is that such a group behaves as a single, central server. As
a consequence, under the assumption of having only crash failures, when a
client sends a request, it should expect k + 1 identical answers. If a server had
crashed, fewer responses would be returned, but they would be the same.
The first problem that we need to solve is that concurrent requests are all
handled in the same order. To this end, PBFT adopts a primary-backup model
with a total of 3k + 1 replica servers. To keep matters simple, let us assume for
now that the primary is nonfaulty. In that case, a client C sends a request to
execute operation o to the primary (denoted as P in Figure 8.16). The primary
has a notion of the current collection of nonfaulty replica servers, expressed
in terms of a view v, which is simply a number. The primary assigns a
timestamp t to o, which is then incremented to be used for a subsequent
request. The primary subsequently sends a (signed) pre-prepare message
PRE-PREPARE(£,y,o) to the backups.
A (nonfaulty) backup will accept to pre-prepare if it is in v and has never
accepted an operation with timestamp t inv before. Each backup that accepts
to pre-prepare sends an (again signed) message prepare(£,17,o) to the others,
including the primary. A key observation is that when a nonfaulty replica
server S has received 2k messages prepare(t, v, o) that all match the pre-prepare
message S itself received by the primary (i.e., all have the same value for t,
v, and o, respectively), there is consensus among the nonfaulty servers on
the order of which operation goes first. To see why, let a prepare certificate
PC(f, v, o) denote a certificate that is based on such a set of 2k + 1 messages.
Let PC(t, v, c/) be another prepare certificate with the same values for t and v
respectively, but with a different operation o'. Because each prepare certificate
is based on 2k + 1 values from a total of 3k + 1 replica servers, the intersection
of two certificates will necessarily be based on messages from a subset of at
least k + 1 servers. Of this subset, we know that there is at least one nonfaulty
server, which will have sent the same prepare message. Hence, o = o'.
After a replica server has a prepare certificate, it commits to the operation
by broadcasting commit(£,z7,o) to the other members in v. Each server S,
in turn, collects 2k of such commit messages from other servers, leading to
commit certificate to execute operation o. At that point, it executes o and
sends a response to the client. Again, with its own message and the 2k other
messages, S knows that there is consensus among the nonfaulty servers on
which operation to actually execute now. The client collects all the results and
takes as the answer the response that is returned by at least k + 1 replicas, of
which it knows that there is at least one nonfaulty replica server contributing
to that answer.
So far so good. However, we also need to deal with the situation that
the primary fails. If a backup detects that the primary fails, it broadcasts
a view-change message for view v + 1. What we wish to establish is that a
request that was still being processed at the time the primary failed, will
eventually get executed once and only once by all nonfaulty servers. To this end,
we first need to ensure that there are no two commit certificates with the same
timestamp that have different associated operations, regardless the view that
each of them is associated with. This situation can be prevented by having
a quorum of 2k + 1 commit certificates just as before, but this time based on
prepare certificates. In other words, we want to regenerate commit certificates,
but now for the new view, and only to make sure that a nonfaulty server is
not missing any operation. In this respect, note that we may be generating a
certificate for an operation that a server S had already executed (which can be
observed by looking at timestamps), but that certificate will be ignored by S
as long as it keeps an account of its execution history.
A backup server will broadcast a (signed) message view-change(v + 1, P),
with P being the set of its prepare certificates. (Note that we ignore garbage
collecting issues.) PBFT includes a deterministic function primary(zt>) known
to all backups that returns who the next primary will be given a view w. This
backup will wait until it has a total of 7k + 1 view-change messages, leading
to a view-change certificate X of prepare certificates. The new primary then
broadcasts new-view(z? + 1,X, O), where O consists of pre-prepare messages
such that:
• pre-prepare (£,y + l,o) G O if the prepare certificate PC(t,vf,o) G X
such that there is no prepare certificate PC(t,v",o') with v" > v',
or
• pre-prepare(t,v + l,none) G O if there is no prepare certificate
PC(t,v',o') ex.
What happens is that any outstanding, pre-prepared operation from a previous
view is moved to the new view, but considering only the most recent view
that led to the installment of the current new view. Simplifying matters a bit,
each backup will check O and X to make sure that all operations are indeed
authentic and broadcast prepare messages for all pre-prepare messages in O.
We have skipped many elements of PBFT that deal with its correctness
and above all its efficiency. For example, we did not touch upon garbage
collecting logs or efficient ways of authenticating messages. Such details
can be found in [Castro and Liskov, 2002]. A description of a wrapper
that will allow the incorporation of Byzantine fault tolerance with legacy
applications is described in [Castro et al., 2003]. Notably the performance
of Byzantine fault tolerance has been subject to much research, leading to
many new protocols (see, for example, Zyzzyva [Kotla et al., 2009] and
Abstract [Guerraoui et al., 2010]), yet even these new proposals often rely on
the original PBFT implementation. That there is still room for improvement
when actually using PBFT for developing robust applications is discussed by
Chondros et al. [2012]. For example, PBFT assumes static membership (i.e.,
clients and servers are known to each other in advance), but also assumes that
a replica server's memory acts as a stable, persistent storage. These and other
shortcomings along with the inherent complexity of Byzantine fault tolerance
have formed a hurdle for its widespread usage.

## Some limitations on realizing fault tolerance
**Brief**. The prioce of organizing replicated processes into a group helps to increase fault tolerance is a potential loss of performance
* *Explain*. Processes in a fault-tolerant group may need to exchange numerous messages before reaching a decision
* *Question of interest*. Whether realizing specific forms of fault tolerance, e.g. being able
to withstand arbitrary failures, is always possible?

### On reaching consensus
As we mentioned, if a client can base its decisions through a voting mechanism,
we can tolerate that k out of 2k + 1 processes are lying about their result. The
assumption we are making, however, is that processes do not team up to
produce a wrong result. In general, matters become more intricate if we
demand that a process group reaches consensus, which is needed in many
cases. There are three requirements for reaching consensus [Fischer et al.,
1985]:
• Processes produce the same output value
• Every output value must be valid
• Every process must eventually provide output
Some examples where reaching consensus is necessary include electing a
coordinator, deciding whether or not to commit a transaction, and dividing
up tasks among workers. When the communication and processes are all
perfect, reaching consensus is often straightforward, but when they are not,
problems arise.
The general goal of distributed consensus algorithms is to have all the
nonfaulty processes reach consensus on some issue, and to establish that
consensus within a finite number of steps. The problem is complicated by the
fact that different assumptions about the underlying system require different
solutions, assuming solutions even exist. Turek and Shasha [1992] distinguish
the following cases:
1. Synchronous versus asynchronous systems. Rephrasing our description
somewhat, a system is synchronous if and only if the processes are
known to operate in a lock-step mode. Formally, this means that there
should be some constant c > 1, such that if any process has taken c + 1
steps, every other process has taken at least 1 step.
2. Communication delay is bounded or not. Delay is bounded if and
only if we know that every message is delivered with a globally and
predetermined maximum time.
3. Message delivery is ordered (in real time) or not. In other words, we
distinguish the situation where messages from the different senders are
delivered in the order that they were sent in real global time, from the
situation in which we do not have such guarantees.
4. Message transmission is done through unicasting or multicasting.
As it turns out, reaching consensus is possible only for the situations
shown in Figure 8.17. In all other cases, it can be shown that no solution exists.
Note that most distributed systems in practice assume that processes behave
asynchronously, message transmission is unicast, and communication delays
are unbounded. As a consequence, we need to make use of ordered (reliable)
message delivery, such as provided by TCP. And again, in practical situations
we assume synchronous behavior to be the default, but take into account that
there may be unbounded delays as well. Figure 8.17 illustrates the nontrivial
nature of distributed consensus when processes may fail.
Reaching consensus may not be possible. Fischer et al. [1985] proved that
if messages cannot be guaranteed to be delivered within a known, finite time,
no consensus is possible if even one process is faulty (albeit if that one process
fails silently). The problem with such systems is that arbitrarily slow processes
are indistinguishable from crashed ones (i.e., you cannot tell the dead from
the living). These and other theoretical results are surveyed by Barborak et al.
[1993] and Turek and Shasha [1992].

### Consistency, availability, and partitioning
Strongly related to the conditions under which consensus can (not) be reached,
is when consistency can be reached. Consistency in this case means that
when we have a process group to which a client is sending requests, that
the responses returned to that client are correct. We are dealing with a
safety property: a property that asserts that nothing bad will happen. For
our purposes, the type of operations we consider are those that seem to be
executed in a clearly defined order by a single, centralized server. By now,
we know better: these operations are executed by a process group in order to
withstand the failures of k group members.
We introduced process groups to improve fault tolerance, and, more 
specifically, to improve availability. Availability is typically a liveness property:
eventually, something good will happen. In terms of our process groups,
we aim to eventually get a (correct) response to every request issued by a
client. Being consistent in responses while also being highly available is not
an unreasonable requirement for services that are part of a distributed system.
Unfortunately, we may be asking too much.
In practical situations, our underlying assumption that the processes in
a group can indeed communicate with each other may be false. Messages
may be lost; a group may be partitioned due to a faulty network. In 2000,
Eric Brewer posed an important theorem which was later proven to be correct
by Gilbert and Lynch [2002]:
CAP Theorem: Any networked system providing shared data can 
provide only two of the following three properties:
• C: consistency, by which a shared and replicated data item appears
as a single, up-to-date copy
• A: availability, by which updates will always be eventually 
executed
• P: Tolerant to the partitioning of process group (e.g., because of a
failing network).
In other words, in a network subject to communication failures, it is 
impossible to realize an atomic read/write shared memory that guarantees
a response to every request [Gilbert and Lynch, 2012].
This has now become known as the CAP theorem, first published as [Fox
and Brewer, 1999]. As explained by Brewer [2012], one way of understanding
the theorem is to think of two processes unable to communicate because of a
failing network. Allowing one process to accept updates leads to inconsistency,
so that we can only have properties {C,P}. If the illusion of consistency is to
be provided while the two processes cannot communicate, then one of the
two processes will have to pretend to be unavailable, implying having only
{A, P}. However, only if the two processes can communicate, is it possible to
maintain both consistency and high availability, meaning that we have only
{C, A}, but no longer property P.
Note also the relationship with reaching consensus; in fact, where 
consensus requires proving that processes produce the same output, providing
consistency is weaker. This also means that if achieving CAP is impossible,
than so is consensus.
The CAP theorem is all about reaching a trade-off between safety and live-
ness, based on the observation that obtaining both in an inherently unreliable
system cannot be achieved. Practical distributed systems are inherently 
unreliable. What Brewer and his colleagues observed is that in practical distributed
systems, one simply has to make a choice to proceed despite the fact that
another process cannot be reached. In other words, we need to do something
when a partition manifests itself through high latency.
The bottom line when it seems that partitioning is taking place, is to
proceed (tolerating partitions in favor of either consistency or availability),
while simultaneously starting a recovery procedure that can mitigate the
effects of potential inconsistencies. Exactly deciding on how to proceed is
application dependent: in many cases having duplicate keys in a database
can easily be fixed (implying that we should tolerate an inconsistency), while
duplicate transfers of large sums of money may not (meaning that we should
decide to tolerate lower availability). One can argue that the CAP theorem
essentially moves designers of distributed systems from theoretical solutions
to engineering solutions. The interested reader is referred to [Brewer, 2012] to
see how such a move can be made.

## Failure detection
**Failure detection**. One of the cornerstones of fault tolerance in distributed systems
* *Objective*. For a group of processes, nonfaulty members should be able to decide who is still a member, and who is not
    
    $\to$ We need to be able to detect when a member has failed
* *Two mechanisms for detecting process failures*.
    * *Option 1*. Processes actively send "are you alive?" messages to each other, for which they expect an answer
    * *Option 2*. Processes passively wait until messages come in from different processes
        
        $\to$ This makes sense only when it can be guaranteed that there is enough communication
* *Theoretical support for failure detectors*. There is a huge body of theoretical work on failure detectors
    
    $\to$ What it all boils down to is that a timeout mechanism is used to check whether a process has failed
    * *Idea*. If a process $P$ probes another process $Q$ to see if has failed
        
        $\to$ $P$ is said to suspect $Q$ to have crashed if $Q$ has not responded within some time

**On perfect failure detectors**.
* *Failure detectors in synchronous distributed system*. A suspected crash corresponds to a known crash
* *Failure detectors in partially synchronous systems*. It makes more sense to assume eventually perfect failure detectors
    * *Idea*. A process $P$ will suspect another process $Q$ to have crashed after $t$ time units have elapsed and still $Q$ did not respond to $P$'s probe
        * If $Q$ later does send a message, which is received by $P$, $P$ will
            1. Stop suspecting $Q$
            2. Increase the timeout value $t$
        * If $Q$ does crash, and does not recover, $P$ will continue to suspect $Q$


**Practical problems with using probes and timeouts**.
* Due to unreliable networks, concluding that a process has failed, because it does not return an answer to a probe message, may be wrong
    * *Explain*. It is easy to generate false positives, which may result in a perfectly healthy process removed from a membership list
* Timeouts are just plain crude
    * *Explain*. There is hardly any work on building proper failure detection subsystems that take more into account than only the lack of a reply to a single message
        
        $\to$ This is even more evident when looking at industry-deployed distributed systems
    * *Designing factors of a failure detection subsystem*. There are various issues that need to be taken into account when designing a failure detection subsystem
        * *Example*. Failure detection can take place through 
            * Gossiping, in which each node regularly announces to its neighbors that it is still up and running, or
            * Letting nodes actively probe each other, or
            * A side-effect of regularly exchanging information with neighbors
    * *Obduro's approach (Vogels, 2003)*. Processes periodically gossip their service availability
        
        $\to$ This information is gradually disseminated through the network
        * *Consequence*. 
            * Eventually, every process will know about every other process
            * Eventually, every process will have enough information locally available to decide whether a process has failed or not
        * *Failure detection*. A member, for which the availability information is old, will presumably have failed
* A failure detection subsystem should ideally be able to distinguish network failures from node failures
    * *Solution*. Do not let a single node decide whether one of its neighbors has crashed
        
        $\to$ When noticing a timeout on a probe message, a node requests other neighbors to see whether they can reach the presumed failing node
        * *Consequence*. Positive information can also be shared, i.e. if a node is still alive
            
            $\to$ That information can be forwarded to other interested parties
* When a member failure is detected, how should other nonfaulty processes be informed
    * *A simple and radical approach*. Processes can be joined in a group, which spans a wide-area network
        
        $\to$ The group members create a spanning tree used for monitoring member failures
    * *Detected failure propagation*. 
        1. Members send ping messages to their neighbors
        2. When a neighbor does not respond, the pinging node immediately switches to a state in which it will also no longer respond to pings from other nodes
    * *Consequence*. By recursion, a single node failure is rapidly promoted to a group failure notification