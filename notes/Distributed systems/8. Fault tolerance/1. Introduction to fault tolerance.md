<!-- TOC titleSize:1 tabSpaces:2 depthFrom:1 depthTo:6 withLinks:1 updateOnSave:1 orderedList:0 skip:0 title:1 charForUnorderedList:* -->
# Table of Contents
- [Table of Contents](#table-of-contents)
- [Introduction to fault tolerance](#introduction-to-fault-tolerance)
  - [Basic concepts](#basic-concepts)
  - [Failure models](#failure-models)
  - [Failure masking by redundancy](#failure-masking-by-redundancy)
- [Appendix](#appendix)
  - [Concepts](#concepts)
<!-- /TOC -->

# Introduction to fault tolerance
**Partial failure**. A characteristic feature of distributed systems that distinguishes them from single-machine systems
* *Partial failure*. Part of the system is failing while the remaining part continues to operate, and seemingly correctly
* *Recovery from partial failures*. The system should be constructed to be able to automatically recover from partial failures without seriously affecting the overall performance
    * *Idea*. Whenever a failure occurs, the system should continue to operate in an acceptable way while repairs are being made
        
        $\to$ A distributed system is expected to be fault tolerant

**Process resilience through process groups**. Multiple identical processes cooperate providing the appearance of a single logical process

$\to$ The objective is to ensure that one or more of them can fail without a client noticing
* *Difficulty with process groups*. Reaching consensus among the group members, on which a client-requested operation is to perform
    * *Paxos*. A commonly adopted, yet relatively intricate algorithm

**Reliability of communication**. Achieving fault tolerance and reliable communication are strongly related

$\to$ Next to reliable client-server communication we pay attention to reliable group communication and notably atomic multicasting
* *Atomic multicasting*. Message is delivered to all nonfaulty processes in a group, or to none at all
    
    $\to$ Atomic multicasting makes development of fault-tolerant solutions much easier
    * *Atomicity* An important property in many applications
        
        $\to$ In this context, we pay attention to what are known as distributed commit protocols
* *Distributed commit protocols*. A group of processes are conducted to either jointly commit their local work, or collectively abort and return to a previous system state

**Key technique for handling failures**. Redundancy

## Basic concepts
**Brief**. To understand the role of fault tolerance in distributed systems

$\to$ We need to take a closer look at what it actually means for a distributed system to tolerate faults

**Dependable systems**. Being fault tolerant is strongly related to dependable systems
* *Dependability*. A term covering a number of useful requirements for distributed systems including the following
    * *Availability*. A system is ready to be used immediately
        * *Formal definition*. The probability that the system is operating correctly at any given moment, and is available to perform its functions on behalf of its users
            
            $\to$ A highly available system is one, which will most likely be working at a given instant in time
    * *Reliability*. A system can run continuously without failure
        * *Formal definition*. In contrast to availability, reliability is defined in terms of a time interval instead of an instant in time
            
            $\to$ A highly reliable system is one, which will most likely continue to work without interruption during a relatively long period of time
        * *Reliability versus availability*. 
            * If a system goes down on average for one, seemingly random millisecond every hour
                * It has an availability of more than 99.9999 percent, however
                * It is unreliable
            * If a system that never crashes but is shut down for two specific weeks every August
                * It has high reliability
                * It is only 96 percent availability
    * *Safety*. When a system temporarily fails to operate correctly, no catastrophic event happens
    * *Maintainability*. How easily a failed system can be  repaired
        
        $\to$ A highly maintainable system may also show a high degree of availability, especially if failures can be detected and repaired automatically
        
        >**NOTE**. Automatically recovering from failures is easier said than done

    * *Security*. Dependable systems are also required to provide a high degree of security, especially when it comes to issues such as integrity

**System failure**. A system is said to fail when it cannot meet its promises
* *Explain*. If a distributed system is designed to provide its users with a number of services
    
    $\to$ The system has failed when one or more of those services cannot be completely provided
* *Error*. A part of a system's state, which may lead to a failure
    * *Example*. When transmitting packets across a network
        
        $\to$ it is to be expected that some packets have been damaged when they arrive at the receiver
* *Fault*. The cause of an error, i.e. finding out what caused an error is important
    * *Examples*. 
        * A wrong or bad transmission medium may easily cause packets to be damaged
            
            $\to$ It is relatively easy to remove the fault
        * Transmission errors may also be caused by bad weather conditions, e.g. in wireless networks
            
            $\to$ Changing the weather to reduce or prevent errors is a bit trickier

**Controlling faults**. Building dependable systems closely relates to controlling faults

>**NOTE**. A distinction can be made between preventing, tolerating, removing, and forecasting faults

* *Fault tolerance*. The most important issue for our purposes
    * *Idea*. A system should provide its services even in the presence of faults
    * *Example*. By applying error- correcting codes for transmitting packets
        
        $\to$ It is possible to tolerate, to a certain extent, relatively poor transmission lines and reducing the probability that an error may lead to a failure

**Types of faults**. Faults are generally classified as transient, intermittent, or permanent
* *Transient faults*. Occur once and then disappear
    
    $\to$ If the operation is repeated, the fault goes away
    * *Example*. A bird flying through the beam of a microwave transmitter may cause lost bits on some network
        
        $\to$ If the transmission times out and is retried, it will probably work the second time
* *Intermittent fault*. Occur, then vanish of its own accord, then reappears, and so on
    * *Example*. A loose contact on a connector will often cause an  intermittent fault
* *Permanent fault*. One that continues to exist until the faulty  component is replaced
    * *Example*. Burnt-out chips, software bugs, and disk-head crashes

## Failure models
**Dependency relation in distributed systems**. Consider a distributed system as a collection of servers communicating with one another and with their clients

$\to$ Not adequately providing services means that servers, communication channels, or possibly both, are not doing right
* *Problem*. A malfunctioning server itself may not always be the fault we are looking for
    * *Explain*. If such a server depends on other servers to adequately provide its services
        
        $\to$ The cause of an error may need to be searched for somewhere else.

>**NOTE**. Such dependency relations appear in abundance in distributed systems

* *Classification of failures*. To get a better grasp on how serious a failure actually is
    
    $\to$ Several  classification schemes have been developed
    * *Classification schemes described by Cristian [1991] and Hadzilacos and Toueg [1993]*.

        | Type of failure | Description of server's behavior |
        | --- | --- |
        | Crash failure | Halts, but is working correctly until it halts |
        | Omission failure | Fails to repsond to incoming requests |
        | Receive omission (sub-type) | Fails to receive incoming messages |
        | Send omission (sub-type) | Fails to send messages |
        | Timing failure | Respones lies outside a specified time interval |
        | Response failure | Response is incorrect |
        | Value failure (sub-type) | The value of the response is wrong |
        | State-transition failure (sub-type) | Deviates from the correct flow of control |
        | Arbitrary failure | May produce arbitrary responses at arbitrary times |

**Crash failure**. Occur when a server prematurely halts, but was working correctly until it stopped
* *An important aspect of crash failures*. Once the server has halted, nothing is heard from it anymore
* *Example*. An OS coming to a grinding halt, and for which there is only one solution, i.e. reboot it
    
    >**NOTE**. Many personal computer systems suffer from crash failures so often that people have come to expect them to be normal

**Omission failure**. Occur when a server fails to respond to a request, i.e. several things might go wrong
* *Receive-omission failure*.
    * *Example*. Possibly the server never got the request in the first place
        * The connection between a client and a server has been correctly established, however
        * There was no thread listening to incoming requests
    * *Effects on server state*. A receive-omission failure will generally not affect the current state of the server
        * *Explain*. The server is unaware of any message sent to it
* *Send-omission failure*. When the server has done its work, but fails in sending a response
    * *Example*. When a send buffer overflows while the server was not prepared for such a situation
    * *Effects on server state*. Since the server may be in a state reflecting that it has completed a service for the client
        
        $\to$ If the sending of its response fails, the server has to be prepared for the client to reissue its previous request
* *Other types of omission failures not related to communication*. May be caused by software errors, e.g. infinite loops or improper memory management
    
    $\to$ The server is said to "hang"

**Timing failure**. Occur when the response lies outside a specified real-time interval
* *Example*. Consider streaming videos, providing data too soon may easily cause trouble for a recipient if there is not enough buffer space to hold all the incoming data

**Response failure**. A serious type of failure, by which the server's response is incorrect
* *Types of response failures*.
    * *Value failure*. A server provides the wrong reply to a request
        * *Example*. A search engine that systematically returns Web pages not related to any of the search terms used, has failed
    * *State-transition failure*. A server reacts unexpectedly to an incoming request
        * *Example*. If a server receives a message it cannot recognize
            
            $\to$ A state-transition failure happens if no measures have been taken to handle such messages
            
            >**NOTE**. A faulty server may incorrectly take default actions it should never have initiated

**Arbitrary failures (or Byzantine failures)**. The most serious type of failure

$\to$ When arbitrary failures occur, clients should be prepared for the worst
* *Example*. A server is producing output it should never have produced, but which cannot be detected as being incorrect

**Types of distributed systems**.
* *Motivation*. Consider a process $P$ no longer perceiving any actions from another process $Q$
    
    $\to$ Can $P$ conclude that $Q$ has indeed come to a halt?
* *Asynchronous system*. No assumptions about process execution speeds or message delivery times are made
    
    $\to$ When process $P$ no longer perceives any actions from $Q$, it cannot conclude that $Q$ crashed
    * *Consequence*. $Q$ may just be slow or its messages may have been lost
* *Synchronous system*. Process execution speeds and message-delivery times are bounded
    
    $\to$ When Q shows no more activity when it is expected to do so, $P$ can rightfully conclude that $Q$ has crashed
* *Partially synchronous*. 
    * *Problem*. 
        * Pure synchronous systems exist only in theory
        * Assuming that every distributed system is asynchronous also does not do just to what we see in practice
    * *Partially synchronous*. A more realistic assumption about a distributed system
        * *Idea*. Most of the time, the system behaves as a synchronous system, yet there is no bound on the time that it behaves in an asynchronous fashion
            * *Explain*. Asynchronous behavior is an exception
                
                $\to$ We can normally use timeouts to conclude that a process has indeed crashed, but that occasionally such a conclusion is false
        * *Consequence*. Fault-tolerant solutions may withstand incorrectly detecting that a process halted

**Types of halting failures**. Consider a process $P$ attempting to detect that process $Q$ has failed
* *Fail-stop failures*. Crash failures that can be reliably detected
    * *Explain*. This may occur 
        * Nonfaulty communication links are assumed
        * The failure-detecting process $P$ can place a worst-case delay on responses from $Q$
* *Fail-noisy failures*. Like fail-stop failures, except that $P$ will only eventually come to the correct conclusion that $Q$ has crashed
    
    $\to$ There may be some a priori unknown time in which $P$'s detections of the behavior of $Q$ are unreliable
* *Fail-silent failures*. Communication links are nonfaulty, but that $P$ cannot distinguish crash failures from omission failures
* *Fail-safe failures*. Cover the case of dealing with arbitrary failures by process $Q$, yet these failures are benign, i.e. they cannot do any harm
* *Fail-arbitrary failures*. $Q$ may fail in any possible way
    * *Characteristics*.
        * Failures may be unobservable
        * Failures may be harmful to the otherwise correct behavior of other processes
    * *Consequence*. Having to deal with fail-arbitrary failures is the worst that can happen

## Failure masking by redundancy
**Brief**. If a system is to be fault tolerant

$\to$ The best it can do is to try to hide the occurrence of failures from other processes
* *Key technique for masking faults*. Use redundancy

**Types of redundancy**. 
* *Information redundancy*. Extra bits are added to allow recovery from garbled bits
    * *Example*. A Hamming code can be added to transmitted data to recover from noise on the transmission line
* *Time redundancy*. An action is performed, and then, if need be, it is performed again
    * *Examples*. 
        * If a transaction aborts, it can be redone with no harm
        * Retransmitting a request to a server when lacking an expected response
    * *Usage*. Useful when the faults are transient or intermittent
* *Physical redundancy*. Extra equipment or processes are added to make it possible for the system as a whole to tolerate the loss or  malfunctioning of some components
    
    $\to$ Physical redundancy can be done either in hardware or in software
    * *Example*. Extra processes can be added to the system so that if a small number of them crash
        
        $\to$ The system can still function correctly

# Appendix
## Concepts
**Traditional metrics**.

**Omission and commission failures**. It has become somewhat of a habit to associate the occurrence of Byzantine
failures with maliciously operating processes
* *"Byzantine"*. Refer to the Byzantine Empire, a time (330-1453) and place (the Balkans and modern Turkey), in which endless conspiracies, intrigue, and untruthfulness were alleged to be common in ruling circles
* *Problem*. It may not be possible to detect whether an act was actually benign or malicious
    * *Example*. Is a networked computer running a poorly engineered operating system that adversely affects the performance of other computers acting maliciously?
* *Consequence*. It is better to make the following distinction, which effectively excludes
judgement:
    * *Omission failure*. Occur when a component fails to take an action that it should have taken
    * *Commission failure*. Occur when a component takes an action that it should not have taken
* *Conclusion*. This illustrates that a separation between dependability and security may at times be pretty difficult to make

**Triple modular redundancy**.