<!-- TOC titleSize:1 tabSpaces:2 depthFrom:1 depthTo:6 withLinks:1 updateOnSave:1 orderedList:0 skip:0 title:1 charForUnorderedList:* -->
# Table of Contents
- [Table of Contents](#table-of-contents)
- [Recovery](#recovery)
  - [Introduction](#introduction)
  - [Checkpointing](#checkpointing)
    - [Coordinated checkpointing](#coordinated-checkpointing)
    - [Independent checkpointing](#independent-checkpointing)
  - [Message logging](#message-logging)
    - [Characterizing message-logging schemes](#characterizing-message-logging-schemes)
  - [Recovery-oriented computing](#recovery-oriented-computing)
<!-- /TOC -->

# Recovery
**Brief**. Once a failure has occurred, it is essential that the process, where the failure happened, can recover to a correct state

## Introduction
**Recovery from an error**. A fundamental issue of fault tolerance, i.e. an error is that part of a system, which may lead to a failure
* *Idea of error recovery*. Replace an errorneous state with an error-free state

**Forms of error recovery**.
* *Backward recovery*. Bring the system from its present errorneous state back into a previously correct state
    
    $\to$ It is necessary to record the system's state from time to time, and to restore such a recorded state when things go wrong
    * *Checkpoint*. Each time part of the system's present state is recorded, a checkpoint is made
* *Forward recovery*. An attempt is made to bring the system in a correct new state, from which it can continue to execute
    
    $\to$ It is necessary to know in advance which errors may occur, i.e. only in that case is it possible to correct those errors and move to a new state
* *Example*. Consider implementing a reliable communication protocol to deal with packet loss
    * *Backward recovery solution*. The sender retransmit the packet, i.e. we attempt to go back to a previous correct state, i.e. the one in which the lost packet is being sent
    * *Forward recovery solution*. Use erasure correction
        * *Idea*. A missing packet is constructed from other, successfully delivered packets

        >**NOTE**. If not enough packets have been delivered, the sender will have to continue transmitting packets until a previously lost packet can be constructed

* *Usage*. Backward error recovery is widely applied as a general mechanism for recovering from failures in distributed system
    * *Explain*. Backward error recovery is applicable method independent of any specific system or process

        $\to$ It can be integrated into the middleware layer of a distributed system as a general-purpose service 

**Drawbacks of backward error recovery**.
* Restoring a system or process to a previous state is relatively costly in terms of performance
    * *Solution*. Devise very cheap mechanisms, by which components are simply rebooted
* Since backward error recovery mechanisms are independent of the distributed application, for which they are actually used

    $\to$ Once recovery has taken plance, the same or similar failure may happen again
    * *Solution*. To provide such guarantees, handling errors often requires that the application gets into the loop of recovery

        $\to$ Full-fledged failure transparency cannot be provided by backward error recovery mechanisms
* Although backward error recovery requires checkpointing, some state can never be rolled back to
    * *Example*. Once a possibly malicious person has taken the $1.000, which suddenly came rolling out of the incorrectly functioning automated teller machine
    
        $\to$ There is only a small chance that money will be stuffed back in the machine

**Message logging**. Checkpointing allows recovery to a previous correct state, yet taking a checkpoint is costly and may have a severe performance penalty

$\t$ Many fault-tolerant distributed systems combine checkpointing with message logging
* *Logging methods*.
    * *Sender-based logging*. After a checkpoint has been taken, a process logs its messages before sending them off
    * *Receiver-based logging*. The receiving process logs an incoming message before delivering it to the application it is executing
* *Recovery from failure with message logging*. When a receiving process crashes, it is necessary to restore the most recently checkpointed state

    $\to$ From there on, replay the messages which have been sent
    * *Consequence*. Combining checkpoints with message logging makes it possible to restore a state lying beyond the most recent checkpoint, without the cost of checkpointing
* *Recovery from failure without message logging*. Processes will be restored to a checkpointed state

    $\to$ From there on, their behavior may be different than it was before the failure occurred
    * *Example*. Since communication times are not deterministic, messages may be delivered in a different order

        $\to$ This leads to different reactions by the receivers
    * *Benefits of message logging*. With message logging, an actual replay of the events happening since the last checkpoint takes place

        $\to$ Such a replay makes it easier to interact with the outside world

## Checkpointing
**Distributed snapshot**. Backward error recovery requires recording a consistent global state, i.e. a distributed snapshot
* *Idea*. If a process $P$ has recorded the receipt of a message, then there should be a process $Q$, which has recorded the sending of the message

**Recovery line**. To recover after a process or system failure requires the construction of a consistent global state from local states saved by each process

$\to$ It is best to recover to the most recent distributed snapshot, i.e. a recovery line

<div style="text-align:center">
    <img src="https://i.imgur.com/OOK2s8u.png">
    <figcaption>A recovery line</figcaption>
</div>

* *Conclusion*. A recovery line corresponds to the most recent consistent collection of checkpoints

### Coordinated checkpointing
**Coordinated checkpointing**. All processes synchronize to jointly write their state to local storage
* *Implementation*. Use a two-phase blocking protocol
    1. A coordinator multicasts a $\text{checkpoint-request}$ message to all processes
    2. When a process receives the message, it takes a local checkpoint
    3. The process queues any subsequent message handed to it by the application it is executing
    4. The process acknowledges to the coordinator that it has taken a checkpoint
    5. When the coordinator has received an acknowledgment from allprocesses
        
        $\to$ It multicasts a $\text{checkpoint-done}$ message to allow the blocked processes to continue
* *Benefits*. The saved state is automatically globally consistent
    * *Explain*. No incoming message will be registered as part of a checkpoint, i.e.
        * Any message following a request for taking a checkpoint is not considered to be part of the local checkpoint
        * Outgoing messages, as handed to the checkpointing process by the application it is running, are queued locally until the $\text{checkpoint-done}$ message is received
* *Improvement*. Send a checkpoint request only to processes, which depend on the recovery of the coordinator, and ignore other processes
    * *Dependency of processes on coordinator*. A process is dependent on the coordinator if it has received a message, which is directly or indirectly causally related to a message sent by the coordinator since the last checkpoint

        $\to$ This leads to the notion of incremental snapshot
    * *Taking incremental snapshot*.
        1. The coordinator sends a checkpoint request only to processes it had sent a message to since it last took a checkpoint
        2. When a process $P$ receives such a request, it forwards the request to all processes, to which $P$ had sent  amessage since the last checkpoint

            $\to$ This repeats for other processes

            >**NOTE**. A process forwards the request only once
        
        3. When all processes have been identified, a second multicast is used to actually trigger checkpointing, and to let the processes continue where they had left off

### Independent checkpointing
**Independent checkpointing**. Each process records its local state from time to time in an uncoordinated fashion

$\to$ To discover a recovery line requires that each process is rolled back to its most recently saved state

>**NOTE**. If these local states jointly do not form a distributed snapshot, further rolling back is required

* *Domino effect*. The process of a cascaded rollback, e.g. consider the following diagram

    <div style="text-align:center">
        <img src="https://i.imgur.com/YTTRPL0.png">
        <figcaption>The domino effect</figcaption>
    </div>

    1. When $P_2$ crashes, its state is restored to the most recently saved checkpoint, so does $P_1$

        $\to$ However, these local states do not form a distributed snapshot
    2. $P_2$ is rolled back to an ealier state, yet this state cannot be used as part of a distributed snapshot

        $\to$ $P_1$ needs to roll back to a previous state

**Implementation**. Require that dependencies are recorded so that processes can jointly roll back to a consistent global state
* *Assumptions*.
    * $\text{CP}_i(m)$ is the $m$-th checkpoint taken by $P_i$
    * $\text{INT}_i(m)$ is the interval between $\text{CP}_i(m-1)$ and $\text{CP}_i(m)$
* *Checkpointing procedure*.
    1. When $P_i$ sends a message in $\text{INT}_i(m)$, it piggybacks the pair $(i,m)$ to the receiving process
    2. When $P_j$ receives a message in $\text{INT}_j(n)$, along with the pair $(i,m)$

        $\to$ It records the dependency $\text{INT}_i(m)\to\text{INT}_j(n)$
    3. Whenever $P_j$ takes checkpoint $\text{CP}_j(n)$, it saves this dependency to its local storage, along with the rest of the recovery information, which is part of $\text{CP}_j(n)$
* *Rolling-back procedure*. Consider $P_1$ requiring to roll back to $\text{CP}_i(m-1)$

    $\to$ For consistency, all processes, which have received messages from $P_i$ sent in $\text{INT}_i(m)$, are rolled back to a checkpointed state preceding the receipt of such messages
    * *Explain*. $P_j$ needs to be rolled back at least to checkpoint $\text{CP}_j(n-1)$

        $\to$ If $\text{CP}_j(n-1)$ does not lead to a distributed snapshot, further rolling back is required
* *Calculation of the recovery line*. Require an analysis of the interval dependencies recorded by each process when a checkpoint is taken
    * *Drawbacks*.
        * These calculations are complex
        * The coordination between processes is not the performance dominating factor

            $\to$ The overhead of having to save the state to local stable storage is dominating
    * *Consequence*. Coordinated checkpointing, which is much simpler than independent checkpointing, is often more popular

## Message logging
**Message logging**. A technique to reduce the number of checkpoints, since checkpointing can be an expensive operation
* *Basic idea*. If the transmission of messages can be replayed, we can still reach a distributed snapshot without having to restore it from local storages

    $\to$ A checkpointed state is taken as a starting point, and all messages, which have been sent since, are retransmitted and handled accordingly
* *Piecewise deterministic execution model*. Message logging works fine under the assumption of piecewise deterministic execution model
    * *Piecewise deterministic execution model*. The execution of each process is assumed to take place as a series of intervals, in which events take place
        * *Example of events*. The execution of an instruction, the sending of a message, etc.
        * *Interval in piecewise deterministic execution model*. 
            * *Initialization*. Start with a nondeterministic event, e.g. the receipt of a message
            * *Iteration*. From that moment on, the execution of the process is completely deterministic
            * *Termination*. End with the last event before a nondeterministic event occurs
    * *Replaying of intervals*. An interval can be replayed with a known result, in a completely deterministic way, provided it is replayed starting with the same nondeterministic event as before
        * *Consequence*. If we record all nondeterministic events in the model

            $\to$ It is possible to completely replay the entire execution of a process in a deterministic way

**Message logging moment**. It is important to know precisely when messages are to be logged

$\to$ Many existing message-logging schemes can be easily characterized if we focus on how they deal with orphan processes
* *Orphan process*. A process surviving the crash of another process, but whose state is inconsistent with the crashed process after its recovery

### Characterizing message-logging schemes
**Stable message for message-logging schemes**.
* *Requirements on message format*. Eahc message $m$ is considered to have a header containing all information necessary to retransmit $m$, and to properly handle it
    * *Example*. Each header identifies
        * The sender and the receiver
        * A sequence number to recognize it as a duplicate
        * A delivery number to decide when exactly the message should be handed over to the receiving application
* *Stable message*. A message is stable if it cannot be lost, e.g. since it has been written to reliable, local storage

    $\to$ These messages can be used for recovery by replaying their transmission

**Dependencies and copies of messages**.
* *Dependencies of messages*. Each message $m$ leads to a set $\text{DEP}(m)$ of processes depending on the delivery of $m$
    * *Explain*. 
        * $\text{DEP}(m)$ consists of processes, to which $m$ has been delivered
        * If another message $m^*$ is causally dependent on the delivery of $m$, and $m^*$ has been delivered to a process $Q$

            $\to$ $Q$ will be contained in $\text{DEP}(m)$
    * *Causally dependency of messages*. A message $m^*$ is causally dependent on the delivery of another message $m$ if
        * $m^*$ was sent by the same processe, which previously delivered $m$, or
        * $m^*$ was sent by the process, which had delivered another message, which was causally dependent on the delivery of $m$
* *Copies of messages*. 
    * $\text{COPY}(m)$ consists of processes, which have a copy of $m$, yet not reliably stored it, i.e.
    * When a process $Q$ delivers $m$, it becomes a member of $\text{COPY}(m)$
    * $\text{COPY}(m)$ consists of processes, which could hand over a copy of $m$, which can be used to replay the transmission of $m$

    >**NOTE**. If processes in $\text{COPY}(m)$ crash, replaying the transmission of $m$ is not feasible

**Orphan process**.
* *Assumptions*.
    * $\text{FAIL}$ is the collection of crashd processes
    * $Q$ is one of the survivors
* *Orphan process*. $Q$ is an orphan process if
    
    $$\exists m,Q\in\text{DEP}(m)\land \text{COPY}(m)\subseteq \text{FAIL}$$

* *Interpretation*. $Q$ is orphan when it depends on $m$, but there is no way to replay $m$'s transmission

**Avoidance of orphan processes**. We need to ensure that if each process in $\text{COPY}(m)$ crashed

$\to$ No surviving process is left in $\text{DEP}(m)$, i.e. all processes in $\text{DEP}(m)$ should have crashed
* *Idea*. The condition can be enforced if we ensure that whenever a process becomes a member of $\text{DEP}(m)$

    $\to$ It also becomes a member of $\text{COPY}(m)$
* *Pessimistic logging protocols*. For each nonstable message $m$, ensure that there is at most one process dependent on $m$
    * *Explain*. Each nonstable message $m$ is delivered to at most one process
        * *Consequence*. As soon as $m$ is delivered to a process $P$, then $P\in\text{COPY}(m)$
    * *Worst case*. $P$ crashes without $m$ ever having been logged
        * *Solution*. $P$ is not allowed to send any messages after the deliver of $m$, without having ensured that $m$ has been written to reliable storage

            $\to$ No other processes will become dependent on the delivery of $m$ to $P$, without having the possibility of replaying the transmission of $m$
* *Optimistic logging protocol*. Consider a message $m$, for which each process in $\text{COPY}(m)$ has crashed

    $\to$ Any orphan process in $\text{DEP}(m)$ is rolled back to a state, in which it no longer belongs to $\text{DEP}(m)$
    * *Consequence*. The dependencies must be kept track, hence complicate the implementation
* *Conclusion*. Pessmistic logging is simplier than optimistic approaches, hence is preferred in practical distributed system design

## Recovery-oriented computing
**Recovery-oriented computing**. A related way of handling recovery is to start over again
* *Underlying principle*. It may be much cheaper to optimize for recovery

    $\to$ This is aiming for systems, which are free from failures for a long time

**Rebooting**. A flavor of recovery-oriented computing, which simply reboots part of a system
* *Fault localization*. To reboot only a part of the system, the fault must be properly localized

    $\to$ At this point, rebooting means deleting all instances of the identified components, along with the threads operating on them, and to restart the associated requests

    >**NOTE**. Fault localization may be a nontrivial exercise

* *Components decoupling*. To enable rebooting as a practical recovery technique, components must be largely decoupled
    
    $\to$ There are few or no dependencies between different components
    * *Explain*. If there are strong dependencies, then fault localization and analysis may still require that a complete server needs to be restarted

        $\to$ At this point, applying traditional recovery techniques as given above may be more efficient

**Checkpointing and recovery**. A flavor of recovery-oriented computing
* *Idea*. Apply checkpointing and recovery techniques, but continue execution in a changed environment
    * *Motivation*. Many failures can be avoided if 
        * Programs are given some more buffer space
        * Memory is zeroed before allocated
        * Changing the ordering of message delivery while keeping their semantics, etc.
    * *Consequence*. We should tackle software failures
* *Consequence*. Software execution is highly deterministic, hence changing an execution environment may save the day, without repairing anything