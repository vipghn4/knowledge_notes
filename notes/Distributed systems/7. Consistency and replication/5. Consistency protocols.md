<!-- TOC titleSize:1 tabSpaces:2 depthFrom:1 depthTo:6 withLinks:1 updateOnSave:1 orderedList:0 skip:0 title:1 charForUnorderedList:* -->
# Table of Contents
- [Table of Contents](#table-of-contents)
- [Consistency protocols](#consistency-protocols)
  - [Continuous consistency](#continuous-consistency)
    - [Bounding numerical deviation](#bounding-numerical-deviation)
    - [Bounding staleness deviations](#bounding-staleness-deviations)
    - [Bounding ordering deviations](#bounding-ordering-deviations)
  - [Primary-based protocols](#primary-based-protocols)
    - [Remote-write protocols](#remote-write-protocols)
    - [Local-write protocols](#local-write-protocols)
  - [Replicated-write protocols](#replicated-write-protocols)
    - [Active replication](#active-replication)
    - [Quorum-based protocols](#quorum-based-protocols)
  - [Cache-coherence protocols](#cache-coherence-protocols)
  - [Implementing client-centric consistency](#implementing-client-centric-consistency)
- [Appendix](#appendix)
  - [Discussion](#discussion)
<!-- /TOC -->

# Consistency protocols
**Consistency protocols**. Describe an implementation of a specific consistency model

## Continuous consistency
**Continuous consistency**. Yu and Vahdat (2000) have developed a number of protocols to tackle the three forms of consistency

### Bounding numerical deviation
**Idea**. Consider write operations on a data item $x$
* *Assumptions*.
    * $W(x)$ is a write with an associated value $\text{val}(W(x))$, or $\text{val}(W)$ representing the numerical value, by which $x$ is updated
        * $\text{val}(W) > 0$ is assumed
    * *Write's origin*. Each write $W$ is initially submitted to one out of the $N$ available replica servers

        $\to$ In this case, the server becomes the write's origin, denoted as $\text{origin}(W)$
    * *Log of writes*. At any specific moment in time, there are several submitted writes required to be propagated to all servers
        
        $\to$ Each server $S_i$ will keep track of a log $L_i$ of writes, which it has preformed on its own local copy of $x$
    * *The effect of performing the writes executed by $S_i$ that originated from $S_j$*.

        $$\text{TW}[i,j] = \sum \{\text{val}(W):\text{origin}(W)=S_j \land W\in L_i\}$$

        * *Explain*. $\text{TW}[i,i]$ represents the aggregated writes submitted to $S_i$
* *Objective for any time $t$*. Let the current value $v_i$ of $x$ at $S_i$ deviate within bounds from the actual value $v$ of $x$
    * *Actual value of $x$*. Determined by all submitted writes, i.e. if $v_0$ is the initial value of $x$, then

        $$v = v_0 + \sum_{k=1}^N \text{TW}[k,k],\quad v_i = v_0 + \sum_{k=1}^N \text{TW}[i,k]$$
    * *Upperbound on absolute deviation*. Since $v_i\leq v$, thus, for every server $S_i$, we associate an upperbound $\delta_i$ we want to enforce

        $$v-v_i\leq \delta_i$$
* *Write propagation*. Writes submitted to $S_i$ will need to be propagated to all other servers, according to several ways

**Epidemic protocol**. Allow rapid dissemination of updates
* *Observations*. 
    * When $S_i$ propagates a write originating from $S_j$ to $S_k$

        $\to$ $S_k$ can learn about $\text{TW}[i,j]$ at the time the write was sent
        * *Explain*. $S_k$ can maintain a view $\text{TW}_k[i,j]$ of what it believes $S_i$ will have as the value for $\text{TW}[i,j]$
    * We have that

        $$0\leq \text{TW}_k[i,j] \leq \text{TW}[i,j] \leq \text{TW}[j,j]$$
* *Idea*. When $S_k$ notices that $S_i$ has not been staying in the right pace with updates submitted to $S_k$

    $\to$ It forwards writes from its log to $S_i$
    * *Consequence*. This forwarding effectively advances the view $\text{TW}_k[i,k]$ that $S_k$ has of $\text{TW}[i,k]$

        $\to$ This makes $\text{TW}[i,k]-\text{TW}_k[i,k]$ smaller
* *Implementation*. $S_k$ advances its view on $\text{TW}[i,k]$ when an application submits a new write increasing $\text{TW}[k,k] - \text{TW}_k[i,k]$ beyond $\delta_i/(N-1)$

    $\to$ $v-v_i\leq \delta_i$ is ensured

### Bounding staleness deviations
**Approach 1**. 
* *Idea*. Server $S_k$ keep a real-time vector clock $\ce{RVC_k}$ where $\ce{RVC_k}[i] = t_i$ means $S_k$ has seen all writes submitted to $S_i$ up to time $t_i$

    >**NOTE**. $t_i$ denotes the time local to $S_i$

* *Bounding staleness protocol*. If the clocks between the replica servers are loosely synchronized, then whenever $S_k$ notes that $t_k - \ce{RVC_k}[i]$ is about to exceed a specified limit

    $\to$ $S_k$ starts pulling in writes originated from $S_i$ with a timestamp later than $\ce{RVC_k}[i]$ 
* *Bounding staleness versus bounding numerical deviations*.
    * *Bounding staleness deviation*. A replica server is responsible for keeping its copy of $x$ up to date, regarding writes which have been issued elsewhere
    * *Bounding numerical deviation*. The server originating a write must keep replicas up to date by forwarding writes
* *Why using pull rather than push protocol for bounding staleness*. The problem with pushing writes in the case of staleness is that when it is unknown in advance what the maximal propagation time will be

    $\to$ No guarantees can be given for consistency
    * *Consequence*. By pulling in updates, multiple servers can help to keep a server's copy of $x$ fresh

### Bounding ordering deviations
**Motivation**. Ordering deviations in continuous consistency are caused by the fact that a replica server tentatively applies updates submitted to it

$\to$ Each server will have a local queue of tentative writes , for which the actual order in which they are to be applied to the local copy of $x$ still needs to be determined

**Bounding ordering deviation**. Ordering deviation is bounded by specifying the maximal length of the queue of tentative writes
* *Enforcing ordering consistency* Detecting when ordering consistency needs to be enforced is simple
    * *Idea*. When the length of the local queue exceeds a specified maximal length

        $\to$ A server will no longer accept any newly submitted writes, and attempt to commit tentative writes by negotiating with other servers, in which order its writes should be executed
    * *Explain*. We need to enforce a globally consistent ordering of tentative writes

## Primary-based protocols
**Problem**. As soon as consistency models become difficult to understand for application developers

$\to$ They are ignored even if performance could be improved
* *Explain*. If the semantics of a consistency model are not intuitively clear, application developers will have a hard time building correct applications

    $\to$ Simplicity is appreciated

**Primary-based protocols**. Prevail in sequential consistency
* *Idea*. Each data item $x$ in the data store has an associated primary, which is responsible for coordinating write operations on $x$
* *Implementation*.
    * *Option 1*. The primary is fixed at a remote server
    * *Option 2*. Write operations can be carried out locally after moving the primary to the process, where the write operation is initiated

### Remote-write protocols
**Primary-backup protocols**., The simplest primary-based protocol supporting replication
* *Idea*. 
    * All write operations need to be forwarded to a fixed single server
    * Read operations can be carried out locally
* *Procedure*.

    <div style="text-align:center">
        <img src="https://i.imgur.com/lRigfcC.png">
        <figcaption>The principle of a primary-backup protocol</figcaption>
    </div>

    1. A process wants wanting to perform a write operation on data item $x$ forwards the operation to the primary server for $x$
    2. The primary performs the update on its local copy of $x$, then forwards the update to the backup server
    3. Each backup server performs the update as well, and sends an ACK back to the primary
    4. When all backups have updated their local copy, the primary sends an ACK back to the initial process
* *Performance problem*. It may take a relatively long time, before the process initiating the update is allowed to continue

    $\to$ An update is implemented as a blocking operation
* *Non-blocking alternative approach*. As soon as the primary has updated its local copy of $x$

    $\to$ It returns an ACK, then tells the backup servers to perform the update as well
    * *Pros*. Write operations may speed up considerably
    * *Cons*. Fault tolerance must be introduced, since the client process cannot ensure that the update operation is backed up by several other servers

**Sequential consistency via primary-backup protocols**. This is straightforward, since the primary can order all incoming writes in a globally unique time order

$\to$ All processes see all write operations in the same order, no matter which backup server they use to perform read operations
* *Guarantee of most recent writes*. With blocking protocols, processes will always see the effects of their most recent write operation

    >**NOTE**. This cannot be guaranteed with a nonblocking protocol without taking special measures

### Local-write protocols
**Local-write protocols**. The primary copy migrates between processes, which wish to perform a write operation

<div style="text-align:center">
    <img src="https://i.imgur.com/14aKDwC.png">
    <figcaption>Primary-backup protocol where the primary migrates to the process wanting to perform an update</figcaption>
</div>

* *Idea*. When a process wants to update data item $x$, it locates the primary copy of $x$, and moves it to its own location
* *Pros*. Multiple, successive write operations can be carried out locally, while reading processes can still access their local copy
* *Cons*. The advantage can be achieved only if a nonblocking protocol is followed by which updates are propagated to the replicas, after the primary has finished with locally performing the updates

**Local-write protocol applied to mobile computers**. These protocols can be applied to mobile computers, which are able to operate in disconnected mode
* *Idea*. 
    1. Before disconnecting, the mobile computer becomes the primary server for each item it expects to update
    2. While being disconnected, all update operations are carried out locally

        $\to$ Other processes can still perform read operations, but with no updates
    3. When connecting again, updates are propagated from the primary to the backups

        $\to$ The data store is brought in a consistent state again

**Nonblocking local-write primary-based protocols for distributed file systems**.
* *Idea*. There may be a fixed central server, through which normally all write operations take place
    1. The server temporarily allows one of the replicas to perform a series of local updates, since this may considerably speed up performance
    2. When the replica server is done, the updates are propagated to the central server
    3. The updates are distributed to other replica servers from the central server

## Replicated-write protocols
**Replicated-write protocols**. Write operations can be carried out at multiple replicas rather than only one

### Active replication
**Active replication**. Each replica has an assocaited process carrying out update operations
* *Update propagation*. Updates are generally propagated by means of the write operation causing the update

    $\to$ This is in contrast to other protocols
    * *Explain*. The operation is sent to each replica

    >**NOTE**. It is also possible to send the update

* *Drawback*. Operations need to be carried out in the same order everywhere

    $\to$ A total-ordered multicast mechanism is required
* *Total-ordering multicast mechanism*. Use a central coordinator, i.e. a sequencer
    1. Forward each operation to the sequencer
    2. The sequencer assigns the operation a unique sequence number
    3. The sequencer forwards the operation to all replicas
    4. Operations are carried out in the order of their sequence number

### Quorum-based protocols
**Quorum-based protocols**. Support replicated writes using voting
* *Idea*. Require clients to request and acquire the permission of multiple servers before reading or writing a replicated data item
* *Example*. Consider a distributed file system and a file replicated on $N$ servers
    * *File update rule*.
        1. A client contacts at least half the servers plus one, i.e. a majority
        2. The client gets the servers to agree to do the update
        3. Once the servers have agreed, the file is changed, and a new version number is associated with the new file
        4. The version number is used to identify the version of the file, and is the same for all newly updated files
    * *File reading rule*.
        1. The client contacts at least half the servers plus one, asking them to send the version numbers associated with the file
        2. If all the version numbers are the same, this must be the most recent version
            * *Explain*. An attempt to update only the remaining servers would fail since there are not enough of them

**More general scheme**.
* *Reading operation*. To read a file existing in $N$ replicas

    $\to$ A client needs to assemble a read quorum, i.e. an arbitrary collection of any $N_R$ servers, or more
* *Writing opertion*. To modify a file, a write quorum of at least $N_W$ servers is required
* *Values of $N_R$ and $N_W$*. Must satisfy the constraints

    $$N_R + N_W > N,\quad N_W > N/2$$

    * *Explain*.
        * The first constraint is used to prevent read-write conflicts
        * The second constraint prevents write-write conflicts

## Cache-coherence protocols
**Caches**. A special case of replication, which is controlled by clients instead of servers
* *Cache-coherence protocols*. Ensure that a cache is consistent with the server-initiated replicas
* *Solutions to cache-coherence*. 
    * *Solutions for shared-memory multirprocessor systems*. Many solutions based on support from the underlying hardware
    * *Solutions for middleware-based distributed systems*. Software-based solutions to caches are more interesting
* *Criteria to classify caching protocols*.
    * Coherence detection strategy
    * Coherence enforcement strategy

**Coherence detection strategy**. Refer to when inconsistencies are actually detected
* *Classification of methods based on coherence detection strategy*.
    * *Static solutions*. 
        1. A compiler is assumed to perform the necessary analysis prior to execution
        2. The compiler determines which data may lead to inconsitencies since they may be cached
        3. The compiler then inserts instructions to avoid inconsistencies
    * *Dynamic solutions*. Inconsistencies are detected at runtime
* *Classification of dynamic detected-based protocols*. Based on when, during a transaction, the detection is done
    * *Option 1*. When during a transaction, a cached data item is accessed
        1. The client needs to verify whether that data item is still consistent with the version stored at the possibly replicated server
        2. The transaction cannot proceed to use the cached version, until its consistency has been definitely validated
    * *Option 2*. Let the transaction proceed while verification is taking place
        * *Assumption*. The cached data were up to date when the transaction started

            $\to$ If this assumption later proves to be false, the transaction will have to abort
    * *Option 3*. Verify whether the cached data are up to date only when the transaction commits
        1. The transaction just starts operating on the cached data
        2. After all the work has been done, accessed data are verifed for consistency
        3. When stale data were used, the transaction is aborted

**Coherence enforcement strategy**. Determine how caches are kept consistent with the copies stored at servers
* *Simplest solution*. Disallow shared data to be cached
    * *Idea*.
        * Shared data are kept only at the servers, which maintain consistency using one of the primary-based or replication-write protocols
        * Clients are allowed to cache only private data
    * *Drawback*. This solution can offer only limited performance improvements
* *Allow-caching solution*. Cache coherence can be enforced with two options
    * *Option 1*. Let a server send an invalidation message to all caches whenever a data item is modified
    * *Option 2*. Propagate the udpate to clients

    >**NOTE**. Dynamically choosing between two options is sometimes supported in client-server databases

**Cache data modification**.
* *Read-only cache*. Update operations can be performed only by servers, which follow some distribution protocol to ensure that updates are propagated to caches

    $\to$ In many cases, a pull-based approach is followed
    * *Pull-based approach*. A client detects that its cache is stale, and requests a server for an update
* *Write-through cache*. Allow clients to directly modify the cached data, and forward the update to the servers
    * *Idea*. Similar to a primary-based local-write protocol, where the client's cache has become a temporary primary
    * *Guarantee sequential consistency*. It is required that the client has been granted the exclusive write permissions

        $\to$ Otherwise write-write conflicts may occur
    * *Performance*. Potentially improved over other schemes as all operations can be carried out locally
* *Write-back cache*. Further improvements can be made to write-through if we delay the propagation of updates by allowing multiple writes to take place before informing the servers

## Implementing client-centric consistency
**Naive implementation**. Implementing client-centric consistency is relatively straightforward if performance issues are ignored
* *Assumptions*.  Each write operation $W$ is assigned a globally unique identifier
    
    $\to$ The identifier is assigned by the server, to which the write had ben submitted
    * *Origin of $W$*. The server, to which the write had been submitted
* *Idea*. For each client, we keep track of two sets of writes
    * *The read set*. Consist of the writes relevant for the read operations performed by the clietn
    * *The write set*. Consist of writes performed by the client

**Monotonic-read consistency implementation**.
* *Procedure*.
    1. When a client performs a read operation at a server

        $\to$ The server is handed the client's read set to check whether all the identified writes have taken place locally
    2. If not, the server contacts other servers to ensure that it is brought up to date before carrying out the read operation
        * *Alternative option*. The read operation is forwarded to a server, where the write operations have already taken place
    3. After the read operation is performed, the write operations, which have taken place at the selected server and are relevant for the read operation, are added to the client's read set
* *Requirements*.
    * It should be possible to determine exactly where the write operations identified in the read set have taken place
        * *Example*. The write identifier can include the identifier of the server, to which the operation was submitted

            $\to$ That server is required to, for example, log the write operation so that it can be replayed at another server
    * Write operations should be performed in the order they were submitted
        * *Write operation ordering*. Can be achieved by letting the client generate a globally unique sequence number, which is included in the write identifier

**Monotonic-write consistency implementation**. Similar to monotonic reads
* *Procedure*.
    1. When a client initiates a new write operation at a server

        $\to$ The server is handed over the client's write set
    2. The server then ensures that the identified write operations are performed first, and in the correct order
    3. After performing the new operation, that operation's write identifier is added to the write set
* *Client response time*. Bringing the current server up to date with the client's write set may introduce a considerable increase in client's response time
    * *Explain*. The size of the write set may be prohibitively large

**Read-your-writes consistency**. Require that the server, where the read operation is performed, has seen all the write operations in the client's write set
* *Option 1*. The writes can be fetched from other servers, before the read operation is performed

    $\to$ This may lead to a poor response time
* *Option 2*. The client-side software can search for a server, where the identified write operations in the client's write set have already been performed

**Writes-follow-reads consistency**.
1. The selected server is brought up to date with the write operation in the client's read set
2. The server then add the identifier of the write operation to the write set, along with the identifiers in the read set

# Appendix
## Discussion
**Achieving scalability**.

**Client-side caching in NFS**.

**Improving efficiency of client-centric consistency**.