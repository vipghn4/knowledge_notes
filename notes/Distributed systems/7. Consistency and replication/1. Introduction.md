<!-- TOC titleSize:1 tabSpaces:2 depthFrom:1 depthTo:6 withLinks:1 updateOnSave:1 orderedList:0 skip:0 title:1 charForUnorderedList:* -->
# Table of Contents
- [Table of Contents](#table-of-contents)
- [Introduction](#introduction)
  - [Reasons for replication](#reasons-for-replication)
  - [Replication as scaling technique](#replication-as-scaling-technique)
<!-- /TOC -->

# Introduction
**Replication of data**. An important issue in distributed systems, i.e. to enhance reliability or improve performance
* *Major problem*. How to to keep replicas consistent, i.e. when one copy is updated, we need to ensure that the other copies are updated as well

    $\to$ Otherwise, the replicas will no longer be the same

## Reasons for replication
**Reliability increment**. Data are replicated to increase the reliability of a system, i.e.
* If a file system has been replicated, it is possible to continue working after one replica crashes by switching to one of the other replicas
* It is possible to provide better protection against corrupted data
    * *Example*. If there are multiple copies of a file, and every read and write operation is performed on each copy

        $\to$ We can safeguard ourselves against a single, failing write operation, by considering the value returned by at least two copies as being the correct one, i.e. like blockchain

**Performance increment**. Replication for performance is important, when a distributed system needs to scale in terms of size or in terms of geographical area it covers
* *Scaling w.r.t size*. Occur, e.g. when an increasing number of processes needs to access data, which are managed by a single server

    $\to$ Performance can be improved by replicating the server and dividing the workload among processes accessing the data
* *Scaling w.r.t a geographical area*. By placing a copy of data in proximity of the process using them, the time to access the data decreases

    $\to$ The performance as perceived by the process increases
    * *Problem*. More network bandwidth is consumed keeping all replicas up to date

**Cost of replication**. Having multiple copies may lead to consistency problems, i.e. each modification have to be carried out on all copies to ensure consistency

$\to$ Exactly when and how those modifications need to be carried out determines the cost of replication
* *Example*. Consider improving access times to Web pages via replication, then the access time as perceived by the user is excellent

    $\to$ If the user always wants to have the latest version of a page, he may be in for bad luck
    * *Explain*. If the page has been modified in the meantime, modifications will not have been propagated to cached copies, making those copies out-of-date
* *Solution 1*. Forbid the browser to keep local copies in the first place, effectively letting the server be fully in charge of replication
    * *Drawback*. This leads to poor access times if no replica is placed near the user
* *Solution 2*. Let the Web server invalidate or update each cached copy
    * *Drawback*. This requires the server to keep track of all caches, and send them messages

        $\to$ This degrades the overall performance of the server

## Replication as scaling technique
**Replication as scaling technique**. Replication and caching for performance are widely applied as scaling techniques

**Performance issue**. Scalability issues generally appear in the form of performance problems

$\to$ Placing copies of data close to the processes using them can improve performance through reduction of access time, hence solve scalability problems
* *Trade-off*. Keeping copies up to date may require more network bandwidth
    * *Assumptions*.
        * A process $P$ accesses a local replica $N$ times per second
        * The replicate is updated $M$ times per second, where each update completely refreshes the previous version of the local replica
    * *Problem*. If $N\ll M$, i.e. the access-to-update ratio is very flow

        $\to$ Many updated versions of the local replica will never be accessed by $P$, rendering the network communication for those versions useless
    * *Solution*. Not install a local replica close to $P$, or apply a different strategy for updating the replica

**Consistency issue**. A collection of copies is consistent when the copies are always the time

$\to$ A read operation performed at any copy will always return the same result
* *Problem*. When an update operation is performed on one copy, the update should be propagated to all copies before a subsequent operation takes place, no matter at which copy that operation is initiated or performed
* *Tight consistency*. This type of consistency is sometimes informally, and imprecisely referred to as tight consistency, as provided by synchronous replication
    * *Key idea*. An update is performed at all copies as a single atomic operation, or transaction
    * *Problem*. Implementing atomicity involving a large number of replicas, which may be widely dispersed across a large-scale network, is inherently difficult when operations are also required to complete quickly
        * *Explain*. 
            * We have to synchronize all replicas, i.e. all replicas first need to reach agreement on when exactly an update is to be performed locally
            * Global synchronization simply takes a lot of communication time, especially when replicas are spread across a WAN
* *General solution*. Relax the consistency constraints, i.e. if we can relax the requirement that updates need to be executed as atomic operations

    $\to$ We may be able to avoid instantaneous global synchronizations, and thus gain performance
    * *Cost*. Copies may not always be the same everywhere
    * *Extent of relaxation*. Depend highly on the access and update patterns of the replicated data, and the purpose, for which those data are used