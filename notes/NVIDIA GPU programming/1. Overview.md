---
title: 1. Overview
tags: NVIDIA GPU programming
---

# Table of Contents
[toc]

# 1. Introduction

<div style="text-align:center">
    <img src="/media/Q5damfx.png">
</div>

**GPU characteristics**. 
* GPU is specialized for computing-intensive, highly parallel computation

    $\to$ More transistors are devoted to data processing, rather than data caching and flow control, i.e. CPU
* GPU is especially well-suited to address problems, which can be expressed as data-parallel computations

**CUDA programming models**.
* *Challenges in parallel systems*. Develop application software which can transparently scales its parallelism to leverage the increasing number of processor cores
    * *CUDA parallel programming model*. Overcome the challenge while maintaining a low learning curve for programmers familiar with C
* *Key abstractions*. A hierarchy of thread groups, shared memories, and barrier synchronization

**Streaming multiprocessors (SMs)**. A GPU is built around an array of streaming multiprocessors

<div style="text-align:center">
    <img src="/media/LuYm42M.png">
    <figcaption>GPU automatic scalability</figcaption>
</div>

* *Programming model*. A multithreaded program is partitioned into blocks of threads, which execute indepedently from each other

    $\to$ A GPU with more multiprocessors will automatically execute the program faster

# 2. Programming model
## Kernels
**CUDA C**. Extend C by allowing the programmer to define C functions, i.e. kernels, which, when called, are executed $N$ times in parallel by $N$ different CUDA threads

**Syntax**. 
* *Declaration*. Use `__global__` declaration specifier, e.g.

    ```c=
    __global__ void VecAdd(float* A, float* B, float* C){
        int i = threadIdx.x;
        C[i] = A[i] + B[i];
    }
    ```
* *Usage*. Pass the number of CUDA threads to the `<<<...>>>` execution configuration syntax, e.g.

    ```c=+
    int main(){
        ...
        VecAdd<<<1, N>>>(A, B, C);
    }
    ```

## Thread hierarchy
### Threads
**`threadIdx`**. A 3-component vector, for convenience
* *Thread index*.
    * *1D block of size $D_x$*. `threadIdx.x`
    * *2D block of size $(D_x, D_y)$*. `threadIdx.x` and `threadIdx.y`
        * *Flatten map*. A thread of index $(x, y)$ is $(x + y D_x)$ if flattened
    * *3D block of size $(D_x, D_y, D_z)$*. `threadIdx.x`, `threadIdx.y`, and `threadIdx.z`
        * *Flatten map*. A thread of index $(x, y, z)$ is $(x + y D_x + z D_x D_y)$ if flattened
* *Example code*.

    ```c=
    int main(){
        ...
        // Kernel invocation with one block of N * N * 1 threads
        int numBlocks = 1;
        dim3 threadsPerBlock(N, N);
        MatAdd<<<numBlocks, threadsPerBlock>>>(A, B, C);
    }
    ```

**Number of threads**.
* *Number of threads per block*. There is a limit to the number of threads per block
    * *Explain*. All threads of a block are expected to reside on the same processor core and must share the limited memory resources of that core

    >**NOTE**. On current CPUs, a thread block may contain up to 1024 threads

* *Total number of threads*. $N_\text{threads} = N_\text{threads_per_block} \cdot N_\text{blocks}$

>**NOTE**. At a time the gpu will run at least as many blocks as SMs are in the card

### Blocks
**Block organization**. Organized into 1D, 2D, or 3D grid of thread blocks 

<div style="text-align:center">
    <img src="/media/K3Z8SOK.png">
    <figcaption>Grid of thread blocks</figcaption>
</div>

* *Number of thread blocks in a grid*. Usually dictated by the size of the data being processed, or the number of processors in the system, which it can greately exceed
    * *Example code*.

        ```c=
        int main(){
            ...
            dim3 threadsPerBlock(16, 16);
            dim3 numBlocks(N/threadsPerBlock.x, N/threadsPerBlock.y);
            MatAdd<<<numBlocks, threadsPerBlock>>>(A, B, C);
        }
        ```

    * *A common choice for thread block size*. $16 \times 16$

**Block indexing**. Use `blockIdx` variable
* *Example code*.
    
    ```c=
    __global__ void MatAdd(float A[N][N], float B[N][N], float C[N][N]){
        int i = blockIdx.x * blockDim.x + threadIdx.x;
        int j = blockIdx.y * blockDim.y + threadIdx.y;
        if (i < N && j < N) C[i][j] = A[i][j] + B[i][j];
    }
    ```

* *`blockDim`*. Specify the dimension of the thread block

**Thread block execution**. 
* *Dependencies between blocks*. Thread blocks are required to executed independently, i.e. it must be possible to execute them in any order, in parallel or in series
    * *Purpose*. Allow thread blocks to be scheduled in any order across any number of cores

        $\to$ Programmers can write code that scales with the number of cores
* *Block shared memory*. Threads within a block can cooperate via a shared memory, and by synchronizing their execution to coordinate memory accesses
    * *Synchronization points declaration in kernel*. Call `__syncthreads()` instrinsic function
        * *Explain*. `__syncthreads()` acts as a barrir, at which all threads in the block must wait before any is allowed to proceed
    
    >**NOTE**. For efficient cooperation, shared memory is expected to be a low-latency memory near each processor core
    
    >**NOTE**. `__syncthreads()` is expected to be lightweight for efficiency

## Memory hierarchy

<div style="text-align:center">
    <img src="/media/jff8TNt.png">
    <figcaption>Memory hierarchy</figcaption>
</div>

**Memory hierarchy in CUDA threads**. CUDA threads may access data from multiple memory spaces during execution, i.e.
* *Thread private local memory*. Each thread has private local memory
* *Block shared memory*. Each thread block has shared memory visible to all threads of the block, and with the same lifetime as the block
* *Global memory*. All threads have access to the same global memory
* *Constant memory*. All threads have access to this read-only memory
    * *Usage*. Optimized for broadcast, i.e. when the threads in a warp all read the same memory location

        $\to$ When a read is being broadcast to the threads, constant memory is much faster than texture memory
* *Texture memory*. All threads have access to this read-only memory

>**NOTE**. Global, constant, and texture memory spaces are optimized for different memory usages

>**NOTE**. Texture memory also offers different addressing modes, and data filtering (for some specific data formats)

>**NOTE**. Global, constant, and texture memory spaces are persistent across kernel launches by the same application

## Heterogeneous programming

<div style="text-align:center">
    <img src="/media/OVY8OuW.png">
    <figcaption>Heterogeneous programming</figcaption>
</div>

**Execution environment**. CUDA threads execute on a physically seprate *device*, which operates as a coprocessor to the *host* running the C program
* *Example*. The kernels execute on a GPU and the rest of the C program executes on a CPU
* *Memory space*. Both the host and the device maintain their own separate memory spaces in DRAM, i.e. host memory and device memory

    $\to$ The progarm manages the globa, constant, and texture memory space visible to kernels through calls to the CUDA runtime, ei.e. memory allocation, deallocation, and data transfer between host and device memory

**Unified memory**. Provide managed memory to bridge the host and device memory spaces
* *Managed memory*. Accessible from all CPU and GPUs in the system, as a single, coherent memory image with a common address space

## Compute capability
**Compute capability of a device**. Also called SM version. A version number identifying the features supported by the GPU hardware
* *Usage*. Used by applications at runtime to determine which hardware features and/or instructions are available on the present GPU
* *Format*. $X\dot Y$ where $X$ is the major revision number and $Y$ is the minor revision number
    * *Major revision number*. Devices with the same major revision number are of the same architecture
    * *Minor revision number*. Correspond to an incremental improvement to the core architecture, e.g. including new features

# Appendix
## Concepts
**Data-parallel computations**. The same program is executed on many data elements in parallel, with high arithmetic intensity
* *Characteristics*.
    * The same program is executed for each data element

        $\to$ There is a lower requirement for sophisticated flow control
    * The program is executed on many data elements and has high arithmetic intensity

        $\to$ The memory access latency can be hidden with calculations, instead of big data caches
* *Idea*. Map data elements to parallel processing threads

**Arithmetic intensity**. The ratio of arithmetic operations to memory operations

**Just-in-time (JIT) compilation**. The code loaded by an application at runtime is compiled further to binary code by the device driver

## Discussion
**Passing multi-dimensional arrays or pointers to CUDA kernel?**.

**Hardware perspective of threads and blocks**. An SM is able to execute several thread blocks in parallel. As soon as one of its thread block has completed execution, it takes up the serially next thread block

<div style="text-align:center">
    <img src="/media/5QxFL8m.png">
    <figcaption>A pictorial correlation of a programmer's perspective versus a hardware perspective of a thread block in GPU</figcaption>
</div>

* *SM structure*.
    * *Execution cores*. Single precision floating-point units, double precision floating-point units, and special function units (SFUs)
    * *Caches*. L1 cache, shared memory, constant cache, texture cache
    * *Schedulers for warps*
    * *A substantial number of registers*

<div style="text-align:center">
    <img src="/media/94SKuMB.png">
    <figcaption>Hardware structure of a streaming multiprocessor</figcaption>
</div>

* *Thread block execution*. The hardware schedules thread blocks to an SM

    >**NOTE**. In general, an SM can handle multiple thread blocks at the same time

    * Whenever an SM executes a thread block, all the threads inside the block are executed at the same time

* *Number of thread blocks of an SM*. 8

<div style="text-align:center">
    <img src="/media/XLRJyrf.png">
    <figcaption>Volta GV100 streaming multiprocessor</figcaption>
</div>

<div style="text-align:center">
    <img src="/media/jdIRiuW.jpg">
    <figcaption>Volta GV100 Full GPU with 84 SM Units</figcaption>
</div>

<div style="text-align:center">
    <img src="/media/IF6KYQ0.png">
    <figcaption>3D view of Volta GV100 with 84 SM units</figcaption>
</div>

**Streaming multiprocessor with tensor cores**.

<div style="text-align:center">
    <img src="/media/uRoSB8S.png">
    <figcaption>Each subcore (processing block) in the NVIDIA Tesla V100 PCI-E architecture contains 2 TCUs.</figcaption>
    <figcaption>In total, 640 TCUs are available â€” achieving a theoretical peek of 113 TFLOPS.</figcaption>
</div>

<div style="text-align:center">
    <img src="/media/3IEEc9v.png">
    <figcaption>Volta GPU subcore execution flow.</figcaption>
</div>

**Warps**. A thread block is composed of warps
* *Warp*. A set of 32 threads within a thread block, such that all the threads in a warp execute the same instruction
    * *Other definitions*. 
        * Groups with threads with consecutive thread indexes are bundled into warps
        * The unit of allocation of work within streaming multiprocessors

            >**NOTE**. Each streaming multiprocessor is shared by many warps, possibly from multiple blocks
        
    * *Memory allocation for warps*.
        * Registers are partitioned between warps
        * Shared memory is partitioned between blocks
* *Warp execution*. One full warp is executed on a single CUDA core

    <div style="text-align:center">
        <img src="/media/ObDeDKZ.png">
        <figcaption>Execution of warps</figcaption>
    </div>

    * *Explain*. A thread block is divided into a number of warps for execution on the cores of an SM

        >**NOTE**. The size of a warp depends on the hardware
    
    * *Single-instruction, multiple-thread(SIMT)*. The execution style of threads is SIMT
        * *Explain*. Multiple threads are processed by a single instruction in lock-step

            $\to$ Each thread executes the same instruction, but possibly on different data
            
            >**NOTE**. The term "lockstep" originates from army usage, where it refers to synchronized walking, in which marchers walk as closely together as physically practical

    * *Warp selection*. These threads are selected serially by the SM
        * *Thread block selection*. Once a thread block is launched on a multiprocessor (SM)

            $\to$ All of its warps are resident until their execution finishes
            * *Consequence*. A new block is not launched on an SM until 
                * There is sufficient number of free registers for all warps of the new block, and 
                * There is enough free shared memory for the new block
        * *Warp selection*. The warp scheduler selects read-to-run warp, and issue the next instruction to that warp's active threads

    * *Warp context switching*. Consider a warp of 32 threads executing an instruction
        * *Scenario*. If one or both of its operands are not read, e.g. have not yet been fetched from global memory

            $\to$ Process called "context switching" takes place which transfers control to another warp
        * *Context switching procedure*. 
            * When switching away from a particular warp

                $\to$ All the data of that warp remains in the register file so that it can be quickly resumed when its operands become ready

                >**NOTE**. This differs from traditional CPU thread context switching, which requires saving and restoring allocated register values and the PC to off-chip memory, or cache

            * When an instruction has no outstanding data dependencies, i.e. both of its operands are ready

                $\to$ The respective warp is considered to be ready for execution

                >**NOTE**. If more than one warps are eligible for execution
                >$\to$ The parent SM uses a warp scheduling policy for deciding which warp gets the next fetched instruction

        * *Warp scheduling policies*. Round Robin (RR), Least Recently Fetched (LRF), Fair (FAIR), and Thread block-based CAWS (critically aware warp scheduling)
* *Warp optimization*. 
    * *Idle threads*. The proportion of idle threads at a time must not be too large

        >**NOTE**. This may be the direct consequence of warp divergence (below)

    * *Warp divergenve*. When different threads in a warp need to do different things
        * *Warp branching execution*. 
            1. Individual threads composing a SIMT warp start together at the same program address, but they are otherwise free to branch and execute independently
            2. A warp serially executes each path, disabling some of the threads
                * *Explain*. All threads execute all conditional branches
                    * *Example*. If half of threads take `if` and another half take `else`

                        $\to$ The warp executes both branches, one after another
                    
                    >**NOTE**. If the branches are big
                    >$\to$ `nvcc` compiler inserts code to check if all threads in the warp take the same branch, then branches accordingly
            3. When all paths complete, the threads reconverge
        
        >**NOTE**. Divergence only occurs within a warp
        >* *Explain*. Different warps execute independently

        * *Worst case*. When one thread needs expensive branch, while the rest do nothing
        * *Consequences*. Programmers can ignore SIMT executions, but they can achieve performance improvements if threads in a warp do not diverge
    * *Shared memory bank conflict*. 
        * *Memory bank*. The shared memory of each block is divided into memory banks, each of which can only service one address per cycle

            $\to$ Shared memory can be accessed in parallel
        * *Memory bank organization*. 
                * *Bank organization*.
                    * *Compute capability 5.x*
                        * Successive 32-bit words are assigned to successive banks
                        * Each bank as a bandwidth of 32 bits every clock cycle
                    * *Compute capability 3.x*.
                        * There are two modes, i.e. successive 32-bit words or 64-bit words are assigned to successive banks
                        * Each bank has a bandwidth of 64 bits every clock cycle
                * *The number of memory banks*. Can be found on NVIDIA homepage, i.e. 
                    * 16 banks for compute capability 1.x
                    * 32 banks for compute capability 2.x and 3.x

        * *Broadcasting*.
            * *Broadcast*. If all threads of a warp all request exactly the same value

                $\to$ The value will be read once from shared memory and broadcast to the threads
            * *Multicast*. If several threads request the same value from a particular bank

                $\to$ The value will be read once from shared memory
        * *Memory bank conflict*. If different values are requested from a bank

            $\to$ Requests are serialized and performance takes a hit
        * *Consequences*. If different threads of a warp each accesses a different bank from shared memory

            $\to$ The performance is extremely fast
            * *Suggestion*. Have every thread in a warp access consecutive words from shared memory
    * *Memory coealescing*. A technique which allows optimal usage of the global memory bandwidth
        * *Idea*. When parallel threads running the same instruction access to consecutive locations in the global memory

            $\to$ The most favorable access pattern is achieved, i.e. all the accesses are combined into a single request, i.e. coalesced, by the hardware
    * *Redundant transfers from global memory*. We can use shared memory and `__syncthreads()` to avoid redundant transfers from global memory

        >**NOTE**. In fact, we can use shared memory as a tool to eliminate any costly data access to global memory or local memory

        >**NOTE**. Data copy from global memory to shared memory can be asynchronous, giving us the ability to carry out computation during data transfer

**Local memory**. Local memory is named since its scope is local to the thread, not because of its physical location
* *Physical location*. Off-ship

    $\to$ Access to local memory is as expensive as access to global memory
* *Usage*. Hold automatic variables, when `nvcc` determines that there is insufficient register space to hold the variable

>**NOTE**. Device memory allocation and de-allocation with `cudaMalloc` and `cudaFree` are expensive operations
>$\to$ Device memory should be reused and / or sub-allocated by the application whenever possible

**Allocate memory to a pointer in shared memory at runtime**. The purpose of shared memory is to allow the threads in a block to collaborate

$\to$ When we declare an array as `__shared__`, each thread in the block sees the same memory
* *Consequence*. It would not make sense for a given thread to be able to set its own size for an array in shared memory
* *Special case*. We can dynamically specifying the size of a single shared array, which is the same size for all threads
    * *Example*.

        ```c
        __global__ void kernel(const int count) {
            extern __shared__ int a[];
        }
        
        // Kernel invocation
        kernel<<<gridDim, blockDim, a_size>>>(count);
        ```
    
    * *Consequence*. If we need more than one shared array
        
        $\to$ We need to use pointers to offsets within the single allocation

>**NOTE**. Each block in CUDA has a limited shared memory size, which is typically 48kB (depending on the device)
>$\to$ We must be careful about this

## Tricks
**`<` and `==` branching in CUDA**. `<` is much more convergent (warp convergent) than `==