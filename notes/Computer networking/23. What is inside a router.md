---
title: 4. Delay, loss, and throughput in packet-switched networks
tags: Computer networking
---

<!-- TOC titleSize:1 tabSpaces:2 depthFrom:1 depthTo:6 withLinks:1 updateOnSave:1 orderedList:0 skip:0 title:1 charForUnorderedList:* -->
# Table of Contents
- [Table of Contents](#table-of-contents)
- [What is inside a router](#what-is-inside-a-router)
  - [Input processing](#input-processing)
  - [Switching](#switching)
  - [Output processing](#output-processing)
  - [Where does the queuing occur](#where-does-the-queuing-occur)
  - [The rounting control plane](#the-rounting-control-plane)
- [Appendix](#appendix)
  - [Concepts](#concepts)
    - [Concepts](#concepts-1)
    - [Rules for router buffer size](#rules-for-router-buffer-size)
<!-- /TOC -->

# What is inside a router
**Forwarding function of the network layer**. The actual transfer of packets from a router's incoming links to the appropriate outgoing links at that router

>**NOTE**. "Forwarding" and "switching" are often used interchangeably by computer-networking researchers and practitioners

**A generic router architecture**. Consist of four components

<div style="text-align:center">
    <img src="https://i.imgur.com/ZA2WKwf.png">
    <figcaption>Router architecture</figcaption>
</div>

* *Input ports*.
    * *Key functions*.
        * Perform the physical layer function of terminating an incoming physical link at a router

            $\to$ This is the leftmost box of the input port, and the rightmost box of the output port
        * Perform link-layer functions required to interoperate with the link layer at the other side of the incoming link

            $\to$ This is the middle boxes in the input and output ports
        * (Perhaps) perform the lookup function, i.e. the most crucial function

            $\to$ This is the rightmost box of the input port
            * *Idea*. Consult the forwarding table to determine the router output port, to which an arriving packet will be forwarded via the switching fabric
                * *Explain*. Control packets, e.g. packets carrying routing protocol information, are forwarded from an input port to the routing processor
    * *Port*. Refer to the physical input and output router interfaces
        
        $\to$ This is distinctly different from the software ports associated with network applications and sockets
* *Switching fabric*. Connect the router's input ports to its output ports

    $\to$ This is completely contained within the router, i.e. a network inside of a network router
* *Output ports*. Store packets recevied from the switching fabric and transmits these packets on the outgoing link by performing the necessary link-layer and physical-layer functions
    * *Bidirectional links*. When a link is bidirectional, an output port will typically be paired with the input port for that link on the same line card
    * *Line card*. A printed circuit board containing one or more input ports, which is connected to the switching fabric
* *Routing processor*. Execute the routing protocols, maintain routing tables and attached link state information, and computes the forwarding table for the router

    >**NOTE**. This processor also performs network management functions

**Implementation of forwarding function**. A router's input ports, output ports, and switching fabric together implement the forwading function, and are almost always implemented in hardware
* *Router forwarding plane*. The forwarding functions provided by router's input ports, output ports, and switching fabric
    * *Hardware implementation requirement*. Hardware implementation is required for performance perposes
        * *Explain*. If $N$ ports are combined on a line card, as is often done in practice

            $\to$ The datagram-processing pipeline must operate $N$ times faster, which is far too fast for software implementation
    * *Runtime scale*. In the order of nanoseconds
* *Router control plane*. Include executing the routing protocols, responding to attached links which go up or down, and performing management functions
    * *Software implementation requirement*. Router control plane functions are usually implemented in software and execute on the routing processor, which is typically a traditional CPU
    * *Runtme scale*. In the order of milliseconds or seconds

**Analogy of router functionality**. Let suppose that the car interchange is a roundabout, i.e. before a car enters the roundabout, a bit of processing is required
* *Processing step*. 
    1. The car stops at an entry station and indicates its final destination, i.e. not the local roundabout, but the ultimate destionation of its journey
    2. An attendant at the entry station looks up the final destination, determines the roundabout exit leading to the desired destination
    3. The attendant tells the driver which roundabout exit to take
    4. The car enters the roundabout, which may be filled with other cars entering from other input roads and heading to other roundabout exits
    5. The car eventually leaves at the prescribed roundabout exit ramp, where it may encounter other cars leaving the roundabout at that exit
* *Analogy to router components*.
    * *Input ports*. Correspond to the entry road and entry station, with a lookup function to determine to local outgoing port
    * *Switch fabric*. Correspond to the roundabout
    * *Output ports*. Correspond to the roundabout exit roads
* *Possible bottlenecks*. 
    * If cars arrive blazingly fast but the station attendant is slow, then bottleneck occurs
        
        $\to$ How fast must the attendant work to ensure there is no backup on an entry road
    * Even with a blazingly fast attendant, if cars traverse the roundabout slowly, can backups still occur?
    * If most of the entering cars all want to leave the roundabout at the same exit ramp

        $\to$ Can backups occur at the exit ramp or elsewhere?
    * How should the roundabout operate if we want to assign priorities to different cars, or block certain cars from entering the roundabout in the first place?

## Input processing
**Main steps**. Line termination function, link-layer processing, and lookup function

>**NOTE**. The lookup performed in the input port is central to the router's operation
>$\to$ The router uses the forwarding table to look up the output port, to which an arriving packet will be forwarded via the switching fabric

<div style="text-align:center">
    <img src="https://i.imgur.com/LbmfWik.png">
    <figcaption>Input port processing</figcaption>
</div>

**Forwading table**. Computed and updated by the routing processor, with a shadow copy typically stored at each input port
* *Forwading table copying*. The forwading table is copied from the routing processor to the line cards over a separate bus, e.g. a PCI bus
* *Consequence*. With a shadow copy, forwarding decisions can be made locally, at each input port, without invoking the centralized routing processor on a per-packet basis

    $\to$ This avoids a centralized processing bottleneck

**Lookup function**. Simply search through the forwarding table looking for the longest prefix match
* *Lookup algorithm*. At Gigabit transmission rates, this lookup must be performed in nanoseconds

    $\to$ Not only must lookup be performed in hardware, but techniques beyond a simple linear search through a large table are needed
* *Memory access time*. Special attention must be paid to memory access times, resulting in designs with embedded on-chip DRAM and faster SRAM, i.e. used as a DRAM cache, memories
    * *Ternary content address memories (TCAMs)*. Often used for lookup
        * *Idea*. A 32-bit IP address is presented to the memory, which returns the content of the forwarding table entry for that address in essentially constant time
    * *Typical memory size*. The Cisco 8500 has 64K CAM for each input port

**Forwading packets to switching fabric**. Once a packet's output port has been determined via the lookup, the packet can be sent into the switching fabric
* *Packet blocking*. In some design, a packet may be temporarily blocked from entering the switching fabric if packets from other input ports are currently using the fabric

    $\to$ A blocked packet will be queued at the input port and then scheduled to cross the fabric at a later point of time

**Other actions by inpurt ports**.
* Physical- and link-layer processing must occur
* The packet's version number, checksum, and time-to-live field must be checked and the latter two fields rewritten
* Counters used for network management, e.g. the number of IP datagrams received, must be updated

**Match plus action abstraction in networked devices (not just routers)**. The input port steps of looking up an IP address, i.e. match, then sending the packet into the switching fabric, i.e. action, is a specific case of "match plus action" abstraction
* *Other examples*.
    * *Link-layer switches*. Link-layer destination addresses are looked up and several actions may be taken, additional to sending the frame into the switching fabric towards the output port
    * *Firewalls*. Devices which filter out selected incoming packets, i.e. packets whose header matches a given criteria, e.g. a combination of source - destination IP addresses and transport-layer port numbers, may be prevented from being forwarded (action)
    * *Network address translator (NAT)*. An incoming packet, whose transport-layer port number matches a given value, will have its port number rewritten before forwarding (action)

## Switching
**The switching fabric**. The very heart of a router, i.e. through this fabric that packets are actually switched, i.e. forwarded, from an input port to an output port

**Switching methods**.

<div style="text-align:center">
    <img src="https://i.imgur.com/R6zGShd.png">
    <figcaption>Switching techniques</figcaption>
</div>

* *Switching via memory*. The simplest method, used in earliest routers, which were traditional computers
    * *Idea*. Switching between input and output ports is done under direct control of the CPU, i.e. routing processor

        $\to$ Input and output ports functioned as a traditional I/O devices in a traditional OS
    * *Data flow*.
        1. An input port with an arriving packet first signaled the routing processor via an interrupt
        2. The packet was then copied from the input port into processor memory
        3. The routing processor then extracted the destination address from the header
        4. The routing processor looks up the appropriate output port in the forwading table
        5. The routing processor copies the packet to the output port's buffers
    * *Drawback*. 
        * If the memory bandwidth is $B$ packets per second, i.e. can be written into or read from memory

            $\to$ The overall forwading throughput must be less than $B/2$
        * Two packets cannot be forwarded at the same time, even if they have different destination ports
            * *Explain*. Only one memory read and write over the shared system bus can be done at a time
    * *Usage*. In many modern routers, with the destination address lookup and the storing of packet into the appropriate memory location are performed by processing on the input line cards
    * *Analogy to shared-memory processors*. Routers switching via memory look very much like shared-memory multiprocessor
* *Switching via a bus*. An input port transfers a packet directly to the output port over a shared bus, without intervention by the routing processor
    * *Implementation*.
        1. Have the input port prepend a switch-internal label (header) to the packet indicating the local output port, to which this packet is being transferred
            
            $\to$ The input port then transmits the packet onto the bus
        2. The packet is recevied by all output ports, but only the port matching the label will keep the packet
        3. The label is then removed at the output port
    * *Multiple-packet implementation*. If multiple packets arrive to the router at the same time, each at a different input port

        $\to$ All but one must wait, since only one packet can cross the bus at a time
        * *Consequence*. The switching speed of the router is limited to the bus speed
    * *Usage*. Sufficient for routers operating in a small local area and enterprise networks
* *Switching via an interconnection network*. One way to overcome the bandwidth limitation of a single, shared bus
    * *Idea*. Use a more sophisticated interconnection network, e.g. those used in the past to interconnect processors in a multiprocessor computer architecture
        * *Example*. Use a crossbar switch, with $2N$ buses connecting $N$ input ports to $N$ output ports
    * *Data flow*.
        1. A packet arrives from port A and needs to be forwarded to port Y
        2. The switch controller closes the crosspoint at the intersection of busses A and Y
        3. Port A then sends the packet onto its bus, which is picked up only by bus Y
        4. Meanwhile a packet from port B can be forwarded to port X at the same time

            $\to$ Since A-to-Y and B-to-X packets use different input and output busses

## Output processing
**Output port processing**. Take packets stored in the output port's memory and transmit them over the output link

<div style="text-align:center">
    <img src="https://i.imgur.com/NOP9moy.png">
    <figcaption>Output port processing</figcaption>
</div>

* *Processing steps*. Include 
    * Selecting and dequeuing packets for transmission
    * Performing the required link-layer and physical-layer transmission functions

## Where does the queuing occur
**Packet queues**. Required at both the input ports and the output ports
* *Location and extent of queueing*. Depend on the traffic load, the relative speed of the switching fabric, and the line speed
* *Packet loss*. As the queues grow large, the router's memory can eventually be exhausted and packet loss will occur when no memory is available to store arriving packets

    $\to$ Packets are dropped at the router

**Packet drop analysis**.
* *Switching fabric transfer rate*. The rate, at which packets can be moved from input port to output port
* *Assumptions*.
    * The input and output line speeds, i.e. transmission rates, are the same and equal to $R_\text{line}$
    * There are $N$ input ports and $N$ output ports
    * All packets have the same fixed length, and the packets arrive to input ports in a synchronous manner
        * *Synchronous input*. 
            * The time to send a packet on any link equals the time to receive a packet on any link
            * During the sending / receiving time, either zero or one packet can arrive on an input link
    * Switching fabric transfer rate is $R_\text{switch}$
* *Observations*. If $R_\text{switch} \geq N R_\text{line}$, then 
    * Only negligible queuing will occur at the input ports
    * The number of queued packets can grow large enough to exhaust available memory at the output ports

        $\to$ Packets are dropped

**Router buffer size**. Router buffers are required to absorb the fluctuations in traffic load, but how much buffering is required?
* *Rule of thumb for many years*. The amount of buffering $B$ should be equal to an average round-trip time $\text{RTT}$ times the link capacity $C$
    * *Intuition*. $B$ equals to the bandwidth-delay product, which is the total amount of data can exist in the link at a time
    * *Usage*. When the number $N$ of TCP flows is relatively small
* *Buffer size for large number of TCP flows*. $B=\text{RTT} \cdot C / \sqrt{N}$
    * *Explain*. To avoid spending too much memory resource on packet buffer
    * *Intuition*. (288)

**Packet scheduler at the output port**. Choose one packet among those queued for transmission
* *Common algorithms*. FCFS (first-come-first-served), WFQ (weighted fair queuing)
    * *WFQ*. Share the outgoing link fairly among the different end-to-end connections having packets queued for transmission
* *Quality-of-service guarantees*. Packet scheduling plays a crucial role in providing quality-of-service guarantees

**Packet drop policy**. If there is not enough memory to buffer an incoming packet, a decision must be made to drop some packet
* *Dropping policy*.
    * *Drop-tail*. Drop the arriving packet
    * *Drop-body or drop-head*. Remove one or more already-queued packets to make room for the newly arrived ones
        * *Congestion signal*. In some cases, it may be good to drop, or mark the header of, a packet before the buffer is full, to provide a congestion signal to the sender
* *Active queue management (AQM) algorithms*. A set of a number of packet-dropping and -marking policies
    * *The most widely used algorithm*. Random early detection (RED) algorithm
    * *Idea of RED*. A weighted average is maintained for the length of the output queue
        * If the average queue length is less than a threshold $\min_\text{th}$
            
            $\to$ When a packet arrives, it is admitted to the queue
        * If the queue is full, or the average queue length is greater than a threshold $\max_\text{th}$

            $\to$ When a packet arrives, it will be marked or dropped
        * If the packet arrives to find an average queue length in the interval $[\min_\text{th},\max_\text{th}]$
            
            $\to$ The packet is marked or dropped with a probability, which is typically some function of the average queue length, $\min_\text{th}$, and $\max_\text{th}$

**Input queue**. If the switch fabric is not fast enough, relative to the input line speeds, then packet queuing can also occur at the input ports

$\to$ Packets must join input port queues to wait their turn to be transferred through the switching fabric to the output port
* *Head-of-the-line blocking*. When the first packet in the input queue of some input port must wait for other packets from other input ports for queueing

    $\to$ The input queue will grow to unbounded length under certain assumptions

## The rounting control plane
**Routing control plane location**. The routing control plane resides and executes in a routing processor within the router

$\to$ The network-wide routing control plane is thus decentralized, with different pieces, e.g. of routing algorithm, executing at different routers
* *Router interaction*. Different routers interact by sending control messages to each other

**Data plane and software control plane**. Bundled by router and switch vendors into closed, but inter-operable, platforms in a vertically integrated product

# Appendix
## Concepts
### Concepts
**Buffer underflow (or buffer underrun)**. When the buffer is fed at a lower rate than it is being read from

**Aggregate data rate of a router**. The sum of the different data rates on each output link

**Link utilization**. The average traffic over a particular link expressed as a percentage of the total link capacity
* *Link efficiency*. The ratio of the time taken to transmit a frame, or frames, of data, to the total time it takes to transmit and acknowledge the frame or frames
    * *Formula*. $\text{link_eff}=\frac{t_f}{t_f + t_a + t_p + 2 t_d}$
        * $t_f$ is the time taken to transmit a frame or block of data
        * $t_d$ is the propagation delay for both frame and acknowledgement
        * $t_a$ is the time taken to transmit an acknowledgement
        * $t_p$ is the processing time

    >**NOTE**. In many cases, $t_a$ and $t_p$ are ignored

### Rules for router buffer size
**Buffer sizing objectvies**. The objective of buffer sizing has changed overtime
* *Traditional objective*. We are used to thinking of sizing queues so as to prevent them from overflowing and losing packets
    * *Problem*. TCP's sawtooth congestion control algorithm is designed to provide feedback to the sender

        $\to$ No matter how big the buffers are at a bottleneck link, TCP will cause the buffer to overflow
* *Recent objective*. When TCP flows pass through the router, they do not underflow and lose throughput

    $\to$ This is where the rule-of-thumb comes from
    * *Basic idea*. When a router has packets buffered, its outgoing link is always busy
    * *Consequence*. If the outgoing link is a bottleneck, we want to keep it busy as much of the time as possible
        
        $\to$ We need sure the buffer never underflow and goes empty

**TCP congestion control algorithm (recall)**. A single TCP flow passing through a bottleneck link requires a buffer size equal to the bandwidth-delay product

$\to$ This is to prevent the link from going idle and thus losing throughput
* *Scenario*.
    * A single TCP source sends an infinite amount of data with packets of constant size
    * The flow passes through a single router
    * The sender's access link, i.e. input link to the router, is much faster than the receiver's bottleneck link, i.e. output link from the router

        $\to$ Packets are queued at the router
    * The TCP flow has settled into the additive-increase and multiplicative-decrease (AIMD) congestion control mode
* *Assumptions*.
    * $C$ is the capacity of the receiver's bottleneck link
    * $T_p$ is the propagation time between the sender and receiver, and vice versa
* *TCP sender operation*.
    1. The sender transmits a packet each time it receives an ACK, and gradually increases the number of outstanding packets, i.e. the window size

        $\to$ This causes the buffer to gradually fill up
    2. Eventually a packet is dropped, and the sender does not receive an ACK

        $\to$ It halves the window size and pauses
    3. The sender waits for ACKs to arrive, before it can resume transmitting
        * *Explain*. 
            * Before the packet loss, the sender is allowed to have $W_\text{max}$ outstanding packets
            * After the loss, the sender is only allowed to have $W_\text{max} / 2$ outstanding packets

                $\to$ The sender must pause while waiting for the ACKs for $W_\text{max}/2$ packets
            
            >**NOTE**. While TCP measures window size in bytes, we will count window size in packeys for simplicity of presentation, i.e. 1 byte means 1 packet

* *Objective of buffer sizing*. Make sure that while the sender pauses, the router buffer does not go empty and force the bottleneck link to go idle
    * *Idea*. By determining the rate, at which the buffer drains

        $\to$ We can determine the size of the reservoir required to prevent it from going empty
    * *Desired buffer size*. Equal to the distance, in bytes, between the peak and trough of the sawtooth representing the TCP window size

        $\to$ This is shown to correspond to the rule-of-thumb $B=\text{RTT} \times C$

**Buffer sizes' effects on router design complexity**.

>**NOTE**. At the time of writing, a SoTA router linecard runs at aggregate rate of 40 Gb/s, with one or more physical interfaces, has about 250ms of buffering, and thus has 10 Gbits of buffer memory

* *Router backbone buffers*. BUilt from commercial memory devices, e.g. DRAM or SRAM
    * *SoTA SRAM chip capacity*. 36Mbits, i.e. a 40Gb/s linecard would require over 300 chips, i.t. to achieve 10 Gbits buffer memory

        $\to$ The board will be too large, too expensive, and too hot
    * *SoTA DRAM chip capacity*. Up to 1Gbit, i.e. we only need 10 DRAM devices
        * *Problems*. 
            * DRAM has a random access time of about 50ns
                
                $\to$ This is hard to use when a minimum length, i.e. 40 byte, packet can arrive and depart every 8ns, i.e. thus achieve 40Gb/s aggregate rate
            * DRAM access time fall by only 7% per year, thus the problem is going to get worse as line-rates increase in the future
    * *Router buffer implementation in practice*. 
        * *Option 1*. Router linecards use multiple DRAM chips in parallel to obtain the aggregate data-rate, or memory bandwidth, they need
        * *Option 2*. Packets are either scattered across memories in an ad-hoc statistical manner, or use an SRAM cache with a refresh algorithm
    * *Problem with large packet buffer*. A large packet buffer uses a very wide DRAM bus, i.e. hundreds or thousands of signals, with a huge number of fast data pins

        <div style="text-align:center">
            <img src="https://i.imgur.com/XwBXHfC.png">
            <figcaption>Data pins example</figcaption>
        </div>
        
        * *Consequence*. Wide buses consume large amounts of board space, and the fast data pins on modern DRAMs consume a lot of power
    * *Consequence*. It is extremely difficult to build packet buffers at 40 Gb/s and beyond, and given how slowly memory speeds improve

        $\to$ This problem is going to get worse over time
* *On-chip buffer memory*. Substaantial benefits could be gained by placing the buffer memory directly on the chip, which processes the packets, i.e. a network processor or an ASIC
    * *Explain*. Very wide and fast access to a single memory is possible
        * *Explain*. 
            * Local bus, i.e. connecting processor and on-chip caches, usually has large bandwidth and is very fast
            * Local bus must have large bandwidth since the process-to-onchip-cache connection is used very regularly
    * *Commercial packet processor ASICs*. Built with 256Mbits of embedded DRAM

        $\to$ If memories of 2% the delay-bandwidth product, i.e. 10Gbits, were acceptable, then a single-chip packet processor would need no external memories

    >**NOTE**. It is shown later that this buffers this small may take little or no difference to the utilization of backbone links

**Buffer size for a single long-lived TCP flow**.
* *Key to sizing packet buffer*. Make sure that the buffer is large enough, so that while the sender pauses, the buffer does not go empty
* *Sender pause time*. 
    * *ACK rate to the sender*. If the buffer never goes empty, the router at the receiver side must be sending packets onto the bottleneck link at a constant rate $C$

        $\to$ The ACKs arrive to the sender at rate $C$
    * *Sender pause time*. The sender pauses for exactly $(W_\text{max}/2)/C$ seconds for the $W_\text{max}/2$ packets to be acknowledged

        $\to$ The sender then resumes and starts increasing its window size again

        >**NOTE**. Here, we assume that to acknowledge $n$ packets, the receiver will send back $n$ ACK packets, each of which has size 1 byte

* *Constraints on buffer size $B$*. When the sender first pauses at $t_1$, the buffer is full, and thus it drains over a period $B/C$ until $t_2$

    $\to$ The buffer will avoid going empty if the first packet from the sender shows up at the buffer just as it hits empty
    * *Formal*. $(W_\text{max}/2)/C \leq B/C$, or $B\geq W_\text{max}/2$
* *TCP sending rate*. $R = W/\text{RTT}$ where $W$ is the current window size
    * *Consequence*. To send at rate $C$, we require that

        $$C=\frac{W}{RTT}=\frac{W_\text{max}/2}{2T_p}$$
    
        or 

        $$W_\text{max}/2=2T_p \times C$$

* *Desired buffer size*. $B\geq 2T_p\times C=\text{RTT} \times C$

**Rule-of-thumb**. $B = \text{RTT} \times C$
* *Motivation*. Come from a desire to keep a congested link as busy as possible, thus maximize the throughput of the network
* *Fact*. The rule-of-thumb is the amount of buffering required by a single TCP flow, so that the buffer at the bottleneck link never underflows

    $\to$ The router does not lose throughput
* *Effects of the rule*. The buffer occupancy almost hits zero once per packet loss, but never stays empty

    $\to$ This is the behavior we want for the bottleneck link to stay busy
* *Underflow and overflow buffer*.
    * *Underflow buffer*. When the window is halved and the sender pauses waiting for ACKs 
        $\to$ When there is insufficient reserve in the buffer to keep the bottleneck link busy
        * *Consequence*. The buffer goes empty, the link goes idle, and we lose throughput
    * *Overflow buffer*. When the window is halved, the buffer does not completely empty
        * *Consequence*. The queueing delay of the flow is increased by a constant, since the buffer always have packets queued

**Buffer size for synchronized long flows**.
* *Synchronized TCP flows sharing a bottleneck link*. When the TCP flows experience packet drops at roughly the same time
    
    $\to$ Their sawtooths become synchronized and in-phase
    * *Consequence*. The aggregate window process, i.e. the sum of all window size processes, looks like an amplified version of a single flow
* *Buffer size*. Just similar to single-flow case

**Buffer size of desynchronized long flows**.
* *Observation*. When the sawtooths are not synchronized, the less their sum will look like a sawtooth
    * *Explain*. They will smooth each other out, and the distance from the peak to the trough of the aggregate window size will get smaller
    * *Consequence*. We can expect the buffer size requirements to get smaller, as we increased the number of flows, i.e. increase the degree of desynchronization
        * *Explain*. Given that we need as much buffer as the distance from the peak to the trough of the aggregate window size
* *Scenartio*. Consider a set of TCP flows with random and independent start times and propagation delays
    * *Window size independence*. TCP flows are desynchronized enough that the window size processes are independent of each other
    * *Total window size modeling*. We can model the total window size as a bounded random process made up of the sum of the independent sawtooth window sizes
    * *Central limit theorem*. The aggregate window size process will converge to a Gaussian process
    * *Assumptions*.
        * $W=\sum_i W_i$ is the congestion window of all flows
        * $\epsilon$ is the number of dropped packets
* *Queue occupancy at time $t$*. $Q(t)=\sum_{i=1}^n W_i(t) - (2T_p \cdot C) - \epsilon$
    * *Explain*. All outstanding packets are in the queue, i.e. $Q(t)$, on the link, i.e. $2T_p \cdot C$, or have been dropped, i.e. $\epsilon$
    * *Modified formulation when $\epsilon\to 0$*. If the buffer is large enough, and TCP is operating correctly, then $\epsilon$ is negligible, i.e.

        $$Q\approx W - 2T_p\cdot C$$
    
    * *Distribution of $Q$*. Since $W$ has a normal distribution, $Q$ has the distribution of a normal shifted by a constant

        $\to$ We can now pick a buffer size and know the probability that the buffer will underflow and lose throughput
* *Estimating mean and variance of $Q$*.
    * *Mean estimation*. The sum of the mean of $Q$'s constituents, i.e.
        * *Mean of $W_i$*.

            $$\bar{W}_i=\frac{\bar{W}}{n}=\frac{2\bar{T}_p\cdot C + \bar{Q}}{n}\leq \frac{2\bar{T}_p\cdot C + B}{n}$$
    
    * *Variance estimation*. For convenience, we assume that all sawtooths have the same average value

        >**NOTE**. Having different values would still yield the same results

        * *Assumptions*.
            * Each TCP sawtooth is modelled as oscillating with a uniform distribution around its mean $\bar{W}_i$
                * *Minimum value*. $\frac{2}{3} \bar{W}_i$
                * *Maximum value*. $\frac{4}{3} \bar{W}_i$
        * *Standard deviation of $W_i$*. $\sigma_{W_i}=\frac{1}{\sqrt{12}} \bigg(\frac{4}{3}\bar{W}_i - \frac{2}{3}\bar{W}_i\bigg)=\frac{1}{3\sqrt{3}} \bar{W}_i$
        * *Standard deviation of $W$*. $\sigma_W=\sqrt{n} \sigma_{W_i}$
        * *Standard deviation of $Q$*. $\sigma_Q=\sigma_W\leq \frac{1}{3\sqrt 3} \frac{2\bar{T}_p\cdot C + B}{n}$
    * *Link utilization approximation*. If we know the queue occupancy's distribution, we can approximate the link utilization for a given buffer size
        * *Explain*. Whenever the queue size is below a threshold $b$, there is a risk, but not guaranteed that the queue will go empty

            $\to$ We will lose link utilization
            * *Consequence*. If we know $P(Q<b)$ then we have an upper bound on the lost utilization
        * *Lower bound on link utilization*. (Will be derived later)

            $$\text{util}\geq\text{erf}(\frac{3\sqrt{3}}{2\sqrt{2}} \frac{B}{\frac{2 \bar{T}_p \cdot C + B}{\sqrt{n}}})$$
        
    * *Consequence*. We can achieve full utilization with buffers, which are delay-bandwidth product divided by square root of the number of flows, or a small multiple thereof

        $\to$ As the number of flows through a router increases, the amount of required buffer decreases