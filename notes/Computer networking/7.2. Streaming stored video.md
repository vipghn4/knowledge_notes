<!-- TOC titleSize:1 tabSpaces:2 depthFrom:1 depthTo:6 withLinks:1 updateOnSave:1 orderedList:0 skip:0 title:1 charForUnorderedList:* -->
# Table of Contents
- [Table of Contents](#table-of-contents)
- [Streaming stored video](#streaming-stored-video)
  - [UDP streaming](#udp-streaming)
  - [HTTP streaming](#http-streaming)
    - [Prefetching](#prefetching)
    - [Client application buffer and TCP buffers](#client-application-buffer-and-tcp-buffers)
    - [Analysis of video streaming](#analysis-of-video-streaming)
    - [Early termination and repositioning the video](#early-termination-and-repositioning-the-video)
  - [Adaptive streaming and DASH](#adaptive-streaming-and-dash)
  - [Content distribution networks](#content-distribution-networks)
    - [CDN operation](#cdn-operation)
    - [Cluster selection strategies](#cluster-selection-strategies)
  - [Case studies - Netflix, Youtube, and Kankan](#case-studies---netflix-youtube-and-kankan)
    - [Netflix](#netflix)
    - [YouTube](#youtube)
    - [Kankan](#kankan)
- [Appendix](#appendix)
  - [Concepts](#concepts)
  - [Discussion](#discussion)
<!-- /TOC -->

# Streaming stored video
**Streaming video systems categories**. UDP streaming, HTTP streaming, and adaptive HTTP streaming
* *Major types*. HTTP streaming and adaptive HTTP streaming

**Data buffering**. A common characteristic of all three forms of video streaming is the extensive use of client-side application buffering
* *Purposes*. 
    * Mitigate the effects of varying end-to-end delays
    * Mitigate the effects of varying bandwidth between server and client
* *Motivation*. For streaming video, both stored and live, users generally can tolerate a small several second initial delay between when the client requests a video, and when video playout begins at the client
* *Idea*.
    * When the video starts to arrive at the client, the client need not immediately begin playout, but build up a reserve of video in an application buffer
    * Once the client has built up a reserve of several seconds of buffered-but-not-played video

        $\to$ The client can begin video playout
* *Benefits*.
    * Client-side buffering can absorb variations in server-to-client delay
        * *Explain*. If a particular piece of video data is delayed, as long as it arrives before the reserve of received-but-not-played video is exhausted

            $\to$ This long delay will not be noticed
    * If the server-to-client bandwidth briefly drop below the video consumption rate

        $\to$ A user can continue to enjoy continuous playback, as long as the client application buffer does not become completely drained

## UDP streaming
**UDP streaming**. The server transmits video at a rate which matches the client's consumption rate by clocking out the video chunks over UDP at a steady state
* *Example*. If the video consumption rate is 2 Mbps, and each UDP packet carries 8000 bits of video

    $\to$ The server would transmit one UDP packet into its socket every $\frac{8000 \text{bits}}{2 \text{Mbps}} = 4\text{msec}$
* *Explain*. UDP does not employ a congestion-control mechanisms

    $\to$ The server can push packets into the network at the consumption rate of the video without the rate-control restrictions of TCP
* *Data buffering*. UDP streaming typically uses a small client-side buffer, big enough to hold less than a second of video

**Packet encapsulation**. Before passing the video chunks to UDP

$\to$ The server will encapsulate the video chunks within transport packets specially designed for transporting video and audio
* *Implementation*. Use Real-Time Transport Protocol (RTP), or a similar (possibly proprietary) scheme

**Client-server connection**.The client and server maintain, in parallel, a separate control connection, over which the client sends commands regarding session state changes
* *Session state changes*. Pause, resume, reposition, etc.

    $\to$ This control connection is in many ways analogous to the FTP control connection

**Significant drawbacks**.
* Due to the unpredictable and varying amount of available bandwidth between server and client

    $\to$ Constant-rate UDP streaming can fail to provide continuousplayout
* UDP stream requires a media control server, e.g. a RTSP server, to process client-to-server interactivity requests, and to track client state for each ongoing client session

    $\to$ This increases the overall cost and complexity of deploying a large-scale video-on-demand system
* Many firewalls are configured to b lock UDP traffic, preventing the users behind these firewalls from receiving UDP video

## HTTP streaming
**HTTP streaming**. The video is simply stored in an HTTP server, as an ordinary file with a specific URL
* *Procedure*. 
    1. When a user wants to see the video
        
        $\to$ The client establishes a TCP connection with the server, and issues an HTTP GET request for the URL
    2. The server sends the video file, within an HTTP response message, as quickly as possible
        * *Explain*. As quickly as TCP congestion control and flow control will allow
    3. On the client side, the bytes are collected in a client application buffer
    4. Once the number of bytes in the buffer exceeds a predetermined threshold

        $\to$ The client application begins playback
        * *Explain*. The client side periodically grabs video frames from the client application buffer, decompresses the frames, and displays them on the user's screen

**Server-to-client transmission rate**. 
* *Congestion control*. Can vary significantly due to TCP's congestion control mechanism

    $\to$ It is not uncommon for the transmission rate to vary in a saw-tooth manner associated with TCP congestion control
* *Retransmission*. Packets can be significantly delayed due to TCP's retransmission mechanism

**Why TCP for video streaming**.
* Due to its characteristics, the conventional wisdom in the 1990s was that video streaming would never work well over TCP
* Over time, designers of streaming video systems learned that TCP's congestion control and reliable-data transfer mechanisms do not necessarily preclude continuous playout
    * *Explain*. Due to client-side buffering and prefetching

**Benefits of using TCP**. Most video streaming applications today is using HTTP streaming over TCP as its underlying streaming protocol
* Allow the video to traverse firewalls and NATs more easily, which are often configured to block most UDP traffic but to allow most HTTP traffic
* Obviate the need for a media control server, e.g. RTSP server

    $\to$ This reduces the cost of a large-scale deployment over the Internet

### Prefetching 
**Prefetching video frames**. Client-side buffering can be used to mitigate the effects of varying end-to-end and varying available bandwidth

$\to$ For streaming stored video, the client can attempt to download the video at a rate higher than the consumption rate, thus prefetching video frames which are to be consumed in the future
* *Storing prefetched frames*. In the client application buffer naturally
* *Usage*. Naturally used with TCP streaming, since TCP's congestion avoidance mechanism will attempt to use all of the available bandwidth between server and client

### Client application buffer and TCP buffers
**Interaction between client and server for HTTP streaming**.

<div style="text-align:center">
    <img src="https://i.imgur.com/o6RRLkf.png">
    <figcaption>Streaming stored video over HTTP / TCP</figcaption>
</div>

* *Server side*. The portion of the video file in the white has already been sent into the server's socket, while the darkened portion is what remains to be sent
    * *Packet transmission*. After passing through the socket door, the bytes are placed in the TCP send buffer, before being transmitted into the Internet
    * *Full buffer*. If the TCP send buffer is full
        
        $\to$ The server is momentarily prevented from sending more bytes from the video file into the socket
* *Client side*. 
    1. The client application, i.e. media player, reads bytes from the TCP receive buffer, through its client socket

        $\to$ The client application then places the bytes into the client application buffer
    2. At the same time, the client application periodically grabs video frames from the client application buffer, decompresses the frames, and displays them on the user's screen

    >**NOTE**. If the client application buffer is larger than the video file
    >$\to$ The process of moving bytes from the server's storage to the client's application buffer is equivalent to an ordinary file download over HTTP

**Video pausing at client side**. During the pause period, bits are not removed from the client application buffer, even though bits continue to enter the buffer from the server
* *Full client buffer*. If the client application buffer is finite, it may eventually become full, causing back pressure all the way back to the server
    * *Explain*. 
        * Once the client application buffer is full, bytes can no longer be removed from the client TCP receive buffer

            $\to$ The client TCP receive buffer becomes full
        * Once the client receive TCP buffer is full, bytes can no longer be removed from the TCP send buffer

            $\to$ The client send buffer becomes full
        * Once the TCP send buffer becomes full, the server cannot send any more bytes into the socket
* *Consequence*. If we pause the video, the server may be forced to stop transmitting, thus it will be blocked until we resume the video

>**NOTE**. Even during regular playback, i.e. without pausing, if the client application buffer becomes full, back pressure will cause the TCP buffers to become full
>$\to$ This forces the server to reduce its rate

**Server sending rate**. When the client application removes $f$ bits from its fulfilled buffer

$\to$ It creates room for $f$ bits in the client application buffer, allowing the server to send $f$ additional bits
* *Consequence*. The server send rate can be no higher than the video consumption rate at the client

    $\to$ A full client application buffer indirectly imposes a limit on the sending rate of the server when streaming over HTTP

### Analysis of video streaming
**Simple modeling to provide more insight into initial playout delay and freezing due to application buffer depletion**.

<div style="text-align:center">
    <img src="https://i.imgur.com/xoeFFtP.png">
    <figcaption>Analysis of client-side buffering for video streaming</figcaption>
</div>

* *Assumptions*.
    * $B$ is the size, in bits, of the client's application buffer
    * $Q < B$ is the number of bits which must be buffered before the client begins playout
    * $r$ is the video consumption rate
    * TCP's send and receive buffers are ignored
* *Scenario*.
    * The server sends bits at a constant rate $x$ whenever the client buffer is not full

        >**NOTE**. This is a gross simplifcation, since TCP's send rate varies due to congestion control
    
    * At time $t=0$, the application buffer is empty, and video begins arriving to the client application buffer
* *At what time $t=t_p$ does playout begin*. $t_p=Q/x$
* *At what time $t=t_f$ does the client application buffer become full*.
    * If $x<r$, then the client buffer will never become full

        $\to$ Starting at time $t_p$, the buffer will depleted at rate $r$, and only be filled at rate $x<r$
        * *Consequence*. Eventually, the client buffer will empty out entirely, at which time the video will freeze on the screen
            * *Explain*. The client buffer waits another $t_p$ seconds to build up $Q$ bits of video
    * If $x>r$, then starting at time $t_p$, the buffer inreases from $Q$ to $B$ at rate $x-r$

### Early termination and repositioning the video
**HTTP byte-range header**. Used by HTTP streaming systems in the HTTP GET request message to specify the specific range of bytes the client currently wants to retrieve from the desired video
* *Usage*. Useful when we wants to reposition, i.e. jump, to a future point in time in the video
* *Repositioning procedure*.
    1. The client sends a new HTTP request , indicating with the byte-range header, from which byte in the file should the server send data
    2. When the server receives the new HTTP request, it can forget about any earlier request and send bytes beginning with the byte indicated in the byte-range request

**Video early termination**. When a user repositions to a future point in the video, or terminates the video early

$\to$ Some prefetched-but-not-yet viewed data transmitted by the server will go to unwatched, leading to a waste of network bandwidth and server resources
* *Consequence*. Many streaming systems use only a moderate-size client application buffer, or will limit theo amount of prefetched video using the byte-range header in HTTP requests

## Adaptive streaming and DASH
**Drawback of HTTP streaming**. All clients receive the same encoding of the video, despite the large variations in the amount bandwidth available to a client

$\to$ This lead to the development of a new type of HTTP-based streaming, i.e. DASH

**Dynamic adaptive streaming over HTTP (DASH)**. The video is encoded into several different versions, each of which has a different bit rate and hence a different quality level

$\to$ The client dynamically requests chunks of video segments of a few seconds in length from the different versions
* *Idea*. The client selects different chunks one at a time with HTTP GET request messages
    * When the bandwidth is high, the client selects chunks from a high-rate version
    * When the bandwidth is low, the client selects chunks from a low-rate version
* *Storing different video versions*. Each video version is stored in the HTTP server, each with a different URL
    * *Manifest file*. The HTTP server has a manifest file providing a URL for each version, along with its bit rate
* *Chunk request procedure*.
    1. The client requests the manifest file and learns about the various versions
    2. The client selects one chunk at a time by specifying a URL and a byte range in an HTTP GET request message for each chunk
    3. While downloading chunks, the client measures the received bandwidth and runs a rate determination algorithm to select the chunk to request next
        * If the client has a lot of video buffered, and if the measured receive bandwidth is high

            $\to$ It will choose a chunk from high-rate version
        * If the client has little video buffered, and the measured received bandwidth is low

            $\to$ It will choose a chunk froma low-rate version
* *Default video version*. Since a sudden drop in bit rate by changing versions may result in noticeable visual quality degradation

    $\to$ The bit-rate reduction may be achieved by using multiple intermediate versions to smoothly transition to a rate, where the client's consumption rate drops blow its available receive bandwidth
    * *Consequence*. When the network conditions improve, the client can later choose chunks from higher bit-rate versions
* *Consequence*. 
    * DASH allows the client to freely switch among different quality levels
    * DASH can often achieve continuous playout at the best possible quality level without frame freezing or skipping
    * Since the client, rather than the server, maintains the intelligence to determine which chunk to send next

        $\to$ DASH improves server-side scalability
    * The client can use HTTP byte-range request to precisely control the amount of prefetched video, which it buffers locally

**DASH for audio**. The server not only stores many versions of the video, but also many versions of the audio

$\to$ The client dynamically selects both video and audio chunks, and locally synchronizes audio and video playout

## Content distribution networks
**Challenge**. Stream hundreds of millions of video streams to users around the world, while providing continuous playout and high interactivity, everyday
* *Straightforward approach*. Build a single massive data center, store all of its video in he data center, and stream the videos directly from the data center to clients worldwide
    * *Drawbacks*.
        * If the client is far from the data center, server-to-client packets will cross many communication links and likely pass through many ISPs, with some of the ISPs possibly located on different continents
            * *Consequence*. If one of these links provides a throughput lower than the video consumption rate

                $\to$ The end-to-end throughput will also be below the consumption rate, resulting in annoying freezing delays for the user
            
            >**NOTE**. The likelihood of this happening increases as the number of links in the end-to-end path increases
        
        * A popular video will likely be sent many times over the same communication links

            $\to$ This leads to wasted bandwidth, and the Internet video company will by paying its provider ISP for sending the same bytes into the Internet over and over again
        * A single data center represents a single point of failure, i.e. if the data center or its links to the Internet goes down

            $\to$ It would not be able to distribute any video streams

**Content distribution networks (CDNs)**. Used by almost all major video-streaming companies to distribute video streams
* *Functionality of a CDN*. 
    * Manage servers in multiple geographically distributed locations
    * Store copies of the videos, and other types of Web content, including documents, images, and audio, in its servers
    * Attempt to direct each user request to a CDN location, which will provide the best user experience
* *Private CDN*. A CDN owned by the content provider, e.g. Google's CDN distributes YouTube videos and other types of content
* *Third-party CDN*. Distribute content on behalf of multiple content providers, e.g. Akamai's CDN distributes Netflix and Hulu content

**Server placement philosophies adopted by CDNs**.
* *Enter deep*. Enter deep into the access networks of the ISPs, by deploying server clusters in access ISPs all over the world
    * *Goal*. Get close to end users, thus improve user-perceived delay and throughput 
        * *Explain*. By decreasing the number of links and routers between the end user and the CDN cluster, from which it receives content
    * *Drawback*. Due to the highly distributed design, the task of maintaining and managing the clusters becomes challenging
* *Bring home*. Bring the ISPs home by building large clusters at a smaller number, e.g. tens, of key locations and connecting these clusters using a private high-speed network

    $\to$ These CDNs typically place each cluster at a location, which is simultaneously near the PoPs of many tier-1 ISPs
    * *Pros*. Lower maintenance and management overhead
    * *Cons*. Higher delay and lower throughput to end users

**Content replication within a CDN**. Once its clusters are inplace, the CDN replicates content across its clusters
* *Pull strategy*. Many CDNs do not push videos to their clusters, but use a simple pull strategy
    * *Idea*. If a client requests a video from a cluster, which is not storing the video

        $\to$ The cluster retrieves the video, from a central repository  or from another cluster, and stores a copy locally while streaming the video to the client at the same time
    * *Video removal*. When a cluster's storage becomes full, it removes videos which are not frequently requested

### CDN operation
**Main operations**. When a browser in a user's host is instructed to retrieve a specific video identified by a URL

$\to$ The CDN must intercept the request so that it can
* Determine a suitable CDN server cluster for the client at that time
* Redirect the client's request to a server in the cluster

**DNS to intercept and redirect requests**. Most CDNs use DNS to intercept and redirect requests

<div style="text-align:center">
    <img src="https://i.imgur.com/O72PjYs.png">
    <figcaption>DNS redirects a user's request to a CDN server</figcaption>
</div>

* *Simple scenario*. A content provider, NetCinema, employs the third-party CDN company, KingCDN, to distribute its videos to its customers
    * On the NetCinema Web pages, each of its video is assigned a URL including the string "video" and a unique ID for the video itself
* *Interception and redirection with DNS*.
    1. The user visits the Web page at NetCinema
    2. When the user clicks on the link `http://video.netcinema.com/6Y7B23V`

        $\to$ The user's host sends a DNS query for `video.netcinema.com`
    3. The user's local DNS server (LDNS) relays the DNS query to an authoritative DNS server for NetCinema
    4. The authoritative DNS server observes the string `video` in the hostname `video.netcinema.com`, thus, to hand over the DNS query to KindCDN

        $\to$ The NetCinema authoritative DNS server returns to the LDNS a hostname in the KingCDN's domain, e.g. `a1105.kingcdn.com`
    5. From this point, the DNS query enters into KingCDN's private DNS infrastructure

        $\to$ The user's LDNS sends a second query for `a1105.kingcdn.com`
    6. KingCDN's DNS system eventually returns the IP addresses of a KingCDN content server to the LDNS

        $\to$ It is thus here, within the KingCDN's DNS system, that the CDN server, from which the client will receive its content, is specified
    7. The LDNS forwards the IP address of the content-serving CDN node to the user's host
    8. Once the client receives the IP address for a KingCDN content server

        $\to$ It establishes a direct TCP connection with the server at that IP address, and issues an HTTP GET request for the video
    7. If DASH is used, the server first sends to the client a manifest file with a list of URLs, one for each version of the video

        $\to$ The client will dynamically select chunks from the different versions

### Cluster selection strategies
**Cluster selection strategy**. A mechanism for dynamically directing clients to a server cluster or a data center within the CDN
* *Idea*.
    1. The CDN learns the IP address of the client's LDNS server via the client's DNS lookup
    2. The CDN then needs to select an appropriate cluster based on this IP address

**Geographically closest strategy**. Assign the client to the cluster which is geographically closest
* *Idea*. Use a commercial geo-location databases, each LDNS IP address is mapped to a geographic location

    $\to$ When a DNS request is received from a particular LDNS, the CDN chooses the geographically closest cluster
* *Usage*. work reasonably well for a large fraction of the clients
* *Drawback*. 
    * For some clients, the solution may perform poorly
        * *Explain*. The geographically closest cluster may not be the closest cluster along the network path
    * For all DNS-based approaches, some end-users are configured to use remotely located LDNSs
    
        $\to$ The LDNS location may be far from the client's location
    * This strategy ignores the variation in delay and available bandwidth over time of Internet paths

**Real-time measurements strategy**. Try to determine the best cluster for a client based on the current traffic conditions
* *Idea*. periodically perform real-time measurements of delay and loss performance between the clusters and clients
    * *Example*. A CDN can have each of its clusters periodically send probes, e.g. ping messages or DNS queries, to all of the LDNSs around the world
* *Drawback*. LDNSs are configured not to respond to probes
* *Alternative solutions*. 
    * *Option 1*. Use the characteristics of recent and ongoing traffic between the clients and CDN servers
        * *Example*. The delay between a client and a cluster can be estimated by examining the gap between server-to-client SYNACK and client-to-server ACK during the TCP handshake
        * *Drawback*. The clients are redirectted to possibly suboptimal clusters from time to time, to measure the properties of paths to these clusters

            $\to$ The selected clients can suffer significant performance degradation when receiving content
    * *Option 2*. Use DNS query traffic to measure the delay between clients and clusters
        * *Explain*. During the DNS phase, the client's LDNS can be occasionally directed to different DNS authoritative servers at various cluster locations

            $\to$ This yields DNS traffic which can then be measured between LDNS and these cluster locations
        * *Consequence*. The DNS servers continue to return the optimal cluster for the client, so that delivery of videos and other Web objects does not suffer

**IP anycast strategy**.

<div style="text-align:center">
    <img src="https://i.imgur.com/QdnkyqI.png">
    <figcaption>Using IP anycast to router clients to closest CDN cluster</figcaption>
</div>

* *IP anycast*. Have the routers in the Internet route the client's packets to the closest cluster, as determined by BGP
* *Idea*.
    * *IP-anycast configuration stage*. 
        1. The CDN company assigns the same IP address to each of its clusters
        2. The CDN company uses standard BGP to advertise this IP address from each of the different cluster locations
        3. When a BGP router receives multiple route advertisements for this same IP address

            $\to$ It treats these advertisements as providing different paths to the same physical location
        4. Following the standard operating procedures, the BGP router then pick the best route to the IP address, according to its local route selection mechanism
    * *IP-anycast execution stage*. After the configuration stage, the CDN can do its main job of distributing content
        1. When any client wants to see any video, the CDN's DNS returns the anycast address, no matter where the client is located
        2. When the client sends a packet to the given IP address

            $\to$ The packet is routed to the closest cluster as determined in the preconfigured forwarding tables
* *Pros*. The closest cluster to the client is found

**Other aspects affecting cluster selection**.
* Load on the clusters, i.e. clients should not be directed to overloaded clusters
* ISP delivery cost

## Case studies - Netflix, Youtube, and Kankan
### Netflix
**Idea**. In order to rapidly deploy its large-scale service, Netflix has made extensive use of third-party cloud services and CDNs

$\to$ Netflix is an example of a company deploying large-scale online service by renting servers, bandwidth, storage, and database services from third parties while using hardly any infrastructure on its own
* *Used techniques*. Video distribution with multiple CDNs, and adaptive streaming over HTTP

**Netflix video streaming platform**.

<div style="text-align:center">
    <img src="https://i.imgur.com/N2GvLgg.png">
    <figcaption>Netflix video streaming platform</figcaption>
</div>

* *Major components*.
    * The registration and payment servers
    * The Amazon cloud
    * Multiple CDN providers
    * Clients
* *The registration and payment servers*. On its own hardware infrastructure, Netflix maintains registration and payment servers

    $\to$ These servers handle registration of new accounts and capture credit-card payment information
* *Amazon cloud*. Netflix runs its online service, except for basic functions, by employing machines, or virtual machines, in the Amazon cloud
    * *Content ingestion*. Before Netflix can distribute a move to its customers

        $\to$ It must ingest and process the movie
        * *Idea*. Netflix receives studio master versions of movies and uploads them to hosts in the Amazon cloud
    * *Content processing*. The machines in the Amazon cloud create many different formats for each movie, suitable for a diverse array of client video players running on various devices
        * *DASH*. A different version is created for each of these formats and at multiple bit rates

            $\to$ This allows for adaptive streaming over HTTP using DASH
    * *Uploading versions to the CDNs*. Once all of the versions of a movie have been created
        
        $\to$ The hosts in the Amazon cloud upload the versions to the CDNs
* *CDN providers*. To deliver the movies to its customers on demand, Netflix makes extensive use of CDN technology

**Interaction between the client and the various servers involved in movie delivery**.
1. The Web pages for browsing the Netflix video library are served from servers in the Amazon cloud
2. When the user selects a movie to "Play Now", the user's client obtains a manifest file from servers in the Amazon cloud
    * *Manifest file*. Includ a variety of information, including 
        * A ranked list of CDNs
        * The URLs for the different versions of the movie, which are used for DASH playback
    * *CDN rank*. Determined by Netflix, and may change from one streaming session to the next
        
        $\to$ Typically, the client will select the CDN ranked highest in the manifest file
3. After the client selects a CDN, the CDN leverages DNS to redirect the client to a specific CDN server
4. The client and the CDN server then interact using DASH
    * *DASH chunk size*. Approximately four-seconds long

### YouTube
**Idea**. 
* *Similarity as Netflix*. YouTube makes extensive use of CDN technology to distribute its videos
* *Difference from Netflix*. Google does not employ third-party CDNs but uses its own private CDN to distribute YouTube videos

**YouTube private CDN**. Google has installed server clusters in many hundreds of different locations

$\to$ From a subset of about 50 of these locations, Google distributes YouTube videos
* *DNS*. Google uses DNS to redirect a customer request to a specific cluster
* *Google's cluster selection strategy*. Most of the time, this strategy directs the client to the cluster, for which the RTT between client and cluster is the lowest
    * *Load balance*. Sometimes, the client is directed, via DNS, to a more distant cluster
* *No-requested-video cluster*. If a cluster does not have the requested video, instead of fetching from elsewhere and relaying it to the client

    $\to$ The cluster may return an HTTP redirect message redirecting the client to another cluster

**Streaming protocol**. YouTube employs HTTP streaming
* *DASH*. YouTube often makes a small number of different versions available for a video, each with a different bit rate and quality level

**Video uploading**. A few million videos are uploaded to YouTube everyday
* *Video uploading protocol*. YouTube uploaders also upload their videos from client to server over HTTP
* *Uploaded video processing*. YouTube processes each video it receives, converting it to a YouTube video format, and creating multiple versions at different bit rates

    $\to$ This processing takes place entirely within Google data centers

### Kankan
**P2P for video distribution**. Another approach for providing video on demand over the Internet at a large scale, which allows the service provider to significantly reduce its infrastructure and bandwidth costs
* *Idea*. Use P2P delivery, instead of client-server, via CDNs, delivery
* *Usage*. Used with great success by several companies in China, e.g. Kankan, PPTV, and PPs

**High-level description**. P2P video streaming is very similar to BitTorrent file download, i.e.
1. When a peer wants to see a video, 
    
    $\to$ It contacts a tracker, which may be centralized or peer-based using a DHT, to discover other peers in the system having a copy of that video
2. This peer then requests chunks of video file in parallel from these other peers having the file

# Appendix
## Concepts
**Border gateway protocol (BGP)**. A standardized exterior gateway protocol designed to exchange routing and reachability information among autonomous systems (AS) on the Internet
* *Motivation*. As networks interact with each other, they need a way to communicate, and this is accomplished via peering

    $\to$ BGP makes peering possible

    >**NOTE**. BGP is the protocol making the Internet work, by enabling data routing on the Internet

    * *Human analogy*. When someone drops a letter into a mailbox

        $\to$ The BGP processes that piece of mail and chooses a fast, efficient route to deliver that letter to its recipient
* *Autonomous system (AS)*. The Internet is a network of networks, and it is broken up into hundreds of thousands of smaller networks known as AS

    $\to$ Each of these networks is a large pool of routers run by a single organization
    * *Human analogy*. AS's are like individual post office branches

        $\to$ A town may have hundreds of mailboxes, but the mails must go through the local postal branch before being routed to another destination
    * *AS and BGP*. 
        1. The internal routers within an AS forward their outbound transmissions to the AS
        2. BGP routing is then used to get these transmissions to their destination
* *How BGP work*. 
    * When we have a network router connecting to other networks, it does not know which network is the best one to send its data to

        $\to$ BGP considers all the different peering options a router has and chooses the one closest to where the router is
    * Each potential peer communicates the routing information it has and that gets stored within a routing information base (RIB)

        $\to$ BGP can access this information and use it to choose the best peering option

**Delivery modes in networking**. Unicast, broadcast, multicase, and anycast

<div style="text-align:center">
    <img src="https://imgur.com/HtY10s3">
    <figcaption>Delivery modes</figcaption>
</div>

## Discussion
**Why UDP-based video streaming requires RTSP server**. 