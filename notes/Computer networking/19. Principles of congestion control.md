<!-- TOC titleSize:1 tabSpaces:2 depthFrom:1 depthTo:6 withLinks:1 updateOnSave:1 orderedList:0 skip:0 title:1 charForUnorderedList:* -->
# Table of Contents
- [Table of Contents](#table-of-contents)
- [Principles of congestion control](#principles-of-congestion-control)
  - [The causes and the costs of congestion](#the-causes-and-the-costs-of-congestion)
    - [Scenario 1 - Two senders, a router with infinite buffers](#scenario-1---two-senders-a-router-with-infinite-buffers)
    - [Scenario 2 - Two senders and a router with a finite buffers](#scenario-2---two-senders-and-a-router-with-a-finite-buffers)
    - [Scenario 3 - Four senders, routers with finite buffers, and multihop paths](#scenario-3---four-senders-routers-with-finite-buffers-and-multihop-paths)
  - [Approaches to congestion control](#approaches-to-congestion-control)
- [Appendix](#appendix)
  - [Concepts](#concepts)
<!-- /TOC -->

# Principles of congestion control
## The causes and the costs of congestion
### Scenario 1 - Two senders, a router with infinite buffers
**Scenario**.

<div style="text-align:center">
    <img src="https://i.imgur.com/fqHZ5vk.png">
    <figcaption>Congestion scenario 1 - Two connections sharing a single hop with infinite buffers</figcaption>
</div>

* *Host A*. The application in host A is sending data into the connection at an average rate of $\lambda_\text{in}$ bytes / sec
    
    $\to$ These data are original, in the sense that each unit of data is sent into the socket only once
    * *Underlying protocol overhead*. The underlying transport-level protocol is a simple one, i.e. data is encapsulated and sent, without error recovery, flow control, or congestion control
        * *Consequence*. Ignoring the additional overhead due to adding transport- and lower-layer information

            $\to$ The rate at which host A offers traffic to the router is $\lambda_\text{in}$ bytes / sec
* *Host B*. Host B operates in a similar manner, with the sending rate of $\lambda_\text{in}$ bytes / sec as host A
* *Router*. Packets from hosts A and B pass through a router, and over a shared outgoing link of capacity $R$
    * *Router buffer*. The router has buffers allowing it to store incoming packets when the packet-arrival rate exceeds the outgoing link's capacity
    * *Router buffer size*. Infinite

**Per-connection throughput**. Number of bytes per second at the receiver

$\to$ This is a function of the connection-sending rate $\lambda_\text{in}$

<div style="text-align:center">
    <img src="https://i.imgur.com/vjbduBY.png">
    <figcaption>Throughput and delay as a function of host sending rate</figcaption>
</div>

**Effects of sending rate**.
* *From $0$ to $R/2$*. The throughput at the receiver equals the sender's sending rate
* *From $R/2$ to infinity*. The throughput is only $R/2$, i.e. as a consequence of the sharing of link capacity between two connections
    * *Explain*. The link cannot deliver packets to a receiver at a steady-state rate which exceeds $R/2$
    * *Consequence*. No matter how high hosts A and B set their sending rates

        $\to$ They will each never see a throughput higher than $R/2$

**Effects of operating at near-link capacity**.
* *Effects of operating at near-link capacity*. 
    * *From throughput standpoint*. Achieving a per-connection of $R/2$ may actually appear to be a good thing
        * *Explain*. Since the link is fully utilized in delivering packets to their destinations
    * *From delay standpoint*. 
        * As the sending rate approaches $R/2$, the average delay becomes larger and larger
        * When the sending rate exceeds $R/2$, the average number of queued packets in the router is unbounded

            $\to$ The average delay between source and destination becomes infinite
* *Consequence*. Operating at an aggregate throughput of near $R$ may be ideal from a throughput standpoint, but it is far from ideal from a delay standpoint

**Conclusion**. Large queuing delays as the packet-arrival rate nears the link capacity is a cost of a congested network, even in this extremely idealized scenario

### Scenario 2 - Two senders and a router with a finite buffers
**Scenario**. Consider a modified version of scenario 1 with the following modifications

<div style="text-align:center">
    <img src="https://i.imgur.com/NlYiyH5.png">
    <figcaption>Scenario 2 - Two hosts, with retransmissions, and a router with finite buffers</figcaption>
</div>

* The amount of router buffering is assumed to be finite

    $\to$ Packets will be dropped when arriving to an already-full buffer
* Each connection is reliable, i.e. retransmission for dropped packets is implemented

**Offered load to the network**. The rate $\lambda'_\text{in}$, at which the transport layer sends segments, containing original data and retransmited data, into the network
* *Unit of measure*. Bytes per second

**Performance of transmission**. Depend strongly on how retransmission is performed

<div style="text-align:center">
    <img src="https://i.imgur.com/kmWRBuD.png">
    <figcaption>Scenario 2 performance with finite buffers</figcaption>
</div>

**Unrealistic case**. Host A is able to determine whether or not a buffer is free in the router, and thus sends a packet only when a buffer is free

$\to$ No packet loss will occur, i.e. $\lambda_\text{in} = \lambda_\text{in}'$, 
* *Connection throughput*. $\lambda_\text{in}$

    $\to$ From a throughput standpoint, perfornace is ideal, i.e. everything which is sent is recevied
* *Maximum sending rate*. $R/2$, i.e. since packet loss is assumed never to occur
    * *Explain*. If the sending rate exceeds $R/2$ then the router buffer would grow infinitely and packet loss starts to occur
* *Performance illustration*. Figure a in the figure above

**More realistic case**. The sender retransmits only when a packet is known for certain to be lost
* *Performance illustration*. Figure b in the figure above
* *Explain*. Consider the case $\lambda_\text{in}'$ equals $R/2$, the rate, at which data are delivered to the receiver application is $R/3$

    $\to$ Out of $0.5 R$ units of data transmitted, i.e. $0.333 R$ bytes / sec, on average, are original data, and $0.166 R$ bytes / src, on average, are retransmitted data
* *Consequence*. Another cost of a congested network is that the sender must perform retransmissions to compensate for dropped packets, due to buffer overflow

**Realistic case**. The sender may timeout prematurely and retransmit a packet which has been delayed in the queue but not yet lost

$\to$ Both original data packet and retransmission may reach the receiver
* *Consequence*. The receiver needs only one copy of the packet, and will discard the retransmission

    $\to$ The work done by the router in forwarding the retransmitted copy of the original packet was wasted
* *Performance illustration*. Figure c in the figure above
* *Consequence*. Another cost of congested network is that unneeded retransmissions by the sender in the face of large delays may cause a router to use its link bandwidth to forward unneeded copies of a packet

### Scenario 3 - Four senders, routers with finite buffers, and multihop paths
**Scenario**. Four hosts transmit packets, each over overlapping two-hop paths

<div style="text-align:center">
    <img src="https://i.imgur.com/Huf9B3j.png">
    <figcaption>Four senders, routers with finite buffers, and multihop paths</figcaption>
</div>

* Each host uses a timeout / retransmission mechanism to implement a reliable data transfer service
* All hosts have the same value of $\lambda_\text{in}$
* All router links have capacity $R$ bytes / sec

**Sending rates**.

<div style="text-align:center">
    <img src="https://i.imgur.com/bLtQinz.png">
    <figcaption>Performance with finite buffers and multihop paths</figcaption>
</div>

* *Extremely small $\lambda_\text{in}$*. Buffer overflows are rare, and the throughput approximately equals the offered load

    $\to$ This is similar to scenario 2 of unrealistic case
* *Slightly larger $\lambda_\text{in}$*. The throughput is larger, since more original is being transmitted into the network, and overflows are still rare

    $\to$ For small values of $\lambda_\text{in}$, an increase in $\lambda_\text{in}$ results in an increase in $\lambda_\text{out}$
* *Extremely large $\lambda_\text{in}$ and consequentially $\lambda_\text{in}'$*. Consider two paths A-C and B-D
    * *Observations*.
        * A-C and B-D traffic must compete at router $R2$ for the limited amount of buffer space

            $\to$ The amount of A-C traffic successfully getting through $R2$ becomes smaller, as the offered load from $B-D$ gets larger
        * As $\lambda_\text{in}'$ approaches infinity, an empty buffer at $R2$ is immediately filled by a B-D packet

            $\to$ The throughput of A-C connection at $R2$ goes to zero
    * *Consequence*. The A-C end-to-end throughput goes to zero, in the limit of heavy traffic

        $\to$ This give rise to the offered load $\lambda_\text{in}'$, versus throughput $\lambda_\text{out}$ tradeoff

**Packet drop**. Another cost of dropping a packet due to congestion is that the transmission capacity used at each upstream links to forward the packet to the point, at which it is dropped, end up having been wasted

## Approaches to congestion control
**Types of congestion-control approaches**. Classified by whether the network layer provides any explicit assistance to the transport layer for congestion-control purposes
* *End-to-end congestion control*. The network layer provides no explicit support for the transport layer for congestion-control purposes
    
    $\to$ Even the presence of congestion in the network must be inferred by the end systems, based only on observed network behavior, e.g. packet loss and delay
    * *Usage*. Used in TCP, since the IP layer provides no feedback to the end systems regarding network congestion
        * *Indication of network congestion*. 
            * Use TCP segment loss (traditional)
            * Use increasing round-trip delay values (recent proposal)
        * *Idea of congestion control*. Reduce window size
* *Network-assisted congestion control*. Network-layer components, i.e. routers, provide explicit feedback to the sender regarding the congestion state in the network

    <div style="text-align:center">
        <img src="https://i.imgur.com/NgPr4gl.png">
        <figcaption>Feedback pathways for network-indicated congestion information</figcaption>
    </div>

    * *Example*. Use a bit to indicate network congestion
    * *Congestion information feedback*. Fed back from the network to the sender by one of two ways
        * *Direct feedback*. Sent from a network router to the sender, in the form of a choke packet
        * *Network feedback via receiver*. A router marks / updates a field in a packet flowing from sender to receiver to indicate congestion

            $\to$ The receiver then notifies the sender of the congestion indication

            >**NOTE**. This form of notification takes at least a full round-trip time

# Appendix
## Concepts
**Relationship between sending rates and throughput and delay in scenario 1 - Two senders, a router with infinite buffers**.