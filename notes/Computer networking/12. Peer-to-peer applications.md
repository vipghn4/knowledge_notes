---
title: 7. Principles of network applications
tags: Computer networking
---

<!-- TOC titleSize:1 tabSpaces:2 depthFrom:1 depthTo:6 withLinks:1 updateOnSave:1 orderedList:0 skip:0 title:1 charForUnorderedList:* -->
# Table of Contents
- [Table of Contents](#table-of-contents)
- [Peer-to-peer applications](#peer-to-peer-applications)
  - [P2P file distribution](#p2p-file-distribution)
    - [Scalability of P2P architectures](#scalability-of-p2p-architectures)
    - [BitTorrent](#bittorrent)
  - [Distributed hash tables (DHTs)](#distributed-hash-tables-dhts)
    - [Circular DHT](#circular-dht)
    - [Peer churn](#peer-churn)
<!-- /TOC -->

# Peer-to-peer applications
**P2P idea**. There is minimal (or no) reliance on always-on infrastructure servers

$\to$ Instead, pairs of intermittently connected hosts, called peers, communicate directly with each other

**Example applications well-suited for P2P designs**. File distribution, and database distributed over a large community of peers

## P2P file distribution
**Scenario**. Consider a very natural application, i.e. distributing a large file from a single server to a large number of hosts, called peers
* *Client-server file distribution*. The server must send a copy of the file to each of the peers

    $\to$ This places an enormous burden on the server and consumes a large amount of server bandwidth
* *P2P file distribution*. Each peer can redistribute any portion of the file it has received to any other peers
    
    $\to$ This assists the server in the distribution process

**BitTorrent**. The most popular P2P file distribution protocol in 2012

### Scalability of P2P architectures
**A simple quantitative model for distributing a file to a fixed set of peers**.
* *Assumptions*.
    * The server and the peers are connected to the Internet with access link
        * The server and clients are not participating on any other network applications

            $\to$ Their upload and download access bandwidth can be fully devoted to file distribution
    * $u_s$ is the upload rate of the server's access link
    * $u_i$ is the upload rate of the $i$th peer's access link
    * $d_i$ is the download rate of the $i$th peer's access link
    * $d_\text{min} = \min\{d_1,\dots,d_N\}$
    * $F$ is the size of the file to be distributed, in bits
    * $N$ is the number of peers wanting a copy of the file
    * The Internet core has abundant bandwidth, i.e. all of the bottlenecks are in access networks
* *Distribution time*. The time it takes to get a copy of the file to all $N$ peers
* *Distribution time $D_\text{cs}$ of the client-server architecture*.
    * *Observations*.
        * The server must transmit one copy of the file to each of the $N$ peers

            $\to$ The server must transmit $NF$ bits, thus the time to distribute the file is at least $NF/u_s$
        * The peer with the lowest download rate cannot obtain all $F$ bits of the file in less than $F/d_\text{min}$ seconds

            $\to$ The minimum distribution time is at least $F/d_\text{min}$
    * *Conclusion*. $D_\text{cs}\geq \max\{\frac{NF}{u_s},\frac{F}{d_\text{min}}\}$

        $\to$ This provides a lower bound on the minimum distribution time for the client-server architecture
        * *Lower bound achievement*. To achieve the lower bound of $D_\text{cs}$, the server should schedule its transmission so that the lower-rate peers is transmitted sooner
            * *Proof*. 
                * In an ideal case, the distribution time is given as

                    $$D_\text{cs} = NF/u_s + \max\{0, \max_{i=1\dots N} \{F/d_i - (N - i)F/u_s\}\}$$
                
                * In other words, minimizing $D_\text{cs}$ means minimizing 
                    
                    $$\max_{i=1\dots N} \{F/d_i - (N - i)F/u_s\}$$

                    which, in turn, means minimizing

                    $$\max_{i=1\dots N} \{1/d_i - (N - i)/u_s\}$$
                    
                    which, in turns, means minimizing

                    $$m=\max_{i=1\dots N} \{1/d_i + i/u_s\}$$
                
                * Without loss of generality, suppose that $d_1\leq d_2\leq \dots \leq d_N$, and the file transmission order is $1,2,\dots,N$
                    * Let the value of $m$ obtained via this scheduling is $m^*$
                    * Consider swapping any pair $(i,j)$, where $i<j$, in the proposed transmission order, we have that

                        $$\begin{aligned}
                        \max\{\frac{1}{d_i}+\frac{i}{u_s},\frac{1}{d_j}+\frac{j}{u_s}\}&\leq\frac{1}{d_i}+\frac{j}{u_s}\\
                        &=\max\{\frac{1}{d_j}+\frac{i}{u_s},\frac{1}{d_i}+\frac{j}{u_s}\}
                        \end{aligned}$$

                    * Therefore, the proposed scheduling method is optimal
    * *Consequence*. As $N$ approaches inifity, the client-server distribution time approaches infinity, as a linear function of $N$
* *Distribution time $D_\text{P2P}$ of the P2P architecture*. 
    * *Idea of P2P*. When a peer receives some file data, it can use its own upload capacity to redistribute the data to other peers

    >**NOTE**. Calculating the distribution time for the P2P is somewhat more complicated than for the client-server architecture
    >* *Explain*. The distribution time depends on how each peer distributes portions of the file to the other peers

    * *Observations*.
        * At the beginning of the distribution, only the server has the file

            $\to$ To get this file into the community of peers, the server must send each bit of the file at least once into its access link
            * *Consequence*. The minimum distribution time is at least $F/u_s$
        * As with the client-server architecture, the peer with the lowest download rate cannot obtain $F$ bits of the file in less than $F/d_\text{min}$ seconds

            $\to$ The minimum distribution time is at least $F/d_\text{min}$
        * The total upload capacity of the system as a whole is equal to the upload rate of the server, plus the rates of each individual peers

            $\to$ $u_\text{total}=u_s + u_1 + \dots + u_N$
        
        * The total number of bits to be uploaded is $NF$ bits, thus the minimum distribution time is at least $NF/(u_s + u_1 + \dots + u_N)$
    * *Conclusion*. $D_\text{P2P} \geq \max\{\frac{F}{u_s},\frac{F}{d_\text{min}},\frac{NF}{u_s + u_1 + \dots + u_N}\}$

        >**NOTE**. IN reality, chunks of the file are redistributed, rather than individual bits, and the inequality above is a good approximation of the actual minimum distribution time

**Distribution time comparison between client-server and P2P architectures**.

<div style="text-align:center">
    <img src="https://i.imgur.com/2hoxN2b.png">
    <figcaption>Distribution time for P2P and client-server architectures</figcaption>
</div>

**Self-scalability of P2P architecture**. Applications with the P2P architecture can be self-scaling, which is a direct consequence of peers being redistributors and consumers of bits

### BitTorrent
**BitTorrent**. A popular P2P protocol for file distribution
* *Torrent*. The colleciton of all peers participating in the distribution of a particular file
* *Idea*. Peers in a torrent download equal-size chunks of the file from one another, with a typical chunk size of 256 KByes
    * *Joining peers*. When a peer first joins a torrent, it has no chunks, but overtime, it accumulates more and more chunks
    * *Running peers*. While a peer downloads chunks, it also uploads chunks to other peers
    * *Leaving peers*. Once a peer has acquired the entire file, it may selfishly leave the torrent, or altruistically remain in the torrent and continue to upload chunks to other peers
* *Number of peers in a torrent*. Fewer than ten or more than a thousand peers at any instance of time

**Tracker**. Each torrent has an infrastructure node, called a *tracker*
* *Responsibility*. When a peer joins a torrent, it registers itself with the tracker and periodically informs the tracker that it is still in the torrent

    $\to$ The tracker keeps track of the peers participating in the torrent
* *Peer joining*.
    1. A new peer joins the torrent
    2. The tracker randomly selects a subset of peers, e.g. 50, from the set of participating peers
    3. The tracker sends the IP addresses of these 50 peers to the new peer
    4. Possessing the list of peers, the new peer attemps to establish concurrent TCP connections with all the peers on this list
    5. As time evolves, some of these peerse may leave and other may attempt to establish TCP connections with this new peer

        $\to$ A peer's neighboring peers will fluctuate over time
* *Peer functionality*. At any given time, each peer will have a subset of chunks from the file, with different peers having different subsets
    * Periodically, a peer will ask each of his neighboring peers, over TCP connections, for the list of chunks the have

        $\to$ If the new peer has $L$ different neighbors, he will obtain $L$ lists of chunks
    * With this knowledge, the new peer will issue requests, over the TCP connections, for chunks he currently does not have

        $\to$ At any given instant of time, the peer will have a subset of chunks and will know which chunks his neighbors have
    * With this knowledge, the peer has two important decisions to make
        * Which chunks should he request first from his neighbors
        * To which of his neighbors should he send requested chunks
* *Rarest first technique*. A technique for decision making (for question 1) of peers in BitTorrent
    * *Idea*. Determine, among the chunks he does not have, the chunks which are rarest among the peer's neighbors

        $\to$ He then request those rarest chunks first
    * *Consequence*. Rarest chunks get more quickly redistributed, aiming to roughly equalize the number of copies of each chunk in the torrent
* *Clever trading algorithm*. A technique for decision making (for question 2) of peers in BitTorrent
    * *Idea*. 
        * The peer gives priority to the neighbors which are currently supplying him data at the highest rate, e.g. choose 4 most-friendly neighbors

            $\to$ The peer then reciprocates by sending chunks to these peers
        * The peer periodically recalculates the rates and possibly modifies the set of peers to send file chunks
            
            $\to$ These peers, who are sent chunks, are said to be unchocked
        * Every 30 seconds, the peer also picks one additional neighbor at random and sends it chunks

            $\to$ This neighbor is said to be optimistically unchocked
    * *Consequence of optimistically unchocking*. Peers capable of uploading at compatible rates tend to find each other
        * By sending data to a random peer, our peer may become one of that peer's top uploaders

            $\to$ That peer will start to send data to us
        * If the rate, at which that peer sends data to us is high enough, that peer could then, in turn, be one of our top uploaders

## Distributed hash tables (DHTs)
**Centralized database**. Simply contain (key, value) pairs
* *Database query*. We query database with a key, and the database returns the corresponding values
* *Database building*. Straightforward with a client-server architecture which store all the (key, value) pairs in one central server

**Distributed hash table (DHT)**. A distributed P2P database, where (key, value) pairs will be distributed over millions of peers, each of which will only hold a small subset of the totality of the (key, value) pairs
* *Database query*. Each peer can query the distributed database with a particular key

    $\to$ The distributed database will then locate the peers having corresponding (key, value) pairs, and return the key-value pairs to the querying peer
* *Database updating*. Any peer can insert new key-value pairs into the database

**Example of a DHT service in a P2P file sharing system**. A key is the content name, and the value is the IP address of a peer having a copy of the content

$\to$ If peer A and B each have a copy of the latest Linux distribution, then DHT database will include (Linux, IP-A) and (Linux, IP-B)
* *DHT distribution*. The DHT database is distributed over the peers

    $\to$ Some peer C will be responsible for the key "Linux", and will have the corresponding key-value pairs
* *DHT query*. Suppose a peer D wants to obtain a copy of Linux
    1. D needs to know which peers have a copy of Linux before beginning to download it

        $\to$ She queries the DHT with "Linux" as the key
    2. The DHT determines that peer C is responsible for the key "Linux"
    3. The DHT then contacts peer C, obtains from C the key-value pairs (Linux, IP-A) and (Linux, IP-B)
    4. The DHT then passes the obtained key-value pairs to peer D
    5. Peer D can then download the latest Linux distribution from either IP-B or IP-C

**Designing a DHT for general key-value pairs**.
* *Naive approach*. Randomly scatter the (key, value) paris across all the peers, and have each peer maintain a list of the IP addresses of all participating peers
    * *DHT query*. The querying peer sends its query to all other peers

        $\to$ The peers containing the (key, value) pairs matching the queried key can respond with their matching pairs
    * *Drawback*. This approach is not scalable
* *An elegent approach*.
    * *Idea*. 
        * Assign an identifier to each peer, e.g. each identifier is an integer in range $[0,2^n-1]$ for some fixed $n$
        * Each key is required to be an integer in range $[0,2^n-1]$
    * *Creating integer keys from non-integer keys*. Use a hash function mapping each key to an integer in the range $[0,2^n-1]$

        $\to$ This hash function is assumed to be available to all peers in the system
        * *Consequence*. When referring to the "key", we refers to the hash of the original key
    * *Inserting (key, value) into database*.
        * *Central issue*. Define a rule for assigning keys to peers
        * *Natural solution*. Assign each (key, value) pair to the peer, whose identifier is the closest to the key

            $\to$ We need to define what is meant by "closest", for which many conventions are possible
        * *(key, value) inserting procedure*.
            1. The underlying peer, namely peer A, determines the peer whose identifier is closest to the key
            2. Peer A sends a message to that peer, instructing it to store the (key, value) pair
        * *Problem*. How can peer A determine the peer cloest to the key, without keeping track of all peers in the system

### Circular DHT
**Circular organization of peers**. Fix the problem of scalability of DHT
* *Idea*. Each peer only keeps track of its immediate sucessor and immediate predessor (modulo $2^n$)
* *Overlay network*. The circular arrangement of peers is a special case of an overlay network
    * *Idea*. The peers form an abstract logical network, which resides above the underlay computer network consisting of physical links, routers, and hosts
    * *Explain*. The links in an overlay network are not physical links, but are simply virtual liaisons between pairs of peers
* *Querying for desired peer with queried key*. Suppose a peer $m$ wants to determine which peer in the DHT is responsible for key $k$
    1. Peer $m$ creates a message saying "Who is responsible for key $k$?", and sends this message clockwise around the circle
    2. Whenever a peer receives the message, because it knows the identifier of its successor and predessor

        $\to$ It can determine whether it is responsible for, i.e. closest to, the key in question
    3. If a peer is not responsible for the key, it simply sends the message to its successor
    4. Step 2 and 3 repeats until the message arrives at the desired peer, who determines that it is the closest peer to the desired key
    5. The desired peer sends a message back to the querying peer, indicating that it is responsible for key $k$
* *Benefits*. Reduce the amount of overlay information each peer must manage
    * *Explain*. Each peer needs to only be ware of its immediate successor and predecessor
* *Problem*. To find the node responsible for a key, in the worst case, all $N$ nodes in the DHT will have to forward a message around the circle

    $\to$ $N/2$ messages are sent on average

**Trade-off in designing a DHT**. There is a tradeoff between the number of neighbors eahc peer has to track, and the number of messages which the DHT needs to send to resolve a single query
* *Solution*. Use the circular overlay as a function, but add shortcuts so that each peer also keeps track of a relatively small number of shortcut peers scattered about the circle 
* *How many shortcut neighbors each peer should have*. This question has attracted significant attention in the research community
    * *Complexity*. It has been shown that the DHT can be designed so that both the number of neighbors per peer, and the number of messages per query, is $O(\log N)$ where $N$ is the number of peers

        $\to$ Such designs strike a satisfactory compromise between the extreme solutions of using mesh and circular overlay topologies

### Peer churn
**Problem**. In a P2P system, a peer can come or go without warning

$\to$ When designing a DHT, we must concern about maintaining the DHT overlay in the presence of such peer churn
* *Requirements*. To handle peer churn, we require 
    * Each peer to track, i.e. know the IP address of, its first and second successors
    * Each peer must periodically verify that its two successors are alive, e.g. by sending ping messages to them and asking for responses

**Handle leaving peers**. The two peers $P_1$ and $P_2$ preceding the departed peer $P_3$, with $P_2$ be the immediate predecessor, learn that this peer has departed, since it no longer responds to ping messages

$\to$ These preceding peers need to update their successor state information
* *Update procedure*.
    1. $P_2$ replaces its first successor, i.e. $P_3$, with its second successor, e.g. $P_4$
    2. $P_2$ then asks its new first successor for the identifier and IP address of its immediate successor, e.g. $P_5$
    3. $P_2$ then makes peer $P_5$ as its second successor

**Handle joining peers**. If a peer $P$ wants to join the DHT, and at the time of joining, it only knows about $P_1$, i.e. first peer, in the DHT
1. $P$ sends $P_1$ a message, saying "What will be my predecessor and successor?"
2. The message is forwarded through the DHT until it reaches the supposed predecessor $P_d$ and successor $P_s$ of $P$
3. $P_d$ sends information about $P_s$ and $P_d$ to $P$
4. $P$ can now join the DHT by making $P_s$ its successor, and by notifying $P_d$ that it should change its immediate successor to $P$