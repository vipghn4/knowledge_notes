<!-- TOC titleSize:1 tabSpaces:2 depthFrom:1 depthTo:6 withLinks:1 updateOnSave:1 orderedList:0 skip:0 title:1 charForUnorderedList:* -->
# Table of Contents
- [Table of Contents](#table-of-contents)
- [Statistical models, supervised learning, and function approximation](#statistical-models-supervised-learning-and-function-approximation)
  - [A statistical model for the joint distribution $P(X,Y)$](#a-statistical-model-for-the-joint-distribution-pxy)
  - [Supervised learning](#supervised-learning)
  - [Function approximation](#function-approximation)
<!-- /TOC -->

# Statistical models, supervised learning, and function approximation
**Goal**. Find a useful approximation $\hat{f}(x)$ to the function $f(x)$ underlying the predictive relationship between $X$ and $Y$
* *Drawbacks of nearest-neighbor methods*.
    * If the input space dimension is high, the nearest neighbors need not be close to the target point

        $\to$ This can result in large errors
    * If special structure is known to exist, this can be used to reduce bias and variance of the estimates
* *Consequence*. We anticipate using other classes of models for $f(x)$, in many cases specifically designed to overcome the dimensionality problems

## A statistical model for the joint distribution $P(X,Y)$
**Underlying statistical model**. $Y=f(X)+\epsilon$ where $\epsilon$ is zero-mean random error independent of $X$
* *Key points*. $f(x) = E(Y|X=x)$, and $P(Y|X)$ depends on $X$ only through the conditional mean $f(x)$
* *Additive error model*. A useful approximation to the truth
    * *Explain*. For most systems, the input-output pairs $(X,Y)$ will not have a deterministic relationship $Y=f(X)$

        $\to$ There will be other unmeasured variables contributing to $Y$, including the measurement error
    * *Underlying assumption of the additive model*. We can capture all the departures from a deterministic relationship via the error $\epsilon$
* *Error distribution*. The assumption that the errors are i.i.d is not strictly necessary

    $\to$ However, it seems to be at the back of our mind when averaging squared error uniformly in our EPE criterion
    * *Simple modification to avoid independence assumption*. We can have $\text{Var}(Y|X=x)=\sigma(x)$
* *Conclusion*. $P(Y|X)$ can depend on $X$ in complicated ways, but the additive error model precludes these

**Additive error for qualitative outputs $G$**. Additive error models are typically not used for qualitative outputs $G$

$\to$ In this case, the target function $p(X) = P(G|X)$ is modeled directly

## Supervised learning
**Supervised learning**. Suppose that the errors are additive and that the model $Y = f(X)+\epsilon$ is a reasonable assumption

$\to$ Supervised learning attempts to learn $f$ by example through a teacher
* *Idea*.
    1. One observes the system under study, both the inputs and outputs, and assembles a training set of observations

        $$T =\{(x_i,y_i):i=1,\dots,N\}$$

    2. $x_i$ are also fed into an artificial system, i.e. a learning algorithm, which is usually a computer program
        
        $\to$ The learning algorithm produces outputs $\hat{f}(x_i)$ in response to the inputs
* *Learning by example*. The process of modifying its input-output relationship $\hat{f}$ in response to differences $y_i âˆ’\hat{f}(x_i)$ between

    $\to$ This is done by the learning algorithm
    * *Learning output*. Upon completion of the learning process 
        
        $\to$ Hope that the artificial and real outputs will be close enough to be useful for all sets of practical inputs

## Function approximation
**Approach in applied mathematics and statistics**. Derived from the perspective of function approximation and estimation
* *Idea*. 
    * The data pairs $(x_i,y_i)$ are seen as points as a $(p+1)$-dimensional Euclidean space
    * The function $f(x)$ has domain equal to the $p$-dimensional input subspace

        $\to$ This function is related to the data via a model $y_i = f(x_i) + \epsilon_i$

        >**NOTE**. For convenience, in this section we will assume the domain is $\mathbb{R}^p$

* *Goal*. Obtain a useful approximation to $f(x)$ for all $x$ in some region of $\mathbb{R}^p$, given the representations in $\mathcal{T}$
* *Benefits of the function-approximation perspective*. Encourage the geometrical concepts of Euclidean spaces and mathematical concepts of probabilistic inference to be applied

**Linear basis expansions**. Another class of useful approximators
* *Mathematical modeling*.

    $$f_\theta(x) = \sum_{k=1}^K h_k(x) \theta_k$$

    where $h_k$ are suitable set of functions, or transformations, of the input vector $x$
* *Learning algorithm*. Least-square methods to estimate $\theta$, i.e. minimizing the following objective as a function of $\theta$

    $$\text{RSS}(\theta) = \sum_{i=1}^N [y_i - f_\theta(x_i)]^2$$

* *Solution*. 
    * If the basis functions do not have any hidden parameters

        $\to$ We get a simple closed form solution to the minimization problem
    * Otherwise, the solution requires either iterative methods or numerical optimization

**Maximum likelihood estimation**. A more general principle for estimation
* *Motivation*. While least squares is generally very convenient
    
    $\to$ It is not the only criterion used and in some cases would not make much sense
* *Assumptions*. $y_1,\dots,y_N$ is a random sample from a density $P_\theta(y)$
* *Log-probability of the observed sample*.
    
    $$L(\theta) = \sum_{i=1}^N \log P_\theta(y)$$

* *Principle of maximum likelihood*. Assume that the most reasonable values for $\theta$ are those, for which $P_\theta(\mathbf{y})$ is largest
* *Examples*.
    * *Least-square and maximum likelihood*. Least squares for the additive model $Y=f_\theta(X) + \epsilon$, with $\epsilon\sim\mathcal{N}(0,\sigma^2)$ is equivalent to maximum likelihood using

        $$P(Y|X,\theta) = \mathcal{N}(f_\theta(X),\sigma^2)$$

    * *Multinomial likelihood for regression function $P(G|X)$ for qualitative output $G$*.
        * *Assumptions*. $P(G=\mathcal{G}_k|X=x)=p_{k,\theta}(x)$ is the conditional probability for each class $k=1,\dots,K$ given $X$
        * *Log-likelihood function*. The following function is referred to as cross-entropy also

            $$L(\theta) = \sum_{i=1}^N \log p_{g_i,\theta}(x_i)$$