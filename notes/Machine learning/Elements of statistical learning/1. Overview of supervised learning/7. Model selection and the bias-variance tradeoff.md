<!-- TOC titleSize:1 tabSpaces:2 depthFrom:1 depthTo:6 withLinks:1 updateOnSave:1 orderedList:0 skip:0 title:1 charForUnorderedList:* -->
# Table of Contents
- [Table of Contents](#table-of-contents)
- [Model selection and the bias-variance tradeoff](#model-selection-and-the-bias-variance-tradeoff)
<!-- /TOC -->

# Model selection and the bias-variance tradeoff
**Smoothing or complexity parameter**. All the models mentioned and many others have a smoothing or complexity parameter to be determined
* *Example of smoothing or complexity parameter*.
    * The multiplier of the penalty term
    * The width of the kernel
    * The number of basis functions

**Spline models**.
* *Cubic smoothing spline*. The parameter $\lambda$ indexes models ranging from a straight line fit, i.e. $\lambda\to\infty$, to the interpolating model, i.e. $\lambda\to 0$
* *Polynomial spline*. Ranges between a degree-$m$ global polynomial, i.e. infinite window size, to an interpolating fit, i.e. small window size
* *Consequence*. We cannot use residual sum-of-squares on the training data to determine these parameters as well
    * *Explain*. We would always pick those that gave interpolating fits and hence zero residuals
        
        $\to$ Such a model is unlikely to predict future data well at all

**k-nearest-neighbor regression fit $\hat{f}_k(x_0)$ and bias-variance tradeoff**.
* *Assumptions*.
    * The data arise from a model $Y=f(X)+\epsilon$ where $E(\epsilon)=0$ and $\text{Var}(\epsilon)=\sigma^2$
    * The values of $x_i$ in the sample are fixed in advance, i.e. not random
* *Expected prediction error*. Also known as test or generalization error

    $$\begin{aligned}
    \text{EPE}_k(x_0) &= E[(Y-\hat{f}_k(x_0))^2|X=x_0]\\
    &=\sigma^2 + \text{Bias}^2[\hat{f}_k(x_0) + \text{Var}_\mathcal{T}[\hat{f}_k(x_0)]]\\
    &= \sigma^2 + [f(x_0) - \frac{1}{k} \sum_{l=1}^k f(x_{(l)})]^2 + \frac{\sigma^2}{k}
    \end{aligned}$$

    * *Irreducible error*. $\sigma^2$, i.e. the variance of the new test target, which is beyond our control, even if we know the true $f(x_0)$
    * *Bias*. Most likely increase with $k$, if the true function is reasonably smooth
        * *Explain*. As $k$ grows, the neighbors are further away, and anything can happen
    * *Variance*. Decrease as the inverse of $k$
* *Conclusion*. As $k$ varies, there is a bias-variance tradeoff

**Model complexity and bias-variance tradeoff**. As the model complexity of our procedure is increased

$\to$ The variance tends to increase, and the squared bias tends to decrease
* *Model selection*. Typically, we want the model complexity to trade bias off with variance to minimize the test error
* *Estimate of test error*. An obvious one is the training error
    * *Drawback*. Training error is not a good estimate of test error, since it does not properly account for model complexity, i.e.
        * The training error tends to decrease as the model complexity increases

            $\to$ The model adapts itself too closely to the training data, without generalization 
            * *Consequence*. The predictions $\hat{f}(x_0)$ have large variance
        * If the model is not complex, it will underfit and may have large bias, resulting in poor generalization