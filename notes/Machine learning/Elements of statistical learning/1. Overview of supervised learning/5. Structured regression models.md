<!-- TOC titleSize:1 tabSpaces:2 depthFrom:1 depthTo:6 withLinks:1 updateOnSave:1 orderedList:0 skip:0 title:1 charForUnorderedList:* -->
# Table of Contents
- [Table of Contents](#table-of-contents)
- [Structured regression models](#structured-regression-models)
  - [Difficulty of the problem](#difficulty-of-the-problem)
<!-- /TOC -->

# Structured regression models
**Brief**. Even in case of low dimensions, the nearest-neighbor and other local methods can be inappropriate
* *Explain*. More structured approaches can make more efficient use of the data

## Difficulty of the problem
**Difficulty of least-square**. Consider the RSS criterion for an arbitrary function $f$

$$\text{RSS}(f) = \sum_{i=1}^N [y_i - f(x_i)]^2$$

* *Solutions to minimization*. Minimizing $\text{RSS}(f)$ leads to infinitely many solutions, i.e. any $\hat{f}$ passing through the training points $(x_i,y_i)$ is a solution

    $\to$ Any solution chosen may be a poor predictor at test points different from the training points
* *Convergence of solution*. If there are multiple observation pairs $(x_i,y_{il})$ for $l=1,\dots,N$ at each $x_i$

    $\to$ The risk is limited
    * *Explain*. If $N$ were sufficiently large so that repeats were guaranteed and densely arranged

        $\to$ These solutions may all tend to the limiting conditional expectation

**Function space restriction**. To obtaining useful results for finite $N$

$\to$We need to restrict the eligible solutions to a smaller set of functions
* *Idea*. How to decide on the nature of the restrictions is based on considerations outside of the data
* *Restriction representations*.
    * *Option 1*. Encode the restrictions via the parametric representation of $f_\theta$
    * *Option 2*. Build the restrictions into the learning method, either implicitly or explicitly
* *Restrictions leading to unique solution*. Any restrictions on $f$, which lead to a unique minimization solution, do not really remove the ambiguity caused by the multiplicity of solutions
    * *Explain*. There are infinitely many possible restrictions, each leading to a unique solution

        $\to$ The ambiguity has transferred to the choice of constraint

**Complexity restrictions**. The constraints imposed by most learning methods

$\to$ This usually means some kind of regular behavior in small neighborhoods of the input space
* *Interpretation*. We restrict the complexity of the function space locally
* *Explain*. For all input points $x$ sufficiently close to each other in some metric

    $\to$ $\hat{f}$ exhibits some special structure, e.g. nearly constant, linear, or low-order polynomial behavior
    * *Consequence*. The estimator is then obtained by averaging or polynomial fitting in that neighborhood
* *Strength of a constraint*. Dictated by the neighborhood size
    * *Explain*. The larger the size of the neighborhood
        * The stronger the constraint, and 
        * The more sensitive the solution is to the particular choice of constraint
    * *Examples*.
        * Local constant fits in infinitesimally small neighborhoods is no constraint at all
        * Local linear fits in very large neighborhoods is almost a globally linear model, and is very restrictive
* *Nature of a constraint*. Depend on the metric used, e.g.
    * Some methods, e.g. kernel and local regression and tree-based methods, directly specify the metric and size of the neighborhood
    * The nearest-neighbor methods are based on the assumption that the function is locally constant
    * Other methods, e.g. splines, neural networks, and basis-function methods, implicitly define neighborhoods of local behavior

**Dimensionality problems**. The number of fixed-size neighborhoods goes up exponentially with the dimension
* *Isotropic neighborhoods*. A fancy word for a ball, i.e. a ball is the same in all directions
* *Conclusion*.
    * Any method attempting to produce locally varying functions in small isotropic neighborhoods will run into problems in high dimensions

        $\to$ This is due to the curse of dimensionality
    * All methods overcoming the dimensionality problems have an associated, and often implicit and adaptive, metric for measuring neighborhoods

        $\to$ Such metrics does not allow the neighborhood to simultaneously small in all directions