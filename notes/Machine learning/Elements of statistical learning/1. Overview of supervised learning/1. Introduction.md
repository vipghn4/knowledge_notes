<!-- TOC titleSize:1 tabSpaces:2 depthFrom:1 depthTo:6 withLinks:1 updateOnSave:1 orderedList:0 skip:0 title:1 charForUnorderedList:* -->
# Table of Contents
- [Table of Contents](#table-of-contents)
- [Introduction](#introduction)
  - [Introduction](#introduction-1)
    - [Variable types and terminology](#variable-types-and-terminology)
  - [Simple approaches to prediction - Least squares and nearest neighbors](#simple-approaches-to-prediction---least-squares-and-nearest-neighbors)
    - [Linear models and least squares](#linear-models-and-least-squares)
    - [Nearest-neighbor methods](#nearest-neighbor-methods)
    - [From least squares to nearest neighbors](#from-least-squares-to-nearest-neighbors)
<!-- /TOC -->

# Introduction
## Introduction
**Supervised learning**. For each there is a set of variables that might be denoted as inputs, which are measured or preset

$\to$ These have some influence on one or more outputs
* *Example*. The goal is to use the inputs to predict the values of the outputs
* *Terminology*.
    * *Statistical literature*. The inputs are predictors

        $\to$ The outputs are called the responses
    * *Pattern recognition literature*. The inputs are independent variables
        
        $\to$ The outputs are the dependent variables

### Variable types and terminology
**Output variable types**.
* *Quantitative variables*. Some measurements are bigger than others, and measurements close in value are close in nature
* *Qualitative variables*. Also referred to as categorical, discrete variables, or factors
    * *Characteristics*.
        * There is no explicit ordering in the classes
        * There are often descriptive labels rather than numbers are used to denote the classes
    * *Representation*. Typically represented by codes, sometimes referred as targets
        * *Binary qualitative variables*. Use numeric codes, i.e. a single binary digit or bit
        * *Multi-ary qualitative variables*. Use dummy variables, i.e. the most useful and commonly used coding
* *Ordered categorical*. There is an ordering between the values, but no metric notion is appropriate

**Input variable types**. Qualitative and quantitative

**Prediction tasks and methods**. 
* *Tasks*. The distinction in output type led to a naming convention for the prediction tasks
    * *Regression*. When re predict quantitative outputs
    * *Classification*. When we predict qualitative outputs
* *Methods*. The distinction in input type have also led to distinctions in the types of methods used for prediction

**General supervised learning task**. Given the value of an input $X$, make a good prediction of the output $Y$, denoted by $\hat{Y}$

>**NOTE**. $Y$ is usually used for quantitative outputs, $G$ is usually used for qualitative outputs

* *Dataset*. We have available a set of measurements $\{(x_i,y_i)\}_{i=1}^N$ or $\{(x_i,g_i)\}_{i=1}^N$ known as training data

    $\to$ We will construct our prediction rule with this data

## Simple approaches to prediction - Least squares and nearest neighbors
### Linear models and least squares
**Linear model**. Given an input vector $X^T=(X_1,\dots,X_p)$, we predict $Y$ via the model

$$\hat{Y}=\hat{\beta}_0+\sum_{j=1}^p X_j \hat{\beta}_j$$

* *Intercept (or bias in machine learning)*. $\hat{\beta}_0$
* *Convention*.  It is convenient to include the constant variable $1$ in $X$, and $\hat{\beta}_0$ in $\hat{\beta}$
    
    $\to$ The linear model is written in vector form as an inner product

    $$\hat{Y}=X^T \hat{\beta}$$

* *Interpretations*. 
    * *Hyperplane interpretation*. $(X,\hat{Y})$ represents a hyperplane
        * If $\hat{Y}=\hat{\beta}_0+\sum_{j=1}^p X_j \hat{\beta}_j$ is used

            $\to$ The hyperplane is an affine set cutting the $Y$-axis at $(0,\hat{\beta}_0)$
        * If $\hat{Y}=X^T \hat{\beta}$ is used

            $\to$ The hyperplane includes the origin and is a subspace
    * *Function interpretation*. $f(X)=X^T \beta$ is a linear function in the $p$-dimensional input space

        $\to$ $f'(X)=\beta$ is a vector in input space, which points in the steepest uphill direction
* *Fitting linear model to a set of training data*. Use least squares, i.e. pick $\beta$ to minimize

    $$\text{RSS}(\beta) = \sum_{i=1}^N (y_i - x_i^T \beta)^2$$

    * *Solution to least-square*. Solving the normal equation, we got

        $$\hat{\beta} = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X} \mathbf{y}$$
* *Inference with linear model*. Given an arbitrary input $x$, the prediction is $\hat{y}(x) = x^T \hat{\beta}$

**Linear model for binary classification**. We code the response $Y$ as $0$ of negative and $1$ for positive classes

$\to$ $\hat{Y}$ are converted to a fitted class variable $\hat{G}$ according to the rule

$$\hat{G} = \begin{cases}\text{positive} & \hat{Y} > 0.5\\ \text{negative} & \hat{Y} \leq 0.5\end{cases}$$

* *Decision boundary*. Two predicted classes are separated by the decision boundary $\{x:x^T \hat{\beta} = 0.5\}$

    $\to$ The boundary is linear
* *Classification error*. There are several misclassifications on both sides of the decision boundary

**Optimality of linear model**.
* *Scenario 1*. The training data in each class were generated from multivariate Gaussian distributions with uncorrelated components and different means
    * *Optimality*. In this case, a linear decision boundary is the best we can do, and our estimate is almost optimal
* *Scenario 2*. The training data in each class came from a mixture of several low-variance Gaussian distributions, with individual means themselves distributed as Gaussian

    $\to$ This is best described in terms of the generative model
    * *Generative model*. The data is generated as
        1. A discrete variable determining which of the component Gaussians to use
        2. An observation is then generated from the chosen density
    * *Optimality*. A linear decision boundary is unlikely to be optimal

        $\to$ The optimal decision boundary is nonlinear and disjoint, which is much more difficult to obtain

### Nearest-neighbor methods
**Nearest-neighbor methods**. Use observations in the training set $\mathcal{T}$ closest in input space to $x$ form $\hat{Y}$

$\to$ The $k$-nearest neighbor fit for $\hat{Y}$ is defined as

$$\hat{Y}(x) = \frac{1}{k} \sum_{x_i \in N_k(x)} y_i$$

where $N_k(x)$ is the neighborhood of $x$ defined by the $k$ closest points $x_i$ in the training sample
* *Closeness*. Imply a metric, i.e. we find the $k$ observations with $x_i$ closest to $x$ in input space, and average their responses
* *Optimality of kNN*. 
    * *Mixture model*. k-nearest-neighbormethods is more appropriate for the mixture scenario described above
    * *Gaussian model*. k-nearest-neighbormethods is unnecessarily noisy
* *Choosing $k$*. Choosing $k$ based on the training set would result in $k=1$ and loss will always be 0

    $\to$ An independent test set would give us a more satisfactory means for comparing different methods

**Parameters of k-NN**. It appears that k-NN has only one parameter, i.e. $k$

$\to$ However, the effective number of parameters is $N/k$, which is generally bigger than $p$
* *Explain*. If the neighborhoods are non-overlapping, there would be $N/k$ neighborhoods, and we would fit one parameter, i.e. a mean, in each neighbor

**Loss function**. We cannot use sum-of-squared errors on the training set as criterion for picking $k$, since we always pick $k=1$

### From least squares to nearest neighbors
**Linear decision boundary from least squares**. This model has low variance and potentially high bias
* *Pros*. Very smooth and apparently stable to fit
* *Cons*. Rely heavily on the assumption that a linear decision boundary is appropriate

**k-nearest neighbor**. This model is wiggly and unstable, i.e. high variance and low bias
* *Pros*. Do not appear to rely on any stringent assumptions about the underlying data, and can adapt to any situation
* *Cons*. Any particular subregion of the decision boundary depends on a handful of input points and their particular positions

**Variations of two methods**.
* Kernel methods use weights which decrease smoothly to zero with distance from the target point, rather than $0/1$ weights used by k-nearest-neighbors
* Distance kernels are modified in high-dimensional spaces to emphasize some variable more than others
* Local regression fits linear models by locally weights least squares, rather than fitting constants locally
* Linear models fit to a basis expansion of the original inputs allow  arbitrary complex models
* Projection pursuit and neural network models consist of sums of nonlinear transformed linear models