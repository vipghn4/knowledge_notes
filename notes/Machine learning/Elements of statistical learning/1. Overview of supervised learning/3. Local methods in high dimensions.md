<!-- TOC titleSize:1 tabSpaces:2 depthFrom:1 depthTo:6 withLinks:1 updateOnSave:1 orderedList:0 skip:0 title:1 charForUnorderedList:* -->
# Table of Contents
- [Table of Contents](#table-of-contents)
- [Local methods in high dimensions](#local-methods-in-high-dimensions)
  - [Problems with the curse of dimensionality](#problems-with-the-curse-of-dimensionality)
  - [Bias-variance decomposition](#bias-variance-decomposition)
<!-- /TOC -->

# Local methods in high dimensions
**Curse of dimensionality**. 
* *Observation*. It seems that with a reasonably large set of training data

    $\to$ We could always approximate the theoretically optimal conditional expectation by k-NN averaging
* *Curse of dimensionality*. This approach and our intuition breaks down in high dimensions

## Problems with the curse of dimensionality
**Fitting a unit cube in high-dimensional space**. Consider the nearest-neighbor procedure for inputs uniformly distributed in a $p$-dimensional unit hypercube
* *Scenario*. Suppose we send out a hypercubical neighborhood about a target point to capture a fraction $r$ of the observations
    
    $\to$ This corresponds to a fraction $r$ of the unit volume
    * *Expected edge length*. $e_p(r) = r^{1/p}$
* *Problems*. 
    * As $p$ grows, neighborhoods are no longer "local"
    * Reducing $r$ dramatically does not help, since the fewer observations we average
        
        $\to$ The higher is the variance of our fit

**Fittig a unit sphere in high-dimensional space**. Consider $N$ data points uniformly distributed in a $p$-dimensional unit ball centered at the origin

$\to$ We consider a nearest-neighbor estimate at the origin
* *Median distance from the origin to the closest point*.

    $$d(p,N) = \bigg(1 - \frac{1}{2^{1/N}}\bigg)^{1/p}$$

    * *Explain*. 
        * *Assumptions*.
            * $X_1,\dots,X_n$ are distances from the $n$ data points to the origin
            * $V(d)$ be the volume of sphere with radius $d$ in $p$-dimensional space
        * *The probability that $d$ is the minimum distance*.

            $$\begin{aligned}
            P(d\leq \min_{i=1,\dots,n} X_i) &= \prod_{i=1}^n P(X_i\geq d)\\
            &=\bigg(\frac{V(1) - V(d)}{V(1)}\bigg)^n\\
            &=(1 - d^p)^n
            \end{aligned}$$

        * *Median distance from the origin to the closest point*. By solving $P(d\leq \min_{i=1,\dots,n} X_i) = \frac{1}{2}$
* *Problem*. Most data points are closer to the boundary of the sample space than to any other data point

    $\to$ Prediction is much more difficult near the edges of the training sample
    * *Explain*. We must extrapolate from neighboring sample points, rather than interpolate between them

**Sampling density in high-dimensional space**. Proportional to $N^{1/p}$

$\to$ In high dimensions, all feasible training samples sparsely populate the input space

## Bias-variance decomposition
**Bias-variance decomposition**.
* *Assumptions*.
    * $\mathcal{T}$ is the training set
* *Bias-variance decomposition*.

    $$\begin{aligned}
    \text{MSE}(x_0) &= E_\mathcal{T}[(f(x_0) - \hat{y}_0)^2]\\
    &= E_\mathcal{T}[(\hat{y}_0 - E_\mathcal{T}(\hat{y}_0))^2] + [E_\mathcal{T}(\hat{y}_0) - f(x_0)]^2\\
    &= \text{Var}_\mathcal{T}(\hat{y}_0) + \text{Bias}^2(\hat{y}_0)
    \end{aligned}$$

    >**NOTE**. Variance is due to the sampling variance of the training data points

**Nearest neighbor in high dimensions**.
* *Problem of interest*.
    * *True relationship between $X$ and $Y$*. $Y = f(X) = \exp(-8\|X\|^2)$
    * *Estimator*. 1-nearest-neighbor estimator
    * *Point to estimate*. $x_0 = 0$
* *Curse of dimensionality*.
    * When $p$ is small, the nearest neighbor is very close to $x_0$

        $\to$ Both the bias and variance are small
    * As $p$ increases, the nearest neighbor tends to stray further from the target point

        $\to$ Both bias and variance are incurred
    * As $p$ increases further, the estimate tends to be $0$ more often, i.e.
        * The MSE levels off at $1.0$, so does the bias
        * The variance starts dropping

**General phenomena of curse of dimensionality**. The complexity of functions of many variables can grow exponentially with the dimension
* *Consequence*. If we wish to be able to estimate such functions with the same accuracy as function in low dimensions
    
    $\to$ We need the size of our training set to grow exponentially

**Bias-variance decomposition for linear model**. 
* *Assumptions*.
    * The true relationship between $X$ and $Y$ is 
        
        $$Y=X^T\beta + \epsilon$$ 
        
        where $\epsilon\sim\mathcal{N}(0,\sigma^2)$
    * The regression model is $\hat{y}_0 = x_0^T\hat{\beta}$

      $\to$ Under this model, the least-square estimate is unbiased
* *Regression model*. We can represent the regression model as regressing the noise term $\epsilon$, i.e.

    $$\hat{y}_0 = x_0^T \beta + [\mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1} x_0]^T \epsilon$$

* *Bias-variance decomposition*.

    $$\begin{aligned}
    \text{EPE}(x_0) &= E_{y_0|x_0} E_\mathcal{T}[(y_0 - \hat{y}_0)^2]\\
    &=\text{Var}(y_0|x_0) + \text{Var}_\mathcal{T}(\hat{y}_0) + \text{Bias}^2(\hat{y}_0)\\
    &= \sigma^2 + E_\mathcal{T}[x_0^T (\mathbf{X}^T \mathbf{X})^{-1} x_0 \sigma^2] + 0^2
    \end{aligned}$$

    $\to$ We have incurred an additional variance $\sigma^2$ on the prediction error, since our target is not deterministic

* *Analysis*. Assuming that $N$ is large, $\mathcal{T}$ is selected at random, and $E(X) = 0$, then $\mathbf{X}^T \mathbf{X}\to N \text{Cov}(X)$, hence

    $$\begin{aligned}
    E_{x_0}[\text{EPE}(x_0)]&\sim E_{x_0}[x_0^T \text{Cov}(X)^{-1} x_0 \sigma^2 / N + \sigma^2]\\
    &= \sigma^2 \frac{p}{N} + \sigma^2
    \end{aligned}$$

    * *Consequence*. The expected EPE increases linearly as a function of $p$, with slope $\frac{\sigma^2}{N}$
* *Conclusion*. By imposing some heavy restrictions on the class of models being fitted

    $\to$ We can avoid curse of dimensionality