<!-- TOC titleSize:1 tabSpaces:2 depthFrom:1 depthTo:6 withLinks:1 updateOnSave:1 orderedList:0 skip:0 title:1 charForUnorderedList:* -->
# Table of Contents
- [Table of Contents](#table-of-contents)
- [Classes of restricted estimators](#classes-of-restricted-estimators)
  - [Roughness penalty and Bayesian methods](#roughness-penalty-and-bayesian-methods)
  - [Kernel methods and local regression](#kernel-methods-and-local-regression)
  - [Basis functions and dictionary methods](#basis-functions-and-dictionary-methods)
- [Appendix](#appendix)
  - [Concepts](#concepts)
<!-- /TOC -->

# Classes of restricted estimators
**Classes of restricted estimators**. The variety of nonparametric regression techniques or learning methods fall into a number of different classes, depending on the nature of the restrictions imposed

>**NOTE**. These classes are not distinct, and indeed some methods fall in several classes

* *Smoothing parameters*. Each class has one or more associated parameter, i.e. smoothing parameters

    $\to$ These parameters control the effective size of the local neighborhood

## Roughness penalty and Bayesian methods
**Roughness penalty**. 
* *Roughness penalty*. The class of function is controlled by explicitly penalizing $\text{RSS}(f)$ with a roughness penalty

$$\text{PRSS}(f;\lambda) = \text{RSS}(f) + \lambda J(f)$$

* *Penalty function $J(f)$*. Large for functions $f$, which vary too rapidly over small regions of input space
* *Examples*.
    * *Least-square criterion with roughness penalty on second-order derivatives*.

        $$\text{PRSS}(f,\lambda) = \sum_{i=1}^N [(y_i - f(x_i))]^2 + \lambda \int f''(x)^2 dx$$

    * *Additive penalties*. $J(f) = \sum_{j=1}^p J(f_i)$, i.e. combined with additive functions $f(X) = \sum_{j=1}^p f_j(X_j)$ to create additive models with smooth coordinate functions
    * *Projection pursuit regression*. $f(X) = \sum_{m=1}^M g_m(\alpha_m^T X)$ where 
        * $\alpha_m$ is adaptively chosen
        * $g_m$ can have an associated roughness penalty

**Penalty function (or regularization method)**. 
* *Penalty function*. Express our prior belief that 
    * The type of functions we seek exhibit a certain type of smooth behavior
    * The type of functions we seek can usually be cast in a Bayesian framework
* *Explain*. The penalty $J$ corresponds to a log-prior, and $\text{PRSS}(f;\lambda)$ corresponds to the log-posterior distribution

    $\to$ Minimizing $\text{PRSS}(f;\lambda)$ amounts to finding the posterior mode

## Kernel methods and local regression
**Kernel methods and local regression**. 
* *Idea*. Explicitly provide estimates of the regression function, or conditional expectation, by specifying 
  * The nature of the local neighborhood
  * The nature of the class of regular functions fitted locally
* *Kernel function*. The local neighborhood is specified by a kernel function $K_\lambda(x_0,x)$
    * *Kernel function*. Assign weights to points $x$ in a region around $x_0$, e.g.
        * *Gaussian kernel function*. $K_\lambda(x_0,x) = \frac{1}{\lambda} \exp \bigg(-\frac{\|x-x_0\|^2}{2\lambda}\bigg)$

**Kernel estimate**.
* *Nadaraya-Watson weighted average*. The simplest form of kernel estimate

    $$\hat{f}(x_0)=\frac{\sum_{i=1}^N K_\lambda(x_0,x_i)\cdot y_i}{\sum_{i=1}^N K_\lambda(x_0,x_i)}$$

* *Local regression estimate of $f(x_0)$*. $f_\hat{\theta}(x_0)$, where $\hat{\theta}$ minimizes

    $$\text{RSS}(f_\theta,x_0) = \sum_{i=1}^N K_\lambda(x_0,x_i) [y_i - f_\theta(x_i)]^2$$

    and $f_\theta$ is some parametrized function, e.g. a lower-order polynomial
* *Nearest-neighbor methods*. Can be seen as kernel methods having a more data-dependent metric, i.e.
    * *Assumptions*.
        * $x_{(k)}$ is the training observation ranked $k$th in distance from $x_0$
        * $I(S)$ is the indicator of the set $S$
    * *Metric for k-nearest-neighbor*.

        $$K_k(x,x_0) = I(\|x-x_0\|\leq \|x_{(k)} - x_0\|)$$

## Basis functions and dictionary methods
**Basis functions and dictionary methods**. The model for $f$ is a linear expansion of basis functions

$$f_\theta(x) = \sum_{i=1}^M \theta_m h_m(x)$$

where each $h_m$ is a function of the input $x$

$\to$ "Linear" here refers to the action of $\theta$
* *Examples*. Linear regression, polynomial regression, etc.

**Basis functions**.
* *Polynomial splines of degree $K$*. Can be represented by an appropriate sequence of $M$ spline basis functions, determined by $M-K$ knots
    * *Model parameter*. Can be the total degree of the polynomial, or the number of knots in case of splines
* *Radial basis functions*. Symmetric $p$-dimensional kernels located at particular centroids, i.e.

    $$f_\theta(x) = \sum_{m=1}^M K_\lambda(\mu_m,x)\theta_m$$

    * *Gaussian kernel*. $K_\lambda(\mu,x)=\exp\bigg(-\frac{\|x-\mu\|^2}{2\lambda}\bigg)$
    * *Model parameter*. The centroids $\mu_m$ and scales $\lambda_m$

        $\to$ We want the data to dictate them
* *Problem*. Including model parameters changes the regression problem from a straightforward linear problem to a combinatorially hard nonlinear problem
    * *Practical solution*. Shortcuts such as greedy algorithms or two stage processes are used

**Dictionary methods**. The adaptively chosen basis function methods, where one has available a possibly infinite set or dictionary $\mathcal{D}$ of candidate basis functions, from which to choose

$\to$ Models are built up by employing some kind of search mechanism
* *Single-layer feed-forward neural network with linear output weights*. The model has the form

$$f_\theta(x) = \sum_{i=1}^M \beta_m \sigma(\alpha_m^T x + b_m)$$

where $\sigma(\cdot)$ is the sigmoid activation function

$\to$ This model can be seen as an adaptive basis function model
* *Model parameters*. The directions $\alpha_m$, the bias $b_m$, and the weights $\beta_m$

# Appendix
## Concepts
**Spline**. A special function defined piecewise by polynomial
* *Univariate polynomial case*. Spline is a piecewise polynomial function $S:[a,b]\to\mathbb{R}$
    * *Assumptions*.
        * $[a,b]$ is covered by $k$ ordered, disjoint subintervals

            $$a\leq t_0\leq t_1\leq \dots\leq t_{k-1}\leq t_k=b$$
        * $P_i:[t_i,t_{i+1}]\to\mathbb{R}$ is a polynomial
    * *Spline*. A spline $S$ is defined by

        $$\forall i=0,\dots,k-1,\forall t\in[t_i,t_{i+1}),S(t) = P_i(t)$$
    
    * *Knots*. The $k+1$ points $t_0,\dots,t_k$
        * *Knot vector*. $\mathbf{t}=(t_0,\dots,t_k)$
    * *Degree of spline $S$*. If $P_i$ each have degree at most $n$

        $\to$ $S$ is of degree $\leq n$, or of order $n+1$
* *Motivation*. Before computers were used, numerical calculations were done by hand
    
    $\to$ Polynomials were generally preferred because they were easier to work with
    * *Consequence*. Through the advent of computers splines have gained importance
    * *History*. Splines were first used as a replacement for polynomials in interpolation, then as a tool to construct smooth and flexible shapes in computer graphics