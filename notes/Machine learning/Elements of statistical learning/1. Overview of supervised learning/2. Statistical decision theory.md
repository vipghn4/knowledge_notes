<!-- TOC titleSize:1 tabSpaces:2 depthFrom:1 depthTo:6 withLinks:1 updateOnSave:1 orderedList:0 skip:0 title:1 charForUnorderedList:* -->
# Table of Contents
- [Table of Contents](#table-of-contents)
- [Statistical decision theory](#statistical-decision-theory)
  - [Framework for quantitative output](#framework-for-quantitative-output)
  - [Framework for qualitative output](#framework-for-qualitative-output)
- [Appendix](#appendix)
  - [Concepts](#concepts)
<!-- /TOC -->

# Statistical decision theory
**Taget**. Develop a small amount of theory providing a framework for developing models

## Framework for quantitative output
**Problem formulation**.
* *Assumptions*.
    * $X\in\mathbb{R}^p$ is a real valued random input vector
    * $Y\in\mathbb{R}$ is a real valued random output variable, with joint distribution $P(X,Y)$
* *Problem*. Seek a function $f(X)$ for predicting $Y$, given values of $X$

**Loss function**. The problem above requires a loss function $L(Y,f(X))$ for penalizing errors in prediction
* *Example*. Squared error loss, i.e. $L(Y,f(X)) = (Y - f(X))^2$

    $\to$ We will consider this loss function from now on, in this section
* *Criterion for choosing $f$*. Based on expected prediction error, i.e.

    $$\begin{aligned}
    \text{EPE}(f) &= E[(Y - f(X))^2]\\
    &= \int [y - f(x)]^2 P(dx, dy)
    \end{aligned}$$

    * *Consequence*. By conditioining on $X$, i.e. factoring $P(X,Y)$ as $P(X,Y) = P(Y|X) P(X)$, where $P(Y|X) = \frac{P(Y,X)}{P(X)}$

        $$\text{EPE}(f) = E_X E_{Y|X} ([Y - f(X)]^2 | X)$$

* *Pointwise minimization of EPE*. $f(x) = \arg\min_c E_{Y|X} ([Y - c]^2|X=x)$
    * *Solution*. $f(x) = E(Y|X=x)$, i.e. since expected value minimizes MSE

        $\to$ This is known as the regression function

**Nearest-neighbor methods**. Directly implement the pointwise minimization of EPE, using the training data, i.e.

$$\hat{f}(x) = \text{Ave}(y_i|x_i \in N_k(x))$$
* *Approximations of kNN*. Two approximations to pointwise minimization of EPE is used
    * Expectation is approximated by averaging over sample data
    * Conditioning at a point is relaxed to conditioning on some region close to the target point
* *Convergence to pointwise minimization of EPE*.
    * *Observations*.
        * For large training sample size $N$, the points in $N_k(x)$ are likely to be close to $x$
        * As $k$ gets large, the average will get more stable
    * *Consequence*. Under mild regularity conditions on $P(X,Y)$, as $N,k\to\infty$ so that $k\N\to 0$

        $$\hat{f}(x)\to E(Y|X=x)$$
* *Problems with kNN*. 
    * We often do not have very large samples, i.e. if the linear or some more structured model is appropriate

        $\to$ We can usually get a more stable estimate than kNN, although such knowledge has to be learned from the data as well
    * As the dimension $p$ gets large, so does the metric size of the k-NN

        $\to$ Settling for nearest neighborhood as a surrogate for conditioning will fail us miserably
        * *Consequence*. The convergence still holds, but the convergence rate decreases as $p$ increases

**Linear regression**.Consider the case where $f(x)$ is approximately linear in its arguments, i.e. $f(x)\approx x^T\beta$

$\to$ This is a model-based approach
* *Optimal $\beta$*. $\beta = [E(X X^T)]^{-1} E(XY)$

    $\to$ We have not conditioned on $X$, we use our knowledge of the functional relationship to pool over values of $X$
    * *Explain*. We do not condition on $x$'s neighbors, instead, we use our knowledge that $Y$ is approximately a linear function of $X$
* *Least-square methods*. The least-square solution amounts to replace the expectation in optimal $\beta$ by averages over the training data

**Comparison between k-NN and least-squares**. Both k-NN and least squares end up approximating conditional expectations by average, but they differ dramatically in terms of model assumptions
* *Least squares assumption*. $f(x)$ is well approximated by a globally linear function
* *k-NN assumption*. $f(x)$ is well approximated by a locally constant function

    $\to$ This seems more palatable, we may have to pay a price for this flexibility

**Modern techniques** Many of the more modern techniques described in this book are model-based

$\to$ Although far more flexible than the rigid linear model
* *Additive models*. Additive models assume that

    $$f(X)=\sum_{k=1}^p f_j(X_j)$$

    $\to$ This retains the additivity of the linear model, but each coordinate function $f_j$ is arbitrary
    * *Optimal estimate for additive model*. Use techniques like k-NN to approximate univariate conditional expectations simultaneously for each $f_j$
    * *Consequence*. By imposing some, often unrealistic, model assumptions, e.g. additivity in this case 
        
        $\to$ The problem of estimating a conditional expectation in high-dimensions are swept away
* *$L_1$ loss function for regression*. Consider the $L_1$ loss function $E|Y-f(X)|$

    $\to$ The solution in this case is the conditional median, i.e.

    $$\hat{f}(x) = \text{median}(Y|X=x)$$

    which is more robust to noise than those for the conditional mean
    * *Drawback*. $L_1$ criteria have discontinuities in their derivatives

        $\to$ This hinders their widespread use
    * *Consequence*. Squared loss is analytically convenient and hence the most popular

## Framework for qualitative output
**Loss functional for penalizing categorial output variable $G$**.
* *Estimate for $G$*. An estimate $\hat{G}$ will assume values in $\mathcal{G}$, i.e. the set of possible classes
* *Loss function*. 
    * *Characteristics*. Can be represented by a $K\times K$ matrix $\mathbf{L}$ where $K=\text{card}(\mathcal{G})$
        * $\mathbf{L}$ is zero on the diagonal and nonnegative elsewhere
        * $L(k,l)$ is the price paid for classifying an observation belonging to class $\mathcal{G}_k$ as $\mathcal{G}_l$
* *Zero-one loss*. All misclassifications are charged as a single unit
    * *Expected prediction error*.

        $$\text{EPE} = E[L(G,\hat{G}(X))]$$

        where the expectation is taken w.r.t $P(G,X)$
    * *Consequence*. By conditioning, we can write

        $$\text{EPE} = E_X \sum_{k=1}^K L[\mathcal{G}_k,\hat{G}(X)] P(\mathcal{G}_k|X)$$
* *Pointwise minimization of EPE*.

    $$\hat{G}(x) = \arg\min_{g\in\mathcal{G}} \sum_{k=1}^K L(\mathcal{G}_k,g) P(\mathcal{G}_k|X=x)$$

    * *Pointwise minimization of EPE for 0-1 loss*. The solution of pointwise minimization of EPE for 0-1 loss is the Bayes classifier

        $$\hat{G}(x) = \arg\min_{g\in\mathcal{G}} [1 - P(g|X=x)]$$

    * *Bayes rate*. The error rate of Bayes classifier

**Nearest-neighbor methods**. Directly approximate Bayes classifier, i.e. a majority vote in a nearest neighborhood amounts to exactly this
* *Assumptions of k-NN*. 
    * Conditional probability at a point is relaxed to conditional probability within a neighborhood of a point
    * Probabilities are estimated by training-sample proportions

**Dummy-variable approach**. Assume $G$ is coded via a variable $Y$, followed by squared error loss estimation
* *Optimal estimator*. $\hat{f}(X) = E(Y|X)$ where $E(Y_k|X) = P(G=\mathcal{G}_k|X)$
    * *Consequence*. The dummary-variable regression procedure is another way of representing the Bayes classifier
* *Drawback*. Although this theory is exact, empirical problems can occur, depending on the regression model used
    * *Example*. If linear regression is used, $\hat{f}(X)$ need not be positive

        $\to$ We may be suspicious about it as an estimate of a probability

# Appendix
## Concepts
**Model-based approach**. We specify a model for the prediction function

**Conditioning in probability**. Beliefs depend on the available information, i.e. this idea is formalized in probability theory by conditioning