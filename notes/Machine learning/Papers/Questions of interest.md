**Questions of interest**.
* Why using MLP and $\tanh$ activation for NLP tasks
* Intuition of relative position representations
* Why scaled dot product attention use $\sqrt{d_k}$
* Intuition of multihead attention
* Why FFN is used in Transformer
* Why embedding weights in Transformer are scaled by $\sqrt{d_k}$
* Why addition is used to fuse embedding vectors in BERT-like models
* Intuition of spatial-aware attention score in LayoutLMv2