---
title: 7. Acceptance testing
tags: Software engineering
---

<!-- TOC titleSize:1 tabSpaces:2 depthFrom:1 depthTo:6 withLinks:1 updateOnSave:1 orderedList:0 skip:0 title:1 charForUnorderedList:* -->
# Table of Contents
* [7. Acceptance testing](#7-acceptance-testing)
  * [Communicating requirements](#communicating-requirements)
    * [Premature precision](#premature-precision)
    * [The uncertainty principle](#the-uncertainty-principle)
    * [Estimation anxiety](#estimation-anxiety)
    * [Late ambiguity](#late-ambiguity)
  * [Acceptance tests](#acceptance-tests)
    * [The definition of "done"](#the-definition-of-done)
    * [Communication](#communication)
    * [Automation](#automation)
    * [Extra work](#extra-work)
    * [Who writes acceptance tests, and when?](#who-writes-acceptance-tests-and-when)
    * [The developer's role](#the-developers-role)
    * [Test negotiation and passive aggression](#test-negotiation-and-passive-aggression)
    * [Acceptance tests and unit tests](#acceptance-tests-and-unit-tests)
    * [GUIs and other complications](#guis-and-other-complications)
    * [Testing through the right interface](#testing-through-the-right-interface)
    * [Continuous integration](#continuous-integration)
    * [Stop the presses](#stop-the-presses)
* [Appendix](#appendix)
  * [Tricks and advices](#tricks-and-advices)
  * [Discussions](#discussions)
<!-- /TOC -->

# 7. Acceptance testing
## Communicating requirements
**Principle**. One of the most common communication issues between programmers and business is the requirements
* *Work flow of collecting requirements*.
    * Business people state what they believe they need
    * The programmers then build what they believe the business described
* *Problem*. Communication of requirements is extremely difficult

    $\to$ The process is fraught with error

### Premature precision
**Principle**. Both business and programmers are tempted to fall into the trap of premature precision

**Premature precision**. Both business and programmers want a precision which simply cannot be achieved, and often willing to waste a fortune trying to attain it
* Business people want to know exactly what they are going to get before they authorize a project
* Developers want to know exactly what they are supposed to deliver before they estimate the project

### The uncertainty principle
**The uncertainty principle**. Things appear different on paper than they do in a working system
* *Explain*. When the business actually sees what they specified running in a system, they realize that it was not what they wanted at all

    $\to$ They then have a better idea of what they really want

>**NOTE**. The more precise we make our requirements, the less relevant they become as the system is implemented
>* *Explain*. When we demonstrate a feature to the business, it gives them more information than they had before
    >$\to$ The new information impacts on how they see the whole system

### Estimation anxiety
**Principle**. Programmers know they must estimate the system and often think that this requires precision, but it does not
* *Explain*.
    * Even with perfect information, our estimates will have a huge variance
    * The uncertainty principle makes hash out of early precision

        $\to$ The requirements will change, making the precision moot

**Professional behavior**.
* *Good understanding*.
    * Understand that estimates can, and should, be made based on low precision requirements
    * Recognize that the estimates are estimates
* *Consequence*. We should include error bars with our estimates so that the business understands the uncertainty

### Late ambiguity
**Principle**. The solution to premature precision is to defer precision as along as possible
* *Explain*. Professional developers do not flesh out a requirement until they are about to develop it
* *Consequence*. The professional behavior can lead to late ambiguity

**Professional behavior**. Make sure that all ambitguity is removed from the requirements

## Acceptance tests
**Acceptance tests**. Tests written by a collaboration of the stakeholders and the programmers to define when a requirement is done
* *Examples*.
    * *Test 1*.

        ```
        given the command LogFileDirectoryStartupCommand
        given that the old_inactivate_logs directory does not exist
        when the command is executed
        then the old_inactive_logs directory should exist
        and it should be empty
        ```

    * *Test 2*.

        ```
        given the command LogFileDirectoryStartupCommand
        given that the old_inactive_logs directory exists
        and that it contains a file named x
        When the command is executed
        Then the old_inactive_logs directory should still exist
        and it should still contain a file named x
        ```

### The definition of "done"
**Principle**. Done means done, i.e. all code written, all tests pass, QA and the stakeholders are accepted
* *Tests*. Help us get this level of done-ness and still make quick progress from iteration to iteration
    * *Explain*. We create a set of automated tests that, when they pass, we will meet all of the above crieria
* *Acceptance tests*. When the acceptance tests for our feature pass, we are done

**Professional behavior**.
1. Drive the definition of their requirements all the way to automated acceptance tests
2. Work with stakeholder's and QA to ensure the automated tests are a complete specification of done

### Communication
**Principle**. The purpose of acceptance test is communication, clarity, and precision
* *Explain*. By agreeing to them, the developers, stakeholders, and testers all understand what the plan for the system behavior is

>**NOTE**. Achieving this kind of clarity is the responsibility of all parties

**Professional behavior**. Work with stakeholders and testers to ensure that all parties know what is about to be built

### Automation
**Principle**. Acceptance tests should always be automated

>**NOTE**. There is a place for manual testing elsewhere in the software lifecycle, but acceptance of tests should never be manual due to cost
>$\to$ Professional developers do not let this kind of situation happen

**Professional responsibility**. Ensure that acceptance tests are automated
* *Tools*. There are many open-source and commercial tools which facilitate the automation of acceptance tests

### Extra work
**Principle**. Writing acceptance tests is not really extra work at all
* *Explain*. Writing acceptance tests is simply the work of specifying the system, at a very details level

    >**NOTE**. Specifying at this level of detail is the only way that
    >* Programmers can know what *done* means
    >* Stakeholders can ensure that the system they are paying for really does what they needed
    >* Acceptance tests are automated successfully

* *Consequence*. Acceptance tests are massive time and money savers

### Who writes acceptance tests, and when?
**Problem**.
* *Ideal world*. The stakeholders and QA would collaborate to write acceptance tests, and developers review them for consistency
* *Real world*. Stakeholders seldom have time to dive into the required level of detail

    $\to$ They delegate the responsibility to BAs, QA, or even developers
    * *Consequence*. Developers must write these tests

**Who writes acceptance tests**.
* Developers write acceptance tests
* BAs typically write the "happy path" versions of acceptance tests, i.e. describe the features which have business value
* QA typically writes the "unhappy path" tests, e.g. boundary conditions, exceptions, and corner cases

>**NOTE**. Developers who write the tests must not be the same as the developers who implement the tested features

**When to write acceptance tests**. Acceptance tests should be written as late as possible, typically a few days before the feature is implemented
* *Explain*. Due to late precision

### The developer's role
**Principle**. Implementation work on a feature begins when the acceptance tests for that feature are ready
* *Developers' role*.
    1. Execute the acceptance tests for the new feature and see how they fail
    2. Work to connect the acceptance test to the system
    3. Start making the test pass by implementing the desired feature

### Test negotiation and passive aggression
**Problem**. Test authors are human and make mistakes. Sometimes, the tests as written do not make a lot of sense once we start implementing them
* *Examples*.
    * The tests may be too complicated
    * The tests may be awkward
    * The tests may contain silly assumptions
    * The tests may be wrong
* *Consequence*. It can be very annoying if we are the developer who has to make the test pass

**Professional behavior**. Negotiate with the test author for a better test

>**NOTE**. Never take the passive-aggressive option, i.e. only do what the test says, without concerning anything

* *Explain*. It is our job to help our team create the best software we can

### Acceptance tests and unit tests
**Principle**. Acceptance tests are not unit tests
* *Unit tests*. Written by programmers for programmers
    * *Description*. A formal design documents describing the lowest level structure and behavior of the code
    * *Audience*. Programmers
* *Acceptance tests*. Written by the business for the business
    * *Description*. A formal requirements documents specifying how the system should behave from the business' point of view
    * *Audience*. Business and programmers

>**NOTE**. Sometimes, acceptance tests and unit tests may test the same thing, but they do so differently
>* Unit test dig into the guts of the system making calls to methods in particular classes
>* Acceptance tests invoke the system much further out, at the API or sometimes even UI level

### GUIs and other complications
**Problem**. It is hard to specify the GUIs beforehand. It can be done, but it is seldom done well
* *Explain*. People want to fiddle with GUIs, e.g. try different fonts, colors, page-layouts, etc.
* *Consequence*. Writing acceptance tests for GUIs is hard
* *Solution*. Design the system so that we can treat the GUI as if it were an API, rather than a set of buttons, sliders, etc.

**Single responsibility principle (SRP)**.
* *Principle*.
    * We should separate those things which change for different reasons
    * We should group up those things which change for the same reasons
* *SRP and GUIs*. The layout, format, and workflow of the GUI will change for many reasons, but the underlying capability of the GUI will remain the same
* *Consequence*. When writing acceptance tests for a GUI, we test the underlying abstractions which do not change very frequently

**Example**. There can be several buttons on a page
* *Bad practice*. Create tests which click on the buttons based on their positions on the page
* *Good practice*. Click on the buttons based on their names, or unique ID (if available)

### Testing through the right interface
**Principle**. It is better to write tests which invoke the features of the system through a real API, rather than through the GUI

>**NOTE**. This API should be the same API used by the GUI

* *Explain*. We should separate GUIs from our business rules, i.e. testing through the GUI is always problematic, unless we are testing only the GUI, since
    * The GUI is likely to change, making the tests very fragile

### Continuous integration
**Principle**. Run unit tests and acceptance tests several times per day in a CI system

**CI system**. Everytime someone commits a module, the CI system should
1. Kick off a build
2. Run all the tests in the system
3. Email the results to everyone on the team

### Stop the presses
**Principle**. It is very important to keep the CI tests running at all times. They should never fail
* *Good practice*. If the CI tests fail, the whole team should stop the current tasks and focus on getting the broken tests to pass again
    * *Explain*. If we do not fix the CI tests rightaway, we may forget to fix them later

        $\to$ We may release with bugs

# Appendix
## Tricks and advices
**GUI separation**. Decouple the GUI and the business rules
* *Example*. Keep the GUI tests to a minimum, since they are fragile, i.e. the GUI is volatile

## Discussions
**Unit tests and acceptance tests are not tests**. The primary function of unit tests and acceptance tests is not testing
* *Explain*. Unit tests and acceptance tests are documents first, and tests second
* *Primary purpose of tests*. Formally document the design, structure, and behavior of the syste
