---
title: 1. Overview
tags: Software engineering
---

<!-- TOC titleSize:1 tabSpaces:2 depthFrom:1 depthTo:6 withLinks:1 updateOnSave:1 orderedList:0 skip:0 title:1 charForUnorderedList:* -->
# Table of Contents
* [Introduction](#introduction)
  * [Organization and architecture](#organization-and-architecture)
  * [Structure and function](#structure-and-function)
    * [A hierarchical system](#a-hierarchical-system)
    * [Function](#function)
    * [Structure](#structure)
  * [Expansions](#expansions)
* [Computer evolution and performance](#computer-evolution-and-performance)
  * [The Von Neumann machine](#the-von-neumann-machine)
    * [Overview](#overview)
    * [Memory](#memory)
    * [Control unit](#control-unit)
  * [Generations of computers](#generations-of-computers)
  * [Designing for performance](#designing-for-performance)
    * [Microprocessor speed](#microprocessor-speed)
    * [Performance balance](#performance-balance)
  * [Multicore, MICS, and GPGPUs](#multicore-mics-and-gpgpus)
  * [Embedded systems](#embedded-systems)
  * [Performance assesment](#performance-assesment)
    * [Clock speed and instructions per second](#clock-speed-and-instructions-per-second)
      * [The system clock](#the-system-clock)
      * [Instruction execution rate](#instruction-execution-rate)
    * [Benchmark](#benchmark)
  * [Expansions](#expansions)
<!-- /TOC -->

# Introduction
## Organization and architecture
**Computer architecture**. Attributes of a system visible to a programmer
* *Another interpretation*. Attributes having a direct impact on the logical execution of a program
* *Example*.
    * The instruction set
    * The number of bits to represent various data types
    * I/O mechanisms
    * Techniques for addressing memory

**Computer organization**. The operational units and their interconnections, which realize the architectural specification
* *Examples*.
    * Hardware details transparent to the programmer, e.g. control signals
    * Interfaces between the computer and peripherals
    * The memory technology used

## Structure and function
### A hierarchical system
**A hierarchical system** is a set of interrelated subsystems, each of which, intern, hierarchical in structure until we reach some lowest level of elementary subsystem

**Components and interrelationships**. At each level, the system consists of a set of *components* and their *interrelationships*

**The behavior of each level** depends on a simplified, abstracted characterization of the system at the next lower level

**Structure and function**. At each leve, the designer is concerned with
* *Structure*. The way, in which the components interrelated
* *Function*. The operation of each individual components as part of the structure

### Function

<div style="text-align:center">
    <img src="/media/tvVVQLz.png">
    <figcaption>A Functional view of the Computer</figcaption>
</div>

**Data processing**.

<div style="text-align:center">
    <img src="/media/wF4Qj5l.png">
    <figcaption>Data processing on data in storage</figcaption>
</div>

<div style="text-align:center">
    <img src="/media/vRVstk7.png">
    <figcaption>Data processing on data en route between storage and external environment</figcaption>
</div>

**Data storage**.

<div style="text-align:center">
    <img src="/media/xu9muWw.png">
    <figcaption>Computer as a data storage device</figcaption>
</div>

* *Short-term data storage function* is used to temporarily store pieces of data being worked on, at any given moment
* *Long-term data storge function* is used to store files of data for subsequent retrieval and update

**Data movement**.

<div style="text-align:center">
    <img src="/media/Z8wexBr.png">
    <figcaption>Computer as a data movement device</figcaption>
</div>

* *Input-output (I/O)* is when data are received from or delivered to a device, which is directly connected to the computer
* *Data communications* is when data are moved over longer distances, to or from a remote device

**Control**. This function controls the three functions above
* The control is exercised by ones who provide the computer with instructions
* Within the computer, a control unit manages the computer's resources and organizes the performance of its functional parts, in response to the instructions

### Structure

<div style="text-align:center">
    <img src="/media/kLazDYQ.png">
    <figcaption>The computer</figcaption>
</div>

**Overview**. The computer interacts with its external environment via its linkages to the environment, i.e.
* *Peripheral devices*
* *Communication lines*

<div style="text-align:center">
    <img src="/media/PjvlVLc.png">
    <figcaption>The Computer; Top-level structure</figcaption>
</div>

**Internal structure of the computer**. There are four main structural components
* *Central processing unit (CPU)*. Also called *processor*. Controls the operation of the computer and performs its data processing functions
* *Main memory*. Stores data
* *I/O*. Moves data between the compuiter and its external environment
* *System interconnection*. Provides communication among CPU, main memory, and I/O, e.g. system bus

**Major structural components of a CPU**.
* *Control unit (CU)*. Controls the operation of the CPU and hence the computer
* *Arithmetic and logic unit (ALU)*. Performs the computer's data processing functions
* *Registers*. Provides storage internal to the CPU
* *CPU interconnection*. Provides for communication among the CU, ALU, and registers

## Expansions
**A family of computer models**. All models have the same architecture, but with differences in organization
* *Consequent*. Different models have different price and performance characteristics

**Data processing on the fly**. Data come in and get processed, and the result go out immediately

**Computer's operating environment** consists of devices serving as sources and destinations

**A peripheral** is a device directly connected to the computer

**A system bus** consists of a number of conducting wires, to whcih all the components attach

# Computer evolution and performance
## The Von Neumann machine
### Overview
**Stored-program concept**.
* A program can be represented in a form suitable for storing in memory along with the data
* A computer can get the program's instructions by reading them from memory
* A program can be set or altered byu setting the values of portion of memory

<div style="text-align:center">
    <img src="/media/IfSnZRI.png">
    <figcaption>Structure of the IAS computer</figcaption>
</div>

**IAS computer - by von Neumann**. IAS computer consists of
* *A main memory*. Stores both data and instructions
* *An arithmetic and logic unit (ALU)*. Capable of operating on binary data
* *A control unit*. Interprets the instructions in memory and causes them to be executed
* *Input/output (I/O)*. Equipment operated by the control unit

>**NOTE**. Unless otherwise noted, *instruction* refers to a machine instruction, which is directly interpreted and executed by the processor

>**NOTE**. IAS computer is the prototype of all subsequent general-purpose computers

**Motivation**.
* *Motivation for ALU*. A computer has to perform elementary arithmetic operations most frequently, i.e. $+,-,\cdot,/$

    $\to$ It should contain specialized organs for these operations
* *Motivation for CU*. If the device is to be all-purpose, then a distinction must be made between the instructions and the control organs
    * The former must be stored in some way
    * The latter are represented by operating parts of the device, i.e. the CU
* *Motivation for the memory*. Any device carrying out long and complicated sequences of operations must have a considerable memory
* *Motivation for the Input*. The device must have organs to transfer information from its outside recording medium to its specific parts CU and memory. These organs form its input
* *Motivation for the output*. The device must have organs to trasnfer from its specific parts CU and memory to its outside recording medium. These organs form its output

### Memory
**Memory of the IAS** consists of $1000$ storage locations, called *words*, of $40$ bits each

**Data and instruction representations**.
* *Data representation*. Each number is represented by a word, i.e. a sign bit and a 39-bit value
* *Instruction representation*. A word may contain two 20-bit instructions, each of which consists of
    * An 8-bit operation code (opcode), i.e. the operation to be performed
    * A 12-bit address, i.e. designating one of the words in memory

### Control unit

<div style="text-align:center">
    <img src="/media/JV40ZyE.png">
    <figcaption>Expanded structure of the IAS computer</figcaption>
</div>

**The control unit** operates the IAS by fetching instructions from memory and executing them one at a time

**Registers**. Both the control unit and the ALU contain storage locations, called *registers*.
* *Memory buffer register (MBR)*. Contains the input / output of the control unit, i.e.   
    * Contains a word to be stored in memory, or sent to the I/O unit, or
    * Is used to receive a word from memory, or from the I/O unit
* *Memory address register (MAR)*. Specifies the address in memory of the word to be written from or read into the MBR
* *Instruction register (IR)*. Contains the 8-bit opcode instruction being executed
* *Instruction buffer register (IBR)*. Employed to hold temporarily the right-hand instruction, i.e. the second one, from a word in memory
* *Program counter (PC)*. Contains the address of the next instruction pair to be fetched from memory
* *Accumulator (AC) and multiplier quotient (MQ)*. Employed to hold temporarily operands and results of ALU operations

<div style="text-align:center">
    <img src="/media/ZBarW4l.png">
    <figcaption>Partial flowchart of IAS operation</figcaption>
</div>

**Instruction cycle**. The IAS operates by iteratively performing *an instruction cycle*, each of which consists of two subcycles
* *Fetch cycle*.
    * The opcode of the next instruction is loaded into the IR. This instruction can be
        * Taken from the IBR (if available), or
        * Obtained from memory by loading a word into the MBR, then down to the IBR, IR, and MAR
    * The address portion is loaded into MAR
* *Execute cycle*. Once the opcode is in the IR, the this cycle is performed
    * Control circuitry interprets the opcode and executes the instruction by sending appropriate control signals to
        * Cause data to be moved, or
        * Cause an operation to be performed by the ALU

>**NOTE**. Normally, the CU executes instructions in sequence from memory

**Instructions of IAS computer**
* *Data transfer*. Move data between memory and ALU registers, or between two ALU registers
* *Unconditional branch*. Change the instruction sequence by a branch instruction for repetitive operations
* *Conditional branch*. Change the instruction sequence by a branch instruction depending on a condition, i.e. decision points
* *Arithmetic*. Operations performed by the ALU
* *Address modify*. Permits addresses to be computed in the ALU, then inserted into instructions stored in memory, i.e. for addressing flexibility

## Generations of computers

<div style="text-align:center">
    <img src="/media/JuXBGA6.png">
    <figcaption>Vacuum tube, transitor, and integrated circuits</figcaption>
</div>

**First generation of computers** are made of vacuum tubes
* *Vacuum tubes* are used by John Mauchly to build general-purpose computer in 1943
* *Data representation*. The memory consisted of 20 accumulators, each capable of holding a 10-digit decimal number
    * A ring of 10 vacuum tubes represented each digit
    * At any time, only one vacuum tube was in the ON state
* *Drawback*.
    * (Main) The computer has to be programmed manually by setting switches and plugging and unplugging cables
    * Vacuum tube requires wires, metal plates, a glass capsule, and a vacuum

**Second generation of computers** are made of transitors
* *Transitor* is smaller, cheaper, and dissipates less heat than a vacuum tube, but can be used in the same way as a vacuum tube to construct computers
* *Other improvements*.
    * More complex arithmetic and logic units and control units
    * The use of high-level programming languages
    * The provision of system sofware
* *Drawback*. The manufacturing process, from transitor to circuit board, was expensive and cumbersome
    * *Explain*. Discrete components were manufactured separately, packaged in their own container, and soldered or wired together onto circuit boards, which were then installed in computers

        $\to$ Whenever a device called for a transitor, a little tube of metal containing a pinhead-sized piece of silicon had to be soldered to a circuit board

**Third generation of computers** are made of integrated circuits
* *Two fundamental types of components* (of a digital computer) to perform storage, movement, processing, and control functions are
    * *Gate*, i.e. a device implementing a simple Boolean or logical function
    * *Memory cell*, i.e. a device to store one bit of data

>**NOTE**. *Gate* means that they control data flow in much the same way canal gates control the flow of water

* *Formation of a computer*. By interconnecting large numbers of gates and memory cells, we can construct a computer
    * *Consequence*. A computer consists of gates, memory cells, and interconnections among these elements
* *Fundamental components and basic functions*
    * *Data storage*. Provided by memory cells
    * *Data processing*. Provided by gates
    * *Data movement*. The paths among components are used to move data
        * From memory to memory, and
        * From memory through gates to memory
    * *Control*. The path among components can carry control signals

<div style="text-align:center">
    <img src="/media/GgWF0e3.png">
    <figcaption>Relationship among wafer, chip, and gate</figcaption>
</div>

* *Integrated circuit* exploits the fact that transistors, resistors, and conductors can be fabricated from a semiconductor, e.g. silicon
    * *Idea*. We can fabricate an entire circuit in a tiny piece of silicon, rather than assemble discrete components

        $\to$ Many transistors can be produced at the same time on a single wafer of silicon
    * *Key concept to manufacture circuits*.
        1. A thin wafer of silicon is divided into a matrix of small areas
        2. The identical circuit pattern is fabricated in each area, and the wafer is broken up into chips
            * *Chip*. Each chip consists of many gates and/or memory cells plus a number of input and output attachment points
        3. Each chip is packaged in housing protecting it and providing pins for attachment to devices beyond the chip
        4. A number of these packages can be interconnected on a printed circuit board to produce larger and more complex circuits

<div style="text-align:center">
    <img src="/media/iuDrt4L.png">
    <figcaption>Chip manutfacturing process</figcaption>
</div>

**Moore's law**. The number of transistors, which could be put on a single chip was doubling every year
* *Consequences*.
    * The cost of a chip has remained virtually unchanged

        $\to$ The cost of computer logic and memory circuitry has fallen at a dramatic rate
    * Logic and memory elements are placed closer together on a more densely packaged chips

        $\to$ The electrical path length is shortened, increasing operating speed
    * The computer becomes smaller
    * There is a reduction in power and cooling requirements
    * The interconnections on integrated circuit are much more reliable than solder connections
        * *Explain*. More circuitry on each chip implies fewer interchip connections

**Other ideas**
* Includes an *Instruction Backup Register* used to buffer the next instruction

    $\to$ The CU fetches two adjacent words from memory for an instruction fetch
* Uses *data channels*
* Use *multiplexor*, i.e. the central termination point for data channels, CPU, and memory
    * *Explain*. The multiplexor schedules access to memory from the CPU and data channels, allowing these devices to act independently
* Apply *integrated circuit chips* to construct CU, ALU, and memory
* Use *microprocessors*, i.e. all components of a CPU are contained on a single chip

## Designing for performance
### Microprocessor speed
**Problem**. The raw speed of the microprocessor will not achieve its potential unless it is fed a constant stream of work to do, in the form of computer instructions
* *Explain*. Anything getting in the way of the smooth flow undermines the power of the processor

**Idea**. Try to make the processor as busy as possible

**Techinques**
* *Pipelining*. The processor overlaps operations by moving data or instructions into a conceptual pipe with all stages of the pipe processing simultaneously

    $\to$ The processor can simultaneously work on multiple instructions
* *Branch prediction*. The processor looks ahead in the instruction code fetched from memory and predicts which branches, or groups of instructions, are likely to be processed next

    $\to$ If the processor guesses right, it can prefetch the correct instructions and buffer them so that the processor is kept busy
* *Data flow analysis*. The processor analyzes which instructions are dependent on eeach other's results, or data, to create an optimized schedule of instructions
    * *Explain*. Instructions are scheduled to be executed when ready, independent of the original program order

        $\to$ This prevents unnecessary delay
* *Speculative execution*. The processor speculatively execute instructions ahead of their actual order in the program execution, holding the results in temporary lcoations

    $\to$ The processor is kept as busy as possible by executing instructions, which are likely to be needed

### Performance balance
**Problem**. To boost up the performance of a computer, we need to boost up the minimum performance among its components
* *Main source of problem* is the interface between processor and main memory, i.e.
    * The processor speed has grown rapidly
    * The speed with which data can be transferred between main memory and the processor has lagged badly

**Key concept**. Balance the throughput and processing demands of
* The processor components
* Main memory
* I/O devices, and
* The interconnection structures

**Some ideas**.
* Increase the number of bits retrieved at one time
    * *Example*. make DRAMs wider rather than deeper by using wide bus data paths
* Change the DRAM interface to make it more efficient by including a cache, or other buffering scheme on the DRAM chip
* Reduce the frequency of memory access by complex and efficient cache structures between processor and main memory
    * *Example*.
        * Use one or more caches on the processor chip
        * Use one or more off-chip caches close to the processor chip
* Increase the interconnect bandwidth between processor and memory by
    * Using higher-speed buses, and
    * Using a hierarchy of buses to buffer and structure data flow
* Develop sophisticated applications to support the use of peripherals with intensive I/O demands

## Multicore, MICS, and GPGPUs
**Multicore**. The use of multiple processors on the same chip
* *Larger cache*. With more processors, larger shared cache is required

**Many integrated core (MIC)**. Involving a homogeneous collection of general-purpose processors on a single chip

**GPU**. A core designed to perform parallel operations on graphics data
* *Example*. encode and render 2D and 3D graphics, as well as process video
* *Applications*. GPUs are increasingly being used as vector processors
    * *Explain*. GPUs perform parallel operations on multiple sets of data
* *General-purpose computing on GPU (GPGPU)*. When a broad range of applications are supported by a GPU

## Embedded systems
**Embedded system**. A combination of computer hardware and software, and maybe additional mechanical or other parts, designed to perform a dedicated function

**Realtime constraint**. Often, embedded systems are tightly coupled to their environment

$\to$ Realtime constraints are imposed by the need to interact with the environment

<div style="text-align:center">
    <img src="/media/iHfMq43.png">
    <figcaption>Possible organization of an embedded system</figcaption>
</div>

## Performance assesment
### Clock speed and instructions per second
#### The system clock
**The system clock**. Operations performed by a processor are governed by a *system clock*
* *Clock pulse*. All operations typically begin with the pulse of the clock
* *Speed measurement*. The speed of a processor is dictated by the pulse frequency produced by the clock
    * *Unit of measurement*. Cycles per second, or Hertz (Hz)

**Clock rate**. Also called *clock speed*. The rate of pulses
* *A clock cycle*. Also called *a clock tick*. One increment, or pulse, of the clock
* *Cycle time*. The time between pulses

**Clock cycles of an instruction**. The execution of an instruction involves multiple discrete steps

$\to$ Most instructions on most processors require multiple clock cycles to complete
* *Consequence*. A straight comparison of clock speeds on different processors does not tell the performance
    * *Explain*.
        * Different instructions take different number of clock cycles
        * When pipelining is used, multiple instructions are being executed simultaneously

#### Instruction execution rate
**Clock frequency**. A processor is driven by a clock with *a constant frequency* $f$ or, equivalently, a constant cycle time
$$\tau=\frac{1}{f}$$

**Average cycles per instruction (CIP)**. Let $I_c$ be the number of machine instructions executed for a program until it completes, or for some defined time interval. Then the CPI for a program is defined as
$$\text{CPI}=\frac{\sum_{i=1}^n \text{CPI}_i \cdot I_i}{I_c}$$
where $\text{CPI}_i$ is the number of cycles for instructions of type $i$, and $I_i$ be the number of executed instruction of type $i$, for a given program

**The processor time** $T$ needed to execute a given program is given as
$$T=I_c\cdot \text{CPI} \cdot \tau$$
However, execution involves data processing (processor) and data I/O from memory. Thus
$$T=I_c\cdot[p+(m\cdot k)] \cdot \tau$$
where $p$ is the number of processor cycles to decode and execute the instruction, $m$ is the number of memory references, and $k$ is the ratio between memory cycle time and processor cycle time

**Aspects deciding the performance**. The performance factors $I_c, p, m, k, \tau$ are affected by four system attributes
* *Instruction set architecture*. The design of the instruction set
* *Compiler technology*, i.e.  how effective the compiler is, in producing an efficient machine language program from a high-level language program
* *Processor implementation*
* *Cache and memory hierarchy*

<div style="text-align:center">
    <img src="/media/0M70EfR.png">
    <figcaption>Performance factors and system attributes</figcaption>
</div>

**Millions of instructions per second (MIPS rate)** is a common measure of performance for a processor
$$\text{MIPS rate}=\frac{I_c}{T\cdot 10^6}=\frac{f}{\text{CPI}\cdot 10^6}$$

**Millions of floating-point operations per second (MFLOPS)** is common in many scientific and game applications
$$\text{MFLOPS rate}=\frac{\text{# executed floating-point operations in a program}}{\text{Execution time}\cdot 10^6}$$

**Amdahl's law** deals with potential speedup of a program using multiple processors, compared to a single one
$$\begin{aligned}\text{Speedup}&=\frac{\text{Time to execute program on a single processor}}{\text{Time to execute program on }N\text{ parallel processors}}\\
&=\frac{T(1-f)+Tf}{T(1-f)+\frac{T}{N}f}\\
&=\frac{1}{(1-f)+\frac{f}{N}}
\end{aligned}$$
where $f$ is the fraction of execution time involving code, which is infinitely parallelizable with no scheduling overload. $(1-f)$ is the fraction of execution time involving code, which is inherently serial. $N$ is the number of processors, and $T$ is the total execution time of the program using one processor.
* *Consequence*
    * When $f$ is is small, the use of parallel processors has little effect
    * As $N\to\infty$, speedup is bound by $\frac{1}{1-f}$
* *Generalization*.
$$\text{Speedup}=\frac{\text{Execution time before enhancement}}{\text{Execution time after enhancement}}$$

>**NOTE**. Amdahl's law illustrates the problems in the development of multicore machines with an ever-growing number of cores, i.e. the software running on such machines must be adapted to a highly parallel execution environment to exploit the power of parallel processing

### Benchmark
**Desirable characteristics of a benchmark program**.
* It is written in a high-level language, i.e. portable across different machines
* It is representative of particular kind of programming style, e.g. system programming, numerical programming, etc.
* It can be measured easily
* It has wide distribution

## Expansions
**The ALU, CU, and memory** correpsond to associative neurons in the human nervous system. It remains to discuss the sensory, or afferent, and the motor, or efferent, neurons, i.e. the input and output organs of the device

**System sofware** provides the ability to load programs, move data to peripherals, and libraries to perform common computations

**A data channel** is an independent I/O module with its own processor and instruction set
* *I/O instructions*.
    * The CPU does not execute detailed I/O instructions
    * Instructions are stored in main memory to be executed by a special-purpose processor int the data channel itself
* *I/O instruction execution*.
    1. The CPU initiates an I/O transfer by sending a control signal to the data channel

        $\to$ This asks the data channel to execute a sequence of instructions in memory
    2. The data channel performs its task independently of the CPU
    3. The data channel signals the CPU when the operation is complete
* *Advantage*. This arrangement relives the GPU of a considerable processing burden

**A discrete component** is a single, self-contained transitor

**A family of compatible computers**.
* *Idea*.
    * A customer with modest requirements and a budget to match could start with the relatively inexpensive model
    * If the customer's needs grew, it was possible to upgrade to better machine without sacrificing the investment in already-developed software
* *Characteristics*
    * *Similar or identical instruction set*
        * In many cases, the exact same set of machine instructions is supported on all members of the family

            $\to$ A program executing on a machine will also execute on any other
        * In some cases, the lower end of the family has an instruction set, which is a subset of that of the top end of the family

            $\to$ Programs can move up but not down
    * *Similar or identical operating system*. The same basic operating system is available for all family members
    * *Increasing speed*
    * *Increasing number of I/O ports*
    * *Increasing memory size*
    * *Increasing cost*

**CISC and RISC**.
* *CISC*. Complex instruction set computers
    * *Example*. Intel x86 architecture
* *RISC*. Reduced instruction set computer
    * *Example*. ARM architecture

>**NOTE**. To obtain a reliable comparison of the performance of various computers, it is preferable to run a number of different benchmark programs then average the result
