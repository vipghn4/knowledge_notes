---
title: 4. Cache memory
tags: Computer architecture
---

<!-- TOC titleSize:1 tabSpaces:2 depthFrom:1 depthTo:6 withLinks:1 updateOnSave:1 orderedList:0 skip:0 title:1 charForUnorderedList:* -->
# Table of Contents
* [Cache memory](#cache-memory)
  * [Computer memory system overview](#computer-memory-system-overview)
    * [Characteristics of memory systems](#characteristics-of-memory-systems)
    * [The memory hierarchy](#the-memory-hierarchy)
  * [Cache memory principles](#cache-memory-principles)
  * [Elements of cache design](#elements-of-cache-design)
    * [Cache addresses](#cache-addresses)
    * [Cache size](#cache-size)
    * [Mapping function](#mapping-function)
    * [Replacement algorithm](#replacement-algorithm)
    * [Write policy](#write-policy)
    * [Line size](#line-size)
    * [Number of caches](#number-of-caches)
  * [Pentium 4 cache organization](#pentium-4-cache-organization)
  * [ARM cache](#arm-cache)
* [Appendix](#appendix)
  * [Concepts](#concepts)
  * [Discussions](#discussions)
<!-- /TOC -->

# Cache memory
## Computer memory system overview
### Characteristics of memory systems
**Motivation**. The complex subject of computer memory is more manageable if we classify memory systems according to their key characteristics

**Types of memory systems based on characteristics**.
* *Location*. Refer to whether memory is internal and external to the computer
    * *Internal memory*. Often equated with main memory, but there are other forms of internal memory, e.g. registers, internal CU memory, cache, etc.
    * *External memory*. Peripheral storage devices, e.g. disk or tape, which are accessible to the processor via I/O controllers
* *Capacity*.
    * *Internal memory*. Capacity is typically expressed in terms of bytes, i.e. 1 byte = 8 bits, or words, i.e. 1 word is 8, 16, or 32 bits
    * *External memory*. Capacity is typically expressed in terms of bytes
* *Unit of transfer*.
    * *Internal memory*. The unit of transfer is equal to the number of electrical lines into and out of the memory module

        $\to$ This may be equal to the word length, but often larger, e.g. 64, 128, or 256 bits
    * *Word*. The natural unit of organization of memory
        * *Word size*. Typically equal to the number of bits used to represent an integer and to the instruction length
        * *Exceptions in word size*.
            * CRAY C90 has a 64-bit word length but uses a 46-bit integer representation
            * Intel x86 architecture has a wide variety of instruction lengths, expressed as multiples of bytes, and a word size of 32 bits
    * *Addressable unit*. The relationship between the length (in bits) $A$ of an address, and the number $N$ of addressable units is $2^A=N$
        * In some system, the addressable unit is the word
        * Many system allow addressing at the byte level
    * *Unit of transfer*.
        * *Main memory*. Unit of transfer is the number of bits read out of or written to memory at a time

            >**NOTE**. The unit of transfer need not equal a word or an addressable unit

        * *External memory*. Data are often transferred in much larger units than a word

            $\to$ These are referred as blocks
            * *Case study*. NVIDIA GPUs
* *Method of accessing*.
    * *Sequential access*. Memory is organized into units of data, called records

        $\to$ Access must be made in a specific linear sequence
        * *Addressing information*. Stored addressing information is used to separate records and assist in the retrieval process
        * *Read-write mechanism*. A shared read-write mechanism is used

            $\to$ This muse be moved from its current location to the desired location, passing and rejecting each intermediate record
            * *Consequence*. The time to access an arbitrary record is highly variable
        * *Example*. Tape units
    * *Direct access*. Involve a shared read-write mechanism, but individual blocks or records have a unique address based on physical location

        $\to$ Address is accomplished by direct access to reach a general vicinity plus sequential searching, counting, or waiting to reach the final location
        * *Consequence*. Access time is variable
        * *Example*. Disk units
    * *Random access*. Each addressable location in memory has a unique, physically wired-in addressing mechanism
        * *Access time*. The time to access a given location is independent of the sequence of prior accesses and is constant

            $\to$ Any location can be selected at random and directly addressed and accessed
        * *Example*. Main memory and cache systems
    * *Associative*. A random access type of memory which enables one to make a comparison of desired bit locations within a word for specified match

        $\to$ The comparison will be done for all words simultaneously
        * *Consequence*.
            * A word is retrieved based on a portion of its contents, rather than its address
            * Each location has its own addressing mechanism, and retrieval time is constant independent of location or prior access patterns
        * *Example*. Cache memories (115)
* *Capacity and performance*.
    * *Access time (latency)*.
        * *Random-access memory*. Access time is the time it takes to perform read or write operation
            * *Explain*. The time from the instant that an address is presented to the memory, to the instant that data have been stored or made available for use
        * *Non-random-access memory*. Access time is the time it takes to position the read-write mechanism at desired location
    * *Memory cycle time*. Consist of the access time, plus any additional time required before a second access can commerce

        >**NOTE**. This concept is primarily applied to random-access memory

        * *Example of additional time*. Maybe required for trasients to
            * Die out on signal lines, or

                <div style="text-align:center">
                    <img src="/media/PqbB78r.png">
                    <figcaption>Transient and stable signal</figcaption>
                </div>

                * *Explain*. After the transient response has died out, we see the steady-state response
            * Regenerate data if they are read destructively

        >**NOTE**. Memory cycle is concerned with the system bus, not the processor

    * *Transfer rate*. The rate at each which data can be transferred into or out of a memory unit
        * *Random-access memory*. Transfer rate is $1/\text{cycle\_time}$
        * *Non-random-access memory*. The following relationship holds
            * *Assumptions*.    
                * $T_n$ is the average time to read or write $n$ bits
                * $T_A$ is the average access time
                * $n$ is the number of bits
                * $R$ is the transfer rate, i.e. in bits per second (bps)
            * *Conclusion*. $T_n=T_A+\frac{n}{R}$
* *Physical types of memory*.
    * *Most common types*. Semiconductor of memory, magnetic surface memory, and optical and magneto-optical
* *Physical characteristics of data storage*.
    * *Volatile memory*. Information decays naturally or is lost when electrical power is switched off
        * *Example*. Semiconductor memory
    * *Non-volatile memory*. Information once recorded remains without deterioration untill deliberately changed

        $\to$ No electrical power is required to retain information
        * *Example*. Magnetic-surface memories
    * *Non-erasable memory*. Known as read-only memory (ROM)
* *Organization*. This refers to the physical arrangement of bits to form words

    >**NOTE**. A key design issue for random-access memory

### The memory hierarchy
**Design constraints on a computer's memory**.
* *Key characteristics*. Capacity (how much), access time (how fast), and cos t(how expensive)
    * *How much*. If the capacity is there, applications will likely be developed to use it
    * *How fast*. To achieve greatest performance, the memory must be able to keep up with the processor
    * *How expensive*. For practical system, the cost of memory must be reasonable in relationship to other components
* *Trade-off between characteristics*.
    * *Option 1*. Faster access time, greater cost per bit
    * *Option 2*. Greater capacity, smaller cost per bit
    * *Option 3*. Greater capacity, slower access time
* *Solution*. Use a memory hierarchy, rather than a single memory component

**Typical memory hierarchy**.

<div style="text-align:center">
    <img src="/media/RJq6qyT.png">
    <figcaption>The memory hierarchy</figcaption>
</div>

* *General structure*. As one goes down the hierarchy, the following occur
    * Decreasing cost per bit
    * Increasing capacity
    * Increasing access time
    * Decreasing frequency of access of the memory by the processor

        >**NOTE**. This final point is the key of success

* *Consequence*. Smaller, more expensive, faster memories are supplemented by larger, cheaper, slower memories
* *Satisfactions of conditions*. The use of two levels of memory to reduce average access time works in principle, but only if conditions 1 and 4
    * By employing a variety of techonologies, a spectrum of memory systems exists that satisfies conditions 1, 2, and 3
    * Condition 4 is also generally valid

**Locality of reference principle**. The basis for the validity of condition 4
* *Motivation*. During the execution of a program, memory references by the processor, for both instructions and data, tend to cluster
    * *Explain*.
        * Programs typically contain a number of iterative loops and subroutines

            $\to$ Once a loop or subroutine is entered, there are repeated references to a small set of instructions
        * Operations on tables and arrays involve access to a clustered set of data words

            $\to$ Over a long period of time, the clusters in use change, but over a short period of time, the processor primarily working with fixed clusters of memory references
    * *Consequence*. It is possible to organize data across the hierarchy so that the percentage of accesses to each successively lower level is substantially less than that of the level above
    * *Generalization*. This principle can be applied across more than two levels of memory
* *Implementation*.
    * *The registers internal to the processor*. Fastest, smallest, and most expensive
        * *Number of registers*. A processor typically contains a few dozen of registers, although some machines contain hundreds of registers
    * *Main memory*. The principal internal memory system of the computer
    * *Cache*. Main memory is usually extended with a higher-speed, smaller cache
        * *Visibility*. The cache is not usually visible to the programmer, or to the processor
            * *Explain*. It is a device for staging the movement of data between main memory and processor registers to improve performance
    * *Secondary memory (or auxiliary memory)*. External, non-volatile memory, which are used to store program and data files and are usually visible to the programmer only in terms of files and records
        * *Virtual memory*. Disk is also used to provide an extension to main memory, known as virtual memory
    * *Other forms of memory*. These forms use a semiconductor technology which is slower and less expensive than that of main memory

        >**NOTE**. This memory does not fit into the hierarchy, but is a side branch, i.e. data can be moved between main memory and expanded storage, but not between the expanded storage and external memory

        * *Example*. Optical and magneto-optical disks
    * *Additional levels*. Can be effectively added to the hierarchy in software
        * *Example*. Disk cache

## Cache memory principles
**Purpose of cache memory**. Designed to combine the memory access time of expensive, high speed memory  with the large memory size of less expensive, lower-speed memory

<div style="text-align:center">
    <img src="/media/2LuW7Jj.png">
    <figcaption>Cache and main memory</figcaption>
</div>

**Cache memory**. Contain a copy of portions of main memory
* *Idea*. There is a relatively large and slow main memory, together with a smaller, faster cache memory
* *Mechanism*.
    1. When the processor attempts to read a word of memory

        $\to$ A check is made to determine if the word is in the cache
    2. If the word is in the cache, it is delivered to the processor. Otherwise, a block of memory consisting of some fixed number of words is read into the cache, and then the word is delivered to the processor
* *Consequence*. Due to the locality of reference, when a block of data is fetched into the cache to satisfy a single memory reference

    $\to$ It is likely that there will be future references to that same memory location, or to other words in the block
* *Multi-level caches*. The accessing speed decreases and the capacity increases as the level of the cache grows, i.e. the cache is closer to main memory
    * *Example*. L1 cache is fastest, then L2, then L3

**Structure of a cache / main-memory system**.

<div style="text-align:center">
    <img src="/media/4kQ25BH.png">
    <figcaption>Structure of cache / main memory system</figcaption>
</div>

* *Main memory*. Consist of $2^n$ addressable words, with each word having a unique $n$-bit address
    * *Memory block*. For mapping purposes, the memory is considered to consist of a number of fixed-length blocks of $K$ words each

        $\to$ There are $M=2^n/K$ blocks in main memory
* *Cache*. Consist of $m$ blocks, called *lines*
    * *Cache line structure*. Each line contains
        * $K$ words
        * A tag of a few bits, i.e. usually a portion of the main memory address
        * Control bits (not shown), e.g. a bit to indicate whether the line has been modified since being loaded into the cache
    * *Cache line size*. The length of a line, not including tag and control bits
    * *Number of cache lines*. Considerably less than the number of memory blocks, i.e. $m\ll M$
* *Main-memory-to-cache data transfer*. At any time, some subset of the blocks of memory resides in lines in the cache
    1. If a word in a block of memory is read, it is transferred to one of the cache lines
    2. Since there are more blocks than lines, an individual line cannot be uniquely and permanently dedicated to a particular block

        $\to$ Each line includes a tag identifying which particular block is currently being stored

**Cache read operation**.

<div style="text-align:center">
    <img src="/media/jGYmEyf.png">
    <figcaption>Cache read operation flow chart</figcaption>
</div>

1. The processor generates a read address (RA) of a word to be read
2. If the word is in the cache, it is delivered to the process. Otherwise, the block containing the word is loaded into the cache, and the word is delivered to the processor

    >**NOTE**. Loading the block into memory and delivering the word to processor are done in parallel

**Typical contemporary cache oragnization**.

<div style="text-align:center">
    <img src="/media/o2MBFJy.png">
    <figcaption>Typical cache organization</figcaption>
</div>

* *Mechanism*.
    * When a cache hit occurs, the data and address buffers are disabled and communication is only between processor and cache, without system bus traffic
    * When a cache miss occurs, the desired address is loaded onto the system bus, and the data are returned through the data buffer to both the cache and the processor
* *Other organizations*. The cache is physically interposed between the processor and the main memory for all data, address, and control lines

    $\to$ For a cache miss, the desired word is first read into the cache, and then transferred from cache to processor

## Elements of cache design
### Cache addresses
**Virtual memory**. Almost all non-embedded processors, and many embedded processors, support virtual memory

<div style="text-align:center">
    <img src="/media/mlG8IxH.png">
    <figcaption>Logical and physical caches</figcaption>
</div>

>**NOTE**. MMU stands for hardware memory management unit

**Logical and physical caches**. When virtual addresses are used, the system designer may choose to use  
* *Logical cache (or virtual cache)*. Place the cache between the processor and the MMU
    * *Mechanism*. Stores data using virtual addresses

        $\to$ The processor accesses the cache directly, without going through the MMU
    * *Pros and cons*.
        * *Pros*. Logical cache access speed is faster than for a physical cache
            * *Explain*. The cache can respond before the MMU performs an address translation
        * *Cons*. Most virtual memory systems supply each application with the same virtual memory address space
            * *Explain*. Each application sees a virtual memory that starts at address 0

                $\to$ The same virtual address in two different applications refers to two different physical addresses
            * *Consequences*.
                * The cache memory must be completely flushed with each application context switch, or
                * Extra bits must be added to each cache line to identify which virtual address space this address refers to
* *Physical cache*. Place the cache between the MMU and main memory
    * *Mechanism*. Store data using main memory physical address

### Cache size
**Objective**. We would like the size of the cache to be
* Small enough so that the overall average cost per  bit is close to that of main memory alone
* Large enough so that the overall average access time is close to that of the cache alone

**Motivations for minimizing cache size**.
* The larger the cache, the larger the number of gates involved in addressing the cache

    $\to$ Larger caches tend to be slightly slower than small ones, even when built with the same integrated circuit technology and put in the same place on chip and circuit board
* The available chip and board area also limits the cache size
* Because the performance of the cache is very sensitive to the nature of the workload

    $\to$ It is impossible to arrive at a single optimum cache size

**Cache sizes of some processors**.

<div style="text-align:center">
    <img src="/media/TAHNhYN.png">
    <figcaption>Cache sizes of some processors</figcaption>
</div>

### Mapping function
**Mapping algorithms**.
* *Required algorithms*.
    * Algorithm for mapping main memory blocks into cache lines
    * Algorithm for determining which main memory block currently occupies a cache line
* *Consequences of mapping function*. The choice of mapping functions dictates how the cache is organized
* *Mapping algorithms*. Direct, associative, and set associative

**Direct mapping**. Map each block of main memory into only one possible cache line

<div style="text-align:center">
    <img src="/media/GsibO5K.png">
    <figcaption>Direct mapping</figcaption>
</div>

* *Formal*.
    * *Assumption*.
        * $i$ is the cache line number
        * $j$ is the main memory block number
        * $m$ is the number of lines in the cache
    * *Mapping function*. $i = j \text{ mod } m$
* *Implementation*. Easily implemented using the main memory address

    <div style="text-align:center">
        <img src="/media/0Ulyj34.png">
        <figcaption>Direct-mapping cache organization</figcaption>
    </div>

    * *Memory address structure*. For purposes of cache access, each main memory address can be viewed as consisting of three fields
        * *Least significant $w$ bits*. Identify a unique word or byte within a block of main memory
        * *The remaining $s$ bits*. Specify one of the $2^s$ blocks of main memory
            * *Tag*. $s - r$ most significant bits
            * *Line field*. Remaning $r$ bits, i.e. identify one of the $m=2^r$ lines of the cache
    * *Summary*.
        * *Address length*. $s+w$ bits
        * *Number of addressable units*. $2^{s+w}$ words or bytes
        * *Block size*. Equal to line size and equal $2^w$ words or bytes
        * *Number of blocks in main memory*. $\frac{2^{s+w}}{2^w}=2^s$
        * *Number of lines in cache*. $m=2^r$
        * *Size of cache*. $2^{r+w}$ words or bytes
        * *Size of tag*. $s-r$ bits
* *Pros and cons*.
    * *Pros*. Simple and inexpensive to implement
    * *Cons*. There is a fixed cache location for a given block, thus if a program is to reference words repeatedly from two different blocks which map into the same line

        $\to$ The blocks will be continually swapped in the cache, and the hit artio will be low, i.e. thrashing
* *Victim cache*. An approach to lower the miss penalty of direct mapping
    * *Idea*. Remeber what was discarded in case it is required again

        $\to$ Since the discarded data has been fetched, it can be used again at a small cost

**Associative mapping**. Permit each main memory block to be loaded into any line of the cache

<div style="text-align:center">
    <img src="/media/9MOv6YM.png">
    <figcaption>Associative mapping</figcaption>
</div>

* *Memory address structure*. The cache control logic interprets a memory address simply as a tag and a word field

    <div style="text-align:center">
        <img src="/media/qUaXN70.png">
        <figcaption>Fully associative cache organization</figcaption>
    </div>

    * *Tag*. Uniquely identify a block of main memory

        $\to$ To determine whether a block is in the cache, the cache control logic must simultaneously examine every line's tag for a match
* *Cache line number identification*. No field in the address corresponds to the line number

    $\to$ The number of lines in the cache is not determined by the address format
* *Summary*.
    * *Address length*. $s+w$ bits
    * *Number of addressable units*. $2^{s+w}$ words or bytes
    * *Block size*. Equal to line size and equal $2^w$ words or bytes
    * *Number of blocks in main memory*. $\frac{2^{s+w}}{2^w}=2^s$
    * *Number of lines in cache*. Undetermined
    * *Size of tag*. $s$ bits

**Set-associative mapping**. A compromise exhibiting the strengths of both direct and associative approaches, while reducing their disadvantages

<div style="text-align:center">
    <img src="/media/OTcMJNE.png">
    <figcaption>Set-associative mapping implementations</figcaption>
</div>

* *Idea*. The cache consists of a number of sets, each of which consists of a number of lines
* *$k$-way set-associative mapping*.
    * *Assumption*.
        * $m$ is the number of lines in the cache
        * $\nu$ is the number of sets
        * $k$ is the number of lines in each set
        * $i$ is the cache set number
        * $j$ is the main memory block number
    * *Relationships between quantities*.

        $$m=\nu \times k,\quad i=j\text{ mod }\nu$$

    * *Explain*. Block $B_j$ can be mapped into any of the lines of set $j$
    * *Usage*. For large values of $k$, i.e. large degrees of associativity
* *Implementation*.

    <div style="text-align:center">
        <img src="/media/N2rTBjx.png">
        <figcaption>Set-associative mapping implementations</figcaption>
    </div>

    * *Set-associative mapping as $\nu$ associative caches*. Each word maps into all the cache lines in a specific set

        $\to$ Set-associative cache can be physically implemented as $\nu$ associative caches
    * *Set-associative mapping as $k$ direct mapping caches*. Set-associative cache can be physically implemented as $k$ direct mapping caches
        * *Explain*. Each direct-mapped cache is referred to as a *way*, consisting of $\nu$ lines
            * The first $\nu$ lines of main memory are direct mapped into the $\nu$ lines of each way
        * *Usage*. For small values of $k$, i.e. small degrees of associativity
* *Summary*.
    * *Address length*. $s+w$ bits
    * *Number of addressable units*. $2^{s+w}$ words or bytes
    * *Block size*. Equal to line size and equal to $2^w$ words or bytes
    * *Number of blocks in memory*. $\frac{2^{s+w}}{2^w}=2^s$
    * *Number of lines in set*. $k$
    * *Number of sets*. $\nu=2^d$
    * *Number of lines in cache*. $m=k\nu=k \cdot 2^d$
    * *Size of cache*. $k\times 2^{d+w}$ words or bytes
    * *Size of tag*. $s-d$ bits

**Choosing parameters for set-associative mapped caches**.

<div style="text-align:center">
    <img src="/media/Ul2y32q.png">
    <figcaption>Varying associativity over cache size</figcaption>
</div>

* *Most common parameters*. $\nu=m/2$ and $k=2$, i.e. two lines per set
* *A modest addition improvement for relatively small additional cost*. $\nu=m/4$ and $k=4$

    >**NOTE**. Further increases in the number of lines per set have little effect

### Replacement algorithm
**Problem**. Once the cache has been filled, when a new block is brought into the cache

$\to$ One of the existing blocks must be replaced
* *Direct mapping*. There is only one possible line for any particular block

$\to$ No choice is possible
* *Associative and set-associative mappings*. A replacement technique is required

    >**NOTE**. To achieve high speed, such an algorithm must be implemented in hardware

**Most common algorithms**.
* *Least-recently used (LRU)*. Replace that block in the set, which has been in the cache longest with no reference to it
    * *Implementation*.
        * *Two-way set-associative*. Each line includes a `USE` bit, which is set to 1 when the line is referenced, and the bit of the other line in that set is set to 0

            $\to$ When a block is to be read into the set, the line whose `USE` bit is 0 is used
        * *Fully associative cache*. The cache mechanism maintains a separate list of indexes to all the lines in the cache. When a line is referenced, it moves to the front of the list

            $\to$ For replacement, the link at the back of the list is used
    * *Hit ratio*. Since we are assuming that more recently used memory locations are more likely to be referenced

        $\to$ LRU should give the best hit ratio

    >**NOTE**. Due to its simplicity of implementation, LRU is the most popular replacement algorithm

* *First-in-first-out (FIFO)*. Replace that block in the set, which has been in the cache longest
    * *Implementation*. Use round-robin or circular buffer technique
* *Least frequently used (LFU)*. Replace the block in the set with fewest references
* *Random replacement*. Pick a line at random from among the candidate lines
    * *Performance*. Simulation studies have shown that random replacement provides only slightly inferior performance to an algorithm based on usage

### Write policy
**Situations when a block resident in the cache is to be replace**.
* *Case 1*. If the old block in the cache has not been altered, then it may be overwritten with a new block without first writing out the old block
* *Case 2*. If at least one write operation has been performed on a word in the line to be replaced of the cache

    $\to$ Main memory must be updated by writing the line of cache out to the block of memory before bringing in the new block

**Problems with write policies adn trade-offs**.
* More than one device may have access to main memory, e.g. I/O module and processor

    $\to$ If a word has been altered only in the cache, then the corresponding memory word is invalid
* If the I/O device has altered main memory, then the cache word is invalid
* When multiple processors are attached to the same bus and each processor has its own local cache

    $\to$ If a word is altered in one cache, it could conceivably invalidate a word in other caches

**Write through**. The simplest write policy
* *Idea*. All write operations are made to main memory, as well as to the cache, ensuring that main memory is always valid
* *Pros*. Any other processor-cache module can monitor traffic to main memory to maintain consistency within its own cache
* *Cons*.
    * The policy generates substantial memory traffic and may create a bottleneck
    * For systems with multiple processors having their own cache, this write policy does not synchronize between processors' caches

**Write back**. An alternative technique which minimizes memory writes
* *Idea*. Update are made only in the cache
* *Implementation*. When an update occurs, a dirty bit, or useb bit, associated with the line is set

    $\to$ When a block is replaced, it is written back to main memory if and only if the dirty bit is set
* *Cons*. Portions of main memory are invalid, thus accesses by I/O modules can be allowed only through the cache

    $\to$ This makes for complex circuitry and a potential bottleneck

>**NOTE**. Write-back, with cache coherency mechanisms, is the standard caching methodology for multiple processor architectures today

**Cache coherency**. A system with multiple processors having their own cache must make sure that caches are consistent with each other
* *Possible approaches*.
    * *Bus watching with write through*. Each cache controller monitors the address lines to detect write operations to memory by other bus masters

        $\to$ If another master writes to a location in shared memory, which also resides in the cache memory, the cache controller performs an action to ensure cache coherency

        >**NOTE**. This strategy depends on the use of a write-through policy by all cache controllers

        * *Possible actions*.
            * *Flush of the cache entry*. When a processor writes on a shared cache block, all the shared copies of other caches are updated through bus watching

                $\to$ This method broadcasts a write data to all caches throughout a bus, but it incurs larger bus traffic than write-invalidate protocol
            * *Invalidation of the cache entry*. When a processor writes on a shared cache block, all teh shared copies in the other caches are invalidated through bus watching

                $\to$ This method ensures that only one copy of a datum can be exclusively read and written by a processor, all other copies in other caches are invalidated

        * *Pros and cons*.
            * *Pros*. Faster if there is enough bandwidth, since all transactions are a request / response seen by all processors
            * *Cons*. Limited scalability
    * *Hardware transparency*. Additional hardware is used to ensure that all updates to main memory via cache are reflected in all caches

        $\to$ one processor modifies a word in its cache, this update is written to main memory, and any matching words in other caches are similarly updated

    * *Noncacheable memory*. Only a portion of memory is shared by more than one processor, and this is designated as noncacheable

        $\to$ All accesses to shared memory are cache misses

>**NOTE**. Cache coherency is active research field

### Line size
**Problem**. When a block of data is retrieved and placed in the cache, not only the desired word but also some number of adjacent words are retrieved
* *Larger block sizes*.
    * The hit ratio will at first increase due to the principle of locality
    * More useful data are brought into the cache
* *Too large block sizes*. The hit ratio will begin to decrease, since the probability of using the newly fetched information is less than the probability of reusing the information that has to be replaced

### Number of caches
**Multi-level caches**.
* *On-chip cache*. As logic density has increased, it is possible to have a cache on the same chip as the processor
    * *Advantages over caches via external bus*.
        * Reduce processor's external bus activity and thus speed up execution times and increase overall system performance
        * Due to the short data paths internal to the processor, compared with bus lengths, on-chip cache accesses will complete appreciably faster than would even zero-wait state bus cycles
        * During the access of processor to on-chip caches, the bus is free to support other transfers
* *Needs of external cache*. Most contemporary design include both on-chip and external caches
    * *Explain*. Without external caches, if the processor makes an access request for a memory location not in the L1

        $\to$ The processor must access DRAM or ROM memory across the bus
* *Design problems*.
    * *Processor-off-chip-L2-cache communication*. Many designs do not use the system bus as the path for transfer between L2 cache and the processor, but use a separate data path

        $\to$ This reduces the burden on the system bus
    * *On-ship L2 cache*. With the continued shrinkage of processor components, a number of processors now incorporate the L2 cache on the processor chip, improving performance

        $\to$ There will be an off-chip L3 cache
* *Hit rates on L1 and L2 caches*. The potential savings due to the use of an L2 cache depends on the hit rates in both the L1 and L2 caches
    * Several studies have shown that, in general, the use of L2 cache does not improve performance, but complicates all of the design issues related to caches, i.e. size, replacement algorithm, and write policy

**Unified versus split caches**. When the on-chip cache first appeared, many designs consisted of a single cache used to store references to both data and instructions

$\to$ More recently, it is common to split the cache into instruction cache and data cache, which both exist at the same level
* *Potential pros*.
    * For a given cache size, a unified cache has a higher rate than split caches since it balacnes the load between instruction and data fetches automatically
        * *Explain*. If an execution pattern involves many more instruction fetches than data fetches

            $\to$ The cache will tend to fill up with instructions
    * Only one cache needs to be designed and implemented
* *Split caches*. The trend is toward split caches at the L1, and unified caches for higher levels, particularly for superscalar machines, which emphasize parallel instruction execution and the prefetching of predicted future instruction
    * *Pros*. Split cache eliminates contention for the cache between the instruction fetch/decode unit and the execution unit

        $\to$ This is important in any design which relies on the pipelining of instructions
        * *Explain*. Typically, the processor will fetch instructions ahead of time and fill a buffer, or pipeline, with instructions to be executed
            * If we use a unified instruction-data cache, then when the execution unit performs a memory access to load and store data

                $\to$ The request is submitted to the unified cache
            * If, at the same time, the instruction prefetcher issues a read request to the cache for an instruction

                $\to$ That request will be temporarily blocked so that the cache can service th execution unit first, enabling it to complete the currently executing instruction
        * *Consequence*. This cache contention can degrade performance by interfering with efficient use of the instruction pipeline

## Pentium 4 cache organization
**Evolution of cache organization**. Can be seen clearly in the evolution of Intel microprocessors

<div style="text-align:center">
    <img src="/media/Cbjerkk.png">
    <figcaption>Intel cache evolution</figcaption>
</div>

**Pentium 4 organization**.

<div style="text-align:center">
    <img src="/media/Babn6nv.png">
    <figcaption>Pentium 4 block diagram</figcaption>
</div>

* *Instruction fetch / decode unit*. Fetch program instructions from the L2 cache, and decode these into a series of micro-operations, and stores the results in the L1 instruction cache
* *Out-of-order execution logic*. Schedules execution of the micro-operations subject to data dependencies and resource availability

    $\to$ Micro-operations may be scheduled for execution in a different order than they were fetched from the instruction stream

    >**NOTE**. As tim permits, this unit schedules speculative execution of micro-operations which may be required in the future

* *Execution units*. Execute micro-operations, fetching the required data from L1 data cache, and temporarily storing results in registers
* *Memory subsystem*. Include L2 and L3 caches and the system bus, which is used to access main memory when the L1 and L2 caches have a cache miss and to access the system IO resources

**Cache location**. The Pentium 4 instruction cache sits between the instruction decode logic and the execution core
* *Reasoning behind*.
    * Pentium process decodes, or translates, Pentium machine instructions to simple RISC-like instructions called micro operations

        $\to$ The use of simple, fixed-length micro-operations enables the use of superscalar pipelining and scheduling techniques to enhance performance
    * The Pentium machine instructions are cumbersome to decode, i.e. they have a variable number of bytes and many different options

        $\to$ The performance is enhanced if this decoding is done independently of the scheduling and pipelining logic

**Write policy used**. Write-back policy

>**NOTE**. The Pentium 4 processor can be dynamically configured to support write-through caching

**Cache structures**.
* *L1 cache*. Controlled by two bits in one of the control registers, i.e. CD (cache disable) and (NW) (not write-through) bits
    * *Instructions used to control data cache*.
        * *INVD*. Invalidate (flush) the internal cache memory and signals the external cache to invalidate
        * *WBINVD*. Write back and invalidate internal cache, then write back and invaliadte external cache
* *L2 and L3 caches*. Eight-way set-associative with a line size of 128 bytes

## ARM cache
**Evolution of ARM cache**. Can be seen as the evolution of the overall architecture of the ARM family

<div style="text-align:center">
    <img src="/media/bYOM1IS.png">
    <figcaption>ARM cache features</figcaption>
</div>

* *ARM7 models*. Use a unified L1 cache
* *Subsequent models*. Use a split instruction/data cache

**ARM cache and write buffer organization**.

<div style="text-align:center">
    <img src="/media/yfzj7Ha.png">
    <figcaption>ARM cache and write buffer organization</figcaption>
</div>

**Write buffer**. ARM architectures use a small FIFO write buffer to enhance memory write performance
* *Location*. Between the cache and main memory
* *Content*. Consist of a set of addresses and a set of data words
* *Capacity*. Small compared to the cache, and may hold up to 4 independent addresses
    * *Explain*. Data written to the write buffer are not available for reading back into cache until the data have transferred from the write buffer to main memory
* *Mechanism*.
    1. When the processor performs a write to a bufferable area

        $\to$ The data are placed in the write buffer at processor clock speed, and the processor continues execution

    2. A write occurs when data in the cache are written back to main memory
        * The data to be written are transferred from the cache to the write buffer
        * The write buffer then performs the external write in parallel
        * If the write buffer is full, then the processor is stalled until there is sufficient space in the buffer
        * As non-write operations proceed, the write buffer continues to write to main memory until the buffer is completely empty

# Appendix
## Concepts
**Memory records and blocks**. A block, sometimes called a physical record, is a sequence of bytes or bits

**Transients**. Any sudden change in a signal is regarded as a trasient
* *Transient response*. Every system takes a certain time to get into the steady state

    $\to$ The time before this steady state is the transient time

* *Steady resnpose*. When the response is fixed, i.e. it does not have any fluctuation in it

**Disk cache**. A portion of main memory can be used as a buffer to hold data temporarily which is to be read out to disk (disk cache)
* *Benefits*. Improve performance in two ways
    * Disk writes are clustered, i.e. instead of many small transfers of data, we have a few large transfers of data

        $\to$ This improves disk performance and minimizes processor involvement
    * Some data destined for write-out may be referenced by a program, before the next dump to disk

        $\to$ The data are retrieved rapidly from the software cache, rather than slowly from the disk

**High-performance computing (HPC)**. Deal with supercomputers and their software, especially for scientific applications involving large amounts of data, vector, and matrix computation, and the use of parallel algorithms
* *Cache design for HPC*. Quite different than for other hardware platform
    * Many researchers found that HPC applications perform poorly on computer architectures which employ caches
    * Other researchers have since shown that a cache hierarchy can be useful in improving performance if the application software is tuned to exploit the cache

**Cache line prefetching**. Hardware speculatively prefetches cache lines
* *Idea*. When we are accessing memory addresses, the hardware will look for trends and when it begins to recognize the trend, it will start cache prefetching
* *Prefetching strategies*.
    * *Forward traversals through cache line $n$*. Prefetch line $n+1$
    * *Reverse traversals through cache line $n$*. Prefetch line $n-1$
* *Implications*.
    * *Locality counts*. If we read or write at address $A$, then contents near $A$ already cached, e.g. on the same cache line, or on nearby cache line which was prefetched
    * *Predictable access counts*. Predictable means forward or backward traversals
    * *Linear array traversals very cache-friendly*. Due to execllent locality and predictable traversal pattern

        $\to$ Big-O wins for large $n$, but hardware caching takes early lead
        * *Example*.
            * Linear array search can beat $\log_2 n$ searchers of heap-based BSTs
            * $\log_2 n$ binary search of sorted array can beat $O(1)$ searches of heap-based hash tables

**False sharing in cache coherency**. If two cores $C_1$ and $C_2$ acceses addresses $A_1$ and $A_2 = A_1 + 1$ respectively

$\to$ Independent pieces of memory are accessed, thus concurrent access is safe
* *Problem*. Due to cache coherency mechanism, if $C_1$ writes to $A_1$, then $A_2$ is invalidated for $C_2$
* *Consequences*. When working with multithreading, avoid references to global variables, or variables in main thread
    * *Explain*. Accessing to global variables or local arrays may lead to cache coherency handling or false sharing
* *Experience from experts*.
    * During our Beta1 performance milestone in Parallel Extensions, most of our performance problems cam down to stamping out false sharing in numerous places

## Discussions
**Random access and direct access differences (need confirmation)**.
* *Direct access*. Jump right into the containing block's address and then use sequential access to access the desired location within the block
* *Random access*. Jump right into the desired location in memor

**Intel Core i7-9xx processor cache hierarchy**.

<div style="text-align:center">
    <img src="https://i.imgur.com/zbNPypm.png">
    <figcaption>Core i7-9xx cache hierarchy</figcaption>
</div>

* *L1 cache*. For each core, there is 1 32KB I-cache and 1 32KB D-cache. These caches is shared by 2 HW threads, which competes for what goes into the caches
    * *Latency*. 4 cycles
* *L2 cache*. For each core, there is 1 256KB L2 cache holding both data and instructions. The cache size is 256KB and is shared by 2 HW threads.
    * *Latency*. 11 cycles
* *L3 cache*. All 4 cores shares this 8MB cache, which holds both instruction and data
    * *Latency*. 39 cycles

    >**NOTE**. L3 cache is shared by every running processing, including the OS

* *Main memory latency*. 107 cycles

**Cache optimization**.
* *Overview*.
    * Compact, well-localized code, which runs the same instructions over and over, that fits in cache, so that the loop fits entirely in the cache, is the fastest. Everything else is slow
    * Compact data structures, which is well-localized and fits in the cache, will also be fastest
    * Data structure traversals which touch only cached data are fastest
* *For data*.
    * Where practical, use linear array traversals
    * Use as much of a cache line as possible
    * Be alert for false sharing in multithreading systems
* *For code*.
    * Fit working set in cache, i.e. avoid iteration over heterogeneous sequences with virtual calls
    * Make "fast paths" branch-free sequences
    * Use inline cautiously, i.e. reduce branching and facilitates code-reducing optimization, but code duplication reduces effective cache size
