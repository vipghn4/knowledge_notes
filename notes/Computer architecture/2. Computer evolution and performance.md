---
title: 2. Computer evolution and performance
tags: Computer architecture
---

# Table of Contents
[toc]

# Computer evolution and performance
## A brief history of computers
### The Von Neumann machine
**Stored-program concept**. If a program could be represented in a form suitable for storing in memory alongside the data

$\to$ A computer could get its instructions by reading them from memory, and a program could be set or altered by setting the values of a portion of memory

**General structure of Neumann's hand-designed computer**.
* *Main memory*. Store data and instruction
* *Arithmetic and logic unit (ALU)*. Operate on binary data
* *Control unit*. Interpret the instructions in memory and causes them to be executed
* *Input/output (I/O)*. Operated by the control unit

<div style="text-align:center">
    <img src="/media/BZd8akw.png">
    <figcaption>Neumann's hand-designed computer</figcaption>
</div>

**von Neumann machines**. With rare exceptions, all of today's computers have this same general structure and function

$\to$ These computers are referred as von Neumann machines

**Memory of the first Neumann's machine**. Consist of 1000 storage locations, called *words*, of 40 binary digits (bits) each

$\to$ Both data and instructions are stored here

<div style="text-align:center">
    <img src="/media/Vhwvmht.png">
    <figcaption>Memory format of Neumann's first machine</figcaption>
</div>

* *Number representation*. Represented by a word, in binary form, by a sign bit and a 39-bit value
* *Instruction representation*. A word contain two 20-bit instructions, each of which consists of
    * An 8-bit operation code (opcode), i.e. specify the operation to be performed
    * A 12-bit address designating one of the words in memory

**Operation of instructions by CU**.
* *Idea*. Fetch instruction from memory and executing them one at a time

    <div style="text-align:center">
        <img src="/media/hYB3oSM.png">
        <figcaption>Expanded structure of Neumann's first computer</figcaption>
    </div>

* *Expanded structure of Neumann's first computer*. Both the CU and the ALU contain storage locations, called *registers*
    * *Memory buffer register (MBR)*. Contain a word to be stored in memory, or sent to the I/O unit, or is used to receive a word from memory or from the I/O unit
    * *Memory address register (MAR)*. Specify the address in memory of the word to be written or read into the MBR
    * *Instruction register (IR)*. Contain the 8-bit opcode instruction being executed
    * *Instruction buffer register (IBR)*. Employed to hold temporarily the right-hand instruction from a word in memory
    * *Program counter (PC)*. Contain the address of the next instruction pair to be fetched from memory
    * *Accumulator (AC) and multiplier quotient (MQ)*. Employed to hold temporarily operands and results of ALU operations
        * *Example*. The result of multiplying two 40-bit numbers is an 80-bit number

            $\to$ The most significant 40 bits are stored in the AC, and the least ones in the MQ
* *Instruction cycle*.  Consist of two subcycles, i.e. fetch cycle and execution cycle

    <div style="text-align:center">
        <img src="/media/KZ96Esk.png">
        <figcaption>Instruction cycle</figcaption>
    </div>

    * *Fetch cycle*. See the diagram above
    * *Execution cycle*. 
        1. Control circuitry interprets the opcode
        2. Control circuitry sends out the appropriate control signals to cause data to be moved or an operation to be performed by the ALU

**Operation groups**.

<div style="text-align:center">
    <img src="/media/43E9foV.png">
    <figcaption>Instruction set of Neumann's first computer</figcaption>
</div>

* *Data transfers*. Move data between memory and ALU registers, or between two ALU registers
* *Unconditional branch*. Normally, the control unit executes instructions in sequence from memory
    
    $\to$ This can be changed by a branch instruction, which facilitates repetitive operations
* *Conditional branch*. The branch can be made dependent on a condition, allowing decision points
* *Arithmetic*. Operations performed by the ALU
* *Address modify*. Permit addresses to be computed in the ALU, then inserted into instructions stored in memory

    $\to$ This allows a program considerable addressing flexibility

## Designing for performance
### Microprocessor speed
**Microprocessor speed**. Not achieve its potential unless it is a fed a constant stream of work to do in the form of computer instructions

$\to$ Anything getting in the way of the smooth flow undermines the power of processors

**Techniques of feeding the processor**.
* *Pipelining*. A processor can simultaneously work on multiple instructions
    * *Idea*. Moving data or instructions into a conceptual pipe with all stages of the pipe processing simultaneously

        $\to$ Operations are overlapped
* *Branch prediction*. Look ahead in the instruction code fetched from memory and predict which branches, or groups of instructions, are likely to be processed next
* *Data flow analysis*. Analyze which instructions are dependent on each other's results, or data, to create an optimized schedule of instructions

    $\to$ This prevents unnecessary delay
* *Speculative execution*. Speculatively execute instructions ahead of their actual appearance in the program execution, holding the results in temporary locations

    $\to$ The processor can keep its execution engines as busy as possible, by executing instructions which are likely to be needed

### Performance balance
**Processor and memory performances**.
* *Problem*. While processor power has raced ahead at breakneck speed, other critical components of the computer have not kept up

$\to$ Wee need performance balance
    * *Explain*. We need an adjusting of the organization and architecture to compensate for the mismatch among the capabilities of the various components
    * *Example*. While processor speed has grown rapidly, the speed with which data can be transferred between main memory and the processor has lagged badly

        $\to$ The interface between the processor and main memory is the most crucial pathway in the entire computer
        * *Explain*. This interface is responsible for carrying a constant flow of program instructions and data between memory chips and the processor
        * *Consequence*. If memory or the pathway fails to keep pace with the processor's insistent demands

            $\to$ The processor stalls in a wait state, and valuable processing time is lost
* *Solutions*.
    * *Option 1*. Increase the number of bits that retrieved at one time by making DRAMs wider, rather than deeper, by using wide bus data paths
    * *Option 2*. Change the DRAM interface to make it more efficient by including a cache, or other buffering scheme on the DRAM chip
    * *Option 3*. Reduce the frequency of memory access by incorporating increasely complex and efficient cache structures between the processor and main memory

        $\to$ This includes the incorporation of one or more caches on the processor chip, as well as on an off-chip cache close to the processor chip
    * *Option 4*. Increase the interconnect bandwith between processors and memory by using higher-speed buses, and a hierarchy of buses to buffer and structure data flow

**I/O devices handling**. As computers become faster and more capable, more sophisticated applications are developed to support the use of peripherals with intensive I/O demands

<div style="text-align:center">
    <img src="/media/tDjK588.png">
    <figcaption>Typical I/O device data rates</figcaption>
</div>

* *Problem*. There remains the problem of getting that data moved between processor and peripheral
* *Solutions*. Caching and buffering schemes plus the use of higher-speed interconnection buses and more elaborate structures of buses

**Performance balance**. The key in all this is balance
* *Explain*. Designers constantly strive to balance the throughput and processing demands of the processor components, main memory, I/O devices, and the interconnection structures
* *Evolving factors*. The design must constantly be rethought to cope with two constantly evolving factors
    * The rate, at which performance is changing in the various technology areas, e.g. processor, buses, memory, peripherals, differs greatly from one type of element to another
    * New applications and new peripheral devices constantly change the nature of the demand on the system, in terms of typical instruction profile and the data access patterns

## Multicore, MICs, and GPGPUs
**Multicore**. Place multiple processors on the same chip, with a large shared cache
* *Benefits*. Provide the potential to increase performance without increasing the clock rate
* *Motivations*.
    * Studies indicate that, within a processor, the increase in performance is roughly proportional to the square root of the increase in complexity
    * If the software can support the effective use of multiple processors

        $\to$ Doubling the number of processors almost doubles the performance
    * With two processors, larger caches are justified

        $\to$ This is important since the power consumption of memory logic on a chip is much less than that of processing logic
* *Trending*. As the logic density on chips continues to rise

    $\to$ The trend to both more cores and more cache on a single chip continues
    * *Consequence*. As caches became larger, it made performance sense to create multiple levels of cache on a chip
        * First-level cache is dedicated to an individual processor
        * Higher-level caches are shared by all the processors

**Many-integrated-core (MIC)**. Chip manufacturers are now in the process of making a huge leap forward in the number of cores per chip

$\to$ The leap in performance, as well as the challenges in developing software to exploit such a large number of cores, have led to the introduction of MIC
* *Consequence*. The multicore and MIC strategy involves a homogeneous collection of general-purpose processors on a single chip

**GPUs**. At the same time as multicore and MIC, chip manufacturers are pursuing another design option, i.e. a chip with multiple general-purpose processors plus GPUs and specialized cores for video processing and other tasks
* *GPU*. A core designed to perform parallel operations on graphics data

    >**NOTE**. Traditionally found on a plug-in graphics card, it is used to encode and render 2D and 3D graphics, as well as process video

**GPGPUs**. GPUs are increasingly being used as vector processors for a variety of applications which require repetitive computations

$\to$ This blurs the line between GPU and CPU, and we come up with GPGPUs

## Common architectures
**Intel x86 architecture**. Represent the results of decades of design effort on complex instruction set computers (CISCs)

**ARM**. Used in a wide variety of embedded systems and is one of the most powerful and best-designed RISC-based systems on the market
* *RISC*. Reduced instruction set computer

### The evolution of the Intel x86 architecture
**Highlights of the evolution of the Intel product line**.
* *8080*. The world's first general-purpose microprocessor
    * *Specs*. An 8-bit machine, with an 8-bit data path to memory
* *8086*. 16-bit machine, with wider data path and larger registers
    * *Additional features*.
        * Instruction cache, or queue, which prefetches a few instructions before they are executed
* *80286*. An extension of 8086 which enabled addressing a 16-MB memory, instead of just 1MB
* *80386*. A 32-bit machine, with multitasking support

    $\to$ The machine could run multiple programs at the same time
* *80486*. Introduce the use of much more sophisticated and powerful cache technology and sophisticated instruction pipelining
    * *Additional features*.
        * Built-in math coprocessor
        * Offloading complex math operations from the main CPU
* *Pentium*. Introduced the use of superscalar techniques

    $\to$ This allows multiple instructions to execute in parallel
* *Pentium pro*. Continued the move into superscalar organization, with aggressive use of register renaming, branch prediction, data flow analysis, and speculative execution
* *Pentium II*. Incorporated Intel MMX technology, which is designed specifically to process video, audio, and graphics data efficiently
* *Pentium III*. Incorporated addition floating-point instructions to support 3D graphics software
* *Pentium 4*. Include additional floating-point and other enhancements for multimedia
* *Core*. First Intel x86 microprocessor with a dual core, i.e. two processors on a single chip
* *Core 2*. Extend Core's architecture to 64 bits
* *Core 2 Quad*. Provide four processors on a single chip
* *More recent Core*. Up to 10 processors per chip

**Today's terminologies**.
* *x86 computers*. A binary compatibility also with the 32-bit isntruction set of the 80386
* *x86-64*. Refer to the 64-bit extended architecture of x86 instruction set

### Embedded systems and the ARM
**ARM architecture**. Refer to a processor architecture evolved from RISC design principles and is used in embedded systems

**Embedded systems**. Refer to the use of electronics and software within a product, as opposed to a general-purpose computer
* *Definition of embedded system*. A combination of computer hardware and software, and perhaps addition mechanical or other parts, designed to perform a dedicated function

    >**NOTE**. In many cases, embedded systems are part of a larger system or product

* *Requirements and constraints on embedded systems*. Embedded systems have widely varying requirements and constraints, i.e.
    * Small to large systems, implying very different cost constraints, thus different needs for optimization and reuse
    * Relaxed to very strict requirements and combinations of different quality requirements
    * Short to long life times
    * Different environmental conditions, in terms of radiation, vibrations, and humidity
    * Different application characteristics resulting in static versys dynamic loads, slow to fast speed, computer versus interface intensive tasks, etc.
    * Different models of computation, ranging from discrete-event systems to those involving continuous time dynamics

>**NOTE**. Often, embedded systems are tightly coupled to their environment

<div style="text-align:center">
    <img src="/media/tDjK588.png">
    <figcaption>Typical I/O device data rates</figcaption>
</div>

**ARM evolution**.
* *ARM*. A family of RISC-based microprocessors and microcontrollers designed by ARM Inc. at Cambridge, England
    * *ARM chips*. High-speed processors which are known for their small die size, and low power requirements
    
    >**NOTE**. ARM is probably the most widely used embedded processor architecture, and indeed the most widely used processor architecture of any kind in the world

* *ARM evolution*.

    <div style="text-align:center">
        <img src="/media/9bXML2r.png">
        <figcaption>ARM evolution</figcaption>
    </div>

* *ARM processors quality attributes*.
    * *Embedded real-time systems*. Systems for storage, automotive body and power-train, industrial, and networking applications
    * *Application platforms*. Devices running open OS including Linux, Palm OS, etc. in wireless, consumer entertainment, and digital imaging applications
    * *Secure applications*. Smart cards, SIM cards, and payment terminals 

## Performance assessment
### Clock speed and instruction per second

<div style="text-align:center">
    <img src="/media/zVEvbiJ.png">
    <figcaption>Possible organization of an embedded system</figcaption>
</div>

**System clock**. Operations performed by a processor, e.g. fetching instruction, decoding instruction, etc. are governed by a system clock
* *Explain*. All operations typically begin with the pulse of the block

    $\to$ At the most fundamental level, the speed of a processor is dictated by the pulse frequency produced by the clock
* *Unit of measurement*. Cycles per second, or Herz
* *Clock signals generation*. 
    1. Clock signals are generated by a quartz crystal, which generates a constant signal wage while the power is applied
    2. The wave is converted into digital voltage pulse stream provided in a constant flow to the processor circuitry
        * *Example*. A 1-GHz processor receives 1 billions pluses per second
* *Clock rate and clock cycle*. 
    * *Clock rate (or clock speed)*. The rate of pulses
    * *Clock cycle (or clock tick)*. One increment, or pulse, or the clock
    * *Cycle time*. The time between pulses
* *Clock rate and processor*. Clock rate is not arbitrary, but must be appropriate for the physical layout of the processor
    * *Explain*. 
        * Actions in the processor require signals to be sent from one processor element to another
        * When a signal is placed on a line inside the process
          
            $\to$ It takes some finite amount of time for the voltage level to settle down so that an accurate value (1 or 0) is available 
        * Depending on the physical layout of the processor circuits, some signals may change more rapidly than others

            $\to$ Operations must be synchronized and paced so that the proper electrical signal (voltage) values are available for each operation
* *Execution of instruction*. Involve a number of discrete steps

    $\to$ Most instructions on most processors require multiple clock cycles to complete

    >**NOTE**. When pipelining is used, multiple instructions are being executed simultaneously
    >$\to$ A straight comparison of clock speeds on different processors does not tell the whole story about the performance

**Instruction execution rate**. 
* *Instruction count for a program*. Denoted as $I_c$, is the number of machine instructions executed for a program until it runs into completion, or some defined time interval

    >**NOTE**. This is the number of instruction executions, not the number of instructions in the object code of the program

* *Average cycles per instruction (CPI) for a program*. On any given processor, the number of clock cycles required varies for different types of instructions, e.g. load, store, branch, etc. 
* *Overall cycles per instruction*.
    * *Assumptions*.
        * A processor is driven by a clock with a constant frequency $f$

            $\to$ Its cycle time is $\tau=1/f$
        * $\text{CPI}_i$ is the number of cycles required for instruction type $i$
        * $I_i$ is the number of executed instructions of type $i$ for a given program
    * *Overall cycles per instruction*. $\text{CPI}=\frac{\sum_{i=1}^n (\text{CPI}_i \times I_i)}{I_c}$
    * *Processor time required to execute a given program*. $T=I_c\times \text{CPI} \times \tau$
* *Refined formula of processor time to execute a given program*.
    * *Observations*.
        * During the execution of an instruction

            $\to$ Part of the work is done by processor, and part of the time a word is being transferred to or from memory
        * In the latter case, the time to transfer depends on the memory cycle time

            $\to$ Memory cycle time is usually greater than the processor cycle time
    * *Assumptions*.
        * $p$ is the number of processor cycles required to decode and execute instruction
        * $m$ is the number of memory references required
        * $k$ is the ratio between memory cycle time and processor cycle time
    * *Refined formula*. $T=I_c\times [p + (m\times k)] \times \tau$
* *Aspects influencing processor time to execute a given program*.

    <div style="text-align:center">
        <img src="/media/vFlRe7d.png">
        <figcaption>Performance factors and system attributes</figcaption>
    </div>

    * *Instruction set architecture*. The design of the instruction set
    * *Compiler technology*. How effective the compiler is in producing an efficient machine language program, from a high-level language program
    * *Processor implementation*
    * *Cache and memory hierarchy*

**Millions of instructions per second (MIPS)**. $\text{MIPS_rate}=\frac{I_c}{T\times 10^6}=\frac{f}{\text{CPI} \times 10^6}$

**Millions of floating-point operations per second**. $\text{MFLOPS_rate}=\frac{\text{n_executed_floating_point_operations_in_a_program}}{\text{execution_time}\times 10^6}$

### Benchmark
**Problems**. 
* MIPS and MFLOPS have proven inadequate to evaluating the performance of processors
* Differences in instruction sets

    $\to$ The instruction execution rate is not a valid means of comparing performance of different architectures
* The performance of a given processor on a given program may not be useful in determining how that processor will perform on a very different type of application

**Solution**. Use benchmark programs
* *Idea*. The same set of programs can be run on different machines and the execution times compared

**Amdahl's law**. To evaluate a system with multiple processors
* *Assumptions*.
    * $T$ is the total execution time of the program using a single processor
    * $1-f$ is the fraction of the execution time involving the code that is inherently serial
        
        $\to$ $f$ is the proportion of code that is infinitely parallelizable with no scheduling overhead
    * $N$ is the number of processors, which are fully exploited
* *Conclusion*. $\text{speed_up}=\frac{T}{T(1-f) + Tf/N}$

# Appendix
## Dicussions
**Digital circuit and clock rate**. Most integrated circuits (ICs) of sufficiently complexity use a clock signal to synchronize the different parts of the circuit, cycing at a rate slower than the worst case internal propagation delays
* *Problem* As ICs become more complex, the problem of supplying accurate and synchronized clocks to all the circuits becomes increasingly difficult

**Purpose of a computer processor clock**. To synchronize all components on the motherboard, the system has to run at the speed of the slowest component at any given clock domain
* *Explaination 1*. Complex circuits like CPUs are mostly designed with synchronous logic due to several reasons
    * The design of a clocked circuit can be broken down into smaller blocks, with less overhead to manage the interactions between those blocks
        * *Clocked design*. Complex circuit is broken down into progressively smaller blocks by adding latches 
            
            $\to$ The input and output of each block changes only when triggered by a clock pulse
            * *Slowest path in a circuit*. 
                * Within the lowest-level blocks, we are locking at unclocked, purely combinatorial logic

                    $\to$ This design makes it relatively computationally tractable to figure out which are the slowest paths through the circuits
                * We then limit the whole circuit to run no faster than the slowest path
            * *Consequence*. We can set the clock speed above the longest time any signal needs to propagate through any circuit on the board

                $\to$ This prevents signals from arriving before other other signals are ready, keeping everything safe and synchronized
        * *Asynchronous design*. The amount of time for a given block to transform its input into a valid output is unpredictable

            $\to$ Every block needs a mechanism to signal that its output has become valid
            * *Consequence*. This is more complicated than the simple latch circuit used in clocked logic
    * Clocked circuits are easier to debug, i.e. we can run it in slow motion, and more or less freeze the clock to stop and inspect the states of the individual blocks
* *Explaination 2*. Most CPUs are state machines, stepped from state to state by clocks

    $\to$ Without clock, what is going to make it change its state
    * *CPU execution at the simplest level*. A CPU with a clock fetches the information at address $0$ on the falling edge of the clock pulse

        $\to$ Then it steps to the next address
* *Explaination 3*. Its important for computer processors, that they not be clocked too fast since its internal parts have propagation time
    * *Propagation time*. The amount of time which each element takes to produce to correct answer

        $\to$ We cannot clock it faster than it takes for the slowest element to get the right answer

## Concepts
**Specs to care about a computer**.
* *Number of opcodes*
* *Memory cycle time*. The time to access one word of memory
* *Typical speed*. Operations per second

    >**NOTE**. Some computer consider cycle time instead

* *Memory size*
* *Hardwired floating point*
* *Number of index registers*.
* *I/O overlap (channels)*.
* *Instruction fetch overlap*.

**Instruction backup register**. Used to buffer the next instruction

$\to$ The CU fetches two adjacent words from memory for an instruction fetch
* *Consequence*. Except for the occurrence of a branching instruction (15 - 20%)

    $\to$ The CU has to access memory for an instruction on only half the instruction cycles

**Data channels**. A data channel is an independent I/O module with its own processor and instruction set

<div style="text-align:center">
    <img src="/media/MjfuToE.png">
    <figcaption>Configuration of IBM 7094</figcaption>
</div>

* *Idea*. The CPU does not execute detailed I/O instructions

    $\to$ Such instructions are stored in a main memory to be executed by a special-purpose processor in the data channel itself
* *Instruction execution*.
    1. The CPU initiates an I/O transfer by sending a control signal to the data channel, instructing it to execute a sequence of instructions in memory
    2. The data channel performs its task independently of the CPU
    3. The data channel signals the GPU when the operation is complete
* *Benefits*. The CPU is released from a considerable processing burden

**Multiplexor**. The central termination point for data channels, the CPU, and memory
* *Functionality*. Schedule access to the memory from the CPU and data channels

    $\to$ Devices can act independentl