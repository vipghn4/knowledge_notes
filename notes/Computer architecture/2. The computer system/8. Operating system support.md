<!-- TOC titleSize:1 tabSpaces:2 depthFrom:1 depthTo:6 withLinks:1 updateOnSave:1 orderedList:0 skip:0 title:1 charForUnorderedList:* -->
# Table of Contents
- [Table of Contents](#table-of-contents)
- [Operating system support](#operating-system-support)
  - [Operating system overview](#operating-system-overview)
    - [Operating system objectives and functions](#operating-system-objectives-and-functions)
      - [The operating system as a user-computer interface](#the-operating-system-as-a-user-computer-interface)
      - [The operating system as resource manager](#the-operating-system-as-resource-manager)
    - [Types of operating systems](#types-of-operating-systems)
      - [Early systems](#early-systems)
      - [Simple batch systems](#simple-batch-systems)
      - [Multiprogrammed batch systems](#multiprogrammed-batch-systems)
      - [Time-sharing systems](#time-sharing-systems)
  - [Scheduling](#scheduling)
    - [Long-term scheduling](#long-term-scheduling)
    - [Medium-term scheduling](#medium-term-scheduling)
    - [Short-term scheduling](#short-term-scheduling)
      - [Process states](#process-states)
      - [Scheduling techniques](#scheduling-techniques)
  - [Memory management](#memory-management)
    - [Swapping](#swapping)
    - [Partitioning](#partitioning)
    - [Paging](#paging)
    - [Virtual memory](#virtual-memory)
      - [Demand paging](#demand-paging)
      - [Page table structure](#page-table-structure)
    - [Translation lookaside buffer](#translation-lookaside-buffer)
    - [Segmentation](#segmentation)
  - [Pentium memory mangement](#pentium-memory-mangement)
    - [Address spaces](#address-spaces)
    - [Segmentation](#segmentation-1)
    - [Paging](#paging-1)
  - [ARM memory management](#arm-memory-management)
    - [Memory system organization](#memory-system-organization)
    - [Virtual memory address translation](#virtual-memory-address-translation)
    - [Memory-management formats](#memory-management-formats)
    - [Access control](#access-control)
- [Appendix](#appendix)
  - [Concepts](#concepts)
<!-- /TOC -->

# Operating system support
## Operating system overview
### Operating system objectives and functions
**Operating system**. A program controlling the execution of application programs and acting as an interface between applications and the computer hardware
* *Objectives*. 
    * *Convenience*. An OS makes a computer more convenient to use
    * *Efficiency*. An OS allows the computer system resources to be used in an efficient manner

#### The operating system as a user-computer interface
**Hardware and software as a hierarchical organization** The hardware and software used in providing applications to a user can be viewed in a layered or hierarchical fashion

$\to$ The user of those applications, the end user, generally is not concerned with the computer’s architecture

<div style="text-align:center">
    <img src="https://i.imgur.com/Ddvph9v.png">
    <figcaption>Computer hardware and software structure</figcaption>
</div>

* *Explain*. The end user views a computer system in terms of an application

**Application**. Expressed in a programming language and developed by an application programmer
* *Application development*. Developing an application program as a set of processor instructions completely responsible for controlling the computer hardware is a complex task
    
    $\to$ To ease this task, a set of systems programs is provided
* *Utilities*. 
    * *Definition*. Some of these programs implementing frequently used functions that assist in 
        * Program creation
        * Management of files, and
        * Control of I/O devices
    * *Usage*. 
        * A programmer makes use of these facilities in developing an application
        * The application, while it is running, invokes the utilities to perform certain functions

**Operating system**. The most important system program
* *Idea*. Mask the details of the hardware from the programmer and provide the
programmer with a convenient interface for using the system
    
    $\to$ The OS acts as mediator, making it easier for the programmer and for application programs to access and use the facilities and services

**Provided services**.
* *Program creation*. The OS provides a variety of facilities and services, e.g. editors and debuggers, to assist the programmer in creating programs
    * *Utilitiy programs*. Typically, these services are in the form of utility programs
        
        $\to$ These programs are not actually part of the OS but are accessible through the OS
* *Program execution*. 
    * *Required steps for program execution*. A number of steps need to be performed to execute a program, e.g.
        * Instructions and data must be loaded into main memory
        * I/O devices and files must be initialized
        * Resources must be prepared
    * *OS' role*. Handle all of this for the user
* *Access to I/O devices*. Each I/O device requires its own specific set of instructions or control signals for operation
    
    $\to$ The OS takes care of the details so that the programmer can think in terms of simple reads and writes
* *Controlled access to files*. 
    * *File control*. File control must include an understanding of
        * The nature of the I/O device, e.g. disk drive, tape drive, etc.
        * The file format on the storage medium
    * *OS' role*. 
        * Handle the details
        * In the case of a system with multiple simultaneous users
            
            $\to$ the OS can provide protection mechanisms to control access to the files
* *System access*. In the case of a shared or public system, the OS controls access to the system as a whole and to specific system resources
    * *Requirements on the access function*.
        * Must provide protection of resources and data from unauthorized users
        * Must resolve conflicts for resource contention
* *Error detection and response*. A variety of errors can occur while a computer system is running
    * *Types of error*. 
        * Internal and external hardware errors, e.g. a memory error, or a device failure or malfunction
        * Software errors, e.g. arithmetic overflow, attempt to access forbidden memory location, and inability of the OS to grant the request of an application
    * *OS' role*. Make the response that clears the error condition with the least impact on running applications
        * *Example response*. The repsonse may range from
            * Ending the program that caused the error
            * Retrying the operation
            * Reporting the error to the application
* *Accounting*. A good OS collects usage statistics for various resources and monitor performance parameters, e.g. response time
    * *Purposes*. This information is useful in 
        * Anticipating the need for future enhancements
        * Tuning the system to improve performance
        * Billing purposes on a multiuser system

**Key interfaces in a typical computer system**.
* *Instruction set architecture (ISA)*. Define the repertoire of machine language instructions, which a computer can follow
    
    $\to$ This interface is the boundary between hardware and software
    * *Application program's access to ISA* Application programs and utilities may access the ISA directly

        $\to$ For these programs, a subset of the instruction repertoire is available (user ISA)
    * *Additional machine language instructions accessible from the OS*. The OS has access to additional machine language instructions dealing with managing system resources (system ISA)
* *Application binary interface (ABI)*. Define a standard for binary portability across programs
    * *Explain*. The ABI defines the system call interface to 
        * The OS, and
        * The hardware resources and services available in a system through the user ISA
* *Application programming interface (API)*. Give a program access to the hardware resources and services available in a system through the user ISA supplemented with high-level language (HLL) library calls
    * *System calls*. Any system calls are usually performed through libraries
        
        $\to$ Using an API enables application software to be ported easily, through recompilation, to other systems that support the same API

#### The operating system as resource manager
**Computer as a set of resources**. A computer is a set of resources for the movement, storage, and processing of data and for the control of these functions

$\to$ The OS is responsible for managing these resources
* *Control pf data movement, storage, and processing by the OS*. The OS controls the movement, storage, and processing of data
    * *Explain*. By managing the computer’s resources, the OS is in control of the computer’s basic functions
    * *Control mechanism in common sense*.
        * *Idea*.
            * A control mechanism as something external to that, which is controlled, or
            * A control mechanism is something, which is a distinct and separate part of that, which is controlled
        * *Example*. A residential heating system is controlled by a thermostat
            
            $\to$ The thermostat is completely distinct from the heat-generation and heat-distribution apparatus
    * *Control mechanism in OS*. In OS, a control mechanism is unusual in two respects
        * The OS functions in the same way as ordinary computer software
            
            $\to$ It is a program executed by the processor
        * The OS frequently relinquishes control and must depend on the processor to allow it to regain control

**OS as a computer program**. Like other computer programs, the OS provides instructions for the processor
* *Difference from program*. The key is in the intent of the program
    * The OS directs the processor in 
        * The use of the other system resources, and
        * The timing of its execution of other programs
    * For the processor to do any of these things, it must cease executing the OS program and execute other programs, i.e.
        1. The OS relinquishes control for the processor to do some useful work
        2. The OS resumes control long enough to prepare the processor to do the next piece of work

**OS kernel**. A portion of the OS is in main memory, including the kernel and, at a given time, other portions of the OS currently in use

$\to$ The remainder of main memory contains user programs and data

<div style="text-align:center">
    <img src="https://i.imgur.com/d6N1heo.png">
    <figcaption>The OS as resource manager</figcaption>
</div>

* *OS kernel (or nucleus)*. Contain the most frequently used functions in the OS
* *Main memory allocation*. The allocation of main memory is controlled jointly by the OS and memory-management hardware in the processor, i.e.
* *Processor allocation*. The OS decides when an I/O device can be used by a program in execution, and controls access to and use of files
    
    $\to$ The processor itself is a resource, and the OS must determine how much processor time is to be devoted to the execution of a particular user program
    
    >**NOTE**. In the case of a multiple-processor system, this decision must span all of the processors

### Types of operating systems
**Types of operating systems**. Certain key characteristics serve to differentiate various types of operating systems
* *Characteristics dimensions*. Characteristics fall along two independent dimensions
    * Whether the system is batch or interactive
        * *Interactive system*. 
            * The user/programmer interacts directly with the computer, usually through a keyboard/display terminal, to request the execution of a job or to perform a transaction
            * The user may, depending on the nature of the application, communicate with the computer during the execution of the job
        * *Batch system*. The opposite of interactive, i.e.
            * The user’s program is batched together with programs from other users and submitted by a computer operator
                
                $\to$ After the program is completed, results are printed out for the user
            * *Pure batch systems*. Rare today
                
                >**NOTE**. It will be useful to the description of contemporary operating systems to examine batch systems briefly

    * Whether the system employs multiprogramming or not
        * *Multiprogramming system*. The attempt is made to keep the processor as busy as possible, by having it work on more than one program at a time
            * *Idea*. Several programs are loaded into memory, and the processor switches rapidly among them
        * *Uniprogramming system*. Work only one program at a time

#### Early systems
**Early systems**. With the earliest computers, from the late 1940s to the mid-1950s, the programmer interacted directly with the computer hardware

$\to$ There was no OS
* *Interacting with processors*. These processors were run from a console, consisting of display lights, toggle switches, some form of input device, and a printer
* *Programs in early systems*. Programs in processor code were loaded via the input device, e.g. a card reader
    * *Erroneous execution*. If an error halted the program, the error condition was indicated by the lights
        
        $\to$ The programmer could proceed to examine registers and main memory to determine the cause of the error
    * *Successful execution*. If the program proceeded to a normal completion
        
        $\to$ The output appeared on the printer
* *Drawbacks*.
    * *Scheduling*. Most installations used a sign-up sheet to reserve processor time
        * *Idea*. A user could sign up for a block of time in multiples of a half hour or so
        * *Consequences*. 
            * A user might sign up for an hour and finish in 45 minutes
                
                $\to$ This results in wasted computer idle time
            * The user might run into problems, not finish in the allotted time
                
                $\to$ The program is forced to stop before resolving the problem
    * *Setup time*. 
        * *Program setup procedure*. A single program, called a job, could involve 
            1. Loading the compiler plus the high-level language program, i.e. source program, into memory
            2. Saving the compiled program, i.e. object program
            3. Loading and linking together the object program and common functions
        * *Consequences*. Each of these steps could involve mounting or dismounting tapes, or setting up card decks
            
            $\to$ A considerable amount of time was spent just in setting up the program to run
            * *Explain*. If an error occurred, the hapless user typically had to go back to the beginning of the setup sequence

**Serial processing**. This operation mode of early systems, reflecting the fact that users have access to the computer in series
* *Improvement*. Over time, various system software tools were developed to attempt to make serial processing more efficient
    * *Examples*. Libraries of common functions, linkers, loaders, debuggers, and I/O driver routines that were available as common software for all users

#### Simple batch systems
**Simple batch systems**. To improve resource utilization of early systems, simple batch operating systems were developed
* *Idea*. With a monitor, the user no longer has direct access to the processor, i.e.
    1. The user submits the job on cards or tape to a computer operator
    2. The computer operator batches the jobs together sequentially
    3. The computer operator places the entire batch on an input device, for use by the monitor

**Simple batch system under monitor's perspective**. The monitor controls the sequence of events

$\to$ For this to happen, much of the monitor must always be in main memory and available for execution

<div style="text-align:center">
    <img src="https://i.imgur.com/XTifmpE.png">
    <figcaption>Memory layout for a resident monitor</figcaption>
</div>

* *Resident monitor*. The portion of the monitor in main memory and available for execution
* *The rest of the monitor*. Consist of utilities and common functions, which are loaded as subroutines to the user program at the beginning of any job requiring them
* *Monitor operation*. 
    1. The monitor reads in jobs one at a time from the input device, i.e. typically a card reader or magnetic tape drive
    2. As it is read in, the current job is placed in the user program area, and control is passed to this job
    3. When the job is completed, it returns control to the monitor
    4. The monitor immediately reads in the next job
    5. The results of each job are printed out for delivery to the user


**Simple batch system under processor's perspective**. 
* *Processor operation*. At a certain point in time
    1. The processor is executing instructions from the portion of main memory containing the monitor
        
        $\to$ These instructions cause the next job to be read in to another portion of main memory
    2. Once a job has been read in, the processor will encounter in the monitor a branch instruction
        
        $\to$ This instruction instructs the processor to continue execution at the start of the user program
    3. The processor will then execute the instruction in the user’s program until it encounters an ending or error condition
    
        $\to$ Either event causes the processor to fetch its next instruction from the monitor program
* *Terminology*. 
    * *"Control is passed to a job"*. The processor is now fetching and executing instructions in a user program
    * *"Control is returned to the monitor"*. The processor is now fetching and executing instructions from the monitor program

**Scheduling and job setup**.
* *Scheduling problem* The monitor handles the scheduling problem
    * *Idea*. A batch of jobs is queued, and jobs are executed as rapidly as possible, with no intervening idle time
* *Job setup*. The monitor handles job setup as well
    * *Job control language (JCL)*. With each job, instructions are included in a job control language (JCL)
        
        $\to$ JCL is a special type of programming language used to provide instructions to the monitor
    * *Example*. 
        * *Scenario*. A user submits a program written in FORTRAN plus some data to be used by the program
            * Each FORTRAN instruction and each item of data is on a separate punched card or a separate record on tape
            * The job also includes job control instructions denoted by the beginning `$`
        * *Overall job format*.

            $$\begin{aligned}
            \$ \text{JOB}&\\
            \$ \text{FTN}&\\
            \vdots\qquad &\} \text{FORTRAN instructions}\\
            \$ \text{RUN}&\\
            \vdots\qquad &\} \text{Data}\\
            \$\text{END}
            \end{aligned}$$

        * *Job execution*. To execute this job
            1. The monitor reads the `$FTN` line and loads the appropriate compiler from its mass storage, i.e. usually tape
            2. The compiler translates the user’s program into object code, which is stored in memory or mass storage
                
                >**NOTE**. If it is stored in memory, the operation is referred to as "compile, load, and go"

                >**NOTE**. If it is stored on tape, then the `$LOAD` instruction is required
            
            3. The `$LOAD` instruction is read by the monitor, which regains control after the compile operation
            4. The monitor invokes the loader, which loads the object program into memory in place of the compiler and transfers control to it
    * *Consequence*. A large segment of main memory can be shared among different subsystems, although only one such subsystem could be resident and executing at a time

**Desirable hardware features for a batch OS**. The monitor, or batch OS, is simply a computer program

$\to$ It relies on the ability of the processor to fetch instructions from various portions of main memory to seize and relinquish control alternately
* *Other desirable hardware features*.
    * *Memory protection*. While the user program is executing, it must not alter the memory area containing the monitor
        * *Error handling*. If such an attempt is made
            1. The processor hardware should detect an error and transfer control to the monitor
            2. The monitor would then abort the job, print out an error message, and load the next job
    * *Timer*. A timer is used to prevent a single job from monopolizing the system
        * *Idea*. The timer is set at the beginning of each job
            
            $\to$ If the timer expires, an interrupt occurs, and control returns to the monitor
    * *Privileged instructions*. Certain instructions are designated privileged and can be executed only by the monitor
        * *Handling privileged instructions*. If the processor encounters a privileged instruction while executing a user program
            
            $\to$ An error interrupt occurs
            * *Explain*. The processor hardware considers this event as an error and transfers control to the monitor
        * *I/O instructions*. A class of privileged instructions, which let the monitor retain control of all I/O devices
            * *Purpose*. Prevent a user program from accidentally reading job control instructions from the next job
            * *Performing I/O from a user program*. The user program must request that the monitor perform the operation for it
    * *Interrupts*. Give the OS more flexibility in relinquishing control to and regaining control from user programs
        
        $\to$ Processor time alternates between execution of user programs and execution of the monitor
        * *Trade-off*. Some main memory is given over to the monitor and some processor time is consumed by the monitor
            
            $\to$ Both of these are forms of overhead
            
            >**NOTE**. Even with this overhead, the simple batch system improves utilization of the computer

#### Multiprogrammed batch systems
**Drawback of simple batch system**. Even with the automatic job sequencing provided by a simple batch OS

$\to$ The processor is often idle
* *Explain*. I/O devices are slow compared to the processor
    * *Observations*. The processor spends a certain amount of time executing, until it reaches an I/O instruction
        
        $\to$ It must then wait until that I/O instruction concludes before proceeding
    * *Consequence*. This inefficiency is not necessary
        * *Explain*. Suppose that there is room for the OS and more than one user programs
            
            $\to$ When one job needs to wait for I/O, the processor can switch to some other job, which likely is not waiting for I/O
* *Solution*. Use multiprogramming, or multitasking

**Multiprogramming**. The central theme of modern OSes
* *Desirable computer hardware features*. The hardware must support I/O interrupts and DMA
    * *Explain*. With interrupt-driven I/O or DMA,
        1. The processor can issue an I/O command for one job
        2. The process then proceed with the execution of another job
            
            $\to$ Meanwhile, the I/O is carried out by the device controller
        3. When the I/O operation is complete, the processor is interrupted
            
            $\to$ Control is passed to an interrupt-handling program in the OS
        4. The OS will then pass control to another job
* *Drawback*. Multiprogramming operating systems are fairly sophisticated compared to single-program, or uniprogramming, systems
    * *Explain*. 
        * To have several jobs ready to run, the jobs must be kept in main memory
            
            $\to$ This requires some form of memory management
        * If several jobs are ready to run, the processor must decide which one to run
            
            $\to$ This requires some algorithm for scheduling

#### Time-sharing systems
**Drawback of multiprogrammed batch system**. With the use of multiprogramming, batch processing can be quite efficient

$\to$ However, for many jobs, it is desirable to provide a mode, in which the user interacts directly with the computer
* *Example*. For transaction processing, an interactive mode is essential
* *Recent solutions*. Today, the requirement for an interactive computing facility can be, and often is, met by the use of a dedicated microcomputer
* *Early solutions*. That option was not available in the 1960s, when most computers were big and costly
    
    $\to$ Instead, time sharing was developed

**Time-sharing systems**. Multiprogramming can be used to handle multiple interactive jobs, i.e. time sharing
* *"Time sharing"*. The processor’s time is shared among multiple users
* *Idea*. Multiple users simultaneously access the system through terminals
    
    $\to$ The OS interleaves the execution of each user program in a short burst or quantum of computation
* *Consequence*. If there are $n$ users actively requesting service at one time
    
    $\to$ Each user will only see on the average $1/n$ of the effective computer speed, not counting OS overhead.
* *Drawback*. Given the relatively slow human reaction time, the response time on a properly designed system should be comparable to that on a dedicated computer

## Scheduling
**Scheduling**. The key to multiprogramming
* *Types of scheduling*
    * *Long-term scheduling*. The decision to add to the pool of processes to be executed
        * *Explain*. Select processes from the queue, and loads them into memory for execution
    * *Medium-term scheduling*. The decision to add to the number of processes, which are partially or fully in main memory
        * *Explain*. Swapping in and out interrupted processes between main memory and secondary storage
    * *Short-term scheduling (CPU scheduling)*. The decision as to which available process will be executed by the processor
        * *Explain*. Select from a group of processes, which are ready to execute, and allocate CPU to one of them
    * *I/O scheduling*. The decision as to which process' pending I/O request shall be handled by an available I/O device

**Process**. This term was first used by the designers of the Multics OS in the 1960s

$\to$ This is a somewhat more general term than job
* *Proposed definitions of process*.
    * *Definition 1*. A program in execution
    * *Definition 2*. The “animated spirit” of a program
    * *Definition 3*. That entity to which a processor is assigned

### Long-term scheduling
**Long-term scheduler**. Determine which programs are admitted to the system for processing

$\to$ Long-term scheduler controls the degree of multiprogramming, i.e. number of processes in memory
* *Idea*. Once admitted, a job or user program becomes a process and is added to the queue for the short-term scheduler
* *Variations*. In some systems, a newly created process begins in a swapped-out condition, i.e. it is added to a queue for the medium-term scheduler
    * *Swap-out*. A method of removing a process from RAM, and adding it to the hard disk
    * *Swap-in*. A method of removing a program from a hard disk, and putting it back into RAM

**Long-term scheduler in a batch system**. In a batch system, or for the batch portion of a general-purpose OS, newly submitted jobs are routed to disk and held in a batch queue

$\to$ The long-term scheduler creates processes from the queue when it can
* *Questions of interest*. There are two decisions involved, i.e.
    * The scheduler must decide that the OS can take on one or more additional processes
    * The scheduler must decide which job or jobs to accept and turn into processes
        
        $\to$ The criteria used may include priority, expected execution time, and I/O requirements

**Long-term scheduler for interactive programs in a time-sharing system**. A process request is generated when a user attempts to connect to the system
* *Idea*. Time-sharing users are not simply queued up and kept waiting until the system can accept them, i.e.
    1. The OS will accept all authorized comers until the system is saturated, using some predefined measure of saturation
    2. After saturation, a connection request is met with a message indicating that the system is full and the user should try again later

### Medium-term scheduling
**Medium-term scheduling**. Part of the swapping function
* *Swapping-in decision*. Typically based on the need to manage the degree of multiprogramming
* *Swapping-in in a system without virtual memory*. Memory management is an issue
    
    $\to$ The swapping-in decision will consider the memory requirements of the swapped-out processes

### Short-term scheduling
**Long-term scheduling and short-term scheduling**.
* *Long-term scheduler*. Execute relatively infrequently and make the coarse-grained decision of whether or not to take on a new process, and which one to take
* *Short-term scheduler (or dispatcher)*. Execute frequently and make the fine-grained decision of which job to execute next

#### Process states
**Brief**. To understand the operation of the short-term scheduler, we need to consider the concept of a process state

**Process state**. During the lifetime of a process, its status will change a number of times

$\to$ Its status at any point in time is referred to as a state

>**NOTE**. "State" is used since it connotes that certain information exists, which defines the status at that point

* *Minimal set of process states*.
    * *New*. A program is admitted by the high-level scheduler but is not yet ready to execute
        
        $\to$ The OS will initialize the process, moving it to the ready state
    * *Ready*. The process is ready to execute and is awaiting access to the processor
    * *Running*. The process is being executed by the processor
    * *Waiting*. The process is suspended from execution waiting for some system resource, e.g. I/O
    * *Halted*. The process has terminated and will be destroyed by the OS

**Process control block**. For each process in the system, the OS must maintain information indicating the state of the process and other information necessary for process execution

$\to$ Each process is represented in the OS by a process control block
* *Block structure*. Typically contain
    * *Identifier*. Each current process has a unique identifier
    * *State*. The current state of the process, e.g. `new`, `ready`, etc.
    * *Priority*. Relative priority level
    * *Program counter*. The address of the next instruction in the program to be executed
    * *Memory pointers*. The starting and ending locations of the process in memory
    * *Context data*. Data present in registers in the processor while the process is executing
        
        $\to$ These data represent the "context" of the process
        
        * *Process swapping*. The context data plus the program counter are saved when the process leaves the running state
            
            $\to$ They are retrieved by the processor when it resumes execution of the process
    * *I/O status information*. Include outstanding I/O requests, I/O devices, e.g. tape drives, assigned to the process, a list of files assigned to the process, etc.
    * *Accounting information*. Include the amount of processor time and clock time used, time limits, account numbers, etc.
* *Block creation and process execution*. 
    1. `When the scheduler accepts a new job or user request for execution
        
        $\to$ It creates a blank process control block and places the associated process in the `new` state
    2. After the system has properly filled in the process control block
        
        $\to$ The process is transferred to the `ready` state

#### Scheduling techniques
**How main memory is partitioned at a given point in time**. 
* *Main memory for OS kernel*. The kernel of the OS is always resident
* *Main memory for active processes*. There are a number of active processes, each of which is allocated a portion of memory

**Basic functioning of short-term scheduler**.
* *Transferring control from process to OS*. Consider two active processes $A$ and $B$ in the main memory
    1. We begin at a point in time when process $A$ is running
        
        $\to$ The processor is executing instructions from the program contained in $A$’s memory partition
    2. At some later point in time, the processor ceases to execute instructions in $A$ and begins executing instructions in the OS area
        
        $\to$ This will happen for one of three reasons
        * Process $A$ issues a service call, e.g. an I/O request, to the OS
            
            $\to$ Execution of $A$ is suspended until this call is satisfied by the OS
        * Process $A$ causes an interrupt, which is a hardware-generated signal to the processor
            
            $\to$ When this signal is detected, the processor ceases to execute $A$ and transfers to the interrupt handler in the OS
            * *Events causing an interrupt*. A variety of events related to $A$ will cause an interrupt
                * *Examples*. 
                    * An error, e.g. attempting to execute a privileged instruction
                    * A timeout, i.e. to prevent any one process from monopolizing the processor
                        
                        $\to$ Each process is only granted the processor for a short period at a time
        * Some event unrelated to process $A$ that requires attention causes an interrupt
            * *Example*. The completion of an I/O operation
* *Context switching*. When transferring control to the OS
    1. The processor saves the current context data and the program counter for $A$ in $A$’s process control block
    2. The process then begins executing in the OS
    3. The OS may perform some work, e.g. initiating an I/O operation
    4. The short-term-scheduler portion of the OS decides which process should be executed next, e.g. $B$ is chosen
    5. The OS instructs the processor to restore $B$’s context data and proceed with the execution of $B$ where it left off

**Key elements of an OS for multiprogramming and process scheduling**.

<div style="text-align:center">
    <img src="https://i.imgur.com/lteb6z9.png">
    <figcaption>Key elements of an OS for multiprogramming</figcaption>
</div>

1. The OS receives control of the processor
    * At the interrupt handler if an interrupt occurs, or
    * At the service-call handler if a service call occurs
2. The short-term scheduler is invoked to select a process for execution
3. When the OS has finished its immediate tasks, it turns the processor over to the chosen process

**Queuing diagram of processor scheduling**.

<div style="text-align:center">
    <img src="https://i.imgur.com/c1YZcEJ.png">
    <figcaption>Queuing diagram representation of processor scheduling</figcaption>
</div>

* *OS queues*. To do its job, the OS maintains a number of queues
    
    $\to$ Each queue is simply a waiting list of processes waiting for some resource
* *Long-term queue*. A list of jobs waiting to use the system
    
    $\to$ As conditions permit, the high-level scheduler will allocate memory and create a process for one of the waiting items
* *Short-term queue*. Consist of all processes in the ready state
    
    $\to$ Any one of these processes could use the processor next
    * *Process selection*. It is up to the short-term scheduler to pick one
        
        $\to$ Generally, this is done with a round-robin algorithm, giving each process some time in turn
        
        >**NOTE**. Priority levels may also be used

* *I/O queue*. There is an I/O queue for each I/O device
    
    >**NOTE**. More than one process may request the use of the same I/O device
    
    * *Idea*. All processes waiting to use each device are lined up in that device’s queue
* *Queueing procedure*. 
    1. Each process request, e.g. batch job, user-defined interactive job, is placed in the long-term queue
    2. As resources become available, a process request becomes a process 
    3. The process is then placed in the ready state and put in the short-term queue
    4. The processor alternates between executing OS instructions and executing user processes
    5. While the OS is in control, it decides which process in the short-term queue should be executed next

**Process suspending**. A process being executed may be suspended for a variety of reasons
* If it is suspended because the process requests I/O
    
    $\to$ It is placed in the appropriate I/O queue
* If it is suspended because of a timeout, or because the OS must attend to pressing business
    
    $\to$ It is placed in the ready state and put into the short-term queue

**Managing I/O queues**. The OS also manages the I/O queues, i.e.
1. When an I/O operation is completed
    
    $\to$ The OS removes the satisfied process from that I/O queue and places it in the short-term queue
2. The OS selects another waiting process, if any, and signals for the I/O device to satisfy that process’s request

## Memory management
**Main memory in uniprogramming system**. Main memory is divided into two parts, i.e. one for the OS, i.e. resident monitor, and one for the program currently being executed

**Main memory in multiprogramming system**. The "user" part of memory is subdivided to accommodate multiple processes
* *Memory management*. The task of subdivision is carried out dynamically by the OS, i.e. memory management
* *Importance in multiprogramming systems*. Effective memory management is vital in such a system
    * *Explain*. If only a few processes are in memory, then for much of the time all of the processes will be waiting for I/O and the processor will be idle
        
        $\to$ Memory needs to be allocated efficiently to pack as many processes into memory as possible

### Swapping
**Problem of interest**. In multiprogramming systems, memory holds multiple processes and that the processor can move to another process when one process is waiting

$\to$ However, the processor is so much faster than I/O that it will be common for all the processes in memory to be waiting on I/O
* *Consequence*. Even with multiprogramming, a processor could be idle most of the time
* *Naive solution*. Expand main memory to accommodate more processes
    * *Drawbacks*. 
        * Main memory is expensive, even today
        * The appetite of programs for memory has grown as fast as the cost of memory has dropped
            
            $\to$ Larger memory results in larger processes, not more processes

**Swapping**

<div style="text-align:center">
    <img src="https://i.imgur.com/NfNJCVs.png">
    <figcaption>The use of swapping</figcaption>
</div>

* *Long-term process queue*. Have a long-term queue of process requests, typically stored on disk
    * These processes are brought in, one at a time, as space becomes available
    * As processes are completed, they are moved out of main memory
* *Intermediate queue*. A queue of existing processes, which have been temporarily kicked out of memory
    1. When none of the processes in memory are in the ready state, e.g. all are waiting on an I/O operation
        
        $\to$ The processor swaps one of these processes back out to disk into the intermediate queue
    2. The OS brings in another process from the intermediate queue, or it honors a new process request from the long-term queue
    3. Execution continues with the newly arrived process

**Drawback**. Swapping is an I/O operation, and therefore there is the potential for making the problem worse, not better
* *Why using swapping*.
    * Disk I/O is generally the fastest I/O on a system, e.g. compared with tape or printer I/O
        
        $\to$ Swapping will usually enhance performance
    * A more sophisticated scheme, involving virtual memory, improves performance over simple swapping

### Partitioning
**Fixed-size partitions**. The simplest scheme for partitioning available memory
* *Partition sizes* Although the partitions are of fixed size, they need not be of equal size
* *Partition allocation to processes*. When a process is brought into memory, it is placed in the smallest available partition that will hold it
* *Drawback*. Even with the use of unequal fixed-size partitions, there will be wasted memory
    * *Explain*. In most cases, a process will not require exactly as much memory as provided by the partition

**Variable-size partitions**. A more efficient approach to fixed-size partitions
* *Partition allocation to processes*. When a process is brought into memory, it is allocated exactly as much memory as it requires and no more
* *Drawback*. This method starts out well, but eventually it leads to a situation in which there are a lot of small holes in memory
    
    $\to$ As time goes on, memory becomes more and more fragmented, and memory utilization declines
* *Compaction*. A technique for overcoming external fragmentation
    * *Idea*. From time to time, the OS shifts the processes in memory to place all the free memory together in one block
    * *Drawback*. This is a time-consuming procedure, wasteful of processor time

**Swapping and compaction's impact on process instructions**.
* *Swapping and compaction results in dynamic process address space*.
    * A process is not likely to be loaded into the same place in main memory each time it is swapped in
    * If compaction is used, a process may be shifted while in main memory
* *Process instructions*. A process in memory consists of instructions plus data
    
    $\to$ The instructions will contain addresses for memory locations of two types:
    * Addresses of data items
    * Addresses of instructions, used for branching instructions
* *Problem*. The addresses in the instructions are not fixed, i.e. they will change each time a process is swapped in
    
    $\to$ To solve this problem, a distinction is made between logical addresses and physical addresses
    * *Logical address*. A location relative to the beginning of the program
        
        $\to$ Instructions in the program contain only logical addresses
    * *Physical address*. An actual location in main memory
* *Logical-to-physical address translation*. When the processor executes a process, it automatically converts from logical to physical address 
    * *Idea*. Add the current starting location of the process, i.e. its base address, to each logical address

>**NOTE**. This is another example of a processor hardware feature designed to meet an OS requirement

>**NOTE**. The exact nature of this hardware feature depends on the memory management strategy in use

### Paging
**Brief**. Both unequal fixed-size and variable-size partitions are inefficient in the use of
memory

**Paging**.
* *Idea*. 
    * Partition the memory into equal relatively small fixed-size chunks
    * Divide each process is also divided into small fixed-size chunks of some size
    * The chunks of a program, i.e. pages, could be assigned to available chunks of memory, i.e. frames or page frames
* *Consequence*. At most, the wasted space in memory for that process is a fraction of the last page

**Paging with insufficient available memory**. Suppose there are not sufficient unused contiguous frames to hold the process
* *Problem*. Since we can once again use the concept of logical address
    
    $\to$ A simple base address will no longer suffice
* *Page table*. The OS maintains a page table for each process
    * *Page table*. Show the frame location for each page of the process
    * *Logical address*. Within the program, each logical address consists of a page number and a relative address within the page
* *Logical-to-physical address translation*. Done by processor hardware
    * *Requirements*. The processor must know how to access the page table of the current process
    * *Idea*. Presented with a logical address `(page number, relative address)`
        
        $\to$ The processor uses the page table to produce a physical address `(frame number, relative address)`

**Conclusion**. Paging solves the problems raised earlier
* *Paging procedure*. 
    * Main memory is divided into many small equal-size frames
    * Each process is divided into frame-size pages
        
        $\to$ Smaller processes require fewer pages, larger processes require more
    * When a process is brought in, its pages are loaded into available frames, and a page table is set up

### Virtual memory
#### Demand paging
**Brief**. The simple tactic of breaking a process up into pages led to the development of another important concept, i.e. virtual memory

$\to$ To understand virtual memory, we must add a refinement to the paging scheme

**Demand paging**. Each page of a process is brought in only when it is needed, i.e. on demand
* *Principle of locality*. Consider a large process, consisting of a long program plus a number of arrays of data
    * *Explain*. Over any short period of time, execution may be confined to a small section of the program, e.g. a subroutine
        
        $\to$ Perhaps only one or two arrays of data are being used
    * *Consequence*. It would clearly be wasteful to load in dozens of pages for that process when only a few pages will be used, before the program is suspended
        
        $\to$ We can make better use of memory by loading in just a few pages
* *Solution to the principle of locality*.
    * *Idea*. If the program branches to an instruction on a page not in main memory, or if the program references data on a page not in memory
        
        $\to$ A page fault is triggered, telling the OS to bring in the desired page
    * *Consequence*. 
        * At any one time, only a few pages of any given process are in memory
            
            $\to$ More processes can be maintained in memory
        * Time is saved, since unused pages are not swapped in and out of memory
    * *Requirements*. The OS must be clever about how it manages this scheme, i.e.
        * *Page replacement*. When the OS brings one page in, it must throw another page out, i.e.
            * If it throws out a page just before it is about to be used
                
                $\to$ It will have to go get that page again almost immediately
            * Too much of this leads to a condition known as thrashing
                * *Thrashing*. The processor spends most of its time swapping pages rather than executing instructions
* *Solution to thrashing*. The OS tries to guess, based on recent history, which pages are least likely to be used in the near future
    * *A potentially effective technique*. Least recently used (LRU)
        * *Drawback*. In practice, LRU is difficult to implement for a virtual memory paging scheme
            
            $\to$ Several alternative approaches that seek to approximate the performance of LRU are in use

**Consequence of demand paging**. It is possible for a process to be larger than all of main memory

$\to$ One of the most fundamental restrictions in programming has been lifted
* *Program size before the present of demand paging*. Without demand paging, a programmer must be acutely aware of how much memory is available
    * If the program being written is too large
        
        $\to$ The programmer must devise ways to structure the program into pieces that can be loaded one at a time
    * With demand paging, that job is left to the OS and the hardware
        
        $\to$ As far as the programmer is concerned, he or she is dealing with a huge memory, the size associated with disk storage
* *Real memory and virtual memory*.
    * *Real memory*. Since a process executes only in main memory, that memory is referred to as real memory
    * *Virtual memory*. A programmer or user perceives a much larger memory, which is allocated on the disk
        
        $\to$ This is referred to as virtual memory
* *Benefits of virtual memory*. 
    * Allow for very effective multiprogramming
    * Relieve the user of the unnecessarily tight constraints of main memory

#### Page table structure
**Page table**. The basic mechanism for reading a word from memory involves the translation of a logical address `(page number, offset)` into a physical address `(frame number, offset)`, using a page table
* *Page table location*. Since the page table is of variable length, depending on the size of the process
    
    $\to$ We cannot expect to hold it in registers
    * *Consequence*. The table must be in main memory to be accessed
* *Hardware implementation*.

    <div style="text-align:center">
        <img src="https://i.imgur.com/Hshk8jV.png">
        <figcaption>Logical and physical addresses</figcaption>
    </div>

    * *Idea*. When a particular process is running, a register holds the starting address of the page table for that process
        * *Page number of a virtual address*. Used to index the page table and look up the corresponding frame number
            
            $\to$ This is combined with the offset portion of the virtual address to produce the desired real address

**Number of page tables per process**. In most systems, there is one page table per process
* *Problem*. Each process can occupy huge amounts of virtual memory

    $\to$ The amount of page table entries may be large
    * *Consequence*. The amount of memory devoted to page tables alone could be unacceptably high
* *Naive*. Store page tables in virtual memory, rather than real memory
    
    $\to$ Page tables are subject to paging just as other pages are
    * *Requirements*. When a process is running, at least a part of its page table must be in main memory, including the page table entry of the currently executing page
* *Two-level organization of large page tables*. Have a page directory, in which each entry points to a page table
    * *Maximum number of pages*. If the length of the page directory is $X$, and if the maximum length of a page table is $Y$
        
        $\to$ A process can consist of up to $X \times Y$ pages
        
    * *Typical maximum length of a page table*. Restricted to be equal to one page
* *Inverted page table structure*. Variations on this approach are used on the PowerPC, UltraSPARC, and the IA-64 architecture

    <div style="text-align:center">
        <img src="https://i.imgur.com/G8Xt4pg.png">
        <figcaption>Inverted page table structure</figcaption>
    </div>

    * *Idea*. The page number portion of a virtual address is mapped into a hash value using a simple hashing function
        
        $\to$ The hash value is a pointer to the inverted page table
        * *Inverted page table*. Contain the page table entries, with one entry for each real memory page frame rather than one per virtual page
            * *Consequence*. 
                * A fixed proportion of real memory is required for the tables regardless of the number of processes or virtual pages supported
                    
                    $\to$ The number of page table entries reduces to the number of frames in physical memory
                * A single page table is used to represent the paging information of all the processes
        * *Hash collision*. Since more than one virtual address may map into the same hash table entry, i.e. two processes may refer to different addresses within the same physical frame
            * *Chaining solution*. A chaining technique is used for managing the overflow
                * *Idea*. Keep a chain of pages per frame
            * *Experimental results*. The hashing technique results in chains, which are typically short, i.e. between one and two entries
    * *"Inverted"*. Indicate that the page table indexes page table entries by frame number rather than by virtual page number

### Translation lookaside buffer
**Problem of interest**. 
* *Required physical memory accesses for each virtual memory reference*.
    * One access to fetch the appropriate page table entry
    * One access to fetch the desired data
* *Consequence*. A straightforward virtual memory scheme would have the effect of doubling the memory access time
* *Solution*. Most virtual memory schemes make use of a special cache for page table entries, usually called a translation lookaside buffer (TLB)

**Translation lookaside buffer (TLB)**. This cache functions in the same way as a memory cache and contains those page table entries that have been most recently used

<div style="text-align:center">
    <img src="https://i.imgur.com/ca0Nx9y.png">
    <figcaption>Operation of paging and translation lookaside buffer (TLB)</figcaption>
</div>

* *Motivation*. By the principle of locality, most virtual memory references will be to locations in recently used pages
    
    $\to$ Most references will involve page table entries in the cache

**Virtual memory mechanism and cache system**. The virtual memory mechanism must interact with the cache system, i.e. not the TLB cache, but the main memory cache

<div style="text-align:center">
    <img src="https://i.imgur.com/SArswtu.png">
    <figcaption>Translation lookaside table and cache operation</figcaption>
</div>

* *Virtual address format*. Generally be in the form of a page number, offset
* *Interaction with cache system*. 
    1. The memory system consults the TLB to see if the matching page table entry is present
        * If it is, the real, i.e. physical, address is generated by combining the frame number with the offset
        * Otherwise the entry is accessed from a page table
    2. Once the real address is generated, which is in the form of a tag and a remainder
        
        $\to$ The cache is consulted to see if the block containing that word is present
        * If so, it is returned to the processor
        * Otherwise the word is retrieved from main memory

**Processor hardware complexity**. We should understand the complexity of the processor hardware involved in a single memory reference
* *Explain*. The virtual address is translated into a real address, i.e. this involves
    * A reference to a page table, which may be in the TLB, in main memory, or on disk
    * The referenced word may be in cache, in main memory, or on disk
        
        $\to$ In case of disk, the page containing the word must be loaded into main memory and its block loaded into the cache
        
        >**NOTE**. In addition, the page table entry for that page must be updated

### Segmentation
**Segmentation**. Another way in which addressable memory can be subdivided
* *Segmentation versus paging*.
    * *Paging*. 
        * Invisible to the programmer
        * Serve the purpose of providing the programmer with a larger address space
    * *Segmentation*. 
        * Usually visible to the programmer
        * Provided as a convenience for organizing programs and data
        * Provided as a means for associating privilege and protection attributes with instructions and data
* *Idea*. Allow the programmer to view memory as consisting of multiple address spaces or segments
    * *Segment size*. Segments are of variable, indeed dynamic, size
    * *Data allocation in segments*. Typically, the programmer or the OS will assign programs and data to different segments
    * *Number of program segments*. There may be a number of program segments for various types of programs as well as a number of data segments
        
        $\to$ Each segment may be assigned access and usage rights
* Memory references*. Consist of a `(segment number, offset)` form of address

**Benefits over a nonsegmented address space**.
1. Segmentation simplifies the handling of growing data structures
    * *Explain*. If the programmer does not know ahead of time how large a particular data structure will become
        
        $\to$ It is not necessary to guess
        * *Idea*. The data structure can be assigned its own segment
            
            $\to$ The OS will expand or shrink the segment as needed
2. Segmentation allows programs to be altered and recompiled independently, without requiring that an entire set of programs be relinked and reloaded
    * *Explain*. This is accomplished using multiple segments
3. Segmentation lends itself to sharing among processes
    * *Explain*. A programmer can place a utility program, or a useful table of data, in a segment, which can be addressed by other processes
4. Segmentation lends itself to protection
    * *Explain*. Since a segment can be constructed to contain a well-defined set of programs or data
        
        $\to$ The programmer or a system administrator can assign access privileges in a convenient fashion

**Benefits of paging over segmentation**. Paging provides for an efficient form of memory management

$\to$ To combine the advantages of both, some systems are equipped with the hardware and OS software to provide both

## Pentium memory mangement
**Development of microprocessors**. Since the introduction of the 32-bit architecture

$\to$ Microprocessors have evolved sophisticated memory management schemes that build on the lessons learned with medium- and large-scale systems
* *Advantages over large-system antecedents*. In many cases, the microprocessor versions are superior to their larger-system antecedents
    * *Explain*. Since the schemes were developed by the microprocessor hardware vendor and may be employed with a variety of OSes
        
        $\to$ They tend to be quite general purpose
* *Example*. The scheme used on the Pentium II

### Address spaces
**Address spaces**. The Pentium II includes hardware for both segmentation and paging

$\to$ Both mechanisms can be disabled, allowing the user to choose from four distinct views of memory
* *Unsegmented unpaged memory*. The virtual address is the same as the physical address
    * *Usage*. Useful, for example, in low-complexity, highperformance controller applications
* *Unsegmented paged memory*. Memory is viewed as a paged linear address space
    
    $\to$ Protection and management of memory is done via paging
    * *Usage*. Used by some operating systems, e.g. Berkeley UNIX
* *Segmented unpaged memory*. Memory is viewed as a collection of logical address spaces
    * *Advantages over a paged approach*. 
        * This approach affords protection down to the level of a single byte, if necessary
        * This approach guarantees that the translation table needed, i.e. the segment table, is on-chip when the segment is in memory
            
            $\to$ Segmented unpaged memory results in predictable access times
* *Segmented paged memory*. 
    * *Idea*.
        * Segmentation is used to define logical memory partitions subject to access control
        * Paging is used to manage the allocation of memory within the partitions
    * *Usage*. Operating systems such as UNIX System V favor this view

### Segmentation
**Logical address**. 
* *Logical address structure*. When segmentation is used, each logical address consists of 
    * A 16-bit segment reference
        * *Structure*. 
            * 2 bits of the segment reference deal with the protection mechanism
            * 14 bits for specifying a particular segment
    * A 32-bit offset
* *Consequence*.
    * With unsegmented memory, the user’s virtual memory is $2^{32} = 4$ Gbytes
    * With segmented memory, the total virtual memory space as seen by a user is $2^{46} = 64$ terabytes (Tbytes)
* *Physical address space*. The physical address space employs a 32-bit address for a maximum of 4 Gbytes

**Logical address space partitioning**.
* *Amount of virtual memory*. Can actually be larger than the 64 Tbytes
    * *Explain*. The processor’s interpretation of a virtual address depends on which process is currently active
* *Virtual address space partitioning*. Virtual address space is divided into two parts
    * One-half of the virtual address space, i.e. 8K segments $\times$ 4 Gbytes, is global
        
        $\to$ This shared by all processes
    * The remainder is local and is distinct for each process

**Segment protection schemes**. Associated with each segment are two forms of protection, i.e. privilege level and access attribute
* *Privilege levels*.
    * *Privilege levels*. There are four levels, from most protected (level 0) to least protected (level 3)
        * *Data segment classification*. The privilege level associated with a data segment
        * *Program segment clearance*. The privilege level associated with a program segment
    * *Protection scheme*. An executing program may only access data segments, for which its clearance level is lower than, i.e. more privileged, or equal to, i.e. same privilege, the privilege level of the data segment
    * *Deciding privilege levels*. The hardware does not dictate how these privilege levels are to be used
        
        $\to$ This depends on the OS design and implementation
    * *Intended purposes for privilege levels*.
        * *Privilege level 1*. Used for most of the OS
        * *Privilege level 0*. Used for that small portion of the OS devoted to memory management, protection, and access control
            
            $\to$ This leaves two levels for applications
        * *Privilege level 3*. In many systems, applications will reside at level 3 with level 2 being unused 
        * *Privilege level 2*. Specialized application subsystems, which must be protected because they implement their own security mechanisms, are good candidates for level 2
            * *Examples*. Database management systems, office automation systems, and software engineering environments
    * *Restricted instructions w.r.t privilege levels*. The privilege mechanism limits the use of certain instructions
        * *Examples*. 
            * Some instructions, e.g. those dealing with memory-management registers, can only be executed in level 0
            * I/O instructions can only be executed up to a certain level, which is designated by the OS
                
                $\to$ Typically, this will be level 1
* *Access attribute*. 
    * *Access attribute of a data segment*. Specify whether read/write or readonly accesses are permitted
    * *Access attribute of a program segment*. Specify read/execute or read-only access

**Address translation mechanism for segmentation**. Include mapping a virtual address into what is referred to as a linear address

<div style="text-align:center">
    <img src="https://i.imgur.com/6IWAz1Q.png">
    <figcaption>Pentium memory-management formats</figcaption>
</div>

* *Virtual address structure*. Consist of the 32-bit offset and a 16-bit segment selector
* *Segment selector field structure*. Consist of the following fields
    * *Table Indicator (TI)*. Indicate whether the global segment table or a local segment table should be used for translation
    * *Segment Number*. The number of the segment, i.e. this serves as an index into the segment table
    * *Requested Privilege Level (RPL)*. The privilege level requested for this access
* *Segment table entry*. Each entry in a segment table consists of 64 bits as shown

### Paging
**Segmentation and linear address**. Segmentation is an optional feature and may be disabled
* When segmentation is in use, addresses used in programs are virtual addresses
    
    $\to$ These addresses are converted into linear addresses
* When segmentation is not in use, linear addresses are used in programs

**Paging mechanism in Pentium**. A two-level table lookup operation
* *Lookup levels*.
    * *First level lookup*. A page directory, which contains up to 1024 entries
        
        $\to$ This splits the 4-Gbyte linear memory space into 1024 page groups, each with its own page table, and each 4 Mbytes in length
    * *Second level lookup*. Each page table contains up to 1024 entries, each of which corresponds to a single 4-Kbyte page
* *Memory management options for paging*. Memory management has the option of 
    * Using one page directory for all processes
    * Using one page directory for each process
    * Using some combination of the two
* *Page table location*. 
    * The page directory for the current task is always in main memory
    * Page tables may be in virtual memory
* *Access control mechanisms*. Can be provided on a page or page group basis

**Translation lookaside buffer (TLB)**. The Pentium II also makes use of a translation lookaside buffer
* *Buffer structure*. The buffer can hold 32 page table entries
    
    $\to$ Each time that the page directory is changed, the buffer is cleared

**Combination of segmentation and paging mechanisms**. For clarity, the translation lookaside buffer and memory cache mechanisms are not shown

<div style="text-align:center">
    <img src="https://i.imgur.com/jxkul85.png">
    <figcaption>Pentium memory address translation mechanisms</figcaption>
</div>

**Page size extension**. The Pentium II includes a new extension not found on the 80386 or 80486, the provision for two page sizes
* *Idea*. If the PSE, i.e. page size extension, bit in control register 4 is set to 1
    
    $\to$ The paging unit permits the OS programmer to define a page as either 4 Kbyte or 4 Mbyte in size
* *Using different page sizes*.
    * When 4-Mbyte pages are used, there is only one level of table lookup for pages
        * *Page directory access*. When the hardware accesses the page directory, the page directory entry has 
            * The PS bit set to 1
            * Bits 9 through 21 are ignored
            * Bits 22 through 31 define the base address for a 4-Mbyte page in memory
        * *Consequence*. There is a single page table
            
            $\to$ This reduces the memory-management storage requirements for large main memories
    * With 4-Kbyte pages, a full 4-Gbyte main memory requires about 4 Mbytes of memory just for the page tables
        
        $\to$ With 4-Mbyte pages, a single table, 4 Kbytes in length, is sufficient for page memory management

## ARM memory management
**Brief**. ARM provides a versatile virtual memory system architecture that can be tailored to the needs of the embedded system designer

### Memory system organization
**Overview of the memory management hardware in the ARM for virtual memory**.

<div style="text-align:center">
    <img src="https://i.imgur.com/GbMA2n9.png">
    <figcaption>ARM memory system overview</figcaption>
</div>

* *Virtual memory translation hardware*. Use one or two levels of tables for translation from virtual to physical addresses
    * *Translation lookaside buffer (TLB)*. A cache of recent page table entries, i.e. recent translations of virtual memory to physical memory
        * *Idea*. If an entry is available in the TLB
            
            $\to$ The TLB directly sends a physical address to main memory for a read or write operation
            * *Motivation*. Data is exchanged between the processor and main memory via the cache
    * *TLB used with cache address types*.
        * *TLB with logical cache organization*. If a logical cache organization is used
            
            $\to$ The ARM supplies that address directly to the cache, as well as supplying it to the TLB when a cache miss occurs
        * *TLB with physical cache organization*. If a physical cache organization is used
            
            $\to$ The TLB must supply the physical address to the cache
* *Access control bits*. Entries in the translation tables include access control bits
    
    $\to$ This determines whether a given process may access a given portion of memory
    * *Explain*. If access is denied, access control hardware supplies an abort signal to the ARM processor

### Virtual memory address translation
**Brief**. The ARM supports memory access based on either sections or pages, i.e.
* *Supersections (optional)*. Consist of 16-MB blocks of main memory
* *Sections*. Consist of 1-MB blocks of main memory
* *Large pages*. Consist of 64-kB blocks of main memory
* *Small pages*. Consist of 4-kB blocks of main memory

>**NOTE**. Sections and supersections are supported to allow mapping of a large region of memory while using only a single entry in the TLB

**Access control mechanisms**. Extended to 1kB subpages within small pages, and to 16kB subpages within large pages

**Levels of translation table in main memory**.
* *First-level table*. Holds section and supersection translations, and pointers to second-level tables
* *Second-level tables*. Hold both large and small page translations

**Memory-management unit (MMU)**. 
* *Functionality*.
    * Translate virtual addresses generated by the processor into physical addresses to access main memory
    * Derive and checks the access permission
* *TLB miss*. Translations occur as the result of a TLB miss, and start with a first-level fetch
    * A section-mapped access only requires a first-level fetch
    * A page-mapped access also requires a second-level fetch
* *Example*.

    <div style="text-align:center">
        <img src="https://i.imgur.com/73qE6Nv.png">
        <figcaption>ARM virtual memory address translation for small pages</figcaption>
    </div>


    >**NOTE**. A similar two-page lookup procedure is used for large pages
    >
    >* *Explain*. For sections and supersection, only the L1 page table lookup is required

### Memory-management formats
**L1 page table**. Each entry is a descriptor of how its associated 1-MB virtual address range, i.e. a section, is mapped

<div style="text-align:center">
    <img src="https://i.imgur.com/JVeHiHw.png">
    <figcaption>ARMv6 memory-mangement format</figcaption>
</div>

* *Entry format*. Each entry has one of four alternative formats
    * *`Bits [1:0] = 00`*. The associated virtual addresses are unmapped, and attempts to access them generate a translation fault
    * *`Bits [1:0] = 01`*. The entry gives the physical address of an L2 page table, which specifies how the associated virtual address range is mapped
    * *`Bits [1:0] = 01` and `bit 19 = 0`*. The entry is a section descriptor for its associated virtual addresses
    * *`Bits [1:0] = 01` and `bit 19 = 1`*. The entry is a supersection descriptor for its associated virtual addresses
    
    >**NOTE**. Entries with `bits[1:0] = 11` are reserved

**Memory-management formats for memory structured into pages**. A two-level page table access is required
* *Entry format for L1 page table*. `Bits [31:10]` of the L1 page entry contain a pointer to a L2 page table
* *Entry format for L2 page table*. 
    * For small pages, the L2 entry contains a 20-bit pointer to the base address of a 4-kB page in main memory
    * For large pages, the structure is more complex
        * As with virtual addresses for small pages, a virtual address for a large page structure includes 
            * A 12-bit index into the L1 table, and
            * An 8-bit index into the L2 table
        * The page index portion of the virtual address must be 16 bits, i.e. since $64=4\times 2^4$ hence 4 more bits are required
            * *Idea*. To accommodate all of these bits in a 32-bit format
                
                $\to$ There is a 4-bit overlap between the page index field and the L2 table index field
            * *Implementation*. ARM accommodates this overlap by requiring that each page table entry in a L2 page table, which supports large pages, be replicated 16 timesthrough the user ISA
                * *Consequence*. The size of the L2 page table is reduced from 256 entries to 16 entries, if all of the entries refer to large pages

                    $\to$ The L2 table index is reduced from 8 bits to 4 bits, leaving 4 bits for increasing the page index portion of the virtual address from 12 bits to 16 bits, as required
        * A given L2 page can service a mixture of large and small pages
            
            $\to$ We need the replication for large page entries

**Memory-management formats for memory structured into sections**.
* For sections, `bits [31:20]` of the L1 entry contain a 12-bit pointer to the base of the 1-MB section in main memory
* For supersections, `bits [31:24]` of the L1 entry contain an 8-bit pointer to the base of the 16-MB section in main memory
    * *Entry replication*. The L1 table index portion of the virtual address overlaps by 4 bits with the supersection index portion of the virtual address
        
        $\to$ 16 identical L1 page table entries are required
    * *Expansion of the range of physical address space*. The physical address space range can be expanded by up to eight additional address bits, i.e. `bits [23:20]` and `[8:5]`
        * *Explain*. There is no domain for supersections, and the supersection base address field is 4 bit shorter than the section base address field
        * *Choice of number of additional bits*. The number of additional bits is implementation dependent
        * *Usage*. These additional bits can be interpreted as extending the size of physical memory by as much as a factor of $2^8 = 256$
            
            $\to$ Thus, physical memory may in fact be as much as 256 times as large as the memory space available to each individual process

### Access control
**AP access control bits in each table entry**. Control access to a region of memory by a given process
* *Types of access*. 
    * A region of memory can be designated as no access, read only, or read-write
    * The region can be designated as privileged access only, reserved for use by the OS and not by applications

**Domain**. ARM employs the concept of a domain, i.e. a collection of sections and/or pages with particular access permissions

$\to$ This allows multiple processes to use the same translation tables while maintaining some protection from each other
* *Number of supported domains*. The ARM architecture supports 16 domains
* *Domain field in table entry and TLB entry*. Each page table entry and TLB entry contains a field specifying which domain the entry is in
* *Access control for domain*. A 2-bit field in the Domain Access Control Register controls access to each domain
    
    $\to$ Each field allows the access to an entire domain to be enabled and disabled very quickly
    * *Consequence*. Whole memory areas can be swapped in and out of virtual memory very efficiently

**Types of supported domains**.
* *Clients*. Users of domains, i.e. execute programs and access data
    
    $\to$ These userse must observe the access permissions of the individual sections and/or pages making up the domain
* *Managers*. 
    * Control the behavior of the domain, i.e. the current sections and pages in the domain, and the domain access
    * Bypass the access permissions for table entries in that domain
* *Common practice*. One program can be a client of some domains, and a manager of some other domains, and have no access to the remaining domains
    
    $\to$ This allows very flexible memory protection for programs that access different memory resources

# Appendix
## Concepts
**ABI and API**.
* *API*. Define the objects and methods that a module makes available to its clients, at the source-code level
    * *Questions of interest*.
        * How to instantiate its objects?
        * What arguments need to pass to its methods, and what do they return?
    * *Conclusion*. When we want to know what library functions are available and how to use them
        
        $\to$ We are asking about an API
* *ABI*. A lower-level concept defining how the bits and bytes that are passed between a module and its clients
    * *Questions of interest*.
        * What format are they in?
        * Are they pushed onto the stack, passed in registers, or stored elsewhere?
        * Does the stack need to be cleaned up when a function returns, and if so, who's responsible for tending to it?
* *Example*. Consider a function with the following API

    ```cpp
    long long multiply(unsigned int multiplicand, long multiplier);
    ```

    then the corresponding ABI is given as 
    1. Push the return address on the stack as an absolute 64-bit address, in MSB to LSB order
        
        $\to$ The program will continue execution at that address when the function completes
    2. Push a 32-bit value onto the stack in MSB to LSB order
        
        $\to$ It represents the unsigned multiplicand
    3. Store a 64-bit value in the accumulator register, in MSB to LSB order
        
        $\to$ It represents the signed multiplier in two's-complement form.
    4. Jump to address 0xADDADD relative to the beginning of the program to execute the function
    5. Upon completion, take the first 4 bytes on the stack and append the first 4 bytes of the value in the accumulator register to create the signed 64-bit product, in LSB to MSB order, in one's-complement format
    6. The caller must remove the 4 bytes from the stack, and the 4-byte multiplicand and the 8-byte return address, to restore the stack