---
title: 3. A top level view of computer function and interconnection
tags: Computer architecture
---

<!-- TOC titleSize:1 tabSpaces:2 depthFrom:1 depthTo:6 withLinks:1 updateOnSave:1 orderedList:0 skip:0 title:1 charForUnorderedList:* -->
# Table of Contents
* [A top-level view of computer function and interconnection](#a-top-level-view-of-computer-function-and-interconnection)
  * [Computer components](#computer-components)
  * [Computer function](#computer-function)
    * [Instruction fetch and execute](#instruction-fetch-and-execute)
    * [Interrupts](#interrupts)
      * [Interrupts and instruction cycle](#interrupts-and-instruction-cycle)
      * [Multiple interrupts](#multiple-interrupts)
    * [I/O function](#io-function)
  * [Interconnection structures](#interconnection-structures)
  * [Bus interconnection](#bus-interconnection)
    * [Bus structure](#bus-structure)
    * [Multiple-bus hierarchies](#multiple-bus-hierarchies)
    * [Elements of bus design](#elements-of-bus-design)
  * [Point-to-point interconnect](#point-to-point-interconnect)
    * [QPI physical layer](#qpi-physical-layer)
    * [QPI link layer](#qpi-link-layer)
    * [QPI routing layer](#qpi-routing-layer)
    * [QPI protocol layer](#qpi-protocol-layer)
  * [Peripheral component interconnect express (PCIe)](#peripheral-component-interconnect-express-pcie)
    * [PCI physical and logical architecture](#pci-physical-and-logical-architecture)
    * [PCIe physical layer](#pcie-physical-layer)
    * [PCIe data link layer](#pcie-data-link-layer)
    * [PCIe transaction layer](#pcie-transaction-layer)
* [Appendix](#appendix)
  * [Concepts](#concepts)
  * [Discussions](#discussions)
<!-- /TOC -->

# A top-level view of computer function and interconnection
## Computer components
**Types of programs**.
* *Hardwired program*. The process of connecting the various components in the desired configuration in the form of hardware
    * *Explain*. There is a small set of basic logic components which can be combined in various ways to store binary data, and perform arithmetic and logical operations on the data

        $\to$ If a particular computation to be performed, a configuration of logic components designed specifically for that computation could be constructed
* *Software*.
    * *Idea from hardware*. Construct a general-purpose configuration of arithmetic and logic functions

        $\to$ This set of hardware will perform various functions on data depending on control signals applied to the hardware
    * *General-purpose hardware*. The system accepts data and control signals and produces results

        $\to$ Instead of rewiring the hardware for eac new program, the programmer merely needs to supply a new set of control signals
    * *Control signal supply*.
        * *Essence of a program*. The entire program is actually a sequence of step
            * *Step execution*. At each step, some arithmetic or logic operation is performed on some data

                $\to$ For each step, a new set of contorl signals is needed
        * *Consequence*. We can provide a unique code for each possible set of control signals

            $\to$ We then add to the general-purpose hardware a segment which can accept a code and generate control signals
    * *Benefits*. Instead of rewiring the hardware for each new program

        $\to$ We will provide a new sequence of codes, each of which is, in effect, an instruction interpreted by the hardware to generate control signals

**General purpose hardware major components**. The following two components constitutes the CPU
* An instruction interpreter
* A module of general-purpose arithmetic and logic functions

**I/O components**. Data and instructions must be put into the system

$\to$ We need some sort of input module
* *I/O module*. Contain basic components for
    * Accepting data and instructions in some form and converting them into an internal form of signals usable by the system
    * Report results when needed
* *I/O memory*. An I/O module transfers data from external devices to CPU and memory, and vice versa

    $\to$ It contains internal buffers for temporarily holding these data until they can be sent on

**Main memory**.
* *Problems*.
    * An input device will bring instruction and data in sequentially, but a program is not invariably executed sequentially
    * Operations on data may require access to more than just one element at a time, in a predetermined sequence
* *Consequence*. There must be place to store temporarily both instructions and data

    $\to$ This place is memory, or main memory
* *Major internal register of main memory*.
    * *Memory address register (MAR)*. Specify the addresses in memory for the next read or write
    * *Memory buffer register (MBR)*. Contain the data to be written into memory, or receives the data read from memory
* *Memory structure*. Consist of a set of locations, defined by sequentially numbered addresses
    * *Memory cell*. Each location contains a binary number which can be interpreted as either an instruction or data

## Computer function
**Basic function of a computer**. Execute a program consisting of a set of instructions stored in memory

$\to$ The processor does the actual work by executing instructions specified in the program
* *Simplest form of instruction processing*. Consist of two steps
    * *Fetching*. The processor reads, i.e. fetches, instructions from memory one at a time
    * *Execution*. The processor executes each of the fetched instruction
* *Program execution*. Consist of repeating the process of instruction fetch and instruction execution

**Instruction cycle and fetch cycle**.

<div style="text-align:center">
    <img src="/media/U1N25wf.png">
    <figcaption>Basic instruction cycle</figcaption>
</div>

* *Instruction cycle*. The processing required for a single instruction
* *Fetch cycle*. The fetching step within an instruction cycle
* *Execute cycle*. The execution step within an instruction cycle

### Instruction fetch and execute
**Program counter (PC) (recall)**. The register holding the address of the instruction to be fetched next

>**NOTE**. Unless told otherwise, the processor always increments the PC after each instruction fetch, so that it will fetch the next instruction in sequence

**Instruction register (IR) (recall)**. The register holding the fetched instruction
* *Structure*. Contain bits specifying the action the processor is to take

**Fetch cycle**.
1. The processor fetches an instruction from memory using PC
2. The fetched instruction is loaded into IR
3. The processor interprets the instruction and performs the required action

**Types of actions**. An instruction's execution may involve a combination of the following actions
* *Processor-memory*. Data maybe transferred from processor to memory, or from memory to processor
* *Processor-I/O*. Data maybe transferred to or from a peripheral device, by transferring between the processor and an I/O module
* *Data processing*. The processor may perform some arithmetic or logic operation on data
* *Control*. An instruction may specify that the sequence of execution be altered

**Internal CPU registers for instruction execution**.

<div style="text-align:center">
    <img src="/media/xlhVKOb.png">
    <figcaption>Internal CPU registers</figcaption>
</div>

<div style="text-align:center">
    <img src="/media/8tNk0Nv.png">
    <figcaption>Example of program execution with opcodes</figcaption>
</div>

* *Examples of Opcodes*.
    * `0001` refers to *Load AC from memory*
    * `0010` refers to *Store AC to memory*
    * `0101` refers to *Add to AC from memory*

**Instruction cycle state diagram**.

<div style="text-align:center">
    <img src="/media/zOI7iDN.png">
    <figcaption>Instruction cycle state diagram</figcaption>
</div>

>**NOTE**. For any given instruction cycle, some states maybe NULL and others maybe visited more than once

* *Instruction address calculation (iac)*. Determine the address of the next instruction to be exected
    * *Typical implementation*. Add a fixed number to the address of the previous instruction
* *Instruction fetch (if)*. Read instruction from its memory location into the processor
* *Instruction operation decoding (iod)*. Analyze instruction to determine the type of operation to be performed, and operands to be used
* *Operand address calculation (oac)*. If the operation involves reference to an operand in memory, or available via I/O

    $\to$ Determine the address of the operand
* *Operand fetch (of)*. Fetch the operand from memory or read it in from I/O
* *Data operation (do)*. Perform the operation indicated in the instruction
* *Operand store (os)*. Write the result into memory or out to I/O

### Interrupts
**Interrupt**. Virtually all computers provide a mechanism, by which other modules, e.g. I/O or memory, may interrupt the normal processing of the processor
* *Common classes of interrupts*.
    * *Program interrupt*. Generated by some condition occurring as a result of an instruction execution
        * *Example*. Arithmetic overflow, division by zero, illegal machine instruction, or invalid memory access
    * *Timer interrupt*. Generated by a timer within the processor

        $\to$ This allows the OS to perform certain functions on a regular basis
    * *I/O interrupt*. Generated by an I/O controller, to signal normal completion of an operation, request service from the processor, or to signal a variety of error conditions
    * *Hardware failure*. Generated by a failure
* *Usage*. As a way to improve processing efficiency
    * *Explain*. Most external devices are much slower than the processor

        $\to$ We would like the processor to signal the device, then back to its work, then get result from device when things are done (via interrupts)

**I/O program**. Consist of the three sections below
* *A sequent of instructions*. To prepare for the actual I/O operation
    * *Example*.
        * Copy data to be output into a special buffer
        * Prepare the parameters for a device command
* *The actual I/O command*. Without interrupts, once this command is issued

    $\to$ The program must wait for the I/O device to perform the requested function, or periodically poll the device
* *Another sequence of instructions*. To complete the operation
    * *Example*. Set a flag indicating the success or failure of the operation

#### Interrupts and instruction cycle
**Interrupts under user perspective**. An interruption of the normal sequence of execution, i.e. when the interrupt processing is completed, execution resumes

$\to$ User program does not have to contain any special code to accommodate interrupts

>**NOTE**. THe processor and the OS are responsible for suspending the user program, then resuming it at the same point

**Interrupt cycle**.
* *Description*.
    1. The processor checks to see if any interrupts have occurred, indicated by the presence of an interrupt signal
    2. If no interrupt are pending, the processor proceeds to the fetch cycle and fetches the next instruction of the current program
    3. If an interrupt is pending, the processor does the following
        * Suspend execution of the current program being executed, and saves its context, i.e. the address of the next instruction to be executed, and relevant data
        * Set the PC to the starting address of an interrupt handler routine
    4. The processor proceeds to the fetch cycle and fetches the first instruction in the interrupt handler program, which will service the interrupt
    5. When the interrupt handler routine is completed

        $\to$ The processor can resume execution of the user program at the point of interruption
* *Overheads*.
    * Extra instructions must be executed to determine the nature of the interrupt, and to decide on the appropriate action
    * Due to the relatively large amount of time wasted by waiting on an I/O operation

        $\to$ The processor can be employed much more efficiently with the use of interrupts

**Interrupt handler**. A program which services a particular interrupt

>**NOTE**. Interrupt handlers are generally part of the OS

* *Typical functionality*. Determine the nature of the interrupt, and perform actions needed

#### Multiple interrupts
**Approaches for multiple interrupts**.

<div style="text-align:center">
    <img src="/media/q5AqArC.png">
    <figcaption>Interrupt processing approaches</figcaption>
</div>

* *Sequential interrupt processing*. Disable interrupts while an interrupt is being processed
    * *Explain*. The processor can and will ignore the interrupt request signal

        $\to$ If an interrupt occurs during this time, it generally remains pending and will be checked by the processor after the processor has enabled interrupts
    * *Consequence*. Interrupts are handled in strict sequential order
    * *Drawback*. Relative priority or time-critical needs are not taken into account
* *Nested interrupt processing*. Define priorities for interrupts and to allow an interrupt of higher priority to cause a lower-priority interrupt handler to be interrupted
    * *Interrupt service routine (ISR)*. Contain instructions for handling a particular type of interrupt

**Instruction cycle with interrupts**.

<div style="text-align:center">
    <img src="/media/PgpijGl.png">
    <figcaption>Instruction cycle with interrupts</figcaption>
</div>

### I/O function
**Data exchange of I/O module and processor**. Can exchange data directly with the processor, just as the processor can initiate a read or write with memory, designating the address of a specific location

$\to$ The processor can also read or write data to an I/O module

**Data exchange of I/O module and memory**. I/O exchanges can occur directly with memory
* *Explain*. The I/O module issues read or write commands to memory, relieving the processor of responsibility for the exchange

## Interconnection structures
**A computer as a network of basic modules**. A computer is a network of a basic modules

$\to$ There must be paths for connecting the modules

**Interconnection structure**. The collection of paths connecting the modules
* *Dependencies*. The design of this structure depends on the exchanges which must be made among modules

**Types of exchanges required**.

<div style="text-align:center">
    <img src="/media/mwhDkRM.png">
    <figcaption>Computer components</figcaption>
</div>

* *Memory*. Typically consist of $N$ words of equal length, each of which is assigned a unique numerical address
    * *Operations on words*. A word can be read or write from memory

        $\to$ This is indicated by read and write control signals
    * *Location of the operation*. Specified by an address
* *I/O module*. From an internal (to the computer system) point of view, I/O is functionally similar to memory
    * *Operations*. Read, write, and interrupts

    >**NOTE**. An I/O module may control more than one external device

    * *Port*. Each of the interfaces to an external device is a port, with a unique address

        >**NOTE**. There are external data paths for the I/O of data with an external device

* *Processor*.
    * *Operations*.
        * Read in instructions and data
        * Write out data after processing
        * Use control signals to control the overall operation of the system
        * Receive interrupt signals

**Must-supported types of transfers**.
* *Memory-to-processor*. The processor reads an instruction or unit of data from memory
* *Processor-to-memory*. The processor writes a unit of data to memory
* *I/O to processor*. The processor reads data from an I/O device via an I/O module
* *Processor to I/O*. The processor sends data to the I/O device
* *I/O to or from memory*. An I/O module is allowed to exchange data directly with memory, using direct memory access

**Most common types of interconnection structures**.
* The bus and various multiple-bus structures
* Point-to-point interconnection structures with packetized data transfer

## Bus interconnection
**A bus**. A communication pathway connecting two or more devices
* *Key characteristic*. A bus is a shared transmission medium
    * *Explain*. Multiple devices connect to the bus, and a signal transmitted by any one device is available for reception by all other devices attached to the bus
    * *Consequence*. If two devices transmit during the same time period, signal signals will overlap and become garbled

        $\to$ Only one device at a time can successfully transmit
* *Bus line*. A bus typically consists of multiple communication pathways, or lines
    * *Single line functionality*. Each line is capable of transmitting signals representing binary 1 and binary 0

        $\to$ Overtime, a sequence of binary digits can be transmitted across a single line
    * *Multiple line functionality*. Several lines of a bus can be used together to transmit binary digits simultaneously, in parallel
* *Types of bus*. A computer systems contain a number of different buses providing pathways between components at various levels of the computer hierarchy
    * *System bus*. A bus connecting major computer components, i.e. processor, memory, and I/O

    >**NOTE**. The most common computer interconnection structures are based on the use of one or more system buses

### Bus structure
**System bus structure**. Typically consist of 50 to 100s of separate lines, each of which is assigned a particular meaning or function

**Functional groups of bus lines**.

<div style="text-align:center">
    <img src="/media/ZBDHaYk.png">
    <figcaption>Bus interconnection scheme</figcaption>
</div>

* *Data lines*. Provide a path for moving data among system modules
    * *Data bus*. The collection of 32, 64, 128, or more seperate data lines
        * *Data bus width*. The number of data lines within a data bus
    * *Number of data lines implication*. The number of lines determines how many bits can be transferred at a time

        $\to$ The data bus width is a key factor in determining overall system performance
* *Address lines*. Used to designate the source or destination of the data on the data buse
    * *Functionality*. If the processor wishes to read a word, e.g. 8, 16, or 32 bits, of data from memory

        $\to$ It puts the address of the desired word on the address lines
    * *Address line width*. Determine the maximum possible memory capacity of the system, i.e. the amount of memory a system can address
        * *Example*. A system with a 32-bit address bus can address $2^{32}$ memory locations

            $\to$ If each memory location holds one byte, then the addressable memory space is 4GB
    * *Address lines for I/O ports*. Addres lines are generally also used to address I/O ports
    * *Bitstream structure on address lines*.
        * *Higher-order bits*. Used to select a particular module on the bus
        * *Lower-order bits*. Select a memory location or I/O port within the module
* *Control lines*. Used to control the access to and the use of the data and address lines
    * *Motivation*. The data and address lines are shared by all components

        $\to$ There must be a means of controlling their use
    * *Control signals*. Transmit both command and timing information among system modules
        * *Command signals*. Specify operations to be performed
        * *Timing signals*. Indicate the validity of data and address information
    * *Typical control lines*.
        * *Memory write*. Cause data on the bus to  be written into the addressed location
        * *Memory read*. Cause data from the addressed location to be placed on the bus
        * *I/O write*. Cause data on the bus to be ouptut to the addressed I/O port
        * *I/O read*. Cause the data from the addressed I/O port to be placed on the bus
        * *Transfer ACK*. Indicate that data have been accepted from or placed on the bus
        * *Bus request*. Indicate that a module needs to gain control of the bus
        * *Bus grant*. Indicate that a requesting module has been granted control of the bus
        * *Interrupt request*. Indicate that an interrupt is pending
        * *Interrupt ACK*. Acknowledge that the pending interrupt has been recognized
        * *Clock*. Used to synchronize operations
        * *Reset*. Initialize all modules
* *Power distribution lines*. Supply power to the attached modules

**Operation of the bus**.
* *Data sending procedure*. If one module wishes to send data to another, it must
    1. Obtain the use of the bus
    2. Transfer data via the bus
* *Data requesting procedure*. If one module wishes to request data from another, it must
    1. Obtain the use of the bus
    2. Transfer a request to the other module over the appropriate control and address lines
    3. Wait for the requested module to send the data

**Bus clock and data transfer rate**.
* *Bus clock pulse*. Common timing reference for all attached devices
    * *Unit of measurement*. Frequency measured in MHz
* *Bus cycle*. Time interval from one clock pulse to the next
* *Data transfer rate*. Measure of communication capacity
    * *Formula*. $\text{bus\_capacity} = \text{data\_transfer\_unit} \times \text{clock\_rate}$

**Bus protocol**. Govern format, content, timing of data, memory addresses, and control messages sent across bus
* *Approaches for access control*.
    * *Master-slave approach (traditional)*. CPU is bus master and all other devices are slaves
    * *Direct memory access (DMA)*. DMA controller gets data from device and stores in RAM
    * *Peer-to-peer buses*. Any device can become master via bus arbitration protocol

**Parallel and serial bus**.

<div style="text-align:center">
    <img src="/media/qMo56gn.png">
    <figcaption>Parallel and serial buses</figcaption>
</div>

* *Parallel bus (older)*. A bus is a connection of wires which devices plug into

    $\to$ Timing skew has become a problem with parallel bus design
* *Serialbus*. Interconnect one device after another and create a daisy-chain of devices

### Multiple-bus hierarchies
**Problem**. If a great number of devices are connected to the bus, performance will suffer
* *Main causes*.
    * The more devices attached to the bus, the greater the bus length, and hence the greater the propagation delay

        $\to$ This delay determines the time it takes for devices to coordinate the use of the bus
        * *Consequence*. When control of the bus passes from one device to another frequently

            $\to$ The propagation delays can noticeably affect perfomance
    * The bus may become a bottleneck as the aggregate data transfer demand approaches the capacity of the bus
        * *Temporarily solutions*.
            * Increase the data rate which can be carried by the bus
            * Use wider buses
        * *Drawback of temporarily solutions*. The data rates generated by attached devices are growing rapidly

            $\to$ This is a race, which a single bus is ultimately destined to lose
* *Consequence*. Most bus-based computer systems use multiple buses, generally laid out in a hierarchy

**Bus structures**.

<div style="text-align:center">
    <img src="/media/dtespX4.png">
    <figcaption>Example bus configurations</figcaption>
</div>

**Traditional bus structure**.
* *Local bus*. Connect the processor to a cache memory, and may support one or more local devices, e.g. main memory
    * *Cache memory controller*. Connect the cache to this local bus, and to a system bus, to which are attached all of the main memory modules
    * *Local bus in contemporary systems*. The cache is in the same chip as the processor

        $\to$ An external bus or other interconnect scheme is not required, although there also be an external cache
        * *Consequence*.
            * The use of cache structure insulates the processor from a requirement to access main memory frequently
            * Main memory can be move off of the local bus onto a system bus

                $\to$ I/O transfers to and from the memory across the system bus do not interfere with the processor's activity
    * *Definition from PCMag*. A local bus is the pathway between the CPU, memory, and peripheral controller chips

        <div style="text-align:center">
            <img src="/media/06Rw9by.png">
            <figcaption>Local bus</figcaption>
        </div>

* *Expansion buses*. It is possible to connect I/O controllers directly onto the system bus, but it results in performance problems

    $\to$ A more efficient solution is to make use of one or more expansion buses
    * *A expansion bus interface*. Buffer data transfers between the system bus and the I/O controllers on the expansion bus

        $\to$ The system can support a wide variety of I/O devices, and at the same time insulate memory-to-processor traffic from I/O traffic
* *Drawbacks*. Begin to break down as higher and higher performance is seen in the I/O devices

    $\to$ A common approach taken by industry is to use mezzanine architecture

**Mezzanine architecture**. Build a high-speed bus which is closely integrated with the rest of the system, requiring only a bridge between the processor's bus and the high-speed bus
* *Buses*.
    * *Local bus*. Connect the processor to a cache controller, which is in turn connected to a system bus which supports main memory
    * *High-speed bus*. Support connections to high-speed LANs, video, and graphic workstation controllers, as well as interface controllers to local peripheral buses, e.g. SCSI and FireWire
        * *Bridge (or buffering device)*. The cache controller is integrated into a bridge, or buffering device, which connects to the high-speed bus
    * *Expansion bus*. Support lower-speed devices, with an interface buffering traffic between the expansion bus and the high-speed bus
* *Advantage*. The high-speed bus brings high-demand devices into closer integration with the processor, and at the same time is independent of the processor

    $\to$ Differences in processor and high-speed bus speeds and signal line definitions are tolerated

### Elements of bus design
**Bus types**. Bus lines can be separated into two generic types
* *Dedicated bus line*. Permanently assigned either to one function or to a physical subset of computer components
    * *Example*. The use of separated dedicated address and data lines
* *Multiplexed bus line*. Use the same lines for multiple purpose
    * *Example*. Multiplex the address lines and data lines, i.e. the data transfer procedure is as following
        1. The address is placed on the bus, and the Address Valid line is activated
        2. At this point, each module has a specified period of time to copy the address and determine if it is the addressed module
        3. The address is then removed from the bus, and the same bus connections are used for the subsequent read or write data transfer
    * *Pros*. There are fewer bus lines, hence save space and usually cost
    * *Cons*.
        * More complex circuitry is required within each module
        * There is a potential reduction in performance since certain events sharing the same line cannot take place in parallel
* *Physical dedication*. The use of multiple buses, each of which connects only a subset of modules
    * *Pros*. High throughput, since there is less bus contention
    * *Cons*. Increase size and cost of the system

**Method of arbitration**. More than one module may need control of the bus, but only one unit at a time can successfully transmit over the bus

$\to$ Some method of arbitration is required
* *Methods of arbitration*.
    * *Centralized arbitration*. A single hardware device, i.e. a bus controller or arbiter, is responsible for allocating time on the bus

        >**NOTE**. The device maybe a separate module or part of the processor

    * *Distribution arbitration*. Each module contains access control logic and the modules act together to share the bus
* *Objective*. Designate one device, either the processor or an I/O module, as master

    $\to$ The master may then initiate a data transfer with some other device, which acts as slave for this particular exchange

**Timing**. The way in which events are coordinated on the bus
* *Bus clock*. Generally derived from the computer system clock, but is often slower than the master clock
    * *Example*. 66MHz buses are used in systems with a processor clock of over 500MHz
    * *Explain*. Memory access times are typically longer than processor clock cycles
* *Timing methods*.
    * *Synchronous timing*. The occurrence of events on the bus is determined by a clock
        * *Clock line*. The bus includes a clock line upon which a clock transmits a regular sequence of alternating 1s and 0s of equal durection
        * *Clock cycle (or bus cycle)*. A single 1-0 transmission, which defines a time slot
        * *Accessibility*. All other devices on the bus can read the clock line, and all events start at the beginning of a clock cycle
            * *Explain*. Synchronize start and finish events with clock

            >**NOTE**. Other bus signals may change at the leading edge of the clock signal, with a light reaction delay

            >**NOTE**. Most events occupy a single clock cycle

        * *Example*.

            <div style="text-align:center">
                <img src="/media/W186naz.png">
                <figcaption>Timing of synchronous bus operations</figcaption>
            </div>

            1. The processor places a memory address on the address lines during the first clock cycle, and may assert various status lines, i.e.
            2. Later, during the same clock cycle, once the address lines have stabilized, the processor issues an address enable signal
            3. For a read operation,
                * The processor issues a read command at the start of the second cycle
                * A memory module recognizes the address and, after a delay of one cycle, places the data on the data lines
                * The processor reads the data from the data lines and drops the read signal
            4. For a write operation,
                * The processor puts the data on the data lines at the start of the second cycle, and issues a write command after the data lines have stabilized
                * The memory module copies the information from the data lines during the third clock cycle
        * *Example from IDC online for reading operation*.
            1. At the raise of the clock signal, i.e. $t_0$, the processor places the device address on the address lines and sends an appropriate command on the control lines

                $\to$ Information travels over the bus at a speed determined by its physical and electrical characteristics

                >**NOTE**. The clock pulse width must be longer than the maximum propagation delay between two devices connected to the bus
                >$\to$ To allow all devices to decode the address and control signals so that the addressed device, i.e. the slave, can respond at the fall of the clock signal

                >**NOTE**. It is important that slaves take no action or place any data on the bus before the falling edge of the bus signal
                >* *Explain*. The information on the bus is unreliable during this period since signals are changing state

            2. The addressed slave places the requested input data on the data line at the falling edge of the clock signal, i.e. $t_1$
            3. At the end of the clock cycle, i.e. $t_2$, the master strobes the data on the data lines into its input buffer

                >**NOTE**. For the data to be loaded correctly into any storage device, the data must be available at the input of that device for a period greater than the setup time of the device
                >$\to$ $t_2 - t_1$ must be greater than the maximum propagation time on the bus, plus the setup time of the input buffer register of the master

            >**NOTE**. The exact times, at which a signal actually change state, are different from the ideal case due to propagation delays on bus wires, and in the circuits of the devices

    * *Asynchronous timing*. The occurrence of one event on a bus follows and depends on the occurrence of a previous event
        * *Memory reading example*.

            <div style="text-align:center">
                <img src="/media/x8C2nep.png">
                <figcaption>Timing of asynchronous bus operations</figcaption>
            </div>

            1. The processor places address and status signal on the bus
            2. After pausing for these signals to stabilize, it issues a read command, indicating the presence of valid address and control signals
            3. The appropriate memory decodes the address and responds by placing the data on the data line
            4. Once the data lines have stabilized, the memory module asserts the acknowledged line to signal the processor that the data are available
            5. Once the master has read the data from the data lines

                $\to$ It deasserts the read signal
            6. The memory module then drop the data and acknowledged lines
            7. Once the acknowledged line is dropped, the master removes the address information
        * *Memory writing example*.
            1. The master places data on the data lines at the same time that it puts signals on the status and address lines
            2. The memory module responds to the write command by copying the data from the data lines and then asserting the acknowledged line
            3. The master then drops the write signal and the memory module drops the acknowledged signal
* *Pros and cons of timing methods*.
    * *Synchronous timing*. Simpler to implement and test, but less flexible than asynchronous timing

        $\to$ The system cannot take advantage of advnces in device performance
    * *Asynchronous timing*. A mixture of slow and fast devices, using older and newer technology, can share a bus

## Point-to-point interconnect
**Drawbacks of shared bus architecture**. Contemporary systems increasingly rely on point-to-point interconnection rather than shared buses, which was the standard interconnection between main computer components for decades
* *Principal reason*. Electrical constraints encountered with increasing the frequency of wide synchronous buses
    * *Explain*.
        * At higher data rates, it becomes increasingly difficult to perform synchronization and arbitration functions in a timely fashion
        * With the advent of multicore chips, with multiple processors and significant memory on a single chip

            $\to$ The use of shared bus on the same chip magnified the difficulties of increasing bus data rate, and reducing bus latency to keep up with processors
* *Point-to-point interonnect versus shared buses*. Lower latency, higher data rate, and better scalability

**Quick path interconnect (QPI)**. Intel's interconnect architecture introduced in 2008
* *Significant characteristics*.
    * *Multiple direct connections*. Multiple components within the system enjoy direct pairwise connections to other components

        $\to$ This eliminates the need for arbitration found in shared transmission systems
    * *Layered protocol architecture*. As found in network environments, e.g. TCP/IP-based data networks, these processor-level interconnects use a layered protocol architecture
    * *Packetized data transfer*. Data are not sent as a raw bitstream

        $\to$ Data are sent as a sequence of packets, each of which includes control headers and error control codes
* *Typical use of QPI on a multicore computer*.

    <div style="text-align:center">
        <img src="/media/dTQnBNp.png">
        <figcaption>Multicore configuration using QPI</figcaption>
    </div>

    * *QPI links*. Form a switching fabric enabling data to move throughout the network
        * *Direct QPI connections*. Established between each pair of core processors
        * *Indirect QPI connections*. If a core needs to access the memory contorller in another core which is not directly connected

            $\to$ It sends its request through a path of cores which finally ends up at the memory controller of the desired core

        >**NOTE**. Larger systems with eight or more processors can be built using processors with three links and routing traffic through intermediate processors

    * *I/O hub (IOH)*. QPI is used to connect to an I/O module, called an I/O hub
        * *Functionality*. Act as a switch directing traffic to or from I/O devices
        * *PCI Express (PCIe)*. Typically in newer system, the link from the IOH to the I/O device controller uses an PCIe

            $\to$ The IOH translates between the QPI protocols and formats and the PCIe protocols and formats
    * *Main memory*. A core also links to a main memory module, i.e. typically the memory uses dynamic access random memory (DRAM), using a dedicated memory bus

**QPI layers**. QPI is a four-layer protocol architecture

<div style="text-align:center">
    <img src="/media/qZWHtF7.png">
    <figcaption>QPI layers</figcaption>
</div>

<div style="text-align: center">
    <img src="/media/P1jhDqv.png">
    <figcaption>QPI architecture layer</figcaption>
</div>

* *Physical layer*. Consist of actual wires carrying the signals, as well as circuitry and logic to support ancillary features required in the transmission and receipt of 1s and 0s
    * *Unit of transfers*. 20 bits, i.e. 1 Phit (physical unit)
* *Link layer*. Responsible for reliable transmission and flow control
    * *Unit of transfers*. 80 bit Flit (flow control unit)
* *Routing layer*. Provide the framework for directing packets through the fabric
* *Protocol layer*. The high-level set of rules for exchanging data packets between devices
    * *Packet*. Comprised of an integral number of Flits

### QPI physical layer
**QPI port architecture**.

<div style="text-align:center">
    <img src="/media/3bSXAEd.png">
    <figcaption>Physical interface of the Intel QPI interconnect</figcaption>
</div>

* *Architecture*. Consist of 84 individual links grouped as above
    * *Data path*. EAch path consists of a pair of wires which transmits data one bit at a time
        * *Data lane*. A pair of wires
            * *Number of lines*. 20 data lines in each direction, i.e. transmit and receive
        * *Clock lane*. There is one clock lane in each direction
    * *Lane quadrant*. The lans in each direction are grouped into four quadrants of 5 lanes each
* *Capacity*.
    * *Path width*. QPI is capable of transmitting 20 bits in parallel in each direction

        $\to$ The 20-bit unit is referred as a phit

        >**NOTE**. At some applications, the link can also operate at half or quarter widths to reduce power consumption or work around failures

        >**NOTE**. In practice, only 16 of 20 bits are for data transmission, the remaining 4 head bits are for debugging purpose

    * *Signaling speeds of the link in current products*. 6.4 GT/s, i.e. transfers per second

        $\to$ At 20-bits per transfer, which adds up to 16GB/s, and since QPI links involve dedicated bidirectional pairs, the total capacity is 32GB/s

**Differential signaling (or balanced transmission)**. The form of transmission on each lane
* *Balanced transmission*. Signals are transmitted as a current, i.e. electrical signal, traveling down one conductor and returns on the other
* *Binary value of data*. Depend on the voltage difference
    * *Typical setup*.
        * One line has a positive voltage value, and the other line has zero voltage
        * One line is associated with binary 1 and one line is associated with binary 0
* *Low-voltage differential signaling (LVDS)*. The technique used by QPI for binary data transmission
    * *Explain*.
        1. The transmitter injects a small current into one wire or the other, depending on the logic level to be sent
        2. The current passes through a resistor at the receiving end, and then returns in the opposite direction along the other wire
        3. The receiver senses the polarity of the voltage across the resistor to determine the logic level

**Multi-lane distribution**. Another function performed by the physical layer is managing the translation bewteen 80-biit flits and 20-bit phits using multiland distribution

<div style="text-align:center">
    <img src="/media/qbTMv0E.png">
    <figcaption>QPI multilane distribution</figcaption>
</div>

* *Idea*. The flits can be considered as a bit stream which is distributed across the data lanes in a round-robin fashion, i.e. first bit to first lane, second bit to second lane, etc.

    $\to$ This enables QPI to achieve very high data rates by implementing the physical link between two ports as multiple parallel channels

### QPI link layer
**Key functions**. Flow control and error control
* *Operation unit*. Flit (flow contorl unit)

**Flit structure**. 72-bit message payload and 8-bit error control code called cyclic redundacy check (CRC)
* *Flit payload*. May consist of data or message information
    * *Data flits*. Transfer the actual bits of data between cores or between a core and an IOH
    * *Message flits*. Used for functions such as flow control, error control, and cache coherence

**Flow control function**. Required to ensure that a sending QPI entity does not overwhelm a receiving QPI entity by sending data faster than the receiver can process the data and clear buffers for more incoming data
* *Idea*. Use a credit scheme
* *Implementation*.
    1. Durin initialization, a sender is given a set number of credits to send flits to a receiver
    2. Whenever a flit is sent to the receiver, the sender decrements its credit counters by one credit
    3. Whenever a buffer is freed at the receiver, a credit is returned to the sender for that buffer
* *Consequence*. The receiver controls that pace at which data is transmitted over a QPI link

**Error control function**. Occasionally, a bit transmitted at the physical layer is changed during transmission, due to noise or some other phenomenon

$\to$ The error control function detects and recovers from such bit errors, and thus isolates higher layers from experiencing bit errors
* *Procedure*.
    1. Each 80-bit flit includes an 8-bit CRC field, which is a function of the value of the remaining 72 bits

        $\to$ On transmission, sender calculates a CRC value for each flit and inserts that value into the flit
    2. When the flit is received, receiver calculates the CRC value for the 72-bit payload and compares this value with the one given by sender

        $\to$ If two values do not match, an error has been detected
    3. When receiver detects an error, it sends a request to sender to retransmit the error flit
    4. Since sender may have had sufficient credit to send a stream of flits, so that additional flits have been transmitted after the flit in error and before sender receives the request to retransmit

        $\to$ The request is for sender to backup and retransmit the damaged flit plus all subsequent flits

    >**NOTE**. This procedure is similar to how TCP/IP protocol handles data transmission error

###  QPI routing layer
**Key functions**. Determine the course which a packet will traverse across the available system interconnects

**Routing table**. Defined by the firmware and describe the possible paths which a packaet can follow

$\to$ This is similar to functionality of routers

### QPI protocol layer
**Key function**. Cache coherency protocol, i.e. deal with making sure that main memory values held in multiple caches are consistent

**Packet**. The unit of transfer of protocol layer
* *Content definition*. Standardized with some flexibility allowed to meet differing market segment requirements
* *Typical data packet payload*. A block of data being sent to or from a cache

## Peripheral component interconnect express (PCIe)

<div style="text-align: center">
    <img src="/media/TeWr1x3.jpg">
    <figcaption>PCIe illustration</figcaption>
</div>

**Peripheral component interconnect**. A popular high-bandwidth, processor-independent bus functioning as a mezzanine or peripheral bus
* *Advantages over other common bus specifications*. PCI delivers better system performance for high-speed I/O subsystems
* *Drawback*. Bus-based pCI scheme has not been able to keep pace with the data rate demands of attached devices

**Peripheral component interconnect express (PCIe)**. A new version of PCI, which is, as with QPI, a point-to-point interconnect scheme intended to replace bus-based schemes like PCI
* *Key requirements for PCIe*.
    * High capacity to support the needs of higher data rate I/O devices, e.g. Gigabit Ethernet
    * Support time-dependent data streams, i.e. to serve applications such as video-on-demand and audio redistribution
    * Data tagging so that an I/O system can prioritize its flow throughout the platform

### PCI physical and logical architecture
**PCIe typical configuration**.

<div style="text-align: center">
    <img src="/media/GoR3Wan.png">
    <figcaption>Typical configuration using PCIe</figcaption>
</div>

* *Chipset (or host bridge)*. A root complex device connecting the processor and memory subsystem to the PCIe switch fabric comprising one or more PCIe and PCIe switch devices
    * *Functionality*.
        * Act as a buffering device, to deal with difference in data rates between I/O controllers and memory and processor components
        * Translate the PCIe transaction formats and the processor and memory signal and control requirements
    * *PCIe support*. The chipset typically supports multiple PCIe ports
        * Some of PCIe ports  attach directly to a PCIe device
        * One or more ports attach to a switch managing multiple PCIe streams
    * *Types of devices attached to PCIe links from the chipset*.
        * *Switch*. The switch manages multiple PCIe streams
        * *PCIe endpoint*. An I/O device or controller implementing PCIe, e.g. a Gigabit Ethernet switch, a graphics, or video controller
        * *Legacy endpoint*. Intended for existing designs which have been migrated to PCIe

            $\to$ It allows legacy behaviors, e.g. use of I/O space and locked transactions, i.e. legacy mode

            >**NOTE**. PCIe endpoints are not permitted to require the use of I/O space at runtime, and must not use locked transactions
            >* *Consequence*. A system designer can restrict or eliminate legacy behaviors which have negative impacts on system performance and robustness

    * *PCIe/PCI bridge*. Allow older PCI devices to be connected to PCIe-based systems

**PCIe protocol architecture**.
* *Physical layer*. Consist of
    * The actual wires carrying the signals
    * Circuitry and logic to support ancillary features required in the transmission and receipt of the 1s and 0s
* *Data link layer*. Responsible for reliable transmission and flow control
    * *Data unit*. Data link layer packets (DLLPs)
* *Transaction layer*.
    * *Functionalities*.
        * Generate and consume data packets used to implement load / store data transfer mechanisms
        * Manage the flow control of the packets between the two components on a link
    * *Data unit*. Transaction layer packets (TLPs)
* *Software layer*. Generate read and write requests which are transported by the transaction layer to the I/O devices using a packet-based transaction protocol

### PCIe physical layer
**PCIe port**. Each PCIe port consists of a number of bi-directional lanes, rather than uni-directional lanes in QPI
* *Per-direction transfer method*. Use differential signaling over a pair of wires
* *Number of lanes of a PCI port*. 1, 4, 6, 16, or 32

**Multilane distribution**. PCIe uses multilane distribution technique
* *Data processing*. At each physical lane, data are buffered and processed 16 bytes, i.e. 128 bits, at a time
    * *128b/130b encoding*. Each block of 128 bits are encoded into a unique 130-bit codeword for transmission

        $\to$ The effective data rate of an individual lane is reduced by a factor of $128/130$
    * *Motivation*. The additional 2 bits are for synchronization purpose between transmitter and receiver, since PCIe does not use clock lanes for synchronization

### PCIe data link layer
**Purpose**. Ensure reliable delivery of packets across the PCIe link

**Data link layer packets**. Originate at the data link layer of a transmitting device and terminate at the DLL of the device on the other end of the link

<div style="text-align: center">
    <img src="/media/fwjtREE.png">
    <figcaption>The format of a DLLP</figcaption>
</div>

* *Types of DLLPs*.
    * *Flow control packets*. Regulate the rate, at which TLPs and DLLPs can be transmitted across a link
    * *Power management packets*. Used in managing power platform budgeting
    * *TLP ACK and NAK packets*. Used in TLP processing

**Transaction layer packet processing**.
1. The DLL adds two fields to the core of the TLP created by the TL
    * *Added fields*. These fields are processed at each intermediate node on the way from source to destination, whereas the core fields are only used at the destination TL
        * A16-bit sequence number
        * A 32-bit link-layer CRC (LCRC)
2. When a TLP arrives at a device, the DLL strips off the sequence number and LCRC fields and checks the LCRC
    * If no errors are detected, the core portion of the TLP is handed up to the local TL
        * If this receiving device is the intended destination, then the TL processes the TLP
        * Otherwise, the TL determines a route for the TLP and passes it back down to the DLL for transmission over the next link, on the way to the destination
    * If an error is detected, the DLL schedules an NAK DLL packet to return back to the remote transmitter, and the TLP is eliminated
3. When the DLL transmits a TLP, it retains a copy of the TLP
    * If the DLL receives an NAK for the TLP with this sequence number, it retransmits the TLP
    * When the receives an ACK, it discards the buffered TLP

### PCIe transaction layer
**Purpose**. Receive read and write requests from the software above the TL and create request packets for transmission to a destination via the link layer below

**Split transaction technique**. Used by most transactions
* *Procedure*.
    1. A request packet is sent out by a source PCIe device, which then waits for a response, i.e. a completion packet
    2. Completion packet following a request is initiated by the completer only when it has the data and / or status ready for delivery
        * *Packet identifier*. Each packet has a unique identifier enabling completion packets to be directed to the correct originator
* *Pros*. The completion is separated in time from the request, while in typical bus operation, both sides of a transaction must be available to seize and use the bus

    $\to$ Other PCIe traffic may use the link between the request and the completion
* *Posted transactions*. TL messages and some write transactions, which expect no response

**Address spaces and transaction types**. The TL supports several address spaces
* *Memory*. The memory space includes system main memory, and PCIe I/O devices, with certain ranges of memory addresses map into I/O device
* *I/O*. Used for legacy PCI devices, with reserved memory address ranges used to address legacy I/O devices
* *Configuration*. Enable the TL to read and write configuration registers associated with I/O devices
* *Message*. For control signals related to interrupts, error handling, and power management

**Types of transaction**.

<div style="text-align: center">
    <img src="/media/1eRmG47.png">
    <figcaption>PCIe TLP transaction types</figcaption>
</div>

* *Locked operations*. Occur as a result of device drivers requesting atomic access to registers on a PCIe device
    * *Explain*. A device driver can atomically read, modify, and then write to a device register
* *Implementation of locked operation*.
    1. The device driver causes the processor to execute an instruction or set of instructions
    2. The root complex converts these processor instructions into a sequence of PCIe transactions, which perform individual read and write requests for the device driver
    3. If the transactions must be executed atomically, the root complex locks the PCIe link while execute the transactions

        $\to$ This prevents transactions which are not part of the sequence from occurring
* *The set of processor instructions which can cause a lcoked operation*. Depend on the system chip set and processor architecture

**TLP packet assembly**.

<div style="text-align: center">
    <img src="/media/sXl3b60.png">
    <figcaption>Transaction layer packet</figcaption>
</div>

**Data from upper layer software**. Information required for the TL to create the core of the TLP, which consists of
* *Header*. Describe the type of packet and include information required by the receiver to process the packet, i.e. any required routing information
* *Data*. A data field of up to 4096 bytes may be included in the TLP

    >**NOTE**. Some TLPs do not contain a data field

* *ECRC*. An optional end-to-end CRC field enabling the destination TL layer to check for errors and the Header and Data portion of the TLP

**Example of TLP header format for a memory request transaction**.
* *Length*. Length of the data field in double words (DW), where 1 DW = 4 bytes
* *Attributes*. Consist of two bits
    * *Relaxed ordering bit*. Indicate whether strict or related ordering is used
        * *Relaxed ordering*. A transaction may be completed prior to other transactions which were already queued
    * *No-snoop bit*. When set, indicate that no cache coherency issues exist w.r.t this TLP
* *EP*. Poisoned data bit, i.e. if set, indicate the data in this TLP should be considered invalid, although the transaction is beign allowed to complete normally
* *TE*. TLP digest field present, i.e. if set, indicate that the ECRC field is present
* *Traffic class*. A 3-bit traffic class can be assigned to a traffic flow to enable PCIe to prioritize service
* *Type, format*. Occupy 7 bits. Specify transaction type, header size, and whether a data field is present
* *First DW byte enables*. Occupy 4 bits. Indicate whether the corresponding byte in the first DW is valid
    * *Usage*. Have effect of allowing smaller transfers which a full DW and offetting the start and end addresses from the DW boundary
* *Last DW byte enables*. Occupy 4 bits. Indicate whether the corresponding byte in the last DW is valid
    * *Usage*. Have effect of allowing smaller transfers which a full DW and offetting the start and end addresses from the DW boundary

# Appendix
## Concepts
**Ground (or earth) in electrical engineering**.
* *Definitions*.
    * The reference point in an electrical circuit, from which voltages are measured
    * A common return path for electric current, or
    * A direct physical connection to the earth
* *Ground as a reference point in an electrical circuit*. By convention, we define a point in a circuit as a reference point, i.e. the ground

    $\to$ This point carries a voltage of 0V
    * *Explain*. Voltage measurements are relative measurements, i.e. a voltage measurement must be compared to another point in the circuit

        $\to$ Otherwise, the measurement is meaningless

**Single-ended and differential signaling**.
* *Single-ended signaling*.

    <div style="text-align:center">
        <img src="/media/wWjTGb6.png">
        <figcaption>Single-ended signaling</figcaption>
        <figcaption>The orange triangles denote the amplifiers</figcaption>
    </div>

    * *Procedure*.
        1. The electrical signal is transmitted by a voltage (often a varying voltage)

            $\to$ This voltage is referenced to a fixed potential, usually a 0V node referred to as "ground"
        2. A conductor carries the signal and one conductor carries the common reference potential

            $\to$ The current associated with the signal travels from the sender to the receiver and returns to the power supply through the ground connection

            >**NOTE**. In this case, the power supply is the ground point of

    * *Multiple signal transmission*. If multiple signals are transmitted

        $\to$ The circuit will require one conductor for each signal, plus one shared ground connection
* *Differential signaling*. Use two complementary voltage signals to transmit one information signal

    $\to$ One information signal requires a pair of conductors, one carrying the signal and one carrying the inverted signal

    <div style="text-align:center">
        <img src="/media/qRNBKxo.png">
        <figcaption>Single-ended and differential-ended generic timing diagram</figcaption>
    </div>

    <div style="text-align:center">
        <img src="/media/O0CwP2j.png">
        <figcaption>Differential signaling</figcaption>
    </div>

    <div style="text-align:center">
        <img src="/media/Uu8rHtj.png">
        <figcaption>Differential-ended signaling (another illustration)</figcaption>
        <figcaption>The ground point is denoted as the pyramid pointing downwards</figcaption>
    </div>

    * *Receiver information extraction*. The receiver extracts information by detecting the potential difference between the inverted and non-inverted signals
    * *Balanced voltage*.
        * The inverted and non-inverted signals are balanced, i.e. they have equal amplitude and opposiute polarity relative to a common-mode voltage
        * The return currents associated with these voltages are also balanced and thus cancel each other out

            $\to$ Differential signals have ideally zero current flowing through the ground connection
    * *Ground reference*. The sender and receiver do not necessarily share a common ground reference
    * *Implementation*. Reference [here](https://electronics.stackexchange.com/questions/448221/where-does-return-current-flow-for-a-differential-signal)

        <div style="text-align:center">
            <img src="/media/uTnFGAK.png">
            <figcaption>Differential-ended signaling (another illustration)</figcaption>
        </div>


        <div style="text-align:center">
            <img src="/media/iQRRi2C.png">
            <figcaption>Differential-ended signaling (another illustration)</figcaption>
        </div>

        * *Option 1*. Route two tracks on a printed circuit board (PCB), not close together but matched in length, then use them as a differential pair

            $\to$ Each of the individual lines will have its own return current on a nearby ground plane
        * *Option 2*. Make a closely coupled pair of tracks on the PCAm with the line-to-line spacing less than the line-to-ground spacing

            $\to$ Most of the return current will be in the complementary line, but there will be likely some return current in the ground plane near each line
        * *Option 3*. Make a purely differential line, and the only return current for one line will be in the other and vice versa
    * *Pros*.
        * No return current, i.e. we ideally have no return current, thus the ground reference becomes less important

            $\to$ The ground potential can even be different at the sender and receiver or moving around within a certain acceptable range
        * Lower-voltage operation, i.e. single-ended signals must maintain a relatively high voltage to ensure adequate signal-to-noise ratio, due to the differences in ground reference in receiver and sender

            $\to$ Due to the improved resistance to noise, differential signals can use lower voltages and still maintain adequate SNR
* *Pros and cons of signaling methods*.
    * *Single-ended signaling*. Simpler and use less wires
    * *Differential signaling*. Less power consumption, robust to noise, but require more wires

## Discussions
**QPI model (computer architecture) and Open system interconnection (OSI) layers (networking)**.
* *OSI model*. A conceptaul model which characterizes and standardizes the communication functions of a telecommunication or computing system regardless of its underlying internal structure and technology
    * *Objective*. The interoperability of diverse communication systems with standard communication tools
    * *Idea*. Partition the flow of data in a communication system into seven abstraction layers
    * *History*. OSI model was developed starting in the late 1970s to support the emergence of the diverse computer networking methods, which were competing for aplication in the large national networking efforts in France, the UK, and the US
    * *Layer architecture*.
        * *Application layer*. High-level APIs, including resource sharing, remote file access, etc.
            * *Protocol data unit (PDU)*. Data
        * *Presentation layer*. Translation of data between a networking service and an application, including character encoding, data compression, and encryption / decryption
            * *Protocol data unit (PDU)*. Data
        * *Session layer*. Manage communication sessions, i.e. continuous exchange of information in the form of multiple back-and-forth transmissions between two nodes
            * *Protocol data unit (PDU)*. Data
        * *Transport layer*. Reliable transmission of data segments between points on a network, including segmentation, acknowledgement, and multiplexing
            * *Protocol data unit (PDU)*. Segment, datagram
        * *Network layer*. Structure and manage a multi-node network, including addressing, routing, and traffic control
            * *Protocol data unit (PDU)*. Packet
        * *Data link layer*. Reliable transmission of data frames between two nodes connected by a physical layer
            * *Protocol data unit (PDU)*. Data frame
        * *Physical layer*. Transmission and reception of raw bit streams over a physical mediu
            * *Protocol data unit (PDU)*. Bit, symbol
* *OSI model and QPI model*. The layers of thw two models corresponds to each other as following

    <center>

    | QSI layer | OSI layer |
    | --- | --- |
    | Protocol layer | Network layer |
    | Routing layer | Network layer |
    | Link layer | Data link layer |
    | Physical layer | Physical layer |

    </center>

**Legacy (computing)**. Describe old software or hardware which is outdated but still in use
* *Translation to Vietnamese*. Thiết bị kế thừa
* *Legacy mode*. A state, in which a computer system, component, or software application behaves in a different way from its standard operation to support older software, data, or expected behavior
    * *Difference from backward compatibility*. An item legacy mode  often sacrifice newer features or performance, or be unable to access data or run programs it normally could

        $\to$ This is to provide continued access to older data or functionality

**Address spaces**.
* *I/O space*. I/O address space containing addresses to I/O ports
    * *References*. http://www.scs.stanford.edu/05au-cs240c/lab/i386/s08_01.htm
* *Instruction and data space*. Address space for instruction and data

**Address spaces in PCIe**.
* *PCI configuration space (or extended configuration space in PCIe)*. A set of registers, which are mapped to memory locations such as the I/O address space of the CPU
    * *Space size*. A PCI device has a 256 byte configuration space, which is extended to 4KB for PCIe
        * *Explain*. This space consumes memory addresses from the system memory map, i.e. they are mapped into the system memory map, but the actual bits are generally implemented in registers on the peripheral device
    * *Example*. When we read the Vendor ID or Device ID, the target peripheral device will return the data, even though the memory address being used is from the system memory map
* *Memory-mapped I/O*. Use the same address space to address both memory and I/O devices

    <div style="text-align: center">
        <img src="/media/A8jYLkU.png">
        <figcaption>Memory-mapped I/O bus system</figcaption>
    </div>

    * *Example*. A LED light system requests for 1MB of memory-mapped space, and the BIOS assigns it address `0x10000000` to `0x10100000`

        $\to$ This is not consuming physical RAM, just address space

        >**NOTE**. This is way 32-bit systems run into issues with expansion cards like high-end GPUs with GB of RAM

* *Port-mapped I/O (or isolated I/O) and I/O space*. Behave similarly as memory-mapped I/O, except that I/O space operates in a separate memory space, i.e. the x86 I/O space

    <div style="text-align: center">
        <img src="/media/V9nSdAX.png">
        <figcaption>Port-mapped I/O bus system</figcaption>
    </div>

    * *Explain*.
        * Use a special class of CPU instructions designed specifically for performing I/O
        * I/O devices have a separate address space from generally memory
    * *Example*. Address `0x3F8` exists in both I/O space and memory space but are two different things
    * *Implementation*. Use an extra I/O pin on the CPU's physical interface, or an entire bus dedicated to I/O

**Atomic transaction**. The purpose of making transactions atomic is to prevent different transactions from intefering with one another
* *Atomic operation*. If an operation is atomic, it is guaranteed to complete without interrupted once it begins

**Pros and cons of buses and point-to-point interconnections**.
* *Bus interconnections*.
    * *Pros*.
        * *Versatility*.
            * New devices can be added easily
            * Peripherals can be moved between computer system using the same bus standard
        * *Low cost*. A single set of wires is shared in multiple ways
        * *Manageable complexity*. Complexity can be managed by partitioning the design
    * *Cons*.
        * *Communication bottleneck*. The bandwidth of the bus can limit the maximum I/O throughput
        * *Limited speed*. Maximum bus speed is largely limited by
            * Bus length and the number of devices on the bus
            * The wide range of devices to be supported, i.e. widely varying latencies and widely varying data transfer rates
* *PCIe*.
    * *Pros*. Larger bandwidth, better speed, more utility, and less latency
    * *Cons*. Greater power consumption, and support only newer device
