<!-- TOC titleSize:1 tabSpaces:2 depthFrom:1 depthTo:6 withLinks:1 updateOnSave:1 orderedList:0 skip:0 title:1 charForUnorderedList:* -->
# Table of Contents
- [Table of Contents](#table-of-contents)
- [Parallel processing](#parallel-processing)
  - [Multiple processor organizations](#multiple-processor-organizations)
    - [Types of parallel processor systems](#types-of-parallel-processor-systems)
    - [Parallel organizations](#parallel-organizations)
  - [Symmetric multiprocessors](#symmetric-multiprocessors)
    - [Organization](#organization)
    - [Multiprocessor OS design considerations](#multiprocessor-os-design-considerations)
  - [Cache coherence and the MESI protocol](#cache-coherence-and-the-mesi-protocol)
    - [Software solutions](#software-solutions)
    - [Hardware solutions](#hardware-solutions)
      - [Directory protocols](#directory-protocols)
      - [Snoopy protocols](#snoopy-protocols)
    - [The MESI protocol](#the-mesi-protocol)
  - [Multithreading and chip multiprocessors](#multithreading-and-chip-multiprocessors)
    - [Implicit and explicit multithreading](#implicit-and-explicit-multithreading)
    - [Approaches to explicit multithreading](#approaches-to-explicit-multithreading)
    - [Example systems](#example-systems)
  - [Clusters](#clusters)
    - [Cluster configurations](#cluster-configurations)
    - [OS design issues](#os-design-issues)
    - [Cluster computer architecture](#cluster-computer-architecture)
    - [Blade servers](#blade-servers)
    - [Clusters compared to SMP](#clusters-compared-to-smp)
  - [Nonuniform memory access](#nonuniform-memory-access)
    - [Motivation](#motivation)
    - [Organization](#organization-1)
    - [NUMA pros and cons](#numa-pros-and-cons)
  - [Vector computation](#vector-computation)
    - [Approaches to vector computation](#approaches-to-vector-computation)
      - [Methods for vector computation](#methods-for-vector-computation)
      - [Processor organization for vector computation](#processor-organization-for-vector-computation)
  - [IBM 3090 vector facility](#ibm-3090-vector-facility)
    - [Organization](#organization-2)
    - [Registers](#registers)
    - [Compound instructions](#compound-instructions)
    - [Instruction set](#instruction-set)
<!-- /TOC -->

# Parallel processing
## Multiple processor organizations
### Types of parallel processor systems
**Traditional computer as a sequential machine**. Most computer programming languages require the programmer to specify algorithms as sequences of instructions

$\to$ Processors execute programs by executing machine instructions in a sequence and one at a time
* *Instruction execution*. Each instruction is executed in a sequence of operations, i.e. fetch instruction, fetch operands, perform operation, store results

**Parallelism**. The view of the computer as a sequential machine has never been entirely true
* *Examples*. The following examples are performing functions in parallel
    * At the micro-operation level, multiple control signals are generated at the same time
    * Instruction pipelining, at least to the extent of overlapping fetch and execute operations, has been around for a long time
* *Superscalar organization*. Parallelism is taken further with superscalar organization, which exploits instruction-level parallelism
    * *Idea*. Have multiple execution units within a single processor
        
        $\to$ These may execute multiple instructions from the same program in parallel
* *Parallelism in modern computers*. As computer technology has evolved, and as the cost of computer hardware has dropped
    
    $\to$ Computer designers have sought more and more opportunities for parallelism, usually to enhance performance and, in some cases, to increase availability

**Some prominent approaches to parallel organization**. 
* Symmetric multiprocessors (SMPs), i.e. one of the earliest and still the most common example of parallel organization
    * *SMP organization*. Multiple processors share a common memory
    * *Drawback*. This organization raises the issue of cache coherence
* Multithreaded processors and chip multiprocessors
* Clusters, which consist of multiple independent computers organized in a cooperative fashion
    * *Usage*. Increasingly common to support workloads, which are beyond the capacity of a single SMP
* *Nonuniform memory access (NUMA) machines*. Relatively new and not yet proven in the marketplace
    
    $\to$ This is often considered as an alternative to the SMP or cluster approach

**Hardware organizational approaches to vector computation**. These approaches optimize the ALU for processing vectors or arrays of floating-point numbers

$\to$ They are common on supercomputers

### Parallel organizations
**Taxonomy of parallel processor architectures**.

<div style="text-align:center">
    <img src="https://i.imgur.com/GvGbt9Q.png">
    <figcaption>Taxonomy of parallel processor architectures</figcaption>
</div>

* *General organization of the taxonomy of parallel processor architectures*.

    <div style="text-align:center">
        <img src="https://i.imgur.com/MmYFYD6.png">
        <figcaption>Alternative computer organizations</figcaption>
    </div>

* *SISD*. 
    1. The control unit (CU) provides an instruction stream (IS) to a processing unit (PU)
    2. The PU operates on a single data stream (DS) from a memory unit (MU)
* *SIMD*. There is one CU feeding an IS to multiple PUs
    * *Distributed memory*. Each PU may have its own dedicated memory
    * *Shared memory*. All PU have a a shared memory
* *MIMD*. There are multiple CUs, each feeding a separate IS to its own PU
    
    $\to$ The MIMD may be a shared-memory multiprocessor, or a distributedmemory multicomputer

**Design issues relating to SMPs, clusters, and NUMAs**. Complex, involving issues relating to 
* Physical organization
* Interconnection structures
* Interprocessor communication
* OS design
* Application software techniques

>**NOTE**. Our concern here is primarily with organization, although we touch briefly on OS design issues

## Symmetric multiprocessors
**Brief**.
* *Appearance of SMP*. 
    * Until fairly recently, virtually all single-user personal computers and most workstations contained a single general-purpose microprocessor
    * As demands for performance increase and as the cost of microprocessors continues to drop, vendors have introduced systems with an SMP organization
* *"SMP"*. Refer to a computer hardware architecture and the OS behavior reflecting that architecture

**Definition of SMP as a standalone computer system**. 
* *SMP as a standalone computer system*. An SMP can be defined as a standalone computer system with the following characteristics
    * There are two or more similar processors of comparable capability
    * These processors share the same main memory and I/O facilities
    * There processors are interconnected by a bus or other internal connection scheme
        
        $\to$ They are interconnected so that memory access time is approximately the same for each processor
    * All processors share access to I/O devices, either through the same channels or through different channels providing paths to the same device
    * All processors can perform the same functions, hence the term symmetric
    * The system is controlled by an integrated OS providing interaction between processors and their programs at the job, task, file, and data element levels
* *Contrasts with a loosely coupled multiprocessing system*. The last characteristic illustrates one of the contrasts with a loosely coupled multiprocessing system, e.g. a cluster
    * *Cluster*. The physical unit of interaction is usually a message or complete file
    * *SMP*. Individual data elements can constitute the level of interaction, and there can be a high degree of cooperation between processes
        * *Explain*. The OS of an SMP schedules processes or threads across all of the processors

**Benefits over uniprocessor organization**.
* *Performance*. If the work to be done by a computer can be organized so that some portions of the work can be done in parallel
    
    $\to$ A system with multiple processors will yield greater performance than one with a single processor of the same type
* *Availability*. In a SMP, since all processors can perform the same functions, the failure of a single processor does not halt the machine
    
    $\to$ Instead, the system can continue to function at reduced performance
* *Incremental growth*. A user can enhance the performance of a system by adding an additional processor
* *Scaling*. Vendors can offer a range of products with different price and performance characteristics based on the number of processors configured in the system

>**NOTE**. These are potential, rather than guaranteed, benefits
>
>* *Explain*. The OS must provide tools and functions to exploit the parallelism in an SMP system

**Parallel transparency**. The existence of multiple processors is transparent to the user

$\to$ The OS takes care of scheduling of threads or processes on individual processors and of synchronization among processors

### Organization
**Organization**.

<div style="text-align:center">
    <img src="https://i.imgur.com/h3xZttI.png">
    <figcaption>Generic block diagram of tightly coupled multiprocessor</figcaption>
</div>

* *Processors*. There are two or more processors
    * *Processor components*. Each processor is self-contained, including a control unit, ALU, registers, and, typically, one or more levels of cache
    * *Memory access*. Each processor has access to a shared main memory and the I/O devices through some form of interconnection mechanism
    * *Processor-processor interconnection*.
        * *Via memory*. he processors can communicate with each other through memory, where messages and status information left in common data areas
        * *Via direct signals*. Processors may exchange signals directly
* *Memory*. Often organized so that multiple simultaneous accesses to separate blocks of memory are possible
    * *Processor-specific main memory*. Each processor may have its own private main memory and I/O channels in addition to the shared resources

**Time-shared bus**. The most common organization for personal computers, workstations, and servers

$\to$ This is the simplest mechanism for constructing a multiprocessor system

<div style="text-align:center">
    <img src="https://i.imgur.com/UnD9gRM.png">
    <figcaption>Symmetric multiprocessor organization</figcaption>
</div>

* *Structure and interfaces*. Basically the same as for a single-processor system using a bus interconnection
    * *Bus structure*. Consist of control, address, and data lines
* *Features facilitating DMA transfers from I/O subsystems to processors*.
    * *Addressing*. It must be possible to distinguish modules on the bus to determine the source and destination of data
    * *Arbitration*. Any I/O module can temporarily function as master
        
        $\to$ A mechanism is provided to arbitrate competing requests for bus control, using some sort of priority scheme
    * *Time-sharing*. When one module is controlling the bus
        * Other modules are locked out, and
        * Other modules must, if necessary, suspend operation until bus access is achieved
* *Uniprocessor features directly used in SMP organization*. In SMP, there are multiple processors and multiple I/O processors attempting to gain access to one or more memory modules via the bus

**Pros and cons**.
* *Pros*.
    * *Simplicity*. This is the simplest approach to multiprocessor organization
        * *Explain*. The physical interface and the addressing, arbitration, and time-sharing logic of each processor remain the same as in a single-processor system
    * *Flexibility*. It is generally easy to expand the system by attaching more processors to the bus
    * *Reliability*. The bus is essentially a passive medium, and the failure of any attached device should not cause failure of the whole system
* *Cons*. Low performance
    * *Explain*. All memory references pass through the common bus
        
        $\to$ The bus cycle time limits the speed of the system

**Cache**. To improve performance of time-shared bus, we can equip each processor with a cache memory to reduce the number of bus accesses
* *Levels of cache*. Typically, workstation and PC SMPs have two levels of cache
    * The L1 cache is internal, i.e. same chip as the processor
    * The L2 cache either internal or external
    * Some processors now employ a L3 cache
* *Related design considerations*. The use of caches introduces some new design considerations
    * *Cache coherency*. Since each local cache contains an image of a portion of memory, if a word is altered in one cache
        
        $\to$ It could conceivably invalidate a word in another cache
        * *Solution*. The other processors must be alerted that an update has taken place

### Multiprocessor OS design considerations
**SMP OS**. Nanage processor and other computer resources so that the user perceives a single OS controlling system resources

$\to$ Such a configuration should appear as a single-processor multiprogramming system
* *Explain*.
    * In both the SMP and uniprocessor cases, multiple jobs or processes may be active at one time
        
        $\to$ The OS must schedule their execution and to allocate resources
    * A user may construct applications using multiple processes or multiple threads within processes
        
        $\to$ He should not concern about whether one processor or multiple processors will be available
* *Conclusion*. A multiprocessor OS must provide all the functionality of a multiprogramming system plus additional features to accommodate multiple processors

**Key design issues**.
* *Simultaneous concurrent processes*. OS routines need to be reentrant to allow several processors to execute the same IS code simultaneously
    * *Idea*. With multiple processors executing the same or different parts of the OS
        
        $\to$ OS tables and management structures must be managed properly to avoid deadlock or invalid operations
* *Scheduling*. Any processor may perform scheduling, hence conflicts must be avoided
    
    $\to$ The scheduler must assign ready processes to available processors
* *Synchronization*. With multiple active processes having potential access to shared address spaces or shared I/O resources
    
    $\to$ Care must be taken to provide effective synchronization
    * *Synchronization*. A facility that enforces mutual exclusion and event ordering
* *Memory management*. 
    * Memory management on a multiprocessor must deal with all of the issues found on uniprocessor machines
    * The OS needs to exploit the available hardware parallelism, e.g. multiported memories, to achieve the best performance
    * The paging mechanisms on different processors must be coordinated to enforce consistency when several processors share a page or segment and to decide on page replacement
* *Reliability and fault tolerance*. The OS should provide graceful degradation in the face of processor failure
    
    $\to$ The scheduler and other portions of the OS must recognize the loss of a processor and restructure management tables accordingly

## Cache coherence and the MESI protocol
**Cache coherence problem**. Multiple copies of the same data can exist in different caches simultaneously

$\to$ If processors are allowed to update their own copies freely, an inconsistent view of memory can result

**Common write policies**.
* *Write back*. 
    * *Idea*.
        * Write operations are usually made only to the cache
        * Main memory is only updated when the corresponding cache line is flushed from the cache
    * *Drawback*. This can result in inconsistency, i.e. if two caches contain the same line, and the line is updated in one cache
        
        $\to$ The other cache will unknowingly have an invalid value
        * *Consequence*. Subsequent reads to that invalid line produce invalid results
* *Write through*. All write operations are made to main memory and the cache, ensuring that main memory is always valid
    * *Drawback*. Inconsistency can occur unless other caches monitor the memory traffic or receive some direct notification of the update

**MESI (modified/exclusive/shared/invalid) protocol**. The most widely used approach to cache coherence problem

$\to$ A version of this protocol is used on both the Pentium 4 and PowerPC implementations
* *Objective*. For any cache coherence protocol, the objective is to 
    * Let recently used local variables get into the appropriate cache and stay there through numerous reads and write, and
    * Use the protocol to maintain consistency of shared variables that might be in multiple caches at the same time
* *Types of cache coherence approaches*. Generally divided into software and hardware approaches
    
    >**NOTE**. Some implementations adopt a strategy that involves both software and hardware elements
    
    >**NOTE**. The classification into software and hardware approaches is still instructive and is commonly used in surveying cache coherence strategies

### Software solutions
**Brief**. Software cache coherence schemes attempt to avoid the need for additional hardware circuitry and logic by relying on the compiler and OS to deal with the problem
* *Pros*. 
    * The overhead of detecting potential problems is transferred from run time to compile time
    * The design complexity is transferred from hardware to software
* *Cons*. Compiletime software approaches generally must make conservative decisions, leading to inefficient cache utilization

**Compiler-based coherence mechanisms**. 
* *Idea*.
    1. Perform an analysis on the code to determine which data items may become unsafe for caching

        $\to$ Those items are marked accordingly
    2. The OS or hardware prevents noncacheable items from being cached
* *Simplest approach*. Prevent any shared data variables from being cached
    * *Idea*. Since a shared data structure may be exclusively used during some periods and may be effectively read-only during other periods
        
        $\to$ It is only during periods, when at least one process may update the variable and at least one other process may access the variable, that cache coherence is an issue
* *More efficient approaches*. Analyze the code to determine safe periods for shared variables
    
    $\to$ The compiler then inserts instructions into the generated code to enforce cache coherence during the critical periods

### Hardware solutions
**Hardware-based solutions (or cache coherence protocols)**. Provide dynamic recognition at run time of potential inconsistency conditions
* *Benefits*. 
    * Since the problem is only dealt with when it actually arises
        
        $\to$ There is more effective use of caches, leading to improved performance over a software approach
    * These approaches are transparent to the programmer and the compiler, reducing the software development burden
* *Types of protocols*. Differ in a number of particulars, including where the state information about data lines is held, how that information is organized, where coherence is enforced, and the enforcement mechanisms
    
    $\to$ Generally hardware schemes can be divided into two categories, i.e. directory protocols and snoopy protocols

#### Directory protocols
**Directory protocols**. Collect and maintain information about where copies of lines reside
* *Implementation*. Typically, there is 
    * A centralized controller, which is part of the main memory controller
        * The controller checks and issues necessary commands for data transfer between memory and caches or between caches
        * The controller keeps the state information up to date, hence every local action that can affect the global state of a line must be reported to the central controller
        * The controller maintains information about which processors have a copy of which lines
    * A directory that is stored in main memory to store global state information about the contents of the various local caches
* *Work flow*. 
    1. Before a processor can write to a local copy of a line, it must request exclusive access to the line from the controller
    2. Before granting this exclusive access, the controller sends a message to all processors with a cached copy of this line
        
        $\to$ This forces each processor to invalidate its copy
    3. After receiving acknowledgments back from each processor, the controller grants exclusive access to the requesting processor
    4. When another processor tries to read a line, which is exclusively granted to another processor
        
        $\to$ It will send a miss notification to the controller
    5. The controller issues a command to the processor holding that line that requires the processor to do a write back to main memory
        
        $\to$ The line may now be shared for reading by the original processor and the requesting processor
* *Pros*. They are effective in large-scale systems involving multiple buses or some other complex interconnection scheme
* *Cons*. A bottleneck and the overhead of communication between the various cache controllers and the central controller

#### Snoopy protocols
**Snoopy protocols**. Distribute the responsibility for maintaining cache coherence among all of the cache controllers in a multiprocessor
* *Idea*. 
    * A cache must recognize when a line that it holds is shared with other caches
        
        $\to$ When an update action is performed on a shared cache line, it must be announced to all other caches by a broadcast mechanism
    * Each cache controller is able to snoop on the network to observe these broadcasted notifications, and react accordingly
* *Snoopy protocols for bus-based multiprocessor*. Snoopy protocols are ideally suited to a bus-based multiprocessor
    * *Explain*. The shared bus provides a simple means for broadcasting and snooping
    * *Problem*. Since one of the objectives of the use of local caches is to avoid bus accesses
        
        $\to$ Care must be taken that the increased bus traffic required for broadcasting and snooping does not cancel out the gains from the use of local caches


**Basic approaches to the snoopy protocol**. write invalidate and write update (or write broadcast)
* *Write-invalidate protocol*. There can be multiple readers but only one writer at a time
    1. Initially, a line may be shared among several caches for reading purposes
    2. When one cache wants to perform a write to the line, it issues a notice that invalidates that line in the other caches
        
        $\to$ This makes the line exclusive to the writing cache
    3. Once the line is exclusive, the owning processor can make cheap local writes until some other processor requires the same line
* *Write-update protocol*. There can be multiple writers and multiple readers
    * *Idea*. When a processor wishes to update a shared line, the word to be updated is distributed to all others
        
        $\to$  Caches containing that line can update it
* *Comparison between two protocols* Neither of these two approaches is superior to the other under all circumstances
    * *Explain*. Performance depends on the number of local caches and the pattern of memory reads and writes
    * *Consequence*. Some systems implement adaptive protocols that employ both write-invalidate and write-update mechanisms

**MESI**. The write-invalidate approach is the most widely used in commercial multiprocessor systems, such as the Pentium 4 and PowerPC
* *Implementation*. It marks the state of every cache line, using two extra bits in the cache tag, as modified, exclusive, shared, or invalid

    $\to$ The write-invalidate protocol is called MESI

### The MESI protocol
**MESI protocol**. To provide cache consistency on an SMP, the data cache often supports a MESI protocol
* *Cache line states*. The data cache includes two status bits per tag, hence each line can be in one of four states

    <div style="text-align:center">
        <img src="https://i.imgur.com/kW90Dx0.png">
        <figcaption>MESI cache line states</figcaption>
    </div>

    * *Modified*. The line in the cache has been modified, i.e. different from main memory, and is available only in this cache
    * *Exclusive*. The line in the cache is the same as that in main memory and is not present in any other cache
    * *Shared*. The line in the cache is the same as that in main memory and may be present in another cache
    * *Invalid*. The line in the cache does not contain valid data
* *MESI state transition diagram*. Each line of the cache has its own state bits and hence its own realization of the state diagram

    <div style="text-align:center">
        <img src="https://i.imgur.com/sObGFnX.png">
        <figcaption>MESI state transition diagram</figcaption>
    </div>

**Read miss**. 
* *Work flow*. When a read miss occurs in the local cache
    1. The processor initiates a memory read to read the line of main memory containing the missing address
    2. The processor inserts a signal on the bus that alerts all other processor/cache units to snoop the transaction
* *Possible outcomes*.
    * If one other cache has a clean, i.e. unmodified since read from memory, copy of the line in the exclusive state
        1. It returns a signal indicating that it shares this line
        2. The responding processor transitions the state of its copy from exclusive to shared
        3. The initiating processor reads the line from main memory and transitions the line in its cache from invalid to shared
    * If one or more caches have a clean copy of the line in the shared state, each of them signals that it shares the line
        
        $\to$ The initiating processor reads the line and transitions the line in its cache from invalid to shared
    * If one other cache has a modified copy of the line
        1. That cache blocks the memory read and provides the line to the requesting cache over the shared bus
        2. The responding cache changes its line from modified to shared
        3. The line sent to the requesting cache is also received and processed by the memory controller, which stores the block in memory
    * If no other cache has a copy of the line, i.e. clean or modified, then no signals are returned
        
        $\to$ The initiating processor reads the line and transitions the line in its cache from invalid to exclusive

**Read hit**. When a read hit occurs on a line currently in the local cache, the processor reads the required item

$\to$ There is no state change, i.e. the state remains modified, shared, or exclusive

**Write miss**. 
* *Work flow*. When a write miss occurs in the local cache
    1. The processor initiates a memory read to read the line of main memory containing the missing address
        
        $\to$ For this purpose, the processor issues a signal on the bus meaning read-with-intent-to-modify (RWITM)
    2. When the line is loaded, it is immediately marked modified
* *Possible outcomes w.r.t other caches*. Two possible scenarios precede the loading of the line of data
    * *Case 1*. Some other cache may have a modified copy of this line, i.e. state is modify
        1. The alerted processor signals the initiating processor that another processor has a modified copy of the line
        2. The initiating processor surrenders the bus and waits
        3. The other processor gains access to the bus, writes the modified cache line back to main memory
        4. The other processor transitions the state of the cache line to invalid, i.e. since the initiating processor is going to modify this line
        5. The initiating processor issues a signal to the bus of RWITM and then read the line from main memory
        6. The initiating processor modifies the line in the cache, and marks the line in the modified state
    * *Case 2*. No other cache has a modified copy of the requested line
        
        $\to$ No signal is returned, and the initiating processor proceeds to read in the line and modify it
        * *Reactions of other processors*. Meanwhile
            * If one or more caches have a clean copy of the line in the shared state
            
                $\to$ Each cache invalidates its copy of the line
            * If one cache has a clean copy of the line in the exclusive state
                
                $\to$ It invalidates its copy of the line

**Write hit**. When a write hit occurs on a line currently in the local cache, the effect depends on the current state of that line in the local cache
* *Shared*. 
    1. Before performing the update, the processor must gain exclusive ownership of the line
    2. The processor signals its intent on the bus
    3. Each processor with a shared copy of the line in its cache transitions the sector from shared to invalid
    4. The initiating processor performs the update and transitions its copy of the line from shared to modified
* *Exclusive*. The processor already has exclusive control of this line
    
    $\to$ It simply performs the update and transitions its copy of the line from exclusive to modified.
* *Modified*. The processor already has exclusive control of this line and has the line marked as modified
    
    $\to$ It simply performs the update

**L1-L2 cache consistency**. 
* *Brief*. The described cache coherency protocols are in terms of the cooperate activity among caches connected to the same bus or other SMP interconnection facility
    
    $\to$ These caches are typically L2 caches
* *Problem*. Each processor also has an L1 cache, which does not connect directly to the bus and hence cannot engage in a snoopy protocol
    
    $\to$ Some scheme is needed to maintain data integrity across both levels of cache and across all caches in the SMP configuration
* *Idea*. Extend the MESI protocol, or any cache coherence protocol, to the L1 caches
    
    $\to$ Each line in the L1 cache includes bits to indicate the state
    * *Objective*. For any line present in both an L2 cache and its corresponding L1 cache
        
        $\to$ The L1 line state should track the state of the L2 line
* *L1 write-through*. Adopt the write-through policy in the L1 cache, where the write through is to the L2 cache and not to the memory
    
    $\to$ This forces any modification to an L1 line out to the L2 cache, making it visible to other L2 caches
    * *Requirements*. The L1 content must be a subset of the L2 content
        
        $\to$ The associativity of the L2 cache should be equal to or greater than that of the L1 associativity
    * *Usage*. Used in the IBM S/390 SMP
* *L1 write-back*. If the L1 cache has a write-back policy, the relationship between the two caches is more complex

## Multithreading and chip multiprocessors
**Measure of performance of a processor**. The rate at which it executes instructions
* *Assumptions*.
    * $f$ is the processor clock frequency, in MHz
    * $\text{IPC}$ is the number of instructions per cycle
is the average number of instructions executed per cycle
* *Performance measure*.
    
    $$\text{MIPS rate} = f \times \text{IPC}$$

* *Consequence*. Designers have pursued the goal of increased performance on two fronts
    * Increasing clock frequency and
    * Increasing the number of instructions executed or, more properly, the number of instructions completed during a processor cycle

**Improvement of IPC**. Designers have increased IPC by using an instruction pipeline and multiple parallel instruction pipelines in a superscalar architecture
* *Principal problem of pipelined and multiple-pipeline designs*. Maximize the utilization of each pipeline stage

**Improvement of throughput**. Designers created more complex mechanisms
* *Option 1*. 
    * *Idea*.
        * Execute some instructions in a different order from the way they occur in the instruction stream
        * Begin execution of instructions that may never be needed
    * *Drawback*. This approach may be reaching a limit due to complexity and power consumption concerns
* *Option 2 - Multithreading*. Allow for a high degree of instruction-level parallelism without increasing circuit complexity or power consumption
    * *Idea*. The instruction stream is divided into several smaller streams, i.e. threads, for parallel execution

### Implicit and explicit multithreading
**Brief**. The concept of thread used in discussing multithreaded processors may or may not be the same as the concept of software threads in a multiprogrammed OS

**Terminology**.
* *Process*. An instance of a program running on a computer, which embodies two key characteristics
    * *Resource ownership*. A process includes a virtual address space to hold the process image

        $\to$ From time to time, a process may be allocated control or ownership of resources, e.g. main memory, I/O channels, I/O devices, and files
        * *Process image*. The collection of program, data, stack, and attributes that define the process
    * *Scheduling/execution*. The execution of a process follows an execution path, i.e. trace, through one or more programs
        
        $\to$ This execution may be interleaved with that of other processes
        * *Consequence*. 
            * A process has an execution state, e.g. Running, Ready, etc., and a dispatching priority and is the entity
            * A process is scheduled and dispatched by the OS
* *Process switch*. An operation switching the processor from one process to another, by
    1. Saving all the process control data, registers, and other information for the first
    2. Replacing them with the process information for the second
* *Thread*. A dispatchable unit of wo within a process
    * *Thread structure*. 
        * A processor context, including includes the program counter and stack pointer
        * Its own data area for a stack to enable subroutine branching
    * *Thread execution*. A thread executes sequentially and is interruptible so that the processor can turn to another thread
* *Thread switch*. The act of switching processor control from one thread to another within the same process. Typically, this type of switch is much less costly than a process switch

**Multithreading**.  A thread is concerned with scheduling and execution, whereas a process is concerned with both scheduling/execution and resource ownership
* *Explain*. The multiple threads within a process share the same resources
    
    $\to$ A thread switch is much less time consuming than a process switch
* *Supports for threading*. 
    * Traditional OSs, e.g. earlier versions of UNIX, did not support threads
    * Most modern OSs, e.g. Linux, other versions of UNIX, and Windows, do support thread
* *User-level and kernel-level threads*. Both of these may be referred to as explicit threads, defined in software
    * *User-level threads*. Visible to the application program
    * *Kernel-level threads*. Visible only to the OS

**Explicit and implicit multithreading**. All of the commercial processors and most of the experimental processors so far have used explicit multithreading
* *Explicit multithreading*. Concurrently execute instructions from different explicit threads, either by 
    * Interleaving instructions from different threads on shared pipelines, or
    * Parallel execution on parallel pipelines
* *Implicit multithreading*. The concurrent execution of multiple threads extracted from a single sequential program
    
    $\to$ These implicit threads may be defined either statically by the compiler or dynamically by the hardware

### Approaches to explicit multithreading
**Multithreaded processor**.
* *Design issues*. At minimum, a multithreaded processor must provide a separate program counter for each thread of execution to be executed concurrently

    $\to$ The designs differ in the amount and type of additional hardware used to support concurrent thread execution
* *Instruction fetching*. Generally takes place on a thread basis
    * *Idea*. The processor treats each thread separately and may use a number of techniques for optimizing single-thread execution, including 
        * Branch prediction
        * Register renaming
        * Superscalar techniques
    * *Consequence*. Thread-level parallelism is achieved, providing for greatly improved performance when married to instruction-level parallelism

**Principal approaches to multithreading**.
* *Interleaved multithreading (or fine-grained multithreading)*. The processor deals with two or more thread contexts at a time, switching from one thread to another at each clock cycle
    * *Thread switching*. If a thread is blocked due to data dependencies or memory latencies, it is skipped and a ready thread is executed
* *Blocked multithreading (or coarse-grained multithreading)*. The instructions of a thread are executed successively until an event occurs causing delay, e.g. a cache miss
    
    $\to$ This event induces a switch to another thread
    * *Usage*. Effective on an in-order processor, which would stall the pipeline for a delay event, e.g. a cache miss
* *Simultaneous multithreading (SMT)*. Instructions are simultaneously issued from multiple threads to the execution units of a superscalar processor
    
    $\to$ This combines the wide superscalar instruction issue capability with the use of multiple thread contexts
* *Chip multiprocessing (or multicore)*. The processor is replicated on a single chip and each processor handles separate threads
    * *Benefits*. The available logic area on a chip is used effectively without depending on ever-increasing complexity in pipeline design

**Pros and cons of principal approaches**.
* *Approaches 1 and 2*. Instructions from different threads are not executed simultaneously
    
    $\to$ The processor is able to rapidly switch from one thread to another, using a different set of registers and other context information
    * *Pros*. 
        * Better utilization of the processor’s execution resources
        * Avoid a large penalty due to cache misses and other latency events
* *SMT approach*. Involve true simultaneous execution of instructions from different threads, using replicated execution resources
* *Chip multiprocessing*. Enables simultaneous execution of instructions from different threads

**Pipeline architectures with multithreading and without multithreading**.

<div style="text-align:center">
    <img src="https://i.imgur.com/k13cOCo.png">
    <figcaption>Approaches to executing multiple threads</figcaption>
</div>

* *Notation*.
    * *Horizontal row*. Represent the potential issue slot, or slots for a single execution cycle
        * *Explain*. The width of each row corresponds to the maximum number of instructions, which can be issued in a single clock cycle
    * *Vertical dimension*. Represent the time sequence of clock cycles
    * *Empty (shaded) slot*. Represent an unused execution slot in one pipeline
    * *No-op*. Indicated by N
* *Approaches with scalar processor*. The first three figures show different approaches with a scalar, i.e. single-issue, processor
    * *Single-threaded scalar*. The simple pipeline found in traditional RISC and CISC machines, with no multithreading
    * *Interleaved multithreaded scalar*. The easiest multithreading approach to implement
        * *Idea*. By switching from one thread to another at each clock cycle
            
            $\to$ The pipeline stages can be kept fully occupied, or close to fully occupied
            * *Requirements*. The hardware must be capable of switching from one thread context to another between cycles
        * *Thread switching time*. 0 cycle
            * *Explain*. It is assumed that there are no control or data dependencies between threads

                $\to$ This simplifies the pipeline design and therefore should allow a thread switch with no delay
        * *Drawback*. This approach sacrifices single-thread performance
            * *Example*. The multiple threads compete for cache resources, which raises the probability of a cache miss for a given thread
            * *Consequence*. More opportunities for parallel execution are available if the processor can issue multiple instructions per cycle
    * *Blocked multithreaded scalar*. A single thread is executed until a latency event occurs that would stop the pipeline, at which time the processor switches to another thread
        * *Thread switching time*. 1 cycle
            * *Explain*. 
                * Depending on the specific design and implementation, block multithreading may require a clock cycle to perform a thread switch
                * This is true if a fetched instruction triggers the thread switch and must be discarded from the pipeline
* *Approaches with simultaneous instructions*. Figures d through i illustrate a number of variations among processors with hardware for issuing four instructions per cycle
    * *Instruction issuing*. In all these cases, only instructions from a single thread are issued in a single cycle
    * *Alternatives to instruction issuing*.
        * *Superscalar*. The basic superscalar approach with no multithreading
            
            >**NOTE**. Until relatively recently, this was the most powerful approach to providing parallelism within a processor
            
            * *Horizontal loss*. During some cycles, not all of the available issue slots are used
                
                $\to$ During these cycles, less than the maximum number of instructions is issued
            * *Vertical loss*. During other instruction cycles, no issue slots are used
                
                $\to$ These are cycles when no instructions can be issued
        * *Interleaved multithreading superscalar*. During each cycle, as many instructions as possible are issued from a single thread
            
            $\to$ Potential delays due to thread switches are eliminated, i.e. as previously discussed
            * *Drawback*. The number of instructions issued in any given cycle is limited by dependencies within any given thread
        * *Blocked multithreaded superscalar*. Instructions from only one thread may be issued during any cycle, and blocked multithreading is used
        * *Very long instruction word (VLIW)*. Place multiple instructions in a single word
            * *VLIW construction*. Typically, a VLIW is constructed by the compiler, which places operations that may be executed in parallel in the same word
            * *Filling holes with no-ops*. If it is not possible to completely fill the word with instructions to be issued in parallel
                
                $\to$ No-ops are used
        * *Interleaved multithreading VLIW*. Provide similar efficiencies to those provided by interleaved multithreading on a superscalar architecture
        * *Blocked multithreaded VLIW*. Provide similar efficiencies to those provided by blocked multithreading on a superscalar architecture
* *Final two approaches*. Enable the parallel, simultaneous execution of multiple threads
    * *Simultaneous multithreading (SMT)*. Consider a system capable of issuing 8 instructions at a time
        * *Idea*.
            * If one thread has a high degree of instruction-level parallelism
                
                $\to$ It may on some cycles be able fill all of the horizontal slots
            * On other cycles, instructions from two or more threads may be issued
        * *Consequence*. If sufficient threads are active, it should usually be possible to issue the maximum number of instructions on each cycle
            
            $\to$ This provides a high level of efficiency
    * *Chip multiprocessor (multicore)*. Consider a chip containing four processors, each of which has a two-issue superscalar processor
        
        $\to$ Each processor is assigned a thread, from which it can issue up to two instructions per cycle

**Comparison between approaches**. 
* A chip multiprocessor with the same instruction issue capability as an SMT cannot achieve the same degree of instruction-level parallelism
    * *Explain*. The chip multiprocessor is not able to hide latencies by issuing instructions from other threads
* The chip multiprocessor should outperform a superscalar processor with the same instruction issue capability
    * *Explain*. 
        * The horizontal losses will be greater for the superscalar processor
        * It is possible to use multithreading within each of the processors on a chip multiprocessor
            
            $\to$ This is done on some contemporary machines

### Example systems
**Pentium 4**. More recent models of the Pentium 4 use a multithreading technique, which the Intel literature refers to as hyperthreading
* *Idea*. Use SMT with support for two threads, i.e. the single multithreaded processor is logically two processors

**IBM Powers**. Combine chip multiprocessing with SMT
* *Idea*. The chip has two separate processors, each of which is a multithreaded processor capable of supporting two threads concurrently using SMT
* *Experimental results*. By simulation, it is shown that having two two-way SMT processors on a single chip provided superior performance to a single four-way SMT processor
    * *Explain*. Due to cache thrashing, as data from one thread displaces data needed by another thread
        
        $\to$ Additional multithreading beyond the support for two threads might decrease performance
* *Instruction flow diagram*.

    <div style="text-align:center">
        <img src="https://i.imgur.com/Tts29VR.png">
        <figcaption>Power5 instruction data flow</figcaption>
    </div>

    * *Architecture*
        * Only a few of the elements in the processor need to be replicated, with separate elements dedicated to separate threads
        * Two program counters are used
    1. The processor alternates fetching instructions, up to eight at a time, between the two threads
        
        $\to$ All the instructions are stored in a common instruction cache and share an instruction translation facility
        * *Instruction translation facility*. Do a partial instruction decode
    2. When a conditional branch is encountered, the branch prediction facility predicts the direction of the branch and, if possible, calculates the target address
        * *Subroutine return target prediction*. The processor uses a return stack, one for each thread
    3. Instructions move into two separate instruction buffers
    4. On the basis of thread priority, a group of instructions is selected and decoded in parallel
    5. Instructions flow through a register-renaming facility in program order
    7. Logical registers are mapped to physical registers
        * *Registers of Power5*. 120 physical general-purpose registers and 120 physical floating-point registers
    8. The instructions are moved into issue queues
    9. From the issue queues, instructions are issued using symmetric multithreading
        * *Explain*. The processor has a superscalar architecture and can issue instructions from one or both threads in parallel
    9. At the end of the pipeline, separate thread resources are needed to commit the instructions

## Clusters
**Brief**. An important and relatively recent development computer system design is clustering

**Clustering**. An alternative to symmetric multiprocessing as an approach to providing high performance and high availability

$\to$ This is attractive for server applications
* *Cluster*. A group of interconnected, whole computers working together as a unified computing resource, which can create the illusion of being one machine
    * *"Whole computer"*. Indicate a system that can run on its own, apart from the cluster
    * *Node*. Each computer in a cluster is typically referred to as a node

**Benefits of clustering**.
* *Absolute scalability*. It is possible to create large clusters that far surpass the power of even the largest standalone machines
    * *Explain*. A cluster can have tens, hundreds, or even thousands of machines, each of which is a multiprocessor
* *Incremental scalability*. A cluster is configured so that it is possible to add new systems to the cluster in small increments
    
    $\to$ A user can start out with a modest system and expand it as needs grow, without having to go through a major upgrade, in which an existing small system is replaced with a larger system
* *High availability*. Since each node in a cluster is a standalone computer, the failure of one node does not mean loss of service
    
    >**NOTE**. In many products, fault tolerance is handled automatically in software.
* *Superior price/Performance*. By using commodity building blocks, it is possible to put together a cluster with equal or greater computing power than a single large machine, at much lower cost

### Cluster configurations
**Brief**. Clusters are classified in a number of different ways

**Cluster classification based on disk sharing**. Whether the computers in a cluster share access to the same disks

<div style="text-align:center">
    <img src="https://i.imgur.com/a0CppoM.png">
    <figcaption>Cluster configurations</figcaption>
</div>

* *Separated-disk cluster*.
    * *Option 1*. The only interconnection is by means of a high-speed link used for message exchange to coordinate cluster activity
        * *Message link*. Can be a LAN that is shared with other computers, which are not part of the cluster or the link can be a dedicated interconnection facility
    * *Option 2*. Cluster nodes have a link to a LAN or WAN, hence there is a connection between the server cluster and remote client systems
        
        >**NOTE**. Each computer is depicted as being a multiprocessor
        >
        >$\to$ This is not necessary but does enhance both performance and availability

* *Shared-disk cluster*. There is a message link between nodes and a disk subsystem directly linked to multiple computers within the cluster 
    * *RAID disk subsystem*. A common choice for the disk subsystem in clusters
        
        $\to$ The high availability achieved by the presence of multiple computers is not compromised by a shared disk, which is a single point of failure

**Clustering methods**.

<div style="text-align:center">
    <img src="https://i.imgur.com/TgEJen2.png">
    <figcaption>Clustering methods - Benefits and limitations</figcaption>
</div>

**Passive standby**. A common, older method, where one computer handles all of the processing load while the other computer remains inactive, standing by to take over in the event of a failure of the primary
* *Node coordination*. The active, or primary, system periodically sends a heartbeat message to the standby machine
    
    $\to$ Should these messages stop arriving, the standby assumes that the primary server has failed and puts itself into operation
    * *Pros*. Increase availability
    * *Cons*. 
        * Do not improve performance
        * If the only information exchanged between the two systems is a heartbeat message, and if the two systems do not share common disks
            
            $\to$ The standby provides a functional backup without access to the databases managed by the primary
* *Passive standby and clustering*. Passive standby is generally not referred to as a cluster
    * *Explain*. "Cluster" is reserved for multiple interconnected computers, which are all actively doing processing while maintaining the image of a single system to the outside world

        $\to$ "Active secondary" is often used in referring to this configuration

**Active secondary**.
* *Separate server clustering*. Each computer is a separate server with its own disks and there are no disks shared between systems
    * *Benefits*. Provide high performance and high availability
    * *Load balancing*. Some type of management or scheduling software is needed to assign incoming client requests to servers
        
        $\to$ The load should be balanced and high utilization is achieved
    * *Failure tolerance*. If a computer fails while executing an application, another computer in the cluster can pick up and complete the application
        * *Requirements*. Data must constantly be copied among systems so that each system has access to the current data of the other systems
        * *Consequence*. The overhead of this data exchange ensures high availability at the cost of a performance penalty
        * *Improvement*. To reduce the communications overhead, most clusters consist of servers connected to common disks
* *Shared nothing clustering*. A variation on separate server clustering, where the common disks are partitioned into volumes, and each volume is owned by a single computer
    
    $\to$ If that computer fails, the cluster must be reconfigured so that some other computer has ownership of the volumes of the failed computer
* *Shared disk clustering*. Have multiple computers sharing the same disks at the same time
    
    $\to$ Each computer has access to all of the volumes on all of the disks
    * *Requirement*. This approach requires the use of some type of locking facility to ensure that data can only be accessed by one computer at a time

### OS design issues
**Brief**. Full exploitation of a cluster hardware configuration requires some enhancements to a single-system OS

**Failure management**. How failures are managed by a cluster depends on the clustering method used
* *Highly available clusters*. Offer a high probability that all resources will be in service
    * *Idea*. If a failure occurs, a system goes down or a disk volume is lost the queries in progress are lost
        
        $\to$ Any lost query, if retried, will be serviced by a different computer in the cluster
    * *Execution state of partially executed transactions*. The cluster OS makes no guarantee about the state of partially executed transactions
        
        $\to$ This would need to be handled at the application level
* *Fault-tolerant cluster*. Ensure that all resources are always available
    * *Idea*. Use redundant shared disks and mechanisms for backing out uncommitted transactions and committing completed transactions
    * *Failover*. The function of switching applications and data resources over from a failed system to an alternative system in the cluster
    * *Failback*. The restoration of applications and data resources to the original system once it has been fixed
        
        $\to$ This can be automated, but this is desirable only if the problem is truly fixed and unlikely to recur
        * *Explain*. Otherwise, automatic failback can cause subsequently failed resources to bounce back and forth between computers
            
            $\to$ This results in performance and recovery problems

**Load balancing**. A cluster requires an effective capability for balancing the load among available computers

$\to$ This includes the requirement that the cluster be incrementally scalable
* *Incrementally scalable cluster*. When a new computer is added to the cluster, the load-balancing facility should automatically include this computer in scheduling applications
    
    $\to$ Middleware mechanisms need to recognize that services can appear on different members of the cluster and may migrate from one member to another

**Parallelizing computation**. Effective use of a cluster requires executing software from a single application in parallel
* *Parallelizing compiler*. Determine, at compile time, which parts of an application can be executed in parallel
    
    $\to$ These are then split off to be assigned to different computers in the cluster
    * *Performance*. Depend on the nature of the problem and how well the compiler is designed
    * *Drawback*. In general, such compilers are difficult to develop
* *Parallelized application*. The programmer writes the application from the outset to run on a cluster, and uses message passing to move data, as required, between cluster nodes
    
    $\to$ This places a high burden on the programmer
    * *Benefit*. This is the best approach for exploiting clusters for some applications
* *Parametric computing*. 
    * *Usage*. When 
        * The essence of the application is an algorithm or program that must be executed a large number of times, and
        * Each time with a different set of starting conditions or parameters
    * *Example*. A simulation model running a large number of different scenarios and then develop statistical summaries of the results
    * *Requirements*. For this approach to be effective, parametric processing tools are needed to organize, run, and manage the jobs in an effective manner

### Cluster computer architecture
**Typical cluster architecture**.

<div style="text-align:center">
    <img src="https://i.imgur.com/JTXnkI6.png">
    <figcaption>Cluster computer architecture</figcaption>
</div>

* *Cluster node interconnection*. Connected by some high-speed LAN or switch hardware
* *Cluster nodes*. Each node is capable of operating independently
* *Middleware layer of software*. Installed in each computer to enable cluster operation, i.e. 
    * Provide a unified system image to the user, i.e. a single-system image
    * Provide high availability, by means of load balancing and responding to failures in individual components

**Desirable cluster middleware services and functions**.
* *Single entry point*. A user logs onto the cluster rather than to an individual computer
* *Single file hierarchy*. The user sees a single hierarchy of file directories under the same root directory
* *Single control point*. There is a default workstation used for cluster management and control
* *Single virtual networking*. Any node can access any other point in the cluster, even though the actual cluster configuration may consist of multiple interconnected networks
    
    $\to$ There is a single virtual network operation
* *Single memory space*. Distributed shared memory enables programs to share variables
* *Single job-management system*. Under a cluster job scheduler, a user can submit a job without specifying the host computer to execute the job
* *Single user interface*. A common graphic interface supports all users, regardless of the workstation from which they enter the cluster
* *Single I/O space*. Any node can remotely access any I/O peripheral or disk device without knowledge of its physical location
* *Single process space*. A uniform process-identification scheme is used
    
    $\to$ A process on any node can create or communicate with any other process on a remote node
* *Checkpointing*. Periodically save the process state and intermediate computing results, to allow rollback recovery after a failure
* *Process migration*. This function enables load balancing

**Conclusion**.
* The last four items on the preceding list enhance the availability of the cluster
* The remaining items are concerned with providing a single system image

### Blade servers
**Blade server**. A common implementation of the cluster approach
* *Blade server*. A server architecture housing multiple server modules, i.e. blades, in a single chassis
    * *Usage*. Widely used in data centers to save space and improve system management
* *Hardware specification*. Either self-standing or rack mounted, the chassis provides the power supply, and each blade has its own processor, memory, and hard disk

**Example**. The trend at large data centers, with substantial banks of blade servers, is the deployment of 10-Gbps ports on individual servers to handle the massive multimedia traffic provided by these servers

$\to$ The on-site Ethernet switches are needed to interconnect large numbers of servers

<div style="text-align:center">
    <img src="https://i.imgur.com/erZSsrn.png">
    <figcaption>Ethernet configuration for massive blade server site</figcaption>
</div>

### Clusters compared to SMP
**Similarity**. 
* Both clusters and symmetric multiprocessors provide a configuration with multiple processors to support high-demand applications
* Both solutions are commercially available, although SMP schemes have been around far longer

**SMP approach**. 
* Easier to manage and configure than a cluster
* Much closer to the original single-processor model, for which nearly all applications are written
* The principal change required in going from a uniprocessor to an SMP is to the scheduler function
* SMP usually takes up less physical space and draws less power than a comparable cluster
* SMP products are well established and stable

**Cluster approach**. Over the long run, however, the advantages of the cluster approach are likely to result in clusters dominating the high-performance server market
* Clusters are far superior to SMPs in terms of incremental and absolute scalability
* Clusters are superior in terms of availability, since all components of the system can readily be made highly redundant

## Nonuniform memory access
**Brief**. In terms of commercial products, the two common approaches to providing a multiple-processor system to support applications are SMPs and clusters

$\to$ For some years, another approach, known as nonuniform memory access (NUMA), has been the subject of research and commercial NUMA products are now available

**Terminology in NUMA literature**.
* *Uniform memory access (UMA)*. All processors have access to all parts of main memory using loads and stores
    * The memory access time of a processor to all regions of memory is the same
    * The access times experienced by different processors are the same
* *Nonuniform memory access (NUMA)*. All processors have access to all parts of main memory using loads and stores
    * The memory access time of a processor differs depending on which region of main memory is accessed
    * The access times experienced by different processors are the same
        
        >**NOTE**. For different processors, which memory regions are slower and which are faster differ

* *Cache-coherent NUMA (CC-NUMA)*. A NUMA system, in which cache coherence is maintained among the caches of the various processors

    >**NOTE**. A NUMA system without cache coherence is more or less equivalent to a cluster
    
**Commercial NUMA products with much attention**. CC-NUMA systems, which are quite distinct from both SMPs and clusters

$\to$ Usually, but not always, such systems are in fact referred to in the commercial literature as CC-NUMA systems

>**NOTE**. This section is concerned only with CC-NUMA systems

### Motivation
**Drawback of SMP systems**. There is a practical limit to the number of processors that can be used
* *Explain*. 
    * An effective cache scheme reduces the bus traffic between any one processor and main memory
        
        $\to$ However, as the number of processors increases, this bus traffic also increases
    * The bus is used to exchange cache-coherence signals, further adding to the burden
* *Consequence*. The bus becomes a performance bottleneck

    $\to$ Performance degradation seems to limit the number of processors in an SMP configuration to somewhere between 16 and 64 processors
* *Example*. Silicon Graphics’ Power Challenge SMP is limited to 64 R10000 processors in a single system
    
    $\to$ Beyond this number performance degrades substantially

**Drawback of cluster systems**. The processor limit in an SMP is one of the driving motivations behind the development of cluster systems

$\to$ However, with a cluster, each node has its own private main memory
* *Consequence*. Applications do not see a large global memory, hence coherency is maintained in software rather than hardware
    
    $\to$ This memory granularity affects performance and, to achieve maximum performance, software must be tailored to this environment

**NUMA**. One approach to achieving large-scale multiprocessing while retaining the flavor of SMP
* *Objective*. Maintain a transparent system wide memory while permitting multiple multiprocessor nodes, each with its own bus or other internal interconnect system

### Organization
**Typical CC-NUMA organization**.

<div style="text-align:center">
    <img src="https://i.imgur.com/NTZdl8v.png">
    <figcaption>CC-NUMA organization</figcaption>
</div>

* *Nodes*. There are multiple independent nodes, each of which is, in effect, an SMP organization
    
    $\to$ Each node contains multiple processors, each with its own L1 and L2 caches, plus main memory

    >**NOTE**. The node is the basic building block of the overall CC-NUMA organization
    
    * *Node interconnection*. The nodes are interconnected by means of some communications facility, which could be a switching mechanism, a ring, or some other networking facility
* *Memory layout*.
    * *Main memory and addressable memory*.
        * *Main memory*. Each node in the CC-NUMA system includes some main memory
        * *Addressable memory*. From the point of view of the processors, however, there is only a single addressable memory, with each location having a unique system wide address
    * *Memory access*. When a processor initiates a memory access
        1. If the requested memory location is not in that processor’s cache, the L2 cache initiates a fetch operation
        2. If the desired line is in the local portion of the main memory, the line is fetched across the local bus
        3. If the desired line is in a remote portion of the main memory
            1. An automatic request is sent out to fetch that line across the interconnection network
            2. The line is delivered to the local bus
            3. The line is delivered to the requesting cache on that bus
                
            >**NOTE**. All of this activity is automatic and transparent to the processor and its cache

**Cache coherence**. A central concern in this configuration
* *Idea*. Each node must maintain some sort of directory giving it an indication of the location of various portions of memory and also cache status information
* *Example*. Suppose that processor 3 on node 2, i.e. P2-3, requests a memory location 798, which is in the memory of node 1
    
    $\to$ The following sequence occurs
    1. P2-3 issues a read request on the snoopy bus of node 2 for location 798
    2. The directory on node 2 sees the request and recognizes that the location is in node 1
    3. Node 2’s directory sends a request to node 1, which is picked up by node 1’s directory
    4. Node 1’s directory, acting as a surrogate of P2-3, requests the contents of 798, as if it were a processor
    5. Node 1’s main memory responds by putting the requested data on the bus
    6. Node 1’s directory picks up the data from the bus
    7. The value is transferred back to node 2’s directory
    8. Node 2’s directory places the data back on node 2’s bus, acting as a surrogate for the memory that originally held it
    9. The value is picked up and placed in P2-3’s cache and delivered to P2-3
* *Cache coherence protocol on top of hardware mechanisms for remote memory reading*. The preceding sequence explains how data are read from a remote memory using hardware mechanisms, which make the transaction transparent to the processor
    
    $\to$ On top of these mechanisms, some form of cache coherence protocol is needed
    * *Change log*. Node 1’s directory keeps a record that some remote cache has a copy of the line containing location 798
        
        $\to$ There needs to be a cooperative protocol to take care of modifications
    * *Modification broadcasting*. If a modification is done in a cache, this fact can be broadcast to other nodes
        
        $\to$ Each node’s directory receiving the broadcast can determine if any local cache has that line and, if so, cause it to be purged
        * *Memory invalidation*. If the actual memory location is at the node receiving the broadcast notification
            * That node’s directory needs to maintain an entry indicating that that line of memory is invalid
            * The invalidity remains so until a write back occurs
    * *Request of an invalid line*. If another processor, i.e. local or remote, requests the invalid line
        
        $\to$ The local directory must force a write back to update memory before providing the data

### NUMA pros and cons
**Pros of CC-NUMA**.
* CC-NUMA can deliver effective performance at higher levels of parallelism than SMP, without requiring major software changes
    * *Explain*. With multiple NUMA nodes, the bus traffic on any individual node is limited to a demand that the bus can handle

**Cons of CC-NUMA**.
* If many of the memory accesses are to remote nodes, performance begins to break down
    * *Avoidance of performance drop*. This performance breakdown can be avoided
        * The use of L1 and L2 caches is designed to minimize all memory accesses, including remote ones
            * *Consequence*. If much of the software has good temporal locality, remote memory accesses should not be excessive
        * If the software has good spatial locality, and if virtual memory is in use
            
            $\to$ The data needed for an application will reside on a limited number of frequently used pages, which can be initially loaded into the memory local to the running application
        * The virtual memory scheme can be enhanced by including in the OS a page migration mechanism, which moves a virtual memory page to a node frequently using it
* CC-NUMA does not transparently look like an SMP, i.e. software changes will be required to move an OS and applications from an SMP to a CC-NUMA     

    $\to$These include page allocation, already mentioned, process allocation, and load balancing by the OS
* Availability, i.e. this is a rather complex issue and depends on the exact implementation of the CC-NUMA system

## Vector computation
**Brief**. Although the performance of mainframe general-purpose computers continues to improve relentlessly

$\to$ There continue to be applications that are beyond the reach of the contemporary mainframe
* *Needs for special-purpose computers*. There is a need for computers to solve mathematical problems of physical processes
    * *Example*. These occur in disciplines including aerodynamics, seismology, meteorology, and atomic, nuclear, and plasma physics
    * *Conclusion*. Typically, these problems are characterized by the need for high precision and a program that repetitively performs floating-point arithmetic operations on large arrays of numbers
        
        $\to$ Most of these problems fall into the category known as continuous-field simulation
* *Supercomputers*. Developed to handle these types of problems
    * *Capability of supercomputers*. Typically capable of billions of floating-point operations per second
    * *Comparison to main-frame computers*. 
        * Mainframe computers are designed for multiprogramming and intensive I/O
        * Supercomputers are optimized for the described type of numerical calculation
    * *Usage*. The supercomputer has limited use and, because of its price tag, a limited market
* *Array processor*. Another type of system designed to address the need for vector computation
    * *Supercomputer*. Although a supercomputer is optimized for vector computation
        
        $\to$ It is a general-purpose computer, capable of handling scalar processing and general data processing tasks
    * *Array processor*. Do not include scalar processing
        * *Explain*. They are configured as peripheral devices by both mainframe and minicomputer users to run the vectorized portions of programs

### Approaches to vector computation
**Brief**. The main task is to perform arithmetic operations on arrays or vectors of floating-point numbers
* *Approach in general-purpose computers*. In a general-purpose computer, this will require iteration through each element of the array
* *Approach in array processors*. Introduce some form of parallelism

#### Methods for vector computation
**Problem of interest**. Consider the vector multiplication $C = A \cdot B$, where $A$, $B$, and $C$ are $N \times N$ matrices
* *Formula for $C_{i,j}$*. 

    $$c_{i,j} = \sum_{k=1}^N a_{i,k} \cdot b_{k,j}$$

    where $A$, $B$, and $C$ have elements $a_{i,j}$, $b_{i,j}$, and $c_{i,j}$ respectively
* *FORTRAN program for scalar processing*.

    ```fortran
    DO 100 I = 1, N
        DO 100 J = 1, N
            C(I, J) = 0.0
            DO 100 K = 1, N
                C(I, J) = C(I, J) + A(I, K) + B(K, J)
    100 CONTINUE
    ```

**Vector processing**. Assuming that it is possible to operate on a one-dimensional vector of data
* *Fortran code*.

    ```fortran
    DO 100 I = 1, N
        C(I, J) = 0.0 (J = 1, N)
        DO 100 K = 1, N
            C(I, J) = C(I, J) + A(I, K) + B(K, J) (J = 1, N)
    100 CONTINUE
    ```

    where `(J = 1, N)` indicates that operations on all indices `J` in the given interval are to be carried out as a single operation
* *Explain*. 
    * All the elements of the $i$-th row are to be computed in parallel
    * Each element in the row is a summation, and the summations across `K` are done serially rather than in parallel
* *Time complexity*. Only $N^2$ vector multiplications are required for this algorithm, as compared with $N^3$ scalar multiplications for the scalar algorithm

**Parallel processing**. Assuming that we have `N` independent processors functioning in parallel

$\to$ To utilize processors effectively, we must parcel out the computation to the various processors
* *Primitives for parallel processing*.
    * *`FORK n`*. Cause an independent process to be started at location `n`
        
        $\to$ Meanwhile, the original process continues execution at the instruction immediately following the `FORK`
        * *Implementation*. Every execution of a `FORK` spawns a new process
    * *`JOIN`*. Essentially the inverse of the `FORK`, i.e. `JOIN N` causes `N` independent processes to be merged into one that continues execution at the instruction following the `JOIN`
        * *Implementation*. The OS must coordinate this merger
            
            $\to$ The execution does not continue until all `N` processes have reached the `JOIN` instruction
* *Fortran code*.

    ```fortran
    DO 50 J = 1, N - 1
        FORK 100
    50 CONTINUE
    
    J = N
    100 DO 200 I = 1, N
        C(I, J) = 0.0
        DO 200 K = 1, N
            C(I, J) = C(I, J) + A(I, K) + B(K, J)
        200 CONTINUE

    JOIN N
    ```

* *Explain*. Each column of C is computed by a separate process, hence the elements in a given row of `C` are computed in parallel

#### Processor organization for vector computation
**Types of processor organizations for vector computation**. Pipelined ALU, parallel ALUs, and parallel processors

<div style="text-align:center">
    <img src="https://i.imgur.com/QHB8AGe.png">
    <figcaption>Approaches to vector computation</figcaption>
</div>

**Pipelined ALU**.
* *Motivation*. Since floating-point operations are rather complex, there is opportunity for decomposing a floating-point operation into stages
    
    $\to$ Different stages can operate on different sets of data concurrently
* *Example*. Floating-point addition is broken up into four stages, i.e. compare, shift, add, and normalize

    $\to$ As the processing proceeds, four different sets of numbers will be operated on concurrently in the pipeline

    <div style="text-align:center">
        <img src="https://i.imgur.com/Wan2Vkh.png">
        <figcaption>Pipelined processing of floating-point operations</figcaption>
    </div>

* *Suitability for vector processing*. This organization is suitable for vector processing
    * *Explain*. Consider the instruction pipelining of the CPU
        * *Instruction pipelining of the CPU*. The processor goes through a repetitive cycle of fetching and processing instructions
            
            $\to$ Without branches, the processor is continuously fetching instructions from sequential locations
            * *Consequence*. The pipeline is kept full and a savings in time is achieved
        * *Pipelined ALU*. A pipelined ALU will save time only if it is fed a stream of data from sequential locations
    * *Consequence*. A single, isolated floating-point operation is not speeded up by a pipeline
        
        $\to$ The speedup is achieved when a vector of operands is presented to the ALU
* *Improvement*. The pipeline operation can be further enhanced if the vector elements are available in registers rather than from main memory
    * *Vector register*. A large bank of identical registers
    * *Idea*. 
        * The elements of each vector operand are loaded as a block into a vector register
        * The result is also placed in a vector register
    * *Consequence*. 
        * Most operations involve only the use of registers
        * Only load and store operations and the beginning and end of a vector operation require access to memory

**Pipelining within an operation**. An enhancement to pipelined ALU
* *Scenario*. 
    * The operation of interest is $C = A + B$, which is to be applied to vector operands
    * Pipelining allows multiple vector elements to be processed in parallel
* *Idea*. This mechanism can be augmented with pipelining across operations, i.e.
    * *Explain*. There is a sequence of arithmetic vector operations, and instruction pipelining is used to speed up processing
* *Chaining*. One approach to pipelining across operations found on the Cray supercomputers
    * *Basic rule for chaining*. A vector operation may start as soon as 
        * The first element of the operand vector(s) is available, and
        * The functional unit, e.g. add, subtract, multiply, or divide, is free
    * *Idea*. Chaining causes results issuing from one functional unit to be fed immediately into another functional unit and so on
    * *Improvement*. If vector registers are used, intermediate results do not have to be stored into memory
        
        $\to$ These results can be used even before the vector operation that created them runs to completion
* *Example*. Consider computing $C = (s \cdot A) + B$, where $A$, $B$, and $C$ are vectors and $s$ is a scalar
    
    $\to$ The Cray may execute three instructions at once
    * *Idea*. 
        1. Elements fetched for a load immediately enter a pipelined multiplier
        2. The products are sent to a pipelined adder
        3. The sums are placed in a vector register as soon as the adder completes them
    * *Example*.

        ```
        1. Vector load A S Vector Register (VR1)
        2. Vector load B S VR2
        3. Vector multiply s * VR1 S VR3
        4. Vector add VR3 + VR2 S VR4
        5. Vector store VR4 S C
        ```

    * *Explain*. 
        * Instructions 2 and 3 can be chained since they involve different memory locations and registers
        * Instruction 4 needs the results of instructions 2 and 3, but it can be chained with them as well
            
            $\to$ As soon as the first elements of vector registers 2 and 3 are available, the operation in instruction 4 can begin

**Parallel ALUs**. Use multiple ALUs in a single processor, under the control of a single control unit
* *Idea*. The control unit routes data to ALUs so that they can function in parallel
    * *Improvement*. Use pipelining on each of the parallel ALUs
* *Vector element routing*. The control unit routes vector elements to ALUs in a roundrobin fashion until all elements are processed

    $\to$ This type of organization is more complex than a single-ALU CPI

**Parallel processors**. Break the task up into multiple processes to be executed in parallel
* *Requirements*. This organization is effective only if the software and hardware for effective coordination of parallel processors is available
* *Idea*. Computer organizations can be distinguished by the presence of one or more control units
    
    $\to$ Multiple control units imply multiple processors
    * *Consequence*. If the multiple processors can function cooperatively on a given task, they are termed parallel processors
        
>**NOTE**. The term vector processor is often equated with a pipelined ALU organization, although a parallel ALU organization is also designed for vector processing

>**NOTE**. A parallel processor organization may also be designed for vector processing

>**NOTE**. Array processing is sometimes used to refer to a parallel ALU

>**NOTE**. Array processor usually refers to an auxiliary processor attached to a general-purpose processor and used to perform vector computation

**Recent trends**. The pipelined ALU organization dominates the marketplace, i.e.
* Pipelined systems are less complex than the other two approaches
* The control unit and OS design are well developed to achieve efficient resource allocation and high performance

## IBM 3090 vector facility
**Brief**. A good example of a pipelined ALU organization for vector processing is the vector facility developed for the IBM 370 architecture and implemented on the high-end 3090 series

>**NOTE**. This facility is an optional add-on to the basic system but is highly integrated with it

* *Idea*. Resemble vector facilities found on supercomputers, such as the Cray family
* *Vector register*. There is a number of vector registers, each of which is a bank of scalar registers
* *Vector computation*. Consider the computation of $C = A + B$, where $A$ and $B$ are loaded into two vector registers
    * *Data loading*. The data from these registers are passed through the ALU as fast as possible
    * *Result saving*. The results are stored in a third vector register
* *Vector computing facility*. The computation overlap, and the loading of the input data into the registers in a block
    
    $\to$ This results in a significant speeding up over an ordinary ALU operation

### Organization
**IBM vector architecture and similar pipelined vector ALUs**. Provide increased performance over loops of scalar arithmetic instructions in three ways
* The structure of vector data is fixed and predetermined
    
    $\to$ Housekeeping instructions inside the loop can be replaced by faster internal hardware or microcoded machine operations
* Data-access and arithmetic operations on several successive vector elements can proceed concurrently by 
    * Overlapping such operations in a pipelined design, or
    * Performing multiple-element operations in parallel
* The use of vector registers for intermediate results avoids additional storage reference

**General organization of the vector facility**. Although the vector facility is seen to be a physically separate add-on to the processor

$\to$ Its architecture is an extension of the System/370 architecture and is compatible with it

<div style="text-align:center">
    <img src="https://i.imgur.com/TWdjDcs.png">
    <figcaption>IBM 3090 with vector facility</figcaption>
</div>

* *Integration of vector facility in the system/370 architecture*.
    * Existing System/370 instructions are used for all scalar operations
    * Arithmetic operations on individual vector elements produce exactly the same result as do corresponding System/370 scalar instructions
        * *Example*. 
            * One design decision concerned the definition of the result in a floating-point `DIVIDE` operation, i.e.
                * Should the result be exact, as it is for scalar floating-point division
                * Should an approximation be allowed to permit higherspeed implementation with an error in one or more low-order bit positions
            * The decision was made to uphold complete compatibility with the System/370 architecture at the expense of a minor performance degradation
    * Vector instructions are interruptible, and their execution can be resumed from the point of interruption after appropriate action has been taken
        
        $\to$ The resuming action is compatible with the System/370 program-interruption scheme
    * Arithmetic exceptions are the same as, or extensions of, exceptions for the scalar arithmetic instructions of the System/370, and similar fix-up routines can be used
        * *Idea*. A vector interruption index is employed to indicate the location in a vector register affected by an exception, e.g. overflow
            
            $\to$ When execution of the vector instruction resumes, the proper place in a vector register is accessed
    * Vector data reside in virtual storage, with page faults being handled in a standard manner
* *Benefits*. This level of integration provides a number of benefits
    * Existing OS can support the vector facility with minor extensions
    * Existing application programs, language compilers, and other software can be run unchanged
    * Software that could take advantage of the vector facility can be modified as desired

### Registers
**Brief**. A key issue in the design of a vector facility is whether operands are located in registers or memory

* *Register-to-register organization*. The IBM organization is referred to as register to register
    * *Idea*. The vector operands, both input and output, can be staged in vector registers
        
        $\to$ This approach is also used on the Cray supercomputer
* *Memory-to-register organization*. Obtain operands directly from memory

    $\to$ This is used on Control Data machines

**Vector register and instructions**. Consider the following example

<div style="text-align:center">
    <img src="https://i.imgur.com/Yvp4UJV.png">
    <figcaption>Alternative programs for vector calculation</figcaption>
</div>

* *Assumptions*.
    * Each vector $A,B,C$ has a real part $\ce{AR}, \ce{BR}, \ce{CR}$ 
    * Each vector $A,B,C$ has an imaginary part $\ce{AI}, \ce{BI}, \ce{CI}$
* *3090 architecture specification*.
    * Can perform one main-storage access per processor, or clock, cycle, i.e. either read or write
    * Have registers that can sustain two accesses for reading and one for writing per cycle
    * Produce one result per cycle in the arithmetic unit
    * Each instruction is assumed to specify two source operands and a result
* *Memory-to-memory instructions*. Each iteration of the computation requires a total of 18 cycles
* *Pure register-to-register instructions*. Only 12 cycles are required for computation
    * *Explain*. One cycle is required to load each operand to registers, then no I/O is required
    * *Drawback*. The vector quantities must be loaded into the vector registers prior to computation and stored in memory afterward
        
        $\to$ For large vectors, this fixed penalty is relatively small
* *Storage-to-register instructions*. The ability to specify both storage and register operands in one instruction further reduces the time to 10 cycles per iteration
    * *Explain*. 
    
    >**NOTE**. This latter type of instruction is included in the vector architecture, i.e. as compound instructions

**Vector register architecture**.

<div style="text-align:center">
    <img src="https://i.imgur.com/sLeeD42.png">
    <figcaption>Registers for the IBM 3090 vector facility</figcaption>
</div>

* *Overall architecture*. There are sixteen 32-bit vector registers
    * *Register coupling*. The registers can be coupled to form eight 64-bit vector registers
    * *Register data*. Any register element can hold an integer or floating-point value
        
        $\to$ The vector registers may be used for 32-bit and 64-bit integer values, and 32-bit and 64-bit floating-point values
* *Number of scalar elements*. Each register contains from 8 to 512 scalar elements
    * *Trade-off for the choice of actual length*. 
        * The time to do a vector operation consists essentially of the overhead for pipeline startup and register filling plus one cycle per vector element
            
            $\to$ The use of a large number of register elements reduces the relative startup time for a computation
        * This efficiency must be balanced against 
            * The added time required for saving and restoring vector registers on a process switch, and
            * The practical cost and space limits
    * *Consequence*. These considerations led to the use of 128 elements per register in later 3090 implementations
* *Additional registers*.
    * *Vector-mask register*. Contain mask bits, which may be used to select which elements in the vector registers are to be processed for a particular operation
    * *Vector-status register*. Contain control fields, e.g. the vector count, to determine how many elements in the vector registers are to be processed
    * *Vector-activity count*. Keep track of the time spent executing vector instructions
* *Pros and cons*.
    * *Pros*. The operation is decoupled from slower main memory and takes place primarily with registers
    * *Cons*. The programmer or compiler must take them into account for good performance
        * *Example*. Consider a vector register of length $K$, and the operand vector of length $N > K$
            
            $\to$ A vector loop must be performed, in which the operation is performed on $K$ elements at a time
            * *Consequence*. The loop is repeated $N/K$ times

### Compound instructions
**Lack of chaining**. Although instruction execution can be overlapped using chaining to improve performance

$\to$ The designers of the IBM vector facility chose not to include this capability for several reasons
* *Explain*. Due to the cost of including the additional controls and register access paths in the vector facility for generalized chaining, i.e.
    * The System/370 architecture have to be extended to handle complex interruptions, including their effect on virtual memory management
    * The System/370 architecture have to be extended to handle corresponding changes needed in the software

**Compound instructions**. Rather than using chaining, three operations are provided that combine into one instruction, i.e. one opcode, multiplication followed by addition, subtraction, or summation

$\to$ These are the most common sequences in vector computation
* *Storage-to-register instruction*. 
    1. Fetch a vector from storage
    2. Multiply it by a vector from a register
    3. Add the product to a third vector in a register
* *Benefits*. 
    * No need for the use of additional registers for temporary storage of intermediate results
    * Require one less register access, e.g. consider the following chain

        ```
        VR1 = A
        VR1 = VR1 + VR2
        ```
        
        $\to$ Two stores to the vector register `VR1` are required
        * *IBM's storage-to-register ADD instruction*. Only the sum is placed in `VR1`
    * Avoid the need to reflect in the machine-state description the concurrent execution of a number of instructions
        
        $\to$ This simplifies status saving and restoring by the operating system and the handling of interrupts

### Instruction set
**Arithmetic and logical operations defined for the vector architecture**.

<div style="text-align:center">
    <img src="https://i.imgur.com/hgfQtLX.png">
    <figcaption>IBM 3090 vector facility - Arithmetic and logical instructions</figcaption>
</div>

>**NOTE**. In addition, there are memory-to-register load and register-to-memory store instructions

* *Number of operands*. Many of the instructions use a three-operand format
* *Variants of operations* Many instructions have a number of variants, depending on the location of the operands
    * *Explain*.
        * A source operand may be a vector register (V), storage (S), or a scalar register (Q)
        * The target is always a vector register, except for comparison
        * The result of which goes into the vector-mask register
    * *Consequence*. With all these variants, the total number of opcodes is 171
        
        >**NOTE**. This rather large number, however, is not as expensive to implement as might be imagined

* *Major hardware cost*. The arithmetic units and the data paths to feed operands from storage, scalar registers, and vector registers to the vector pipelines

    $\to$ The architecture then can, with little difference in cost, provide a rich set of variants on the use of those registers and pipelines