<!-- TOC titleSize:1 tabSpaces:2 depthFrom:1 depthTo:6 withLinks:1 updateOnSave:1 orderedList:0 skip:0 title:1 charForUnorderedList:* -->
# Table of Contents
- [Table of Contents](#table-of-contents)
- [Parallel processing](#parallel-processing)
  - [Multiple processor organizations](#multiple-processor-organizations)
    - [Types of parallel processor systems](#types-of-parallel-processor-systems)
    - [Parallel organizations](#parallel-organizations)
  - [Symmetric multiprocessors](#symmetric-multiprocessors)
    - [Organization](#organization)
    - [Multiprocessor OS design considerations](#multiprocessor-os-design-considerations)
  - [Cache coherence and the MESI protocol](#cache-coherence-and-the-mesi-protocol)
    - [Software solutions](#software-solutions)
    - [Hardware solutions](#hardware-solutions)
    - [The MESI protocol](#the-mesi-protocol)
  - [Multithreading and chip multiprocessors](#multithreading-and-chip-multiprocessors)
    - [Implicit and explicit multithreading](#implicit-and-explicit-multithreading)
    - [Approaches to explicit multithreading](#approaches-to-explicit-multithreading)
    - [Example systems](#example-systems)
  - [Clusters](#clusters)
    - [Cluster configurations](#cluster-configurations)
    - [OS design issues](#os-design-issues)
    - [Cluster computer architecture](#cluster-computer-architecture)
    - [Blade servers](#blade-servers)
    - [Clusters compared to SMP](#clusters-compared-to-smp)
  - [Nonuniform memory access](#nonuniform-memory-access)
    - [Motivation](#motivation)
    - [Organization](#organization-1)
    - [NUMA pros and cons](#numa-pros-and-cons)
  - [Vector computation](#vector-computation)
    - [Approaches to vector computation](#approaches-to-vector-computation)
    - [IBM 3090 vector facility](#ibm-3090-vector-facility)
<!-- /TOC -->

# Parallel processing
## Multiple processor organizations
### Types of parallel processor systems
**Traditional computer as a sequential machine**. Most computer programming languages require the programmer to specify algorithms as sequences of instructions

$\to$ Processors execute programs by executing machine instructions in a sequence and one at a time
* *Instruction execution*. Each instruction is executed in a sequence of operations, i.e. fetch instruction, fetch operands, perform operation, store results

**Parallelism**. The view of the computer as a sequential machine has never been entirely true
* *Examples*. The following examples are performing functions in parallel
    * At the micro-operation level, multiple control signals are generated at the same time
    * Instruction pipelining, at least to the extent of overlapping fetch and execute operations, has been around for a long time
* *Superscalar organization*. Parallelism is taken further with superscalar organization, which exploits instruction-level parallelism
    * *Idea*. Have multiple execution units within a single processor
        
        $\to$ These may execute multiple instructions from the same program in parallel
* *Parallelism in modern computers*. As computer technology has evolved, and as the cost of computer hardware has dropped
    
    $\to$ Computer designers have sought more and more opportunities for parallelism, usually to enhance performance and, in some cases, to increase availability

**Some prominent approaches to parallel organization**. 
* Symmetric multiprocessors (SMPs), i.e. one of the earliest and still the most common example of parallel organization
    * *SMP organization*. Multiple processors share a common memory
    * *Drawback*. This organization raises the issue of cache coherence
* Multithreaded processors and chip multiprocessors
* Clusters, which consist of multiple independent computers organized in a cooperative fashion
    * *Usage*. Increasingly common to support workloads, which are beyond the capacity of a single SMP
* *Nonuniform memory access (NUMA) machines*. Relatively new and not yet proven in the marketplace
    
    $\to$ This is often considered as an alternative to the SMP or cluster approach

**Hardware organizational approaches to vector computation**. These approaches optimize the ALU for processing vectors or arrays of floating-point numbers

$\to$ They are common on supercomputers

### Parallel organizations

## Symmetric multiprocessors

### Organization

### Multiprocessor OS design considerations

## Cache coherence and the MESI protocol

### Software solutions

### Hardware solutions

### The MESI protocol

## Multithreading and chip multiprocessors

### Implicit and explicit multithreading

### Approaches to explicit multithreading

### Example systems

## Clusters

### Cluster configurations

### OS design issues

### Cluster computer architecture

### Blade servers

### Clusters compared to SMP

## Nonuniform memory access

### Motivation

### Organization

### NUMA pros and cons

## Vector computation

### Approaches to vector computation

### IBM 3090 vector facility