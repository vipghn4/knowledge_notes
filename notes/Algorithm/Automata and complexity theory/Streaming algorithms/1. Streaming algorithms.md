<!-- TOC titleSize:1 tabSpaces:2 depthFrom:1 depthTo:6 withLinks:1 updateOnSave:1 orderedList:0 skip:0 title:1 charForUnorderedList:* -->
# Table of Contents
- [Table of Contents](#table-of-contents)
- [Streaming algorithms](#streaming-algorithms)
  - [Most frequent element](#most-frequent-element)
  - [Number of distinct elements](#number-of-distinct-elements)
<!-- /TOC -->

# Streaming algorithms
**Streaming algorithm**. An algorithm receiving its input as a stream of data, and proceeding by making only one pass through the data
* *Streaming algorithm design objective*. The algorithm should be fast and uses as little memory as possible
* *Streaming algorithm and DFA*. A DFA is a streaming algorithm that
    * Have constant amount of memory
    * Process each data item in its input stream in constant time
    * Solve a decision problem defined on the input
* *Streaming algorithms of interest*. 
    * Algorithms for numerical computation, and other computations with a non-binary output
    * Algorithms whose memory use and processing-time-per-input-item are not constants, as along as we can make them feasibly small even for very large streams
* *Applications*. Appropriate for settings, in which 
    * The data to be processed is not stored anywhere, but is generated automatically, and
    * The data is fed into the streaming algorithm as it is being generated
* *Example*. 
    * Stream of measurements from a sensor network
    * Stream of transactions from an online store
    * Stream of search strings in a search engine
    * Stream of page requests coming to a web server

**Problem of interest**. Use Myhill-Nerode theorem and its related ideas to argue about memory lower bounds for streaming algorithms

## Most frequent element
**Most frequently occurring element (MFE) problem**. Consider passing only once through a sequence $x_1,\dots, x_n$ of elements from a set $\Sigma$, knowing that one value from $\Sigma$ occurs more than $n/2$ times in the sequence

$\to$ Find the most frequently repeated value using only two variables, using a total memory of only $\log_2 n + \log_2 |\Sigma|$ bits
* *Importance*. MFE problem is an important one in many of the settings motivating the streaming model
    * *Example*. Keeping track of the page that receives the most hits, or the best selling item, etc.
* *Difficulty and complexity*. Without the guarantee that there is an element occurring in a majority of places in the sequence
    
    $\to$ There is no memory-efficient way to find a most frequent element

**Application of Myhill-Nerod**.
* *Assumptions*.
    * $\Sigma=\{0,\dots,2^l-1\}$ is an alphabet
    * $L_{n,\Sigma}=\{x_1,\dots,x_n\}\subseteq \Sigma^n$ is a language, in which $0$ is a MFE, for every $n$
* *Theorem*. If there is a deterministic algorithm for MFE using $\leq m(n,\Sigma)$ bits of memory

    $\to$ There is a DFA for $L_{n,\Sigma}$ with $\leq 2^{m(n,\Sigma)}$ states
    * *Proof*.
        * A streaming algorithm using $m$ bits of memory has only $2^m$ distinct internal states

            $\to$ We can construct an automaton with $2^m$ states, which can simulate the execution of the algorithm
        * At the end of the sequence, we define the internal states leading the algorithm to output $0$ are accepting states for the DFA

            $\to$ The other states are not accepting
        * The automaton above can recognize $L_{n,\Sigma}$
* *Application of Myhill-Nerod theorem to $L_{n,\Sigma}$*. $L_{n,\Sigma}$ has $2^{\Omega(nl)}$ distinguishable strings

**Theorem**. Every deterministic streaming algorithm solving the MFE problem must use at least $\Omega(n\cdot l)$ bits of memory (prove later), where
* $n$ is the length of the strings
* $l=\log_2 |\Sigma|$ is the bit-length of one element of $\Sigma$, assuming $2^l>n^2$

## Number of distinct elements
**Distinct elements (DE) problem**. Find how many distinct elements are in the stream

$\to$ We are interested in proving lower bounds for this problem, rather than reducing to a regular language and finding distinguishable strings for it

>**NOTE**. For simplicity, the assumption $|\Sigma|=2^l>n^2$ holds

**Distinguishable streams**. 
* *Distinguishable streams*. Two streams $\mathbf{x}, \mathbf{y}$ are distinguishable for a streaming problem $P$ on inputs of length $n$, if 
    * There is a stream $\mathbf{z}$ such that all the correct answers to problem $P$ for input $\mathbf{x} \cdot \mathbf{z}$ are different from all the correct answers to problem $P$ for input $\mathbf{y} \cdot \mathbf{z}$
    * The streams $\mathbf{x} \cdot \mathbf{z}$ and $\mathbf{y} \cdot \mathbf{z}$ have length $n$
* *Distinguishable streams for MFE problem*. $\mathbf{x},\mathbf{y}$ are distinguishable for the MFE problem if there is a stream $\mathbf{z}$ so that none of the MFEs of $\mathbf{x} \cdot \mathbf{z}$ is also a MFE of $\mathbf{y} \cdot \mathbf{z}$
* *Distinguishable streams for DE problem*. $\mathbf{x},\mathbf{y}$ are distinguishable for the DE problem if there is a stream $\mathbf{z}$ so that the number of DEs of $\mathbf{x} \cdot \mathbf{z}$ is different from the number of DEs of $\mathbf{y} \cdot \mathbf{z}$

**Lemma**. This lemma gives a way to prove memory lower bounds
* *Assumption*. We can find $D(n,\Sigma)$ strings in $\Sigma^*$, which are distinguishable for problem $P$ on inputs of length $n$
* *Conclusion*. Every deterministic streaming algorithm for $P$ must use at least $\log_2 D(n,\Sigma)$ memory on inputs of length $n$

**Theorem**. There are $2^{\Omega(nl)}$ distinguishable strings for the DE problem on inputs of length $n$

$\to$ The DE problem requires $\Omega(nl)$ bits of memory for every deterministic streaming algorithm

**Theorem**. There are $2^{\Omega(nl)}$ distinguishable strings for the problem of approximating DE within a $\pm 20\%$ relative error on inputs of length $n$

$\to$ The DE problem requires $\Omega(nl)$ bits of memory for every deterministic streaming algorithm

**Randomized approximate streaming algorithms**. The DE problem admits very efficient randomized streaming algorithms
* *Example*. It is possible to achieve a $1\%$ approximation with high probability using only $O(l+\log n)$ bits of memory
* *Idea*.
    1. Randomly pick a hash function $h:\Sigma\to[0,1]$, which randomly maps data items to reals in $[0,1]$
    2. Given a sequence $x_1,\dots,x_n$, compute $h(x_i)$ for each $i$, and store the minimum hash value $m$
    3. The output is $1/m$
* *Motivation*. 
    * Given a perfectly random hash function $h$, the following processes are the same
        * The process of evaluating $h(x_i)$ for each $i$ and defining $m$ to be the minimum
        * The probabilistic process as picking $k$ random real numbers in $[0, 1]$, where $k$ is the number of distinct elements in $x_1, \dots x_n$, then defining $m$ to be the minimum
    * The latter process is well understood, and $m$ tends to be approximately $1/k$

        $\to$ $1/m$ is an approximation to $k$
* *Space complexity*. The storage required to implement the algorithm is the memory used to store $h$, plus the memory needed to store the current minimum
    * *Real number representation*. Real numbers are represented with finite precision, which affects the algorithm negligibly
    * *Selection of $h$*. $h$ is picked not as a completely random function, which would require storage space proportional to $|\Sigma|$
        
        $\to$ It is picked as a pairwise independent hash function, which requires storage $O(\log |\Sigma|)$

    >**NOTE**. The analysis of the completely random case needs to be adjust to deal with the more limited randomness property of the hash function used in the implementation

    >**NOTE**. Both the probability of finding a good approximation and the range of approximation can be improved with various techniques